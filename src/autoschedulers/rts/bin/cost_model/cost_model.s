	.section	__TEXT,__text,regular,pure_instructions
	.section	__TEXT,__const
	.p2align	5                               ## -- Begin function cost_model
LCPI0_0:
	.quad	32                              ## 0x20
	.quad	256                             ## 0x100
	.quad	512                             ## 0x200
	.quad	2                               ## 0x2
LCPI0_2:
	.quad	-9223372036854775804            ## 0x8000000000000004
	.quad	64                              ## 0x40
	.quad	128                             ## 0x80
	.quad	16                              ## 0x10
LCPI0_4:
	.long	0                               ## 0x0
	.long	32                              ## 0x20
	.long	1                               ## 0x1
	.long	0                               ## 0x0
	.long	0                               ## 0x0
	.long	32                              ## 0x20
	.long	32                              ## 0x20
	.long	0                               ## 0x0
LCPI0_9:
	.long	0                               ## 0x0
	.long	24                              ## 0x18
	.long	1                               ## 0x1
	.long	0                               ## 0x0
	.long	0                               ## 0x0
	.long	39                              ## 0x27
	.long	24                              ## 0x18
	.long	0                               ## 0x0
LCPI0_12:
	.quad	16777216                        ## 0x1000000
	.quad	1073741824                      ## 0x40000000
	.quad	4294967296                      ## 0x100000000
	.quad	0                               ## 0x0
LCPI0_13:
	.quad	0                               ## 0x0
	.quad	0                               ## 0x0
	.quad	0                               ## 0x0
	.quad	256                             ## 0x100
LCPI0_14:
	.quad	68719476736                     ## 0x1000000000
	.quad	274877906944                    ## 0x4000000000
	.quad	0                               ## 0x0
	.quad	0                               ## 0x0
LCPI0_15:
	.quad	0                               ## 0x0
	.quad	0                               ## 0x0
	.quad	4096                            ## 0x1000
	.quad	8192                            ## 0x2000
LCPI0_16:
	.quad	67108864                        ## 0x4000000
	.quad	0                               ## 0x0
	.quad	0                               ## 0x0
	.quad	268435456                       ## 0x10000000
LCPI0_17:
	.quad	0                               ## 0x0
	.quad	64                              ## 0x40
	.quad	128                             ## 0x80
	.quad	0                               ## 0x0
LCPI0_18:
	.quad	0                               ## 0x0
	.quad	0                               ## 0x0
	.quad	4194304                         ## 0x400000
	.quad	0                               ## 0x0
LCPI0_19:
	.quad	576460752303423488              ## 0x800000000000000
	.quad	144115188075855872              ## 0x200000000000000
	.quad	0                               ## 0x0
	.quad	1152921504606846976             ## 0x1000000000000000
LCPI0_21:
	.quad	0                               ## 0x0
	.quad	4                               ## 0x4
	.quad	8                               ## 0x8
	.quad	288230376151711744              ## 0x400000000000000
LCPI0_22:
	.quad	2305843009213693952             ## 0x2000000000000000
	.quad	16                              ## 0x10
	.quad	32                              ## 0x20
	.quad	4611686018427387904             ## 0x4000000000000000
LCPI0_23:
	.quad	2                               ## 0x2
	.quad	36028797018963968               ## 0x80000000000000
	.quad	72057594037927936               ## 0x100000000000000
	.quad	18014398509481984               ## 0x40000000000000
LCPI0_24:
	.quad	0                               ## 0x0
	.quad	17179869184                     ## 0x400000000
	.quad	0                               ## 0x0
	.quad	0                               ## 0x0
LCPI0_25:
	.quad	512                             ## 0x200
	.quad	0                               ## 0x0
	.quad	1024                            ## 0x400
	.quad	2048                            ## 0x800
LCPI0_26:
	.long	8                               ## 0x8
	.long	0                               ## 0x0
	.long	8                               ## 0x8
	.long	1                               ## 0x1
	.long	0                               ## 0x0
	.long	40                              ## 0x28
	.long	0                               ## 0x0
	.long	7                               ## 0x7
LCPI0_27:
	.long	0                               ## 0x0
	.long	24                              ## 0x18
	.long	1                               ## 0x1
	.long	0                               ## 0x0
	.long	24                              ## 0x18
	.long	1                               ## 0x1
	.long	0                               ## 0x0
	.long	39                              ## 0x27
LCPI0_28:
	.quad	1024                            ## 0x400
	.quad	2048                            ## 0x800
	.quad	512                             ## 0x200
	.quad	8192                            ## 0x2000
LCPI0_29:
	.quad	2                               ## 0x2
	.quad	8                               ## 0x8
	.quad	16                              ## 0x10
	.quad	4                               ## 0x4
LCPI0_30:
	.quad	16384                           ## 0x4000
	.quad	4096                            ## 0x1000
	.quad	32768                           ## 0x8000
	.quad	65536                           ## 0x10000
LCPI0_31:
	.quad	32                              ## 0x20
	.quad	64                              ## 0x40
	.quad	128                             ## 0x80
	.quad	256                             ## 0x100
LCPI0_32:
	.quad	32                              ## 0x20
	.quad	256                             ## 0x100
	.quad	8589934592                      ## 0x200000000
	.quad	128                             ## 0x80
LCPI0_33:
	.quad	2                               ## 0x2
	.quad	536870912                       ## 0x20000000
	.quad	4                               ## 0x4
	.quad	1073741824                      ## 0x40000000
LCPI0_34:
	.quad	2147483648                      ## 0x80000000
	.quad	8                               ## 0x8
	.quad	4294967296                      ## 0x100000000
	.quad	64                              ## 0x40
LCPI0_35:
	.quad	-9223372036821221376            ## 0x8000000002000000
	.quad	67108864                        ## 0x4000000
	.quad	134217728                       ## 0x8000000
	.quad	268435456                       ## 0x10000000
LCPI0_58:
	.quad	4                               ## 0x4
	.quad	5                               ## 0x5
	.quad	6                               ## 0x6
	.quad	7                               ## 0x7
LCPI0_59:
	.quad	0                               ## 0x0
	.quad	1                               ## 0x1
	.quad	2                               ## 0x2
	.quad	3                               ## 0x3
	.section	__TEXT,__literal16,16byte_literals
	.p2align	4
LCPI0_1:
	.quad	-9223372036854775808            ## 0x8000000000000000
	.quad	0                               ## 0x0
LCPI0_3:
	.long	0                               ## 0x0
	.long	32                              ## 0x20
	.long	1                               ## 0x1
	.long	0                               ## 0x0
LCPI0_5:
	.long	0                               ## 0x0
	.long	8                               ## 0x8
	.long	1                               ## 0x1
	.long	0                               ## 0x0
LCPI0_6:
	.long	0                               ## 0x0
	.long	40                              ## 0x28
	.long	8                               ## 0x8
	.long	0                               ## 0x0
LCPI0_7:
	.long	0                               ## 0x0
	.long	7                               ## 0x7
	.long	320                             ## 0x140
	.long	0                               ## 0x0
LCPI0_8:
	.long	0                               ## 0x0
	.long	24                              ## 0x18
	.long	1                               ## 0x1
	.long	0                               ## 0x0
LCPI0_10:
	.long	0                               ## 0x0
	.long	40                              ## 0x28
	.long	1                               ## 0x1
	.long	0                               ## 0x0
LCPI0_11:
	.long	0                               ## 0x0
	.long	7                               ## 0x7
	.long	40                              ## 0x28
	.long	0                               ## 0x0
LCPI0_20:
	.long	1048576                         ## 0x100000
	.long	0                               ## 0x0
	.long	0                               ## 0x0
	.long	0                               ## 0x0
	.section	__TEXT,__literal4,4byte_literals
	.p2align	2
LCPI0_36:
	.long	0xbfb8aa3b                      ## float -1.44269502
LCPI0_37:
	.long	0x3f317200                      ## float 0.693145751
LCPI0_38:
	.long	0x35bfbe8e                      ## float 1.42860677E-6
LCPI0_39:
	.long	4294967169                      ## 0xffffff81
LCPI0_40:
	.long	128                             ## 0x80
LCPI0_41:
	.long	0xb9a797f3                      ## float -3.19659332E-4
LCPI0_42:
	.long	0xbc0b192a                      ## float -0.00848988629
LCPI0_43:
	.long	0xbe2aae1f                      ## float -0.166679844
LCPI0_44:
	.long	0xbf800000                      ## float -1
LCPI0_45:
	.long	0x3f800000                      ## float 1
LCPI0_46:
	.long	0x3a9c2e66                      ## float 0.00119156833
LCPI0_47:
	.long	0x3d2a66bc                      ## float 0.0416018814
LCPI0_48:
	.long	0x3effffde                      ## float 0.499998987
LCPI0_49:
	.long	2155872255                      ## 0x807fffff
LCPI0_50:
	.long	0x3f317218                      ## float 0.693147182
LCPI0_51:
	.long	0x3d9c7946                      ## float 0.0764031857
LCPI0_52:
	.long	0x3e5333c6                      ## float 0.206252187
LCPI0_53:
	.long	0x3eaa99cd                      ## float 0.333204657
LCPI0_54:
	.long	0x3e266e2a                      ## float 0.162529618
LCPI0_55:
	.long	0xbe809085                      ## float -0.251102597
LCPI0_56:
	.long	0xbefffcbe                      ## float -0.499975145
LCPI0_57:
	.long	0xbe266e2a                      ## float -0.162529618
LCPI0_63:
	.long	0x45800000                      ## float 4096
LCPI0_64:
	.long	0x40000000                      ## float 2
LCPI0_65:
	.long	0x3089705f                      ## float 9.99999971E-10
	.section	__TEXT,__literal8,8byte_literals
	.p2align	3
LCPI0_60:
	.quad	8                               ## 0x8
LCPI0_61:
	.quad	16                              ## 0x10
LCPI0_62:
	.quad	24                              ## 0x18
LCPI0_66:
	.quad	32                              ## 0x20
	.section	__TEXT,__text,regular,pure_instructions
	.globl	_cost_model
	.p2align	4, 0x90
_cost_model:                            ## @cost_model
	.cfi_startproc
## %bb.0:                               ## %entry
	pushq	%rbp
	.cfi_def_cfa_offset 16
	.cfi_offset %rbp, -16
	movq	%rsp, %rbp
	.cfi_def_cfa_register %rbp
	pushq	%r15
	pushq	%r14
	pushq	%r13
	pushq	%r12
	pushq	%rbx
	andq	$-32, %rsp
	subq	$12160, %rsp                    ## imm = 0x2F80
	.cfi_offset %rbx, -56
	.cfi_offset %r12, -48
	.cfi_offset %r13, -40
	.cfi_offset %r14, -32
	.cfi_offset %r15, -24
	movq	%rcx, %r14
	movl	%edx, 1112(%rsp)                ## 4-byte Spill
                                        ## kill: def $esi killed $esi def $rsi
	movq	%rsi, 1504(%rsp)                ## 8-byte Spill
                                        ## kill: def $edi killed $edi def $rdi
	movq	%rdi, 192(%rsp)                 ## 8-byte Spill
	movq	48(%rbp), %rdx
	xorl	%ecx, %ecx
	movq	%r8, 3008(%rsp)                 ## 8-byte Spill
	testq	%r8, %r8
	sete	%cl
	xorl	%eax, %eax
	cmpq	$0, 88(%rbp)
	sete	%al
	shlq	$3, %rax
	orq	%rcx, %rax
	vmovdqa	32(%rbp), %xmm0
	vmovq	80(%rbp), %xmm1                 ## xmm1 = mem[0],zero
	vmovq	%rdx, %xmm2
	vpunpcklqdq	%xmm1, %xmm2, %xmm1     ## xmm1 = xmm2[0],xmm1[0]
	vinserti128	$1, %xmm1, %ymm0, %ymm0
	vmovq	%r9, %xmm1
	vmovq	%r14, %xmm2
	vpunpcklqdq	%xmm1, %xmm2, %xmm1     ## xmm1 = xmm2[0],xmm1[0]
	vinserti128	$1, 16(%rbp), %ymm1, %ymm1
	vpxor	%xmm2, %xmm2, %xmm2
	vpcmpeqq	%ymm2, %ymm0, %ymm0
	vpcmpeqq	%ymm2, %ymm1, %ymm1
	vpand	LCPI0_0(%rip), %ymm0, %ymm0
	vmovapd	LCPI0_1(%rip), %xmm2            ## xmm2 = [9223372036854775808,0]
	vblendvpd	%ymm1, LCPI0_2(%rip), %ymm2, %ymm1
	vorpd	%ymm0, %ymm1, %ymm0
	vextractf128	$1, %ymm0, %xmm1
	vorpd	%xmm1, %xmm0, %xmm0
	vpermilps	$238, %xmm0, %xmm1      ## xmm1 = xmm0[2,3,2,3]
	vorpd	%xmm1, %xmm0, %xmm0
	vmovq	%xmm0, %rcx
	orq	%rax, %rcx
	xorl	%eax, %eax
	tzcntq	%rcx, %rax
	cmpl	$9, %eax
	jbe	LBB0_157
## %bb.1:                               ## %no_errors_bb
	movq	%r9, %r13
	movq	40(%rbp), %r15
	movq	16(%rbp), %r12
	movq	16(%rdx), %rax
	movq	%rax, 80(%rsp)                  ## 8-byte Spill
	leaq	32(%rdx), %rsi
	movq	%rdx, %rbx
	leaq	28(%rsp), %rdi
	movl	$4, %edx
	vzeroupper
	callq	_memcpy
	movl	28(%rsp), %eax
	movl	%eax, 688(%rsp)                 ## 4-byte Spill
	movq	24(%rbx), %rax
	movq	%rax, 912(%rsp)                 ## 8-byte Spill
	movl	36(%rbx), %eax
	movl	%eax, 200(%rsp)                 ## 4-byte Spill
	movq	40(%rbx), %rax
	movl	(%rax), %ecx
	movq	%rcx, 400(%rsp)                 ## 8-byte Spill
	movl	4(%rax), %ecx
	movq	%rcx, 464(%rsp)                 ## 8-byte Spill
	movl	8(%rax), %eax
	movl	%eax, 800(%rsp)                 ## 4-byte Spill
	movq	16(%r15), %rax
	movq	%rax, 728(%rsp)                 ## 8-byte Spill
	leaq	32(%r15), %rsi
	leaq	28(%rsp), %rdi
	movl	$4, %edx
	callq	_memcpy
	movl	28(%rsp), %eax
	movl	%eax, 784(%rsp)                 ## 4-byte Spill
	movq	24(%r15), %rax
	movq	%rax, 896(%rsp)                 ## 8-byte Spill
	movl	36(%r15), %eax
	movl	%eax, 768(%rsp)                 ## 4-byte Spill
	movq	40(%r15), %rax
	movl	(%rax), %ecx
	movq	%rcx, 656(%rsp)                 ## 8-byte Spill
	movl	4(%rax), %ecx
	movq	%rcx, 432(%rsp)                 ## 8-byte Spill
	movl	8(%rax), %ecx
	movl	%ecx, 1200(%rsp)                ## 4-byte Spill
	movl	16(%rax), %ecx
	movq	%rcx, 448(%rsp)                 ## 8-byte Spill
	movl	20(%rax), %ecx
	movq	%rcx, 416(%rsp)                 ## 8-byte Spill
	movl	24(%rax), %eax
	movl	%eax, 1128(%rsp)                ## 4-byte Spill
	movq	16(%r12), %r15
	leaq	32(%r12), %rsi
	leaq	28(%rsp), %rdi
	movl	$4, %edx
	callq	_memcpy
	movl	28(%rsp), %eax
	movl	%eax, 2208(%rsp)                ## 4-byte Spill
	movq	24(%r12), %rax
	movq	%rax, 880(%rsp)                 ## 8-byte Spill
	movl	36(%r12), %eax
	movl	%eax, 1632(%rsp)                ## 4-byte Spill
	movq	40(%r12), %rax
	movl	(%rax), %ecx
	movq	%rcx, 144(%rsp)                 ## 8-byte Spill
	movl	4(%rax), %ecx
	movq	%rcx, 176(%rsp)                 ## 8-byte Spill
	movl	8(%rax), %eax
	movl	%eax, 2176(%rsp)                ## 4-byte Spill
	movq	16(%r13), %rax
	movq	%rax, 352(%rsp)                 ## 8-byte Spill
	leaq	32(%r13), %rsi
	leaq	28(%rsp), %rdi
	movl	$4, %edx
	callq	_memcpy
	movl	28(%rsp), %eax
	movl	%eax, 2144(%rsp)                ## 4-byte Spill
	movq	24(%r13), %rax
	movq	%rax, 864(%rsp)                 ## 8-byte Spill
	movl	36(%r13), %eax
	movl	%eax, 2112(%rsp)                ## 4-byte Spill
	movq	%r13, 216(%rsp)                 ## 8-byte Spill
	movq	40(%r13), %rax
	movl	(%rax), %ecx
	movq	%rcx, 160(%rsp)                 ## 8-byte Spill
	movl	4(%rax), %r13d
	movl	8(%rax), %ecx
	movl	%ecx, 1344(%rsp)                ## 4-byte Spill
	movl	16(%rax), %ecx
	movq	%rcx, 224(%rsp)                 ## 8-byte Spill
	movl	20(%rax), %ecx
	movq	%rcx, 288(%rsp)                 ## 8-byte Spill
	movslq	24(%rax), %rbx
	movl	32(%rax), %ecx
	movq	%rcx, 304(%rsp)                 ## 8-byte Spill
	movl	36(%rax), %ecx
	movq	%rcx, 736(%rsp)                 ## 8-byte Spill
	movslq	40(%rax), %rax
	movq	%rax, 336(%rsp)                 ## 8-byte Spill
	movq	32(%rbp), %r12
	movq	16(%r12), %rax
	movq	%rax, 1264(%rsp)                ## 8-byte Spill
	leaq	32(%r12), %rsi
	leaq	28(%rsp), %rdi
	movl	$4, %edx
	callq	_memcpy
	movl	28(%rsp), %eax
	movl	%eax, 1600(%rsp)                ## 4-byte Spill
	movq	24(%r12), %rax
	movq	%rax, 848(%rsp)                 ## 8-byte Spill
	movl	36(%r12), %eax
	movl	%eax, 1568(%rsp)                ## 4-byte Spill
	movq	40(%r12), %rax
	movl	(%rax), %ecx
	movq	%rcx, 64(%rsp)                  ## 8-byte Spill
	movl	4(%rax), %ecx
	movq	%rcx, 56(%rsp)                  ## 8-byte Spill
	movl	8(%rax), %eax
	movl	%eax, 208(%rsp)                 ## 4-byte Spill
	movq	24(%rbp), %r12
	movq	16(%r12), %rax
	movq	%rax, 1144(%rsp)                ## 8-byte Spill
	leaq	32(%r12), %rsi
	leaq	28(%rsp), %rdi
	movl	$4, %edx
	callq	_memcpy
	movl	28(%rsp), %eax
	movl	%eax, 1184(%rsp)                ## 4-byte Spill
	movq	24(%r12), %rax
	movq	%rax, 832(%rsp)                 ## 8-byte Spill
	movl	36(%r12), %eax
	movl	%eax, 1824(%rsp)                ## 4-byte Spill
	movq	40(%r12), %rax
	movl	(%rax), %ecx
	movq	%rcx, 48(%rsp)                  ## 8-byte Spill
	movl	4(%rax), %ecx
	movq	%rcx, 384(%rsp)                 ## 8-byte Spill
	movl	8(%rax), %ecx
	movl	%ecx, 672(%rsp)                 ## 4-byte Spill
	movl	16(%rax), %ecx
	movq	%rcx, 320(%rsp)                 ## 8-byte Spill
	movl	20(%rax), %ecx
	movq	%rcx, 1152(%rsp)                ## 8-byte Spill
	movl	24(%rax), %eax
	movl	%eax, 1120(%rsp)                ## 4-byte Spill
	movq	88(%rbp), %r12
	movq	16(%r12), %rax
	movq	%rax, 1408(%rsp)                ## 8-byte Spill
	leaq	32(%r12), %rsi
	leaq	28(%rsp), %rdi
	movl	$4, %edx
	callq	_memcpy
	movl	28(%rsp), %eax
	movl	%eax, 1792(%rsp)                ## 4-byte Spill
	movq	24(%r12), %rax
	movq	%rax, 1328(%rsp)                ## 8-byte Spill
	movl	36(%r12), %eax
	movl	%eax, 1760(%rsp)                ## 4-byte Spill
	movq	16(%r14), %rax
	movq	%rax, 2400(%rsp)                ## 8-byte Spill
	leaq	32(%r14), %rsi
	leaq	28(%rsp), %rdi
	movl	$4, %edx
	callq	_memcpy
	movl	28(%rsp), %eax
	movl	%eax, 1728(%rsp)                ## 4-byte Spill
	movq	24(%r14), %rax
	movq	%rax, 1696(%rsp)                ## 8-byte Spill
	movl	36(%r14), %eax
	movl	%eax, 536(%rsp)                 ## 4-byte Spill
	movq	%r14, 120(%rsp)                 ## 8-byte Spill
	movq	40(%r14), %rax
	movl	(%rax), %ecx
	movq	%rcx, 2560(%rsp)                ## 8-byte Spill
	movl	4(%rax), %ecx
	movq	%rcx, 256(%rsp)                 ## 8-byte Spill
	movl	8(%rax), %ecx
	movl	%ecx, 1664(%rsp)                ## 4-byte Spill
	movl	16(%rax), %ecx
	movq	%rcx, 40(%rsp)                  ## 8-byte Spill
	movl	20(%rax), %ecx
	movq	%rcx, 112(%rsp)                 ## 8-byte Spill
	movslq	24(%rax), %rcx
	movq	%rcx, 72(%rsp)                  ## 8-byte Spill
	movl	32(%rax), %ecx
	movq	%rcx, 2528(%rsp)                ## 8-byte Spill
	movl	36(%rax), %ecx
	movq	%rcx, 480(%rsp)                 ## 8-byte Spill
	movslq	40(%rax), %rax
	movq	%rax, 2368(%rsp)                ## 8-byte Spill
	movq	80(%rbp), %r12
	movq	16(%r12), %rax
	movq	%rax, 1256(%rsp)                ## 8-byte Spill
	leaq	32(%r12), %rsi
	leaq	28(%rsp), %rdi
	movl	$4, %edx
	callq	_memcpy
	movl	28(%rsp), %eax
	movl	%eax, 528(%rsp)                 ## 4-byte Spill
	movq	24(%r12), %r14
	movl	36(%r12), %eax
	movl	%eax, 520(%rsp)                 ## 4-byte Spill
	movq	40(%r12), %rax
	movl	(%rax), %ecx
	movq	%rcx, 616(%rsp)                 ## 8-byte Spill
	movl	4(%rax), %ecx
	movq	%rcx, 240(%rsp)                 ## 8-byte Spill
	movl	8(%rax), %eax
	movl	%eax, 2080(%rsp)                ## 4-byte Spill
	movq	3008(%rsp), %r12                ## 8-byte Reload
	movq	16(%r12), %rax
	movq	%rax, 712(%rsp)                 ## 8-byte Spill
	leaq	32(%r12), %rsi
	leaq	28(%rsp), %rdi
	movl	$4, %edx
	callq	_memcpy
	movl	28(%rsp), %eax
	movl	%eax, 1312(%rsp)                ## 4-byte Spill
	movq	24(%r12), %rsi
	movq	40(%r12), %rax
	movl	(%rax), %ecx
	movq	%rcx, 624(%rsp)                 ## 8-byte Spill
	movl	4(%rax), %ecx
	movq	%rcx, 1216(%rsp)                ## 8-byte Spill
	movl	8(%rax), %r11d
	movl	16(%rax), %ecx
	movq	%rcx, 2336(%rsp)                ## 8-byte Spill
	movl	20(%rax), %ecx
	movq	%rcx, 128(%rsp)                 ## 8-byte Spill
	movl	24(%rax), %r8d
	movl	32(%rax), %ecx
	movq	%rcx, 504(%rsp)                 ## 8-byte Spill
	movl	36(%rax), %ecx
	movq	%rcx, 104(%rsp)                 ## 8-byte Spill
	movl	40(%rax), %eax
	movl	%eax, 140(%rsp)                 ## 4-byte Spill
	cmpl	$-16, %r13d
	movl	$-16, %edx
	cmovgl	%r13d, %edx
	movl	%r13d, %r9d
	sarl	$31, %r9d
	movq	%r13, 32(%rsp)                  ## 8-byte Spill
	andl	%r13d, %r9d
	movl	36(%r12), %edi
	movq	40(%rbp), %rax
	movq	16(%rax), %rax
	testq	%rax, %rax
	movq	%rdx, 640(%rsp)                 ## 8-byte Spill
	jne	LBB0_4
## %bb.2:                               ## %_halide_buffer_is_bounds_query.exit
	cmpl	$0, 32(%rsp)                    ## 4-byte Folded Reload
	setns	%cl
	cmpl	$9, %edx
	setl	%dl
	testb	%cl, %dl
	movq	640(%rsp), %rcx                 ## 8-byte Reload
	jne	LBB0_4
## %bb.3:                               ## %_halide_buffer_is_bounds_query.exit
	movq	40(%rbp), %rdx
	cmpq	$0, (%rdx)
	je	LBB0_170
LBB0_4:                                 ## %"assert succeeded"
	movl	%edi, 1296(%rsp)                ## 4-byte Spill
	movq	%r14, 816(%rsp)                 ## 8-byte Spill
	movq	%rsi, 544(%rsp)                 ## 8-byte Spill
	movq	48(%rbp), %rdi
	cmpq	$0, 16(%rdi)
	je	LBB0_6
## %bb.5:
	movq	80(%rbp), %r10
	movq	24(%rbp), %rdx
	movq	32(%rbp), %r14
	movl	%r11d, 2048(%rsp)               ## 4-byte Spill
	testq	%rax, %rax
	movq	216(%rsp), %rax                 ## 8-byte Reload
	jne	LBB0_11
	jmp	LBB0_8
LBB0_6:                                 ## %_halide_buffer_is_bounds_query.exit279
	cmpq	$0, (%rdi)
	movq	80(%rbp), %r10
	movq	24(%rbp), %rdx
	movq	32(%rbp), %r14
	je	LBB0_10
## %bb.7:                               ## %after_bb
	movl	%r11d, 2048(%rsp)               ## 4-byte Spill
	testq	%rax, %rax
	movq	216(%rsp), %rax                 ## 8-byte Reload
	jne	LBB0_11
LBB0_8:                                 ## %_halide_buffer_is_bounds_query.exit283
	movq	40(%rbp), %rcx
	cmpq	$0, (%rcx)
	jne	LBB0_11
## %bb.9:                               ## %_halide_buffer_init.exit285
	vmovaps	LCPI0_4(%rip), %ymm0            ## ymm0 = [0,32,1,0,0,32,32,0]
	vmovups	%ymm0, 3072(%rsp)
	movq	40(%rbp), %rsi
	movq	40(%rsi), %r13
	vxorps	%xmm0, %xmm0, %xmm0
	vmovups	%xmm0, (%rsi)
	movq	$0, 16(%rsi)
	movabsq	$8590008322, %rcx               ## imm = 0x200012002
	movq	%rcx, 32(%rsi)
	vmovaps	3072(%rsp), %xmm0
	vmovups	%xmm0, (%r13)
	movq	40(%rsi), %rcx
	vmovapd	3088(%rsp), %xmm0
	vmovupd	%xmm0, 16(%rcx)
	movq	$0, 24(%rsi)
	jmp	LBB0_11
LBB0_10:                                ## %then_bb
	movq	40(%rdi), %rax
	vxorpd	%xmm0, %xmm0, %xmm0
	vmovupd	%xmm0, (%rdi)
	movq	$0, 16(%rdi)
	movabsq	$4295041026, %rcx               ## imm = 0x100012002
	movq	%rcx, 32(%rdi)
	vmovapd	LCPI0_3(%rip), %xmm0            ## xmm0 = [0,32,1,0]
	vmovupd	%xmm0, (%rax)
	movq	$0, 24(%rdi)
	movq	40(%rbp), %rax
	movq	16(%rax), %rax
	movl	%r11d, 2048(%rsp)               ## 4-byte Spill
	testq	%rax, %rax
	movq	216(%rsp), %rax                 ## 8-byte Reload
	je	LBB0_8
LBB0_11:                                ## %after_bb10
	movq	16(%rbp), %r11
	cmpq	$0, 16(%r11)
	jne	LBB0_13
## %bb.12:                              ## %_halide_buffer_is_bounds_query.exit284
	cmpq	$0, (%r11)
	je	LBB0_16
LBB0_13:                                ## %after_bb13
	cmpq	$0, 16(%rax)
	jne	LBB0_17
LBB0_14:                                ## %_halide_buffer_is_bounds_query.exit286
	cmpq	$0, (%rax)
	jne	LBB0_17
## %bb.15:                              ## %then_bb17
	movq	40(%rax), %r13
	vxorpd	%xmm0, %xmm0, %xmm0
	vmovupd	%xmm0, (%rax)
	movq	$0, 16(%rax)
	movabsq	$12884975618, %rcx              ## imm = 0x300012002
	movq	%rcx, 32(%rax)
	vmovaps	LCPI0_5(%rip), %xmm0            ## xmm0 = [0,8,1,0]
	vmovups	%xmm0, (%r13)
	movq	40(%rax), %rcx
	vmovaps	LCPI0_6(%rip), %xmm0            ## xmm0 = [0,40,8,0]
	vmovups	%xmm0, 16(%rcx)
	movq	40(%rax), %rcx
	vmovapd	LCPI0_7(%rip), %xmm0            ## xmm0 = [0,7,320,0]
	vmovupd	%xmm0, 32(%rcx)
	movq	$0, 24(%rax)
	jmp	LBB0_17
LBB0_16:                                ## %then_bb14
	movq	40(%r11), %r13
	vxorpd	%xmm0, %xmm0, %xmm0
	vmovupd	%xmm0, (%r11)
	movq	$0, 16(%r11)
	movabsq	$4295041026, %rcx               ## imm = 0x100012002
	movq	%rcx, 32(%r11)
	vmovapd	LCPI0_5(%rip), %xmm0            ## xmm0 = [0,8,1,0]
	vmovupd	%xmm0, (%r13)
	movq	$0, 24(%r11)
	cmpq	$0, 16(%rax)
	je	LBB0_14
LBB0_17:                                ## %after_bb16
	cmpq	$0, 16(%r14)
	movq	120(%rsp), %rsi                 ## 8-byte Reload
	movq	%r8, %r11
	jne	LBB0_19
## %bb.18:                              ## %_halide_buffer_is_bounds_query.exit288
	cmpq	$0, (%r14)
	je	LBB0_22
LBB0_19:                                ## %after_bb19
	cmpq	$0, 16(%rdx)
	jne	LBB0_23
LBB0_20:                                ## %_halide_buffer_is_bounds_query.exit291
	cmpq	$0, (%rdx)
	jne	LBB0_23
## %bb.21:                              ## %_halide_buffer_init.exit295
	vmovaps	LCPI0_9(%rip), %ymm0            ## ymm0 = [0,24,1,0,0,39,24,0]
	vmovups	%ymm0, 3104(%rsp)
	movq	40(%rdx), %rax
	vxorps	%xmm0, %xmm0, %xmm0
	vmovups	%xmm0, (%rdx)
	movq	$0, 16(%rdx)
	movabsq	$8590008322, %rcx               ## imm = 0x200012002
	movq	%rcx, 32(%rdx)
	vmovaps	3104(%rsp), %xmm0
	vmovups	%xmm0, (%rax)
	movq	40(%rdx), %rax
	vmovapd	3120(%rsp), %xmm0
	vmovupd	%xmm0, 16(%rax)
	movq	$0, 24(%rdx)
	jmp	LBB0_23
LBB0_22:                                ## %then_bb20
	movq	40(%r14), %rax
	vxorpd	%xmm0, %xmm0, %xmm0
	vmovupd	%xmm0, (%r14)
	movq	$0, 16(%r14)
	movabsq	$4295041026, %rcx               ## imm = 0x100012002
	movq	%rcx, 32(%r14)
	vmovapd	LCPI0_8(%rip), %xmm0            ## xmm0 = [0,24,1,0]
	vmovupd	%xmm0, (%rax)
	movq	$0, 24(%r14)
	cmpq	$0, 16(%rdx)
	je	LBB0_20
LBB0_23:                                ## %after_bb22
	movq	88(%rbp), %rax
	cmpq	$0, 16(%rax)
	jne	LBB0_25
## %bb.24:                              ## %_halide_buffer_is_bounds_query.exit293
	cmpq	$0, (%rax)
	je	LBB0_28
LBB0_25:                                ## %after_bb25
	cmpq	$0, 16(%rsi)
	jne	LBB0_29
LBB0_26:                                ## %_halide_buffer_is_bounds_query.exit296
	cmpq	$0, (%rsi)
	jne	LBB0_29
## %bb.27:                              ## %then_bb29
	movq	40(%rsi), %rax
	vxorpd	%xmm0, %xmm0, %xmm0
	vmovupd	%xmm0, (%rsi)
	movq	$0, 16(%rsi)
	movabsq	$12884975618, %rcx              ## imm = 0x300012002
	movq	%rcx, 32(%rsi)
	vmovaps	LCPI0_10(%rip), %xmm0           ## xmm0 = [0,40,1,0]
	vmovups	%xmm0, (%rax)
	movq	40(%rsi), %rax
	vmovapd	LCPI0_11(%rip), %xmm0           ## xmm0 = [0,7,40,0]
	vmovupd	%xmm0, 16(%rax)
	movq	40(%rsi), %rax
	movl	$0, 32(%rax)
	movq	192(%rsp), %rcx                 ## 8-byte Reload
	movl	%ecx, 36(%rax)
	movq	$280, 40(%rax)                  ## imm = 0x118
	movq	88(%rbp), %rax
	movq	$0, 24(%rsi)
	jmp	LBB0_29
LBB0_28:                                ## %then_bb26
	vxorpd	%xmm0, %xmm0, %xmm0
	vmovupd	%xmm0, 16(%rax)
	vmovupd	%xmm0, (%rax)
	movq	$73730, 32(%rax)                ## imm = 0x12002
	cmpq	$0, 16(%rsi)
	je	LBB0_26
LBB0_29:                                ## %after_bb28
	cmpq	$0, 16(%r10)
	jne	LBB0_31
## %bb.30:                              ## %_halide_buffer_is_bounds_query.exit297
	cmpq	$0, (%r10)
	je	LBB0_33
LBB0_31:                                ## %after_bb31
	cmpq	$0, 16(%r12)
	je	LBB0_34
LBB0_32:
	xorl	%r8d, %r8d
	cmpq	$0, 16(%r10)
	je	LBB0_46
LBB0_36:
	xorl	%r13d, %r13d
	movl	%r9d, 632(%rsp)                 ## 4-byte Spill
	cmpq	$0, 16(%rsi)
	je	LBB0_47
LBB0_37:
	xorl	%r10d, %r10d
	movq	%r11, 1272(%rsp)                ## 8-byte Spill
	cmpq	$0, 16(%rax)
	je	LBB0_48
LBB0_38:
	xorl	%r11d, %r11d
	cmpq	$0, 16(%rdx)
	je	LBB0_49
LBB0_39:
	xorl	%r12d, %r12d
	movq	16(%rbp), %rdx
	cmpq	$0, 16(%r14)
	je	LBB0_50
LBB0_40:
	xorl	%eax, %eax
	movq	400(%rsp), %r9                  ## 8-byte Reload
	movq	216(%rsp), %rcx                 ## 8-byte Reload
	cmpq	$0, 16(%rcx)
	je	LBB0_51
LBB0_41:
	xorl	%ecx, %ecx
	cmpq	$0, 16(%rdx)
	je	LBB0_52
LBB0_42:
	xorl	%edx, %edx
	cmpq	$0, 16(%rdi)
	je	LBB0_53
LBB0_43:
	xorl	%esi, %esi
	jmp	LBB0_54
LBB0_33:                                ## %then_bb32
	movq	40(%r10), %rax
	vxorpd	%xmm0, %xmm0, %xmm0
	vmovupd	%xmm0, (%r10)
	movq	$0, 16(%r10)
	movabsq	$4295041026, %rcx               ## imm = 0x100012002
	movq	%rcx, 32(%r10)
	movl	$0, (%rax)
	movq	1504(%rsp), %rcx                ## 8-byte Reload
	movl	%ecx, 4(%rax)
	movq	$1, 8(%rax)
	movq	88(%rbp), %rax
	movq	$0, 24(%r10)
	cmpq	$0, 16(%r12)
	jne	LBB0_32
LBB0_34:                                ## %_halide_buffer_is_bounds_query.exit300
	cmpq	$0, (%r12)
	je	LBB0_44
LBB0_35:                                ## %after_bb34.thread
	cmpq	$0, (%r12)
	sete	%r8b
	cmpq	$0, 16(%r10)
	jne	LBB0_36
LBB0_46:
	cmpq	$0, (%r10)
	sete	%r13b
	movl	%r9d, 632(%rsp)                 ## 4-byte Spill
	cmpq	$0, 16(%rsi)
	jne	LBB0_37
LBB0_47:
	cmpq	$0, (%rsi)
	sete	%r10b
	movq	%r11, 1272(%rsp)                ## 8-byte Spill
	cmpq	$0, 16(%rax)
	jne	LBB0_38
LBB0_48:
	cmpq	$0, (%rax)
	sete	%r11b
	cmpq	$0, 16(%rdx)
	jne	LBB0_39
LBB0_49:
	cmpq	$0, (%rdx)
	sete	%r12b
	movq	16(%rbp), %rdx
	cmpq	$0, 16(%r14)
	jne	LBB0_40
LBB0_50:
	cmpq	$0, (%r14)
	sete	%al
	movq	400(%rsp), %r9                  ## 8-byte Reload
	movq	216(%rsp), %rcx                 ## 8-byte Reload
	cmpq	$0, 16(%rcx)
	jne	LBB0_41
LBB0_51:
	cmpq	$0, (%rcx)
	sete	%cl
	cmpq	$0, 16(%rdx)
	jne	LBB0_42
LBB0_52:
	cmpq	$0, (%rdx)
	sete	%dl
	cmpq	$0, 16(%rdi)
	jne	LBB0_43
LBB0_53:
	cmpq	$0, (%rdi)
	sete	%sil
LBB0_54:                                ## %_halide_buffer_is_bounds_query.exit310
	xorl	%r14d, %r14d
	movq	40(%rbp), %rdi
	cmpq	$0, 16(%rdi)
	movl	$0, %edi
	jne	LBB0_56
## %bb.55:
	movq	40(%rbp), %rdi
	cmpq	$0, (%rdi)
	sete	%dil
LBB0_56:                                ## %_halide_buffer_is_bounds_query.exit311
	orb	%dil, %sil
	orb	%sil, %dl
	orb	%dl, %cl
	orb	%cl, %al
	orb	%al, %r12b
	orb	%r12b, %r11b
	orb	%r11b, %r10b
	orb	%r10b, %r13b
	orb	%r13b, %r8b
	testb	$1, %r8b
	je	LBB0_58
## %bb.57:
	xorl	%r12d, %r12d
	xorl	%esi, %esi
	jmp	LBB0_139
LBB0_58:                                ## %then_bb38
	xorl	%eax, %eax
	cmpl	$73730, 688(%rsp)               ## 4-byte Folded Reload
                                        ## imm = 0x12002
	setne	%al
	movq	%rax, 216(%rsp)                 ## 8-byte Spill
	xorl	%eax, %eax
	cmpl	$1, 200(%rsp)                   ## 4-byte Folded Reload
	sete	%al
	movl	%eax, 120(%rsp)                 ## 4-byte Spill
	xorl	%eax, %eax
	cmpl	$73730, 784(%rsp)               ## 4-byte Folded Reload
                                        ## imm = 0x12002
	sete	%al
	movl	%eax, 376(%rsp)                 ## 4-byte Spill
	xorl	%eax, %eax
	cmpl	$2, 768(%rsp)                   ## 4-byte Folded Reload
	sete	%al
	movl	%eax, 2592(%rsp)                ## 4-byte Spill
	xorl	%eax, %eax
	cmpl	$73730, 2208(%rsp)              ## 4-byte Folded Reload
                                        ## imm = 0x12002
	sete	%al
	movl	%eax, 2688(%rsp)                ## 4-byte Spill
	xorl	%eax, %eax
	cmpl	$1, 1632(%rsp)                  ## 4-byte Folded Reload
	sete	%al
	movl	%eax, 2624(%rsp)                ## 4-byte Spill
	xorl	%r8d, %r8d
	cmpl	$73730, 2144(%rsp)              ## 4-byte Folded Reload
                                        ## imm = 0x12002
	sete	%r8b
	xorl	%r10d, %r10d
	cmpl	$3, 2112(%rsp)                  ## 4-byte Folded Reload
	sete	%r10b
	xorl	%eax, %eax
	cmpl	$73730, 1600(%rsp)              ## 4-byte Folded Reload
                                        ## imm = 0x12002
	sete	%al
	movl	%eax, 1920(%rsp)                ## 4-byte Spill
	xorl	%eax, %eax
	cmpl	$1, 1568(%rsp)                  ## 4-byte Folded Reload
	sete	%al
	movl	%eax, 1376(%rsp)                ## 4-byte Spill
	xorl	%eax, %eax
	cmpl	$73730, 1184(%rsp)              ## 4-byte Folded Reload
                                        ## imm = 0x12002
	sete	%al
	movl	%eax, 2752(%rsp)                ## 4-byte Spill
	xorl	%eax, %eax
	cmpl	$2, 1824(%rsp)                  ## 4-byte Folded Reload
	sete	%al
	movl	%eax, 2720(%rsp)                ## 4-byte Spill
	xorl	%r14d, %r14d
	cmpl	$73730, 1792(%rsp)              ## 4-byte Folded Reload
                                        ## imm = 0x12002
	sete	%r14b
	xorl	%eax, %eax
	cmpl	$0, 1760(%rsp)                  ## 4-byte Folded Reload
	sete	%al
	movl	%eax, 1536(%rsp)                ## 4-byte Spill
	xorl	%eax, %eax
	cmpl	$73730, 1728(%rsp)              ## 4-byte Folded Reload
                                        ## imm = 0x12002
	setne	%al
	shlq	$14, %rax
	movq	%rax, 2656(%rsp)                ## 8-byte Spill
	xorl	%esi, %esi
	cmpl	$3, 536(%rsp)                   ## 4-byte Folded Reload
	setne	%sil
	shlq	$15, %rsi
	xorl	%eax, %eax
	cmpl	$73730, 528(%rsp)               ## 4-byte Folded Reload
                                        ## imm = 0x12002
	setne	%al
	shlq	$16, %rax
	movq	%rax, 88(%rsp)                  ## 8-byte Spill
	xorl	%eax, %eax
	cmpl	$1, 520(%rsp)                   ## 4-byte Folded Reload
	setne	%al
	shlq	$17, %rax
	movq	%rax, 512(%rsp)                 ## 8-byte Spill
	xorl	%eax, %eax
	cmpl	$73730, 1312(%rsp)              ## 4-byte Folded Reload
                                        ## imm = 0x12002
	setne	%al
	shlq	$18, %rax
	movq	%rax, 2432(%rsp)                ## 8-byte Spill
	xorl	%eax, %eax
	cmpl	$3, 1296(%rsp)                  ## 4-byte Folded Reload
	setne	%al
	shlq	$19, %rax
	movq	%rax, 1136(%rsp)                ## 8-byte Spill
	movq	464(%rsp), %rcx                 ## 8-byte Reload
	leal	(%rcx,%r9), %eax
	movl	%eax, 1032(%rsp)                ## 4-byte Spill
	cmpl	$32, %eax
	setl	%r11b
	xorl	%edx, %edx
	movq	656(%rsp), %rax                 ## 8-byte Reload
	testl	%eax, %eax
	sete	%dl
	movl	%edx, 1952(%rsp)                ## 4-byte Spill
	setg	%r12b
	xorl	%edi, %edi
	testl	%r9d, %r9d
	sete	%dil
	movl	%edi, 1984(%rsp)                ## 4-byte Spill
	setg	%dl
	orb	%r11b, %dl
	movb	%dl, 2464(%rsp)                 ## 1-byte Spill
	shrq	$10, %rcx
	andl	$2097152, %ecx                  ## imm = 0x200000
	movq	%rcx, 2496(%rsp)                ## 8-byte Spill
	movq	432(%rsp), %rcx                 ## 8-byte Reload
	addl	%ecx, %eax
	movl	%eax, 1024(%rsp)                ## 4-byte Spill
	cmpl	$32, %eax
	setl	%al
	orb	%r12b, %al
	movb	%al, 3040(%rsp)                 ## 1-byte Spill
	movq	%rcx, %rax
	shrq	$8, %rax
	andl	$8388608, %eax                  ## imm = 0x800000
	movq	%rax, 2016(%rsp)                ## 8-byte Spill
	movq	448(%rsp), %rcx                 ## 8-byte Reload
	cmpl	632(%rsp), %ecx                 ## 4-byte Folded Reload
	setg	%r9b
	movq	640(%rsp), %rax                 ## 8-byte Reload
	leal	24(%rax), %edx
	movq	416(%rsp), %rdi                 ## 8-byte Reload
	leal	(%rdi,%rcx), %eax
	movl	%eax, 1016(%rsp)                ## 4-byte Spill
	cmpl	%eax, %edx
	setg	%r13b
	orb	%r9b, %r13b
	movzbl	%r13b, %ecx
	vmovd	%ecx, %xmm0
	movq	%rdi, %rax
	shrq	$6, %rax
	andl	$33554432, %eax                 ## imm = 0x2000000
	movq	%rax, 608(%rsp)                 ## 8-byte Spill
	movq	144(%rsp), %rax                 ## 8-byte Reload
	testl	%eax, %eax
	setg	%cl
	movq	176(%rsp), %rdx                 ## 8-byte Reload
	addl	%edx, %eax
	movl	%eax, 1008(%rsp)                ## 4-byte Spill
	cmpl	$8, %eax
	setl	%r9b
	orb	%cl, %r9b
	shrq	$4, %rdx
	andl	$134217728, %edx                ## imm = 0x8000000
	movq	%rdx, 1288(%rsp)                ## 8-byte Spill
	movq	160(%rsp), %rdx                 ## 8-byte Reload
	testl	%edx, %edx
	setg	%cl
	movq	32(%rsp), %rax                  ## 8-byte Reload
	addl	%eax, %edx
	movl	%edx, 1000(%rsp)                ## 4-byte Spill
	cmpl	$8, %edx
	setl	%r11b
	orb	%cl, %r11b
	shrq	$2, %rax
	andl	$536870912, %eax                ## imm = 0x20000000
	movq	%rax, 1280(%rsp)                ## 8-byte Spill
	movq	224(%rsp), %rax                 ## 8-byte Reload
	testl	%eax, %eax
	setg	%cl
	movq	288(%rsp), %rdi                 ## 8-byte Reload
	addl	%edi, %eax
	movl	%eax, 992(%rsp)                 ## 4-byte Spill
	cmpl	$40, %eax
	setl	%dl
	orb	%cl, %dl
	movzbl	%dl, %ecx
	vpinsrb	$4, %ecx, %xmm0, %xmm0
                                        ## kill: def $edi killed $edi killed $rdi def $rdi
	andl	$-2147483648, %edi              ## imm = 0x80000000
	movq	%rdi, 2944(%rsp)                ## 8-byte Spill
	movq	304(%rsp), %rax                 ## 8-byte Reload
	testl	%eax, %eax
	setg	%cl
	movq	736(%rsp), %rdi                 ## 8-byte Reload
	addl	%edi, %eax
	movl	%eax, 984(%rsp)                 ## 4-byte Spill
	cmpl	$7, %eax
	setl	%dl
	orb	%cl, %dl
	movzbl	%dl, %ecx
	vpinsrb	$8, %ecx, %xmm0, %xmm0
	vpinsrb	$12, 1920(%rsp), %xmm0, %xmm0   ## 4-byte Folded Reload
                                        ## kill: def $edi killed $edi killed $rdi def $rdi
	andl	$-2147483648, %edi              ## imm = 0x80000000
	shlq	$2, %rdi
	movq	%rdi, 1920(%rsp)                ## 8-byte Spill
	movq	64(%rsp), %rax                  ## 8-byte Reload
	testl	%eax, %eax
	setg	%cl
	movq	56(%rsp), %rdx                  ## 8-byte Reload
	addl	%edx, %eax
	movl	%eax, 976(%rsp)                 ## 4-byte Spill
	cmpl	$24, %eax
	setl	%al
	orb	%cl, %al
	movb	%al, 1088(%rsp)                 ## 1-byte Spill
                                        ## kill: def $edx killed $edx killed $rdx def $rdx
	andl	$-2147483648, %edx              ## imm = 0x80000000
	shlq	$4, %rdx
	movq	%rdx, 720(%rsp)                 ## 8-byte Spill
	movq	48(%rsp), %rax                  ## 8-byte Reload
	testl	%eax, %eax
	setg	%cl
	movq	384(%rsp), %rdi                 ## 8-byte Reload
	addl	%edi, %eax
	movl	%eax, 968(%rsp)                 ## 4-byte Spill
	cmpl	$24, %eax
	setl	%dl
	orb	%cl, %dl
	movzbl	%dl, %ecx
	vmovd	%ecx, %xmm1
                                        ## kill: def $edi killed $edi killed $rdi def $rdi
	andl	$-2147483648, %edi              ## imm = 0x80000000
	shlq	$6, %rdi
	movq	%rdi, 1104(%rsp)                ## 8-byte Spill
	movq	320(%rsp), %rax                 ## 8-byte Reload
	testl	%eax, %eax
	setg	%cl
	movq	1152(%rsp), %rdi                ## 8-byte Reload
	addl	%edi, %eax
	movl	%eax, 960(%rsp)                 ## 4-byte Spill
	cmpl	$39, %eax
	setl	%dl
	orb	%cl, %dl
	movzbl	%dl, %ecx
	vpinsrb	$4, %ecx, %xmm1, %xmm1
	vpinsrb	$8, %r14d, %xmm1, %xmm1
	vpinsrb	$12, 1536(%rsp), %xmm1, %xmm1   ## 4-byte Folded Reload
	movzbl	%r9b, %ecx
	vmovd	%ecx, %xmm2
	vpinsrb	$4, %r8d, %xmm2, %xmm2
	vpinsrb	$8, %r10d, %xmm2, %xmm2
	movzbl	%r11b, %ecx
	vpinsrb	$12, %ecx, %xmm2, %xmm2
                                        ## kill: def $edi killed $edi killed $rdi def $rdi
	andl	$-2147483648, %edi              ## imm = 0x80000000
	shlq	$8, %rdi
	movq	%rdi, 1536(%rsp)                ## 8-byte Spill
	movq	2560(%rsp), %rax                ## 8-byte Reload
	testl	%eax, %eax
	setg	%cl
	movq	256(%rsp), %rdi                 ## 8-byte Reload
	addl	%edi, %eax
	movl	%eax, 952(%rsp)                 ## 4-byte Spill
	cmpl	$40, %eax
	setl	%dl
	orb	%cl, %dl
	movzbl	%dl, %eax
	shlq	$40, %rax
	movq	%rax, 1080(%rsp)                ## 8-byte Spill
                                        ## kill: def $edi killed $edi killed $rdi def $rdi
	andl	$-2147483648, %edi              ## imm = 0x80000000
	shlq	$10, %rdi
	movq	%rdi, 1096(%rsp)                ## 8-byte Spill
	movq	40(%rsp), %rax                  ## 8-byte Reload
	testl	%eax, %eax
	setg	%cl
	movq	112(%rsp), %r12                 ## 8-byte Reload
	addl	%r12d, %eax
	movl	%eax, 944(%rsp)                 ## 4-byte Spill
	cmpl	$7, %eax
	setl	%dl
	orb	%cl, %dl
	movzbl	%dl, %eax
	shlq	$42, %rax
	movq	%rax, 1072(%rsp)                ## 8-byte Spill
                                        ## kill: def $r12d killed $r12d killed $r12 def $r12
	andl	$-2147483648, %r12d             ## imm = 0x80000000
	shlq	$12, %r12
	movq	2528(%rsp), %rax                ## 8-byte Reload
	testl	%eax, %eax
	setg	%cl
	movq	480(%rsp), %r14                 ## 8-byte Reload
	leal	(%r14,%rax), %edx
	movq	192(%rsp), %rax                 ## 8-byte Reload
	movl	%edx, 936(%rsp)                 ## 4-byte Spill
	cmpl	%eax, %edx
	setl	%dl
	orb	%cl, %dl
	movzbl	%dl, %ecx
	shlq	$44, %rcx
	movq	%rcx, 1064(%rsp)                ## 8-byte Spill
                                        ## kill: def $r14d killed $r14d killed $r14 def $r14
	andl	$-2147483648, %r14d             ## imm = 0x80000000
	shlq	$14, %r14
	movq	616(%rsp), %rdx                 ## 8-byte Reload
	testl	%edx, %edx
	setg	%cl
	movq	240(%rsp), %r11                 ## 8-byte Reload
	addl	%r11d, %edx
	movq	1504(%rsp), %rdi                ## 8-byte Reload
	movl	%edx, 2976(%rsp)                ## 4-byte Spill
	cmpl	%edi, %edx
	setl	%dl
	orb	%cl, %dl
	movzbl	%dl, %ecx
	shlq	$46, %rcx
	movq	%rcx, 1056(%rsp)                ## 8-byte Spill
                                        ## kill: def $r11d killed $r11d killed $r11 def $r11
	andl	$-2147483648, %r11d             ## imm = 0x80000000
	shlq	$16, %r11
	movq	624(%rsp), %rdx                 ## 8-byte Reload
	testl	%edx, %edx
	setg	%cl
	movq	1216(%rsp), %r10                ## 8-byte Reload
	addl	%r10d, %edx
	movl	%edx, 600(%rsp)                 ## 4-byte Spill
	cmpl	%edi, %edx
	setl	%dl
	orb	%cl, %dl
	movzbl	%dl, %ecx
	shlq	$48, %rcx
	movq	%rcx, 1048(%rsp)                ## 8-byte Spill
                                        ## kill: def $r10d killed $r10d killed $r10 def $r10
	andl	$-2147483648, %r10d             ## imm = 0x80000000
	shlq	$18, %r10
	movq	2336(%rsp), %rdx                ## 8-byte Reload
	testl	%edx, %edx
	setg	%cl
	movq	128(%rsp), %r8                  ## 8-byte Reload
	addl	%r8d, %edx
	movl	%edx, 1472(%rsp)                ## 4-byte Spill
	cmpl	$39, %edx
	setl	%dl
	orb	%cl, %dl
	movzbl	%dl, %ecx
	shlq	$50, %rcx
	movq	%rcx, 1040(%rsp)                ## 8-byte Spill
                                        ## kill: def $r8d killed $r8d killed $r8 def $r8
	andl	$-2147483648, %r8d              ## imm = 0x80000000
	shlq	$20, %r8
	movq	504(%rsp), %rdx                 ## 8-byte Reload
	testl	%edx, %edx
	setg	%cl
	movq	104(%rsp), %r9                  ## 8-byte Reload
	addl	%r9d, %edx
	movl	%edx, 1440(%rsp)                ## 4-byte Spill
	cmpl	%eax, %edx
	setl	%dl
	orb	%cl, %dl
	movzbl	%dl, %r13d
	shlq	$52, %r13
                                        ## kill: def $r9d killed $r9d killed $r9 def $r9
	andl	$-2147483648, %r9d              ## imm = 0x80000000
	shlq	$22, %r9
	xorl	%edi, %edi
	cmpl	$1, 800(%rsp)                   ## 4-byte Folded Reload
	sete	%dil
	xorl	%edx, %edx
	cmpl	$32, 464(%rsp)                  ## 4-byte Folded Reload
	sete	%dl
	xorl	%ecx, %ecx
	cmpl	$1, 1200(%rsp)                  ## 4-byte Folded Reload
	sete	%cl
	xorl	%eax, %eax
	cmpl	$32, 432(%rsp)                  ## 4-byte Folded Reload
	sete	%al
	vmovd	%eax, %xmm3
	vpinsrb	$4, %ecx, %xmm3, %xmm3
	movzbl	3040(%rsp), %eax                ## 1-byte Folded Reload
	vpinsrb	$8, %eax, %xmm3, %xmm3
	xorl	%eax, %eax
	cmpl	$0, 448(%rsp)                   ## 4-byte Folded Reload
	sete	%al
	vpinsrb	$12, %eax, %xmm3, %xmm3
	movzbl	2464(%rsp), %eax                ## 1-byte Folded Reload
	vmovd	%eax, %xmm4
	vpinsrb	$4, 376(%rsp), %xmm4, %xmm4     ## 4-byte Folded Reload
	vpinsrb	$8, 2592(%rsp), %xmm4, %xmm4    ## 4-byte Folded Reload
	vpinsrb	$12, 1952(%rsp), %xmm4, %xmm4   ## 4-byte Folded Reload
	xorl	%eax, %eax
	cmpl	$32, 416(%rsp)                  ## 4-byte Folded Reload
	sete	%al
	vmovd	%eax, %xmm5
	vpinsrb	$4, 2688(%rsp), %xmm5, %xmm5    ## 4-byte Folded Reload
	vpinsrb	$8, 2624(%rsp), %xmm5, %xmm5    ## 4-byte Folded Reload
	xorl	%eax, %eax
	cmpl	$1, 2176(%rsp)                  ## 4-byte Folded Reload
	sete	%al
	vpinsrb	$12, %eax, %xmm5, %xmm5
	vmovd	120(%rsp), %xmm6                ## 4-byte Folded Reload
                                        ## xmm6 = mem[0],zero,zero,zero
	vpinsrb	$4, 1984(%rsp), %xmm6, %xmm6    ## 4-byte Folded Reload
	vpinsrb	$8, %edx, %xmm6, %xmm6
	vpinsrb	$12, %edi, %xmm6, %xmm6
	vmovd	1376(%rsp), %xmm7               ## 4-byte Folded Reload
                                        ## xmm7 = mem[0],zero,zero,zero
	movzbl	1088(%rsp), %eax                ## 1-byte Folded Reload
	vpinsrb	$4, %eax, %xmm7, %xmm7
	vpinsrb	$8, 2752(%rsp), %xmm7, %xmm7    ## 4-byte Folded Reload
	vpinsrb	$12, 2720(%rsp), %xmm7, %xmm7   ## 4-byte Folded Reload
	orq	2656(%rsp), %rsi                ## 8-byte Folded Reload
	orq	1080(%rsp), %rsi                ## 8-byte Folded Reload
	orq	1072(%rsp), %rsi                ## 8-byte Folded Reload
	orq	1064(%rsp), %rsi                ## 8-byte Folded Reload
	orq	88(%rsp), %rsi                  ## 8-byte Folded Reload
	orq	512(%rsp), %rsi                 ## 8-byte Folded Reload
	orq	1056(%rsp), %rsi                ## 8-byte Folded Reload
	orq	2432(%rsp), %rsi                ## 8-byte Folded Reload
	orq	1136(%rsp), %rsi                ## 8-byte Folded Reload
	orq	1048(%rsp), %rsi                ## 8-byte Folded Reload
	orq	1040(%rsp), %rsi                ## 8-byte Folded Reload
	orq	%r13, %rsi
	orq	216(%rsp), %rsi                 ## 8-byte Folded Reload
	orq	2496(%rsp), %rsi                ## 8-byte Folded Reload
	orq	2016(%rsp), %rsi                ## 8-byte Folded Reload
	orq	608(%rsp), %rsi                 ## 8-byte Folded Reload
	orq	1288(%rsp), %rsi                ## 8-byte Folded Reload
	orq	1280(%rsp), %rsi                ## 8-byte Folded Reload
	orq	2944(%rsp), %rsi                ## 8-byte Folded Reload
	orq	1920(%rsp), %rsi                ## 8-byte Folded Reload
	orq	720(%rsp), %rsi                 ## 8-byte Folded Reload
	orq	1104(%rsp), %rsi                ## 8-byte Folded Reload
	orq	1536(%rsp), %rsi                ## 8-byte Folded Reload
	orq	1096(%rsp), %rsi                ## 8-byte Folded Reload
	orq	%r12, %rsi
	orq	%r14, %rsi
	orq	%r11, %rsi
	vpslld	$31, %xmm0, %xmm0
	vpmovsxdq	%xmm0, %ymm0
	vmovapd	LCPI0_13(%rip), %ymm8           ## ymm8 = [0,0,0,256]
	vblendvpd	%ymm0, LCPI0_12(%rip), %ymm8, %ymm0
	orq	%r10, %rsi
	orq	%r8, %rsi
	vpslld	$31, %xmm1, %xmm1
	vpmovsxdq	%xmm1, %ymm1
	vmovapd	LCPI0_15(%rip), %ymm8           ## ymm8 = [0,0,4096,8192]
	vblendvpd	%ymm1, LCPI0_14(%rip), %ymm8, %ymm1
	vpslld	$31, %xmm4, %xmm4
	vpmovsxdq	%xmm4, %ymm4
	vmovapd	LCPI0_20(%rip), %xmm8           ## xmm8 = [1048576,0,0,0]
	vmovapd	LCPI0_21(%rip), %ymm9           ## ymm9 = [0,4,8,288230376151711744]
	vblendvpd	%ymm4, %ymm8, %ymm9, %ymm4
	vpslld	$31, %xmm2, %xmm2
	vpmovsxdq	%xmm2, %ymm2
	vmovapd	LCPI0_17(%rip), %ymm8           ## ymm8 = [0,64,128,0]
	vblendvpd	%ymm2, LCPI0_16(%rip), %ymm8, %ymm2
	orq	%r9, %rsi
	vpslld	$31, %xmm3, %xmm3
	vpmovsxdq	%xmm3, %ymm3
	vmovapd	LCPI0_19(%rip), %ymm8           ## ymm8 = [576460752303423488,144115188075855872,0,1152921504606846976]
	vblendvpd	%ymm3, LCPI0_18(%rip), %ymm8, %ymm3
	vpslld	$31, %xmm5, %xmm5
	vpsrad	$31, %xmm5, %xmm5
	vpmovsxdq	%xmm5, %ymm5
	vpandn	LCPI0_22(%rip), %ymm5, %ymm5
	vorpd	%ymm0, %ymm4, %ymm0
	vpor	%ymm1, %ymm5, %ymm1
	vorpd	%ymm1, %ymm0, %ymm0
	vpslld	$31, %xmm6, %xmm1
	vpsrad	$31, %xmm1, %xmm1
	vpmovsxdq	%xmm1, %ymm1
	vpslld	$31, %xmm7, %xmm4
	vpmovsxdq	%xmm4, %ymm4
	vmovapd	LCPI0_25(%rip), %ymm5           ## ymm5 = [512,0,1024,2048]
	vblendvpd	%ymm4, LCPI0_24(%rip), %ymm5, %ymm4
	vpandn	LCPI0_23(%rip), %ymm1, %ymm1
	vorpd	%ymm4, %ymm3, %ymm3
	vorpd	%ymm3, %ymm2, %ymm2
	vpor	%ymm2, %ymm1, %ymm1
	vpor	%ymm0, %ymm1, %ymm0
	vextracti128	$1, %ymm0, %xmm1
	vpor	%xmm1, %xmm0, %xmm0
	vpshufd	$238, %xmm0, %xmm1              ## xmm1 = xmm0[2,3,2,3]
	vpor	%xmm1, %xmm0, %xmm0
	vmovq	%xmm0, %rax
	orq	%rax, %rsi
	movabsq	$-9223372036854775808, %rcx     ## imm = 0x8000000000000000
	orq	%rcx, %rsi
	xorl	%eax, %eax
	tzcntq	%rsi, %rax
	cmpl	$62, %eax
	jbe	LBB0_161
## %bb.59:                              ## %no_errors_bb40
	xorl	%eax, %eax
	vmovd	384(%rsp), %xmm0                ## 4-byte Folded Reload
                                        ## xmm0 = mem[0],zero,zero,zero
	vpinsrd	$1, 672(%rsp), %xmm0, %xmm0     ## 4-byte Folded Reload
	vpinsrd	$2, 320(%rsp), %xmm0, %xmm0     ## 4-byte Folded Reload
	cmpl	$0, 144(%rsp)                   ## 4-byte Folded Reload
	vpinsrd	$3, 1152(%rsp), %xmm0, %xmm0    ## 4-byte Folded Reload
	vmovd	64(%rsp), %xmm1                 ## 4-byte Folded Reload
                                        ## xmm1 = mem[0],zero,zero,zero
	vpinsrd	$1, 56(%rsp), %xmm1, %xmm1      ## 4-byte Folded Reload
	vpinsrd	$2, 208(%rsp), %xmm1, %xmm1     ## 4-byte Folded Reload
	setne	%al
	xorl	%esi, %esi
	cmpl	$1, 1664(%rsp)                  ## 4-byte Folded Reload
	setne	%sil
	shlq	$17, %rsi
	xorl	%edx, %edx
	cmpl	$1, 2080(%rsp)                  ## 4-byte Folded Reload
	setne	%dl
	shlq	$18, %rdx
	xorl	%edi, %edi
	cmpl	$1, 2048(%rsp)                  ## 4-byte Folded Reload
	setne	%dil
	orq	%rsi, %rdx
	vpinsrd	$3, 48(%rsp), %xmm1, %xmm1      ## 4-byte Folded Reload
	shlq	$19, %rdi
	orq	%rdi, %rdx
	vmovd	224(%rsp), %xmm2                ## 4-byte Folded Reload
                                        ## xmm2 = mem[0],zero,zero,zero
	vpinsrd	$1, 288(%rsp), %xmm2, %xmm2     ## 4-byte Folded Reload
	vpinsrd	$2, 304(%rsp), %xmm2, %xmm2     ## 4-byte Folded Reload
	vpinsrd	$3, 736(%rsp), %xmm2, %xmm2     ## 4-byte Folded Reload
	vinserti128	$1, %xmm0, %ymm1, %ymm0
	vmovd	176(%rsp), %xmm1                ## 4-byte Folded Reload
                                        ## xmm1 = mem[0],zero,zero,zero
	vpinsrd	$1, 160(%rsp), %xmm1, %xmm1     ## 4-byte Folded Reload
	vpinsrd	$2, 32(%rsp), %xmm1, %xmm1      ## 4-byte Folded Reload
	vpinsrd	$3, 1344(%rsp), %xmm1, %xmm1    ## 4-byte Folded Reload
	vinserti128	$1, %xmm2, %ymm1, %ymm1
	vpcmpeqd	LCPI0_26(%rip), %ymm1, %ymm1
	vextracti128	$1, %ymm1, %xmm2
	vpmovsxdq	%xmm2, %ymm2
	vpcmpeqd	LCPI0_27(%rip), %ymm0, %ymm0
	vextracti128	$1, %ymm0, %xmm3
	vpmovsxdq	%xmm3, %ymm3
	vpmovsxdq	%xmm0, %ymm0
	vpandn	LCPI0_28(%rip), %ymm0, %ymm0
	vpmovsxdq	%xmm1, %ymm1
	vpandn	LCPI0_29(%rip), %ymm1, %ymm1
	vpor	%ymm0, %ymm1, %ymm0
	vpandn	LCPI0_30(%rip), %ymm3, %ymm1
	vpandn	LCPI0_31(%rip), %ymm2, %ymm2
	vpor	%ymm1, %ymm2, %ymm1
	vpor	%ymm1, %ymm0, %ymm0
	vextracti128	$1, %ymm0, %xmm1
	vpor	%xmm1, %xmm0, %xmm0
	vpshufd	$238, %xmm0, %xmm1              ## xmm1 = xmm0[2,3,2,3]
	vpor	%xmm1, %xmm0, %xmm0
	orq	%rax, %rdx
	vmovq	%xmm0, %rax
	orq	%rax, %rdx
	orq	%rcx, %rdx
	xorl	%eax, %eax
	tzcntq	%rdx, %rax
	cmpl	$19, %eax
	jbe	LBB0_163
## %bb.60:                              ## %no_errors_bb104
	movslq	112(%rsp), %rsi                 ## 4-byte Folded Reload
	movq	256(%rsp), %r14                 ## 8-byte Reload
	movslq	%r14d, %rax
	movq	%rsi, %r8
	movq	%rax, 120(%rsp)                 ## 8-byte Spill
	imulq	%rax, %r8
	movslq	480(%rsp), %r10                 ## 4-byte Folded Reload
	movq	%r8, %r12
	imulq	%r10, %r12
	movslq	128(%rsp), %r9                  ## 4-byte Folded Reload
	movslq	1216(%rsp), %rax                ## 4-byte Folded Reload
	movq	%r9, %r13
	movq	%rax, 1344(%rsp)                ## 8-byte Spill
	imulq	%rax, %r13
	movslq	104(%rsp), %r11                 ## 4-byte Folded Reload
	movq	%r13, %rax
	imulq	%r11, %rax
	movq	%rax, 32(%rsp)                  ## 8-byte Spill
	movslq	1128(%rsp), %rax                ## 4-byte Folded Reload
	movq	%rax, %rdi
	shlq	$5, %rdi
	movq	%rdi, %rcx
	negq	%rcx
	movq	%rax, 600(%rsp)                 ## 8-byte Spill
	testl	%eax, %eax
	movq	%rdi, %rax
	movq	%rdi, 216(%rsp)                 ## 8-byte Spill
	cmovnsq	%rdi, %rcx
	xorl	%eax, %eax
	movq	%rcx, 200(%rsp)                 ## 8-byte Spill
	cmpq	$2147483647, %rcx               ## imm = 0x7FFFFFFF
	seta	%al
	movq	%rax, 304(%rsp)                 ## 8-byte Spill
	leaq	(,%rbx,8), %rdi
	leaq	(%rdi,%rdi,4), %rdi
	movq	%rdi, %rax
	negq	%rax
	testl	%ebx, %ebx
	cmovnsq	%rdi, %rax
	xorl	%ecx, %ecx
	movq	%rax, 688(%rsp)                 ## 8-byte Spill
	cmpq	$2147483647, %rax               ## imm = 0x7FFFFFFF
	seta	%cl
	movl	%ecx, 144(%rsp)                 ## 4-byte Spill
	movq	336(%rsp), %rax                 ## 8-byte Reload
	leaq	(,%rax,8), %rdi
	subq	%rax, %rdi
	movq	%rdi, %rcx
	negq	%rcx
	testl	%eax, %eax
	cmovnsq	%rdi, %rcx
	xorl	%eax, %eax
	movq	%rcx, 672(%rsp)                 ## 8-byte Spill
	cmpq	$2147483647, %rcx               ## imm = 0x7FFFFFFF
	seta	%al
	movl	%eax, 160(%rsp)                 ## 4-byte Spill
	movslq	1120(%rsp), %rcx                ## 4-byte Folded Reload
	imulq	$39, %rcx, %rdx
	movq	%rdx, %rdi
	negq	%rdi
	movq	%rcx, %rax
	movq	%rcx, 376(%rsp)                 ## 8-byte Spill
	testl	%ecx, %ecx
	cmovnsq	%rdx, %rdi
	xorl	%eax, %eax
	movq	%rdi, 800(%rsp)                 ## 8-byte Spill
	cmpq	$2147483647, %rdi               ## imm = 0x7FFFFFFF
	seta	%al
	movl	%eax, 176(%rsp)                 ## 4-byte Spill
	shrl	$27, %r14d
	andl	$-16, %r14d
	movq	%r14, 256(%rsp)                 ## 8-byte Spill
	imulq	72(%rsp), %rsi                  ## 8-byte Folded Reload
	movq	%rsi, %rax
	negq	%rax
	cmovlq	%rsi, %rax
	xorl	%ecx, %ecx
	movq	%rax, 656(%rsp)                 ## 8-byte Spill
	cmpq	$2147483647, %rax               ## imm = 0x7FFFFFFF
	seta	%cl
	movl	%ecx, 736(%rsp)                 ## 4-byte Spill
	xorl	%eax, %eax
	movq	%r8, 128(%rsp)                  ## 8-byte Spill
	cmpq	$2147483647, %r8                ## imm = 0x7FFFFFFF
	setg	%al
	movl	%eax, 224(%rsp)                 ## 4-byte Spill
	imulq	2368(%rsp), %r10                ## 8-byte Folded Reload
	movq	%r10, %rax
	negq	%rax
	cmovlq	%r10, %rax
	xorl	%ecx, %ecx
	movq	%rax, 784(%rsp)                 ## 8-byte Spill
	cmpq	$2147483647, %rax               ## imm = 0x7FFFFFFF
	seta	%cl
	movl	%ecx, 64(%rsp)                  ## 4-byte Spill
	xorl	%eax, %eax
	movq	%r12, 640(%rsp)                 ## 8-byte Spill
	cmpq	$2147483647, %r12               ## imm = 0x7FFFFFFF
	setg	%al
	movl	%eax, 56(%rsp)                  ## 4-byte Spill
	movq	240(%rsp), %rax                 ## 8-byte Reload
	shrq	$22, %rax
	andl	$512, %eax                      ## imm = 0x200
	movq	%rax, 288(%rsp)                 ## 8-byte Spill
	movq	1216(%rsp), %rax                ## 8-byte Reload
	shrl	$21, %eax
	andl	$1024, %eax                     ## imm = 0x400
	movq	%rax, 1216(%rsp)                ## 8-byte Spill
	movslq	1272(%rsp), %rax                ## 4-byte Folded Reload
	movq	%rax, 1136(%rsp)                ## 8-byte Spill
	imulq	%rax, %r9
	movq	%r9, %rax
	negq	%rax
	cmovlq	%r9, %rax
	movq	%rax, 768(%rsp)                 ## 8-byte Spill
	cmpq	$2147483647, %rax               ## imm = 0x7FFFFFFF
	movl	$0, %eax
	seta	%al
	shlq	$11, %rax
	movq	%rax, 48(%rsp)                  ## 8-byte Spill
	xorl	%eax, %eax
	movq	%r13, 208(%rsp)                 ## 8-byte Spill
	cmpq	$2147483647, %r13               ## imm = 0x7FFFFFFF
	setg	%al
	shlq	$12, %rax
	movq	%rax, 464(%rsp)                 ## 8-byte Spill
	movslq	140(%rsp), %rax                 ## 4-byte Folded Reload
	movq	%rax, 1416(%rsp)                ## 8-byte Spill
	imulq	%rax, %r11
	movq	%r11, %r14
	negq	%r14
	cmovlq	%r11, %r14
	xorl	%eax, %eax
	cmpq	$2147483647, %r14               ## imm = 0x7FFFFFFF
	seta	%al
	shlq	$13, %rax
	movq	%rax, 448(%rsp)                 ## 8-byte Spill
	xorl	%eax, %eax
	cmpq	$2147483647, 32(%rsp)           ## 8-byte Folded Reload
                                        ## imm = 0x7FFFFFFF
	setg	%al
	shlq	$14, %rax
	movq	%rax, 432(%rsp)                 ## 8-byte Spill
	movq	912(%rsp), %r12                 ## 8-byte Reload
	andl	$2, %r12d
	shlq	$14, %r12
	movq	896(%rsp), %r11                 ## 8-byte Reload
	andl	$2, %r11d
	shlq	$15, %r11
	movq	880(%rsp), %r10                 ## 8-byte Reload
	andl	$2, %r10d
	shlq	$16, %r10
	movq	864(%rsp), %r8                  ## 8-byte Reload
	andl	$2, %r8d
	shlq	$17, %r8
	movq	848(%rsp), %rsi                 ## 8-byte Reload
	andl	$2, %esi
	shlq	$18, %rsi
	movq	832(%rsp), %rdx                 ## 8-byte Reload
	andl	$2, %edx
	shlq	$19, %rdx
	movq	1328(%rsp), %rcx                ## 8-byte Reload
	andl	$2, %ecx
	shlq	$20, %rcx
	movq	1696(%rsp), %rax                ## 8-byte Reload
	andl	$2, %eax
	shlq	$21, %rax
	movq	816(%rsp), %r13                 ## 8-byte Reload
	andl	$2, %r13d
	shlq	$22, %r13
	movq	544(%rsp), %rdi                 ## 8-byte Reload
	andl	$2, %edi
	shlq	$23, %rdi
	movq	%rdi, 544(%rsp)                 ## 8-byte Spill
	xorl	%edi, %edi
	cmpq	$0, 80(%rsp)                    ## 8-byte Folded Reload
	sete	%dil
	movl	%edi, 320(%rsp)                 ## 4-byte Spill
	xorl	%edi, %edi
	cmpq	$0, 728(%rsp)                   ## 8-byte Folded Reload
	sete	%dil
	movl	%edi, 400(%rsp)                 ## 4-byte Spill
	xorl	%edi, %edi
	testq	%r15, %r15
	sete	%dil
	movl	%edi, 384(%rsp)                 ## 4-byte Spill
	xorl	%edi, %edi
	cmpq	$0, 352(%rsp)                   ## 8-byte Folded Reload
	sete	%dil
	movl	%edi, 1152(%rsp)                ## 4-byte Spill
	xorl	%edi, %edi
	cmpq	$0, 1264(%rsp)                  ## 8-byte Folded Reload
	sete	%dil
	movl	%edi, 104(%rsp)                 ## 4-byte Spill
	xorl	%edi, %edi
	cmpq	$0, 1144(%rsp)                  ## 8-byte Folded Reload
	sete	%dil
	movl	%edi, 112(%rsp)                 ## 4-byte Spill
	xorl	%edi, %edi
	cmpq	$0, 1408(%rsp)                  ## 8-byte Folded Reload
	sete	%dil
	movl	%edi, 480(%rsp)                 ## 4-byte Spill
	xorl	%edi, %edi
	cmpq	$0, 2400(%rsp)                  ## 8-byte Folded Reload
	sete	%dil
	movl	%edi, 416(%rsp)                 ## 4-byte Spill
	xorl	%r9d, %r9d
	cmpq	$0, 1256(%rsp)                  ## 8-byte Folded Reload
	sete	%r9b
	xorl	%edi, %edi
	cmpq	$0, 712(%rsp)                   ## 8-byte Folded Reload
	sete	%dil
	shlq	$34, %rdi
	orq	464(%rsp), %rdi                 ## 8-byte Folded Reload
	orq	48(%rsp), %rdi                  ## 8-byte Folded Reload
	orq	432(%rsp), %rdi                 ## 8-byte Folded Reload
	orq	448(%rsp), %rdi                 ## 8-byte Folded Reload
	orq	%r12, %rdi
	orq	%r11, %rdi
	orq	304(%rsp), %rdi                 ## 8-byte Folded Reload
	orq	%r10, %rdi
	orq	%r8, %rdi
	orq	%rsi, %rdi
	orq	%rdx, %rdi
	orq	%rcx, %rdi
	orq	%rax, %rdi
	vmovd	736(%rsp), %xmm0                ## 4-byte Folded Reload
                                        ## xmm0 = mem[0],zero,zero,zero
	vpinsrb	$4, 56(%rsp), %xmm0, %xmm0      ## 4-byte Folded Reload
	vpinsrb	$8, %r9d, %xmm0, %xmm0
	orq	256(%rsp), %rdi                 ## 8-byte Folded Reload
	vpinsrb	$12, 64(%rsp), %xmm0, %xmm0     ## 4-byte Folded Reload
	vmovd	144(%rsp), %xmm1                ## 4-byte Folded Reload
                                        ## xmm1 = mem[0],zero,zero,zero
	vpinsrb	$4, 104(%rsp), %xmm1, %xmm1     ## 4-byte Folded Reload
	vpinsrb	$8, 160(%rsp), %xmm1, %xmm1     ## 4-byte Folded Reload
	orq	%r13, %rdi
	vpinsrb	$12, 112(%rsp), %xmm1, %xmm1    ## 4-byte Folded Reload
	vmovd	480(%rsp), %xmm2                ## 4-byte Folded Reload
                                        ## xmm2 = mem[0],zero,zero,zero
	vpinsrb	$4, 176(%rsp), %xmm2, %xmm2     ## 4-byte Folded Reload
	vpinsrb	$8, 416(%rsp), %xmm2, %xmm2     ## 4-byte Folded Reload
	orq	288(%rsp), %rdi                 ## 8-byte Folded Reload
	vpinsrb	$12, 224(%rsp), %xmm2, %xmm2    ## 4-byte Folded Reload
	vmovd	320(%rsp), %xmm3                ## 4-byte Folded Reload
                                        ## xmm3 = mem[0],zero,zero,zero
	vpinsrb	$4, 400(%rsp), %xmm3, %xmm3     ## 4-byte Folded Reload
	vpinsrb	$8, 384(%rsp), %xmm3, %xmm3     ## 4-byte Folded Reload
	orq	544(%rsp), %rdi                 ## 8-byte Folded Reload
	vpinsrb	$12, 1152(%rsp), %xmm3, %xmm3   ## 4-byte Folded Reload
	vpslld	$31, %xmm0, %xmm0
	vpsrad	$31, %xmm0, %xmm0
	vpmovsxdq	%xmm0, %ymm0
	vpand	LCPI0_32(%rip), %ymm0, %ymm0
	orq	1216(%rsp), %rdi                ## 8-byte Folded Reload
	vpslld	$31, %xmm1, %xmm1
	vpsrad	$31, %xmm1, %xmm1
	vpmovzxdq	%xmm1, %ymm1            ## ymm1 = xmm1[0],zero,xmm1[1],zero,xmm1[2],zero,xmm1[3],zero
	vpand	LCPI0_33(%rip), %ymm1, %ymm1
	vpor	%ymm0, %ymm1, %ymm0
	vpslld	$31, %xmm2, %xmm1
	vpsrad	$31, %xmm1, %xmm1
	vpmovsxdq	%xmm1, %ymm1
	vpslld	$31, %xmm3, %xmm2
	vpmovsxdq	%xmm2, %ymm2
	vmovapd	LCPI0_1(%rip), %xmm3            ## xmm3 = [9223372036854775808,0]
	vblendvpd	%ymm2, LCPI0_35(%rip), %ymm3, %ymm2
	vpand	LCPI0_34(%rip), %ymm1, %ymm1
	vorpd	%ymm0, %ymm2, %ymm0
	vpor	%ymm0, %ymm1, %ymm0
	vextracti128	$1, %ymm0, %xmm1
	vpor	%xmm1, %xmm0, %xmm0
	vpshufd	$238, %xmm0, %xmm1              ## xmm1 = xmm0[2,3,2,3]
	vpor	%xmm1, %xmm0, %xmm0
	vmovq	%xmm0, %rax
	orq	%rdi, %rax
	tzcntq	%rax, %rax
	cmpl	$34, %eax
	jbe	LBB0_165
## %bb.61:                              ## %"for squashed_head1_filter.s0.n.preheader"
	movq	336(%rsp), %r8                  ## 8-byte Reload
	shlq	$2, %r8
	shlq	$2, %rbx
	leaq	3168(%rsp), %rax
	xorl	%ecx, %ecx
	vbroadcastss	LCPI0_36(%rip), %ymm15  ## ymm15 = [-1.44269502E+0,-1.44269502E+0,-1.44269502E+0,-1.44269502E+0,-1.44269502E+0,-1.44269502E+0,-1.44269502E+0,-1.44269502E+0]
	vbroadcastss	LCPI0_37(%rip), %ymm12  ## ymm12 = [6.93145751E-1,6.93145751E-1,6.93145751E-1,6.93145751E-1,6.93145751E-1,6.93145751E-1,6.93145751E-1,6.93145751E-1]
	vbroadcastss	LCPI0_38(%rip), %ymm3   ## ymm3 = [1.42860677E-6,1.42860677E-6,1.42860677E-6,1.42860677E-6,1.42860677E-6,1.42860677E-6,1.42860677E-6,1.42860677E-6]
	vpbroadcastd	LCPI0_39(%rip), %ymm4   ## ymm4 = [4294967169,4294967169,4294967169,4294967169,4294967169,4294967169,4294967169,4294967169]
	vpbroadcastd	LCPI0_40(%rip), %ymm6   ## ymm6 = [128,128,128,128,128,128,128,128]
	vbroadcastss	LCPI0_41(%rip), %ymm14  ## ymm14 = [-3.19659332E-4,-3.19659332E-4,-3.19659332E-4,-3.19659332E-4,-3.19659332E-4,-3.19659332E-4,-3.19659332E-4,-3.19659332E-4]
	vbroadcastss	LCPI0_42(%rip), %ymm7   ## ymm7 = [-8.48988629E-3,-8.48988629E-3,-8.48988629E-3,-8.48988629E-3,-8.48988629E-3,-8.48988629E-3,-8.48988629E-3,-8.48988629E-3]
	vbroadcastss	LCPI0_43(%rip), %ymm9   ## ymm9 = [-1.66679844E-1,-1.66679844E-1,-1.66679844E-1,-1.66679844E-1,-1.66679844E-1,-1.66679844E-1,-1.66679844E-1,-1.66679844E-1]
	vbroadcastss	LCPI0_44(%rip), %ymm0   ## ymm0 = [-1.0E+0,-1.0E+0,-1.0E+0,-1.0E+0,-1.0E+0,-1.0E+0,-1.0E+0,-1.0E+0]
	vmovaps	%ymm0, 2592(%rsp)               ## 32-byte Spill
	vbroadcastss	LCPI0_45(%rip), %ymm0   ## ymm0 = [1.0E+0,1.0E+0,1.0E+0,1.0E+0,1.0E+0,1.0E+0,1.0E+0,1.0E+0]
	vmovaps	%ymm0, 2432(%rsp)               ## 32-byte Spill
	vbroadcastss	LCPI0_46(%rip), %ymm0   ## ymm0 = [1.19156833E-3,1.19156833E-3,1.19156833E-3,1.19156833E-3,1.19156833E-3,1.19156833E-3,1.19156833E-3,1.19156833E-3]
	vmovaps	%ymm0, 544(%rsp)                ## 32-byte Spill
	vbroadcastss	LCPI0_47(%rip), %ymm0   ## ymm0 = [4.16018814E-2,4.16018814E-2,4.16018814E-2,4.16018814E-2,4.16018814E-2,4.16018814E-2,4.16018814E-2,4.16018814E-2]
	vmovaps	%ymm0, 1216(%rsp)               ## 32-byte Spill
	vbroadcastss	LCPI0_48(%rip), %ymm0   ## ymm0 = [4.99998987E-1,4.99998987E-1,4.99998987E-1,4.99998987E-1,4.99998987E-1,4.99998987E-1,4.99998987E-1,4.99998987E-1]
	vmovaps	%ymm0, 256(%rsp)                ## 32-byte Spill
	vbroadcastss	LCPI0_45(%rip), %ymm0   ## ymm0 = [1.0E+0,1.0E+0,1.0E+0,1.0E+0,1.0E+0,1.0E+0,1.0E+0,1.0E+0]
	vmovaps	%ymm0, 2944(%rsp)               ## 32-byte Spill
	movq	192(%rsp), %r12                 ## 8-byte Reload
	movq	352(%rsp), %rdi                 ## 8-byte Reload
	vmovaps	2432(%rsp), %ymm1               ## 32-byte Reload
	vmovdqa	2944(%rsp), %ymm2               ## 32-byte Reload
	vmovaps	544(%rsp), %ymm0                ## 32-byte Reload
	.p2align	4, 0x90
LBB0_62:                                ## %"for squashed_head1_filter.s0.n"
                                        ## =>This Loop Header: Depth=1
                                        ##     Child Loop BB0_63 Depth 2
	movq	%rdi, %rdx
	xorl	%esi, %esi
	.p2align	4, 0x90
LBB0_63:                                ## %"for squashed_head1_filter.s0.s"
                                        ##   Parent Loop BB0_62 Depth=1
                                        ## =>  This Inner Loop Header: Depth=2
	vmovups	(%rdx), %ymm10
	vmulps	%ymm15, %ymm10, %ymm11
	vroundps	$1, %ymm11, %ymm11
	vfmadd231ps	%ymm12, %ymm11, %ymm10  ## ymm10 = (ymm11 * ymm12) + ymm10
	vfmadd231ps	%ymm11, %ymm3, %ymm10   ## ymm10 = (ymm3 * ymm11) + ymm10
	vmovaps	%ymm3, %ymm5
	vmovaps	%ymm12, %ymm3
	vmulps	%ymm10, %ymm10, %ymm12
	vmovaps	%ymm14, %ymm13
	vfmadd213ps	%ymm7, %ymm12, %ymm13   ## ymm13 = (ymm12 * ymm13) + ymm7
	vfmadd213ps	%ymm9, %ymm12, %ymm13   ## ymm13 = (ymm12 * ymm13) + ymm9
	vfmadd213ps	2592(%rsp), %ymm12, %ymm13 ## 32-byte Folded Reload
                                        ## ymm13 = (ymm12 * ymm13) + mem
	vmovaps	%ymm7, %ymm8
	vmovaps	%ymm14, %ymm7
	vmovaps	%ymm0, %ymm14
	vfmadd213ps	1216(%rsp), %ymm12, %ymm14 ## 32-byte Folded Reload
                                        ## ymm14 = (ymm12 * ymm14) + mem
	vfmadd213ps	256(%rsp), %ymm12, %ymm14 ## 32-byte Folded Reload
                                        ## ymm14 = (ymm12 * ymm14) + mem
	vfmadd213ps	%ymm1, %ymm12, %ymm14   ## ymm14 = (ymm12 * ymm14) + ymm1
	vfmadd231ps	%ymm13, %ymm10, %ymm14  ## ymm14 = (ymm10 * ymm13) + ymm14
	vcvttps2dq	%ymm11, %ymm10
	vpslld	$23, %ymm10, %ymm11
	vpaddd	%ymm2, %ymm11, %ymm11
	vfmadd213ps	%ymm1, %ymm14, %ymm11   ## ymm11 = (ymm14 * ymm11) + ymm1
	vmovaps	%ymm7, %ymm14
	vmovaps	%ymm8, %ymm7
	vdivps	%ymm11, %ymm1, %ymm11
	vpcmpgtd	%ymm10, %ymm6, %ymm12
	vpand	%ymm11, %ymm12, %ymm11
	vmovaps	%ymm3, %ymm12
	vmovaps	%ymm5, %ymm3
	vpcmpgtd	%ymm4, %ymm10, %ymm10
	vblendvps	%ymm10, %ymm11, %ymm1, %ymm10
	vmovaps	%ymm10, (%rax,%rsi)
	addq	$32, %rsi
	addq	%rbx, %rdx
	cmpq	$1280, %rsi                     ## imm = 0x500
	jne	LBB0_63
## %bb.64:                              ## %"end for squashed_head1_filter.s0.s"
                                        ##   in Loop: Header=BB0_62 Depth=1
	incq	%rcx
	addq	$1280, %rax                     ## imm = 0x500
	addq	%r8, %rdi
	cmpq	$7, %rcx
	jne	LBB0_62
## %bb.65:                              ## %"end for squashed_head1_filter.s0.n"
	vmovdqa	%ymm4, 3040(%rsp)               ## 32-byte Spill
	leal	-1(%r12), %edx
	movl	%edx, %eax
	sarl	$2, %eax
	movl	$-1, %ecx
	cmovnsl	%eax, %ecx
	leal	4(,%rcx,4), %eax
	cmpl	$16777216, %eax                 ## imm = 0x1000000
	jae	LBB0_167
## %bb.66:                              ## %"assert succeeded162"
	movl	%edx, 608(%rsp)                 ## 4-byte Spill
	movl	%eax, %esi
	shlq	$7, %rsi
	orq	$12, %rsi
	xorl	%edi, %edi
	vzeroupper
	callq	_halide_malloc
	testq	%rax, %rax
	je	LBB0_168
## %bb.67:                              ## %"assert succeeded164"
	movq	%rax, %rsi
	movl	%r12d, %ebx
	testl	%r12d, %r12d
	movq	%rsi, 88(%rsp)                  ## 8-byte Spill
	movq	%rbx, 512(%rsp)                 ## 8-byte Spill
	jle	LBB0_81
## %bb.68:                              ## %"for conv1_stage1.s0.w.preheader"
	movq	80(%rsp), %rax                  ## 8-byte Reload
	vmovups	(%rax), %ymm0
	vmovups	32(%rax), %ymm1
	vmovups	64(%rax), %ymm2
	vmovups	96(%rax), %ymm3
	leaq	-1(%rbx), %rcx
	movl	%ebx, %eax
	andl	$7, %eax
	cmpq	$7, %rcx
	jae	LBB0_70
## %bb.69:
	xorl	%ecx, %ecx
	jmp	LBB0_72
LBB0_70:                                ## %"for conv1_stage1.s0.w.preheader.new"
	movl	%ebx, %edx
	andl	$-8, %edx
	addq	$992, %rsi                      ## imm = 0x3E0
	xorl	%ecx, %ecx
	.p2align	4, 0x90
LBB0_71:                                ## %"for conv1_stage1.s0.w"
                                        ## =>This Inner Loop Header: Depth=1
	vmovaps	%ymm0, -992(%rsi)
	vmovaps	%ymm1, -960(%rsi)
	vmovaps	%ymm2, -928(%rsi)
	vmovaps	%ymm3, -896(%rsi)
	vmovaps	%ymm0, -864(%rsi)
	vmovaps	%ymm1, -832(%rsi)
	vmovaps	%ymm2, -800(%rsi)
	vmovaps	%ymm3, -768(%rsi)
	vmovaps	%ymm0, -736(%rsi)
	vmovaps	%ymm1, -704(%rsi)
	vmovaps	%ymm2, -672(%rsi)
	vmovaps	%ymm3, -640(%rsi)
	vmovaps	%ymm0, -608(%rsi)
	vmovaps	%ymm1, -576(%rsi)
	vmovaps	%ymm2, -544(%rsi)
	vmovaps	%ymm3, -512(%rsi)
	vmovaps	%ymm0, -480(%rsi)
	vmovaps	%ymm1, -448(%rsi)
	vmovaps	%ymm2, -416(%rsi)
	vmovaps	%ymm3, -384(%rsi)
	vmovaps	%ymm0, -352(%rsi)
	vmovaps	%ymm1, -320(%rsi)
	vmovaps	%ymm2, -288(%rsi)
	vmovaps	%ymm3, -256(%rsi)
	vmovaps	%ymm0, -224(%rsi)
	vmovaps	%ymm1, -192(%rsi)
	vmovaps	%ymm2, -160(%rsi)
	vmovaps	%ymm3, -128(%rsi)
	vmovaps	%ymm0, -96(%rsi)
	vmovaps	%ymm1, -64(%rsi)
	vmovaps	%ymm2, -32(%rsi)
	vmovaps	%ymm3, (%rsi)
	addq	$8, %rcx
	addq	$1024, %rsi                     ## imm = 0x400
	cmpq	%rcx, %rdx
	jne	LBB0_71
LBB0_72:                                ## %"for conv1_stage1.s1.w.preheader.unr-lcssa"
	testq	%rax, %rax
	movq	600(%rsp), %rsi                 ## 8-byte Reload
	je	LBB0_75
## %bb.73:                              ## %"for conv1_stage1.s0.w.epil.preheader"
	shlq	$7, %rcx
	movq	88(%rsp), %rdx                  ## 8-byte Reload
	addq	%rdx, %rcx
	addq	$96, %rcx
	shlq	$7, %rax
	xorl	%edx, %edx
	.p2align	4, 0x90
LBB0_74:                                ## %"for conv1_stage1.s0.w.epil"
                                        ## =>This Inner Loop Header: Depth=1
	vmovaps	%ymm0, -96(%rcx,%rdx)
	vmovaps	%ymm1, -64(%rcx,%rdx)
	vmovaps	%ymm2, -32(%rcx,%rdx)
	vmovaps	%ymm3, (%rcx,%rdx)
	subq	$-128, %rdx
	cmpq	%rdx, %rax
	jne	LBB0_74
LBB0_75:                                ## %"for conv1_stage1.s1.w.preheader"
	movq	40(%rsp), %rax                  ## 8-byte Reload
	movl	%eax, %edx
	movq	72(%rsp), %rcx                  ## 8-byte Reload
	imull	%ecx, %edx
	movl	%edx, 1920(%rsp)                ## 4-byte Spill
	movl	$1, %edx
	subl	%eax, %edx
	imull	%ecx, %edx
	movq	%rdx, 2496(%rsp)                ## 8-byte Spill
	movl	$2, %edx
	subl	%eax, %edx
	imull	%ecx, %edx
	movq	%rdx, 2016(%rsp)                ## 8-byte Spill
	movl	$3, %edx
	subl	%eax, %edx
	imull	%ecx, %edx
	movq	%rdx, 1984(%rsp)                ## 8-byte Spill
	movl	$4, %edx
	subl	%eax, %edx
	imull	%ecx, %edx
	movq	%rdx, 1952(%rsp)                ## 8-byte Spill
	movl	$5, %edx
	subl	%eax, %edx
	imull	%ecx, %edx
	movq	%rdx, 2464(%rsp)                ## 8-byte Spill
	movl	$6, %edx
	subl	%eax, %edx
	imull	%ecx, %edx
	movl	%edx, 1536(%rsp)                ## 4-byte Spill
	leaq	(,%rsi,4), %rdx
	xorl	%ecx, %ecx
	vxorps	%xmm13, %xmm13, %xmm13
	.p2align	4, 0x90
LBB0_76:                                ## %"for conv1_stage1.s1.w"
                                        ## =>This Loop Header: Depth=1
                                        ##     Child Loop BB0_77 Depth 2
                                        ##       Child Loop BB0_78 Depth 3
	movq	%rcx, %r9
	shlq	$5, %r9
	movq	%rcx, %r14
                                        ## kill: def $ecx killed $ecx killed $rcx def $rcx
	subl	2528(%rsp), %ecx                ## 4-byte Folded Reload
	imull	2368(%rsp), %ecx                ## 4-byte Folded Reload
	subl	2560(%rsp), %ecx                ## 4-byte Folded Reload
	movl	%ecx, %eax
	subl	1920(%rsp), %eax                ## 4-byte Folded Reload
	cltq
	movq	2496(%rsp), %rsi                ## 8-byte Reload
	addl	%ecx, %esi
	movslq	%esi, %rdi
	movq	2016(%rsp), %rsi                ## 8-byte Reload
	leal	(%rcx,%rsi), %esi
	movslq	%esi, %rsi
	movq	1984(%rsp), %rbx                ## 8-byte Reload
	addl	%ecx, %ebx
	movslq	%ebx, %r12
	movq	1952(%rsp), %rbx                ## 8-byte Reload
	addl	%ecx, %ebx
	movslq	%ebx, %r13
	movq	2464(%rsp), %rbx                ## 8-byte Reload
	addl	%ecx, %ebx
	movslq	%ebx, %r10
	addl	1536(%rsp), %ecx                ## 4-byte Folded Reload
	movslq	%ecx, %r8
	movq	2400(%rsp), %rcx                ## 8-byte Reload
	vmovups	(%rcx,%rax,4), %xmm5
	vmovups	16(%rcx,%rax,4), %xmm6
	vmovups	32(%rcx,%rax,4), %xmm1
	vmovaps	%xmm1, 1216(%rsp)               ## 16-byte Spill
	vmovups	48(%rcx,%rax,4), %xmm1
	vmovaps	%xmm1, 256(%rsp)                ## 16-byte Spill
	vmovups	64(%rcx,%rax,4), %xmm7
	vmovups	80(%rcx,%rax,4), %xmm1
	vmovups	96(%rcx,%rax,4), %xmm0
	vmovups	112(%rcx,%rax,4), %xmm10
	vmovups	128(%rcx,%rax,4), %xmm2
	vmovaps	%xmm2, 544(%rsp)                ## 16-byte Spill
	vmovss	144(%rcx,%rax,4), %xmm2         ## xmm2 = mem[0],zero,zero,zero
	vmovss	%xmm2, 32(%rsp)                 ## 4-byte Spill
	vmovss	148(%rcx,%rax,4), %xmm2         ## xmm2 = mem[0],zero,zero,zero
	vmovss	%xmm2, 40(%rsp)                 ## 4-byte Spill
	vmovss	152(%rcx,%rax,4), %xmm2         ## xmm2 = mem[0],zero,zero,zero
	vmovss	%xmm2, 80(%rsp)                 ## 4-byte Spill
	vmovss	156(%rcx,%rax,4), %xmm2         ## xmm2 = mem[0],zero,zero,zero
	vmovss	%xmm2, 72(%rsp)                 ## 4-byte Spill
	movq	%r9, %rax
	vmovups	(%rcx,%rdi,4), %xmm2
	vmovaps	%xmm2, 352(%rsp)                ## 16-byte Spill
	vmovups	16(%rcx,%rdi,4), %xmm2
	vmovaps	%xmm2, 336(%rsp)                ## 16-byte Spill
	vmovups	32(%rcx,%rdi,4), %xmm2
	vmovaps	%xmm2, 240(%rsp)                ## 16-byte Spill
	movq	728(%rsp), %r11                 ## 8-byte Reload
	xorl	%r9d, %r9d
	vmovups	48(%rcx,%rdi,4), %xmm2
	vmovaps	%xmm2, 144(%rsp)                ## 16-byte Spill
	vmovups	64(%rcx,%rdi,4), %xmm2
	vmovaps	%xmm2, 176(%rsp)                ## 16-byte Spill
	vmovups	80(%rcx,%rdi,4), %xmm2
	vmovaps	%xmm2, 160(%rsp)                ## 16-byte Spill
	vmovups	96(%rcx,%rdi,4), %xmm2
	vmovaps	%xmm2, 224(%rsp)                ## 16-byte Spill
	vmovups	112(%rcx,%rdi,4), %xmm2
	vmovaps	%xmm2, 304(%rsp)                ## 16-byte Spill
	vmovups	128(%rcx,%rdi,4), %xmm2
	vmovaps	%xmm2, 288(%rsp)                ## 16-byte Spill
	vmovss	144(%rcx,%rdi,4), %xmm2         ## xmm2 = mem[0],zero,zero,zero
	vmovss	%xmm2, 736(%rsp)                ## 4-byte Spill
	vmovss	148(%rcx,%rdi,4), %xmm2         ## xmm2 = mem[0],zero,zero,zero
	vmovss	%xmm2, 64(%rsp)                 ## 4-byte Spill
	vmovss	152(%rcx,%rdi,4), %xmm2         ## xmm2 = mem[0],zero,zero,zero
	vmovss	%xmm2, 56(%rsp)                 ## 4-byte Spill
	vmovss	156(%rcx,%rdi,4), %xmm2         ## xmm2 = mem[0],zero,zero,zero
	vmovss	%xmm2, 48(%rsp)                 ## 4-byte Spill
	vmovups	(%rcx,%rsi,4), %xmm2
	vmovaps	%xmm2, 320(%rsp)                ## 16-byte Spill
	vmovups	16(%rcx,%rsi,4), %xmm2
	vmovaps	%xmm2, 400(%rsp)                ## 16-byte Spill
	vmovups	32(%rcx,%rsi,4), %xmm2
	vmovaps	%xmm2, 384(%rsp)                ## 16-byte Spill
	vmovups	48(%rcx,%rsi,4), %xmm2
	vmovaps	%xmm2, 1152(%rsp)               ## 16-byte Spill
	vmovups	64(%rcx,%rsi,4), %xmm2
	vmovaps	%xmm2, 464(%rsp)                ## 16-byte Spill
	vmovups	80(%rcx,%rsi,4), %xmm2
	vmovaps	%xmm2, 448(%rsp)                ## 16-byte Spill
	vmovups	96(%rcx,%rsi,4), %xmm2
	vmovaps	%xmm2, 432(%rsp)                ## 16-byte Spill
	vmovups	112(%rcx,%rsi,4), %xmm2
	vmovaps	%xmm2, 416(%rsp)                ## 16-byte Spill
	vmovups	128(%rcx,%rsi,4), %xmm2
	vmovaps	%xmm2, 480(%rsp)                ## 16-byte Spill
	vmovss	144(%rcx,%rsi,4), %xmm2         ## xmm2 = mem[0],zero,zero,zero
	vmovss	%xmm2, 112(%rsp)                ## 4-byte Spill
	vmovss	148(%rcx,%rsi,4), %xmm2         ## xmm2 = mem[0],zero,zero,zero
	vmovss	%xmm2, 104(%rsp)                ## 4-byte Spill
	vmovss	152(%rcx,%rsi,4), %xmm2         ## xmm2 = mem[0],zero,zero,zero
	vmovss	%xmm2, 128(%rsp)                ## 4-byte Spill
	vmovss	156(%rcx,%rsi,4), %xmm2         ## xmm2 = mem[0],zero,zero,zero
	vmovss	%xmm2, 120(%rsp)                ## 4-byte Spill
	movq	88(%rsp), %rsi                  ## 8-byte Reload
	vmovups	(%rcx,%r12,4), %xmm2
	vmovaps	%xmm2, 640(%rsp)                ## 16-byte Spill
	vmovups	16(%rcx,%r12,4), %xmm2
	vmovaps	%xmm2, 912(%rsp)                ## 16-byte Spill
	vmovups	32(%rcx,%r12,4), %xmm2
	vmovaps	%xmm2, 896(%rsp)                ## 16-byte Spill
	vmovups	48(%rcx,%r12,4), %xmm2
	vmovaps	%xmm2, 880(%rsp)                ## 16-byte Spill
	vmovups	64(%rcx,%r12,4), %xmm2
	vmovaps	%xmm2, 864(%rsp)                ## 16-byte Spill
	vmovups	80(%rcx,%r12,4), %xmm2
	vmovaps	%xmm2, 848(%rsp)                ## 16-byte Spill
	vmovups	96(%rcx,%r12,4), %xmm2
	vmovaps	%xmm2, 832(%rsp)                ## 16-byte Spill
	vmovups	112(%rcx,%r12,4), %xmm2
	vmovaps	%xmm2, 816(%rsp)                ## 16-byte Spill
	vmovups	128(%rcx,%r12,4), %xmm2
	vmovaps	%xmm2, 1328(%rsp)               ## 16-byte Spill
	vmovss	144(%rcx,%r12,4), %xmm2         ## xmm2 = mem[0],zero,zero,zero
	vmovss	%xmm2, 1696(%rsp)               ## 4-byte Spill
	vmovss	148(%rcx,%r12,4), %xmm2         ## xmm2 = mem[0],zero,zero,zero
	vmovss	%xmm2, 1344(%rsp)               ## 4-byte Spill
	vmovss	152(%rcx,%r12,4), %xmm2         ## xmm2 = mem[0],zero,zero,zero
	vmovss	%xmm2, 208(%rsp)                ## 4-byte Spill
	vmovss	156(%rcx,%r12,4), %xmm2         ## xmm2 = mem[0],zero,zero,zero
	vmovss	%xmm2, 200(%rsp)                ## 4-byte Spill
	vmovups	(%rcx,%r13,4), %xmm2
	vmovaps	%xmm2, 688(%rsp)                ## 16-byte Spill
	vmovups	16(%rcx,%r13,4), %xmm2
	vmovaps	%xmm2, 672(%rsp)                ## 16-byte Spill
	vmovups	32(%rcx,%r13,4), %xmm2
	vmovaps	%xmm2, 656(%rsp)                ## 16-byte Spill
	vmovups	48(%rcx,%r13,4), %xmm2
	vmovaps	%xmm2, 800(%rsp)                ## 16-byte Spill
	vmovups	64(%rcx,%r13,4), %xmm2
	vmovaps	%xmm2, 784(%rsp)                ## 16-byte Spill
	vmovups	80(%rcx,%r13,4), %xmm2
	vmovaps	%xmm2, 768(%rsp)                ## 16-byte Spill
	vmovups	96(%rcx,%r13,4), %xmm2
	vmovaps	%xmm2, 1200(%rsp)               ## 16-byte Spill
	vmovups	112(%rcx,%r13,4), %xmm2
	vmovaps	%xmm2, 2208(%rsp)               ## 16-byte Spill
	vmovups	128(%rcx,%r13,4), %xmm2
	vmovaps	%xmm2, 1664(%rsp)               ## 16-byte Spill
	vmovss	144(%rcx,%r13,4), %xmm2         ## xmm2 = mem[0],zero,zero,zero
	vmovss	%xmm2, 1632(%rsp)               ## 4-byte Spill
	vmovss	148(%rcx,%r13,4), %xmm2         ## xmm2 = mem[0],zero,zero,zero
	vmovss	%xmm2, 2176(%rsp)               ## 4-byte Spill
	vmovss	152(%rcx,%r13,4), %xmm2         ## xmm2 = mem[0],zero,zero,zero
	vmovss	%xmm2, 2144(%rsp)               ## 4-byte Spill
	vmovss	156(%rcx,%r13,4), %xmm2         ## xmm2 = mem[0],zero,zero,zero
	vmovss	%xmm2, 2112(%rsp)               ## 4-byte Spill
	vmovups	(%rcx,%r10,4), %xmm2
	vmovaps	%xmm2, 2080(%rsp)               ## 16-byte Spill
	vmovups	16(%rcx,%r10,4), %xmm2
	vmovaps	%xmm2, 2048(%rsp)               ## 16-byte Spill
	vmovups	32(%rcx,%r10,4), %xmm2
	vmovaps	%xmm2, 1600(%rsp)               ## 16-byte Spill
	vmovups	48(%rcx,%r10,4), %xmm2
	vmovaps	%xmm2, 1568(%rsp)               ## 16-byte Spill
	vmovups	64(%rcx,%r10,4), %xmm2
	vmovaps	%xmm2, 1184(%rsp)               ## 16-byte Spill
	vmovups	80(%rcx,%r10,4), %xmm2
	vmovaps	%xmm2, 1824(%rsp)               ## 16-byte Spill
	vmovups	96(%rcx,%r10,4), %xmm2
	vmovaps	%xmm2, 1792(%rsp)               ## 16-byte Spill
	vmovups	112(%rcx,%r10,4), %xmm2
	vmovaps	%xmm2, 1760(%rsp)               ## 16-byte Spill
	vmovups	128(%rcx,%r10,4), %xmm2
	vmovaps	%xmm2, 1728(%rsp)               ## 16-byte Spill
	vmovss	144(%rcx,%r10,4), %xmm2         ## xmm2 = mem[0],zero,zero,zero
	vmovss	%xmm2, 536(%rsp)                ## 4-byte Spill
	vmovss	148(%rcx,%r10,4), %xmm2         ## xmm2 = mem[0],zero,zero,zero
	vmovss	%xmm2, 528(%rsp)                ## 4-byte Spill
	vmovss	152(%rcx,%r10,4), %xmm2         ## xmm2 = mem[0],zero,zero,zero
	vmovss	%xmm2, 520(%rsp)                ## 4-byte Spill
	vmovss	156(%rcx,%r10,4), %xmm2         ## xmm2 = mem[0],zero,zero,zero
	vmovss	%xmm2, 632(%rsp)                ## 4-byte Spill
	vmovups	(%rcx,%r8,4), %xmm2
	vmovaps	%xmm2, 1312(%rsp)               ## 16-byte Spill
	vmovups	16(%rcx,%r8,4), %xmm2
	vmovaps	%xmm2, 1296(%rsp)               ## 16-byte Spill
	vmovups	32(%rcx,%r8,4), %xmm2
	vmovaps	%xmm2, 1376(%rsp)               ## 16-byte Spill
	vmovups	48(%rcx,%r8,4), %xmm2
	vmovaps	%xmm2, 2752(%rsp)               ## 16-byte Spill
	vmovups	64(%rcx,%r8,4), %xmm2
	vmovaps	%xmm2, 2720(%rsp)               ## 16-byte Spill
	vmovups	80(%rcx,%r8,4), %xmm2
	vmovaps	%xmm2, 2688(%rsp)               ## 16-byte Spill
	vmovups	96(%rcx,%r8,4), %xmm2
	vmovaps	%xmm2, 2656(%rsp)               ## 16-byte Spill
	vmovups	112(%rcx,%r8,4), %xmm2
	vmovaps	%xmm2, 2624(%rsp)               ## 16-byte Spill
	vmovups	128(%rcx,%r8,4), %xmm12
	vmovss	144(%rcx,%r8,4), %xmm8          ## xmm8 = mem[0],zero,zero,zero
	vmovss	148(%rcx,%r8,4), %xmm14         ## xmm14 = mem[0],zero,zero,zero
	vmovss	152(%rcx,%r8,4), %xmm15         ## xmm15 = mem[0],zero,zero,zero
	vmovss	156(%rcx,%r8,4), %xmm11         ## xmm11 = mem[0],zero,zero,zero
	.p2align	4, 0x90
LBB0_77:                                ## %"for conv1_stage1.s1.c"
                                        ##   Parent Loop BB0_76 Depth=1
                                        ## =>  This Loop Header: Depth=2
                                        ##       Child Loop BB0_78 Depth 3
	leaq	(%r9,%rax), %r10
	vmovss	(%rsi,%r10,4), %xmm9            ## xmm9 = mem[0],zero,zero,zero
	movq	%r11, %r8
	xorl	%edi, %edi
	.p2align	4, 0x90
LBB0_78:                                ## %"for conv1_stage1.s1.r54$x"
                                        ##   Parent Loop BB0_76 Depth=1
                                        ##     Parent Loop BB0_77 Depth=2
                                        ## =>    This Inner Loop Header: Depth=3
	vmovaps	%xmm10, %xmm3
	vmovups	3264(%rsp,%rdi,4), %xmm10
	vunpcklpd	3232(%rsp,%rdi,4), %xmm10, %xmm2 ## xmm2 = xmm10[0],mem[0]
	vmovaps	%xmm3, %xmm10
	vmovups	3168(%rsp,%rdi,4), %xmm3
	vunpcklps	3200(%rsp,%rdi,4), %xmm3, %xmm3 ## xmm3 = xmm3[0],mem[0],xmm3[1],mem[1]
	vmovss	(%r15,%rdi,4), %xmm4            ## xmm4 = mem[0],zero,zero,zero
	vshufps	$36, %xmm2, %xmm3, %xmm2        ## xmm2 = xmm3[0,1],xmm2[2,0]
	vmovups	3392(%rsp,%rdi,4), %xmm3
	vunpcklpd	3360(%rsp,%rdi,4), %xmm3, %xmm3 ## xmm3 = xmm3[0],mem[0]
	vfmadd213ps	%xmm4, %xmm5, %xmm2     ## xmm2 = (xmm5 * xmm2) + xmm4
	vmovups	3296(%rsp,%rdi,4), %xmm4
	vunpcklps	3328(%rsp,%rdi,4), %xmm4, %xmm4 ## xmm4 = xmm4[0],mem[0],xmm4[1],mem[1]
	vshufps	$36, %xmm3, %xmm4, %xmm3        ## xmm3 = xmm4[0,1],xmm3[2,0]
	vmovups	3520(%rsp,%rdi,4), %xmm4
	vunpcklpd	3488(%rsp,%rdi,4), %xmm4, %xmm4 ## xmm4 = xmm4[0],mem[0]
	vfmadd213ps	%xmm2, %xmm6, %xmm3     ## xmm3 = (xmm6 * xmm3) + xmm2
	vmovups	3424(%rsp,%rdi,4), %xmm2
	vunpcklps	3456(%rsp,%rdi,4), %xmm2, %xmm2 ## xmm2 = xmm2[0],mem[0],xmm2[1],mem[1]
	vshufps	$36, %xmm4, %xmm2, %xmm2        ## xmm2 = xmm2[0,1],xmm4[2,0]
	vmovups	3648(%rsp,%rdi,4), %xmm4
	vunpcklpd	3616(%rsp,%rdi,4), %xmm4, %xmm4 ## xmm4 = xmm4[0],mem[0]
	vfmadd132ps	1216(%rsp), %xmm3, %xmm2 ## 16-byte Folded Reload
                                        ## xmm2 = (xmm2 * mem) + xmm3
	vmovups	3552(%rsp,%rdi,4), %xmm3
	vunpcklps	3584(%rsp,%rdi,4), %xmm3, %xmm3 ## xmm3 = xmm3[0],mem[0],xmm3[1],mem[1]
	vshufps	$36, %xmm4, %xmm3, %xmm3        ## xmm3 = xmm3[0,1],xmm4[2,0]
	vmovups	3776(%rsp,%rdi,4), %xmm4
	vunpcklpd	3744(%rsp,%rdi,4), %xmm4, %xmm4 ## xmm4 = xmm4[0],mem[0]
	vfmadd132ps	256(%rsp), %xmm2, %xmm3 ## 16-byte Folded Reload
                                        ## xmm3 = (xmm3 * mem) + xmm2
	vmovups	3680(%rsp,%rdi,4), %xmm2
	vunpcklps	3712(%rsp,%rdi,4), %xmm2, %xmm2 ## xmm2 = xmm2[0],mem[0],xmm2[1],mem[1]
	vshufps	$36, %xmm4, %xmm2, %xmm2        ## xmm2 = xmm2[0,1],xmm4[2,0]
	vmovups	3904(%rsp,%rdi,4), %xmm4
	vunpcklpd	3872(%rsp,%rdi,4), %xmm4, %xmm4 ## xmm4 = xmm4[0],mem[0]
	vfmadd213ps	%xmm3, %xmm7, %xmm2     ## xmm2 = (xmm7 * xmm2) + xmm3
	vmovups	3808(%rsp,%rdi,4), %xmm3
	vunpcklps	3840(%rsp,%rdi,4), %xmm3, %xmm3 ## xmm3 = xmm3[0],mem[0],xmm3[1],mem[1]
	vshufps	$36, %xmm4, %xmm3, %xmm3        ## xmm3 = xmm3[0,1],xmm4[2,0]
	vmovups	4032(%rsp,%rdi,4), %xmm4
	vunpcklpd	4000(%rsp,%rdi,4), %xmm4, %xmm4 ## xmm4 = xmm4[0],mem[0]
	vfmadd213ps	%xmm2, %xmm1, %xmm3     ## xmm3 = (xmm1 * xmm3) + xmm2
	vmovups	3936(%rsp,%rdi,4), %xmm2
	vunpcklps	3968(%rsp,%rdi,4), %xmm2, %xmm2 ## xmm2 = xmm2[0],mem[0],xmm2[1],mem[1]
	vshufps	$36, %xmm4, %xmm2, %xmm2        ## xmm2 = xmm2[0,1],xmm4[2,0]
	vmovups	4160(%rsp,%rdi,4), %xmm4
	vunpcklpd	4128(%rsp,%rdi,4), %xmm4, %xmm4 ## xmm4 = xmm4[0],mem[0]
	vfmadd213ps	%xmm3, %xmm0, %xmm2     ## xmm2 = (xmm0 * xmm2) + xmm3
	vmovups	4064(%rsp,%rdi,4), %xmm3
	vunpcklps	4096(%rsp,%rdi,4), %xmm3, %xmm3 ## xmm3 = xmm3[0],mem[0],xmm3[1],mem[1]
	vshufps	$36, %xmm4, %xmm3, %xmm3        ## xmm3 = xmm3[0,1],xmm4[2,0]
	vmovups	4288(%rsp,%rdi,4), %xmm4
	vunpcklpd	4256(%rsp,%rdi,4), %xmm4, %xmm4 ## xmm4 = xmm4[0],mem[0]
	vfmadd213ps	%xmm2, %xmm10, %xmm3    ## xmm3 = (xmm10 * xmm3) + xmm2
	vmovups	4192(%rsp,%rdi,4), %xmm2
	vunpcklps	4224(%rsp,%rdi,4), %xmm2, %xmm2 ## xmm2 = xmm2[0],mem[0],xmm2[1],mem[1]
	vshufps	$36, %xmm4, %xmm2, %xmm2        ## xmm2 = xmm2[0,1],xmm4[2,0]
	vfmadd132ps	544(%rsp), %xmm3, %xmm2 ## 16-byte Folded Reload
                                        ## xmm2 = (xmm2 * mem) + xmm3
	vpermilpd	$1, %xmm2, %xmm3        ## xmm3 = xmm2[1,0]
	vaddps	%xmm3, %xmm2, %xmm2
	vmovshdup	%xmm2, %xmm3            ## xmm3 = xmm2[1,1,3,3]
	vaddss	%xmm3, %xmm2, %xmm2
	vmovss	32(%rsp), %xmm3                 ## 4-byte Reload
                                        ## xmm3 = mem[0],zero,zero,zero
	vfmadd231ss	4320(%rsp,%rdi,4), %xmm3, %xmm2 ## xmm2 = (xmm3 * mem) + xmm2
	vmovss	40(%rsp), %xmm3                 ## 4-byte Reload
                                        ## xmm3 = mem[0],zero,zero,zero
	vfmadd231ss	4352(%rsp,%rdi,4), %xmm3, %xmm2 ## xmm2 = (xmm3 * mem) + xmm2
	vmovss	80(%rsp), %xmm3                 ## 4-byte Reload
                                        ## xmm3 = mem[0],zero,zero,zero
	vfmadd231ss	4384(%rsp,%rdi,4), %xmm3, %xmm2 ## xmm2 = (xmm3 * mem) + xmm2
	vmovss	72(%rsp), %xmm3                 ## 4-byte Reload
                                        ## xmm3 = mem[0],zero,zero,zero
	vfmadd231ss	4416(%rsp,%rdi,4), %xmm3, %xmm2 ## xmm2 = (xmm3 * mem) + xmm2
	vmovups	4544(%rsp,%rdi,4), %xmm3
	vunpcklpd	4512(%rsp,%rdi,4), %xmm3, %xmm3 ## xmm3 = xmm3[0],mem[0]
	vmovups	4448(%rsp,%rdi,4), %xmm4
	vblendps	$1, %xmm2, %xmm13, %xmm2        ## xmm2 = xmm2[0],xmm13[1,2,3]
	vunpcklps	4480(%rsp,%rdi,4), %xmm4, %xmm4 ## xmm4 = xmm4[0],mem[0],xmm4[1],mem[1]
	vshufps	$36, %xmm3, %xmm4, %xmm3        ## xmm3 = xmm4[0,1],xmm3[2,0]
	vfmadd132ps	352(%rsp), %xmm2, %xmm3 ## 16-byte Folded Reload
                                        ## xmm3 = (xmm3 * mem) + xmm2
	vmovups	4672(%rsp,%rdi,4), %xmm2
	vunpcklpd	4640(%rsp,%rdi,4), %xmm2, %xmm2 ## xmm2 = xmm2[0],mem[0]
	vmovups	4576(%rsp,%rdi,4), %xmm4
	vunpcklps	4608(%rsp,%rdi,4), %xmm4, %xmm4 ## xmm4 = xmm4[0],mem[0],xmm4[1],mem[1]
	vshufps	$36, %xmm2, %xmm4, %xmm2        ## xmm2 = xmm4[0,1],xmm2[2,0]
	vfmadd132ps	336(%rsp), %xmm3, %xmm2 ## 16-byte Folded Reload
                                        ## xmm2 = (xmm2 * mem) + xmm3
	vmovups	4800(%rsp,%rdi,4), %xmm3
	vunpcklpd	4768(%rsp,%rdi,4), %xmm3, %xmm3 ## xmm3 = xmm3[0],mem[0]
	vmovups	4704(%rsp,%rdi,4), %xmm4
	vunpcklps	4736(%rsp,%rdi,4), %xmm4, %xmm4 ## xmm4 = xmm4[0],mem[0],xmm4[1],mem[1]
	vshufps	$36, %xmm3, %xmm4, %xmm3        ## xmm3 = xmm4[0,1],xmm3[2,0]
	vfmadd132ps	240(%rsp), %xmm2, %xmm3 ## 16-byte Folded Reload
                                        ## xmm3 = (xmm3 * mem) + xmm2
	vmovups	4928(%rsp,%rdi,4), %xmm2
	vunpcklpd	4896(%rsp,%rdi,4), %xmm2, %xmm2 ## xmm2 = xmm2[0],mem[0]
	vmovups	4832(%rsp,%rdi,4), %xmm4
	vunpcklps	4864(%rsp,%rdi,4), %xmm4, %xmm4 ## xmm4 = xmm4[0],mem[0],xmm4[1],mem[1]
	vshufps	$36, %xmm2, %xmm4, %xmm2        ## xmm2 = xmm4[0,1],xmm2[2,0]
	vfmadd132ps	144(%rsp), %xmm3, %xmm2 ## 16-byte Folded Reload
                                        ## xmm2 = (xmm2 * mem) + xmm3
	vmovups	5056(%rsp,%rdi,4), %xmm3
	vunpcklpd	5024(%rsp,%rdi,4), %xmm3, %xmm3 ## xmm3 = xmm3[0],mem[0]
	vmovups	4960(%rsp,%rdi,4), %xmm4
	vunpcklps	4992(%rsp,%rdi,4), %xmm4, %xmm4 ## xmm4 = xmm4[0],mem[0],xmm4[1],mem[1]
	vshufps	$36, %xmm3, %xmm4, %xmm3        ## xmm3 = xmm4[0,1],xmm3[2,0]
	vfmadd132ps	176(%rsp), %xmm2, %xmm3 ## 16-byte Folded Reload
                                        ## xmm3 = (xmm3 * mem) + xmm2
	vmovups	5184(%rsp,%rdi,4), %xmm2
	vunpcklpd	5152(%rsp,%rdi,4), %xmm2, %xmm2 ## xmm2 = xmm2[0],mem[0]
	vmovups	5088(%rsp,%rdi,4), %xmm4
	vunpcklps	5120(%rsp,%rdi,4), %xmm4, %xmm4 ## xmm4 = xmm4[0],mem[0],xmm4[1],mem[1]
	vshufps	$36, %xmm2, %xmm4, %xmm2        ## xmm2 = xmm4[0,1],xmm2[2,0]
	vfmadd132ps	160(%rsp), %xmm3, %xmm2 ## 16-byte Folded Reload
                                        ## xmm2 = (xmm2 * mem) + xmm3
	vmovups	5312(%rsp,%rdi,4), %xmm3
	vunpcklpd	5280(%rsp,%rdi,4), %xmm3, %xmm3 ## xmm3 = xmm3[0],mem[0]
	vmovups	5216(%rsp,%rdi,4), %xmm4
	vunpcklps	5248(%rsp,%rdi,4), %xmm4, %xmm4 ## xmm4 = xmm4[0],mem[0],xmm4[1],mem[1]
	vshufps	$36, %xmm3, %xmm4, %xmm3        ## xmm3 = xmm4[0,1],xmm3[2,0]
	vfmadd132ps	224(%rsp), %xmm2, %xmm3 ## 16-byte Folded Reload
                                        ## xmm3 = (xmm3 * mem) + xmm2
	vmovups	5440(%rsp,%rdi,4), %xmm2
	vunpcklpd	5408(%rsp,%rdi,4), %xmm2, %xmm2 ## xmm2 = xmm2[0],mem[0]
	vmovups	5344(%rsp,%rdi,4), %xmm4
	vunpcklps	5376(%rsp,%rdi,4), %xmm4, %xmm4 ## xmm4 = xmm4[0],mem[0],xmm4[1],mem[1]
	vshufps	$36, %xmm2, %xmm4, %xmm2        ## xmm2 = xmm4[0,1],xmm2[2,0]
	vfmadd132ps	304(%rsp), %xmm3, %xmm2 ## 16-byte Folded Reload
                                        ## xmm2 = (xmm2 * mem) + xmm3
	vmovups	5568(%rsp,%rdi,4), %xmm3
	vunpcklpd	5536(%rsp,%rdi,4), %xmm3, %xmm3 ## xmm3 = xmm3[0],mem[0]
	vmovups	5472(%rsp,%rdi,4), %xmm4
	vunpcklps	5504(%rsp,%rdi,4), %xmm4, %xmm4 ## xmm4 = xmm4[0],mem[0],xmm4[1],mem[1]
	vshufps	$36, %xmm3, %xmm4, %xmm3        ## xmm3 = xmm4[0,1],xmm3[2,0]
	vfmadd132ps	288(%rsp), %xmm2, %xmm3 ## 16-byte Folded Reload
                                        ## xmm3 = (xmm3 * mem) + xmm2
	vpermilpd	$1, %xmm3, %xmm2        ## xmm2 = xmm3[1,0]
	vaddps	%xmm2, %xmm3, %xmm2
	vmovshdup	%xmm2, %xmm3            ## xmm3 = xmm2[1,1,3,3]
	vaddss	%xmm3, %xmm2, %xmm2
	vmovss	736(%rsp), %xmm3                ## 4-byte Reload
                                        ## xmm3 = mem[0],zero,zero,zero
	vfmadd231ss	5600(%rsp,%rdi,4), %xmm3, %xmm2 ## xmm2 = (xmm3 * mem) + xmm2
	vmovss	64(%rsp), %xmm3                 ## 4-byte Reload
                                        ## xmm3 = mem[0],zero,zero,zero
	vfmadd231ss	5632(%rsp,%rdi,4), %xmm3, %xmm2 ## xmm2 = (xmm3 * mem) + xmm2
	vmovss	56(%rsp), %xmm3                 ## 4-byte Reload
                                        ## xmm3 = mem[0],zero,zero,zero
	vfmadd231ss	5664(%rsp,%rdi,4), %xmm3, %xmm2 ## xmm2 = (xmm3 * mem) + xmm2
	vmovss	48(%rsp), %xmm3                 ## 4-byte Reload
                                        ## xmm3 = mem[0],zero,zero,zero
	vfmadd231ss	5696(%rsp,%rdi,4), %xmm3, %xmm2 ## xmm2 = (xmm3 * mem) + xmm2
	vmovups	5824(%rsp,%rdi,4), %xmm3
	vunpcklpd	5792(%rsp,%rdi,4), %xmm3, %xmm3 ## xmm3 = xmm3[0],mem[0]
	vmovups	5728(%rsp,%rdi,4), %xmm4
	vunpcklps	5760(%rsp,%rdi,4), %xmm4, %xmm4 ## xmm4 = xmm4[0],mem[0],xmm4[1],mem[1]
	vshufps	$36, %xmm3, %xmm4, %xmm3        ## xmm3 = xmm4[0,1],xmm3[2,0]
	vmovups	5952(%rsp,%rdi,4), %xmm4
	vunpcklpd	5920(%rsp,%rdi,4), %xmm4, %xmm4 ## xmm4 = xmm4[0],mem[0]
	vblendps	$1, %xmm2, %xmm13, %xmm2        ## xmm2 = xmm2[0],xmm13[1,2,3]
	vfmadd132ps	320(%rsp), %xmm2, %xmm3 ## 16-byte Folded Reload
                                        ## xmm3 = (xmm3 * mem) + xmm2
	vmovups	5856(%rsp,%rdi,4), %xmm2
	vunpcklps	5888(%rsp,%rdi,4), %xmm2, %xmm2 ## xmm2 = xmm2[0],mem[0],xmm2[1],mem[1]
	vshufps	$36, %xmm4, %xmm2, %xmm2        ## xmm2 = xmm2[0,1],xmm4[2,0]
	vmovups	6080(%rsp,%rdi,4), %xmm4
	vunpcklpd	6048(%rsp,%rdi,4), %xmm4, %xmm4 ## xmm4 = xmm4[0],mem[0]
	vfmadd132ps	400(%rsp), %xmm3, %xmm2 ## 16-byte Folded Reload
                                        ## xmm2 = (xmm2 * mem) + xmm3
	vmovups	5984(%rsp,%rdi,4), %xmm3
	vunpcklps	6016(%rsp,%rdi,4), %xmm3, %xmm3 ## xmm3 = xmm3[0],mem[0],xmm3[1],mem[1]
	vshufps	$36, %xmm4, %xmm3, %xmm3        ## xmm3 = xmm3[0,1],xmm4[2,0]
	vmovups	6208(%rsp,%rdi,4), %xmm4
	vunpcklpd	6176(%rsp,%rdi,4), %xmm4, %xmm4 ## xmm4 = xmm4[0],mem[0]
	vfmadd132ps	384(%rsp), %xmm2, %xmm3 ## 16-byte Folded Reload
                                        ## xmm3 = (xmm3 * mem) + xmm2
	vmovups	6112(%rsp,%rdi,4), %xmm2
	vunpcklps	6144(%rsp,%rdi,4), %xmm2, %xmm2 ## xmm2 = xmm2[0],mem[0],xmm2[1],mem[1]
	vshufps	$36, %xmm4, %xmm2, %xmm2        ## xmm2 = xmm2[0,1],xmm4[2,0]
	vmovups	6336(%rsp,%rdi,4), %xmm4
	vunpcklpd	6304(%rsp,%rdi,4), %xmm4, %xmm4 ## xmm4 = xmm4[0],mem[0]
	vfmadd132ps	1152(%rsp), %xmm3, %xmm2 ## 16-byte Folded Reload
                                        ## xmm2 = (xmm2 * mem) + xmm3
	vmovups	6240(%rsp,%rdi,4), %xmm3
	vunpcklps	6272(%rsp,%rdi,4), %xmm3, %xmm3 ## xmm3 = xmm3[0],mem[0],xmm3[1],mem[1]
	vshufps	$36, %xmm4, %xmm3, %xmm3        ## xmm3 = xmm3[0,1],xmm4[2,0]
	vmovups	6464(%rsp,%rdi,4), %xmm4
	vunpcklpd	6432(%rsp,%rdi,4), %xmm4, %xmm4 ## xmm4 = xmm4[0],mem[0]
	vfmadd132ps	464(%rsp), %xmm2, %xmm3 ## 16-byte Folded Reload
                                        ## xmm3 = (xmm3 * mem) + xmm2
	vmovups	6368(%rsp,%rdi,4), %xmm2
	vunpcklps	6400(%rsp,%rdi,4), %xmm2, %xmm2 ## xmm2 = xmm2[0],mem[0],xmm2[1],mem[1]
	vshufps	$36, %xmm4, %xmm2, %xmm2        ## xmm2 = xmm2[0,1],xmm4[2,0]
	vmovups	6592(%rsp,%rdi,4), %xmm4
	vunpcklpd	6560(%rsp,%rdi,4), %xmm4, %xmm4 ## xmm4 = xmm4[0],mem[0]
	vfmadd132ps	448(%rsp), %xmm3, %xmm2 ## 16-byte Folded Reload
                                        ## xmm2 = (xmm2 * mem) + xmm3
	vmovups	6496(%rsp,%rdi,4), %xmm3
	vunpcklps	6528(%rsp,%rdi,4), %xmm3, %xmm3 ## xmm3 = xmm3[0],mem[0],xmm3[1],mem[1]
	vshufps	$36, %xmm4, %xmm3, %xmm3        ## xmm3 = xmm3[0,1],xmm4[2,0]
	vmovups	6720(%rsp,%rdi,4), %xmm4
	vunpcklpd	6688(%rsp,%rdi,4), %xmm4, %xmm4 ## xmm4 = xmm4[0],mem[0]
	vfmadd132ps	432(%rsp), %xmm2, %xmm3 ## 16-byte Folded Reload
                                        ## xmm3 = (xmm3 * mem) + xmm2
	vmovups	6624(%rsp,%rdi,4), %xmm2
	vunpcklps	6656(%rsp,%rdi,4), %xmm2, %xmm2 ## xmm2 = xmm2[0],mem[0],xmm2[1],mem[1]
	vshufps	$36, %xmm4, %xmm2, %xmm2        ## xmm2 = xmm2[0,1],xmm4[2,0]
	vmovups	6848(%rsp,%rdi,4), %xmm4
	vunpcklpd	6816(%rsp,%rdi,4), %xmm4, %xmm4 ## xmm4 = xmm4[0],mem[0]
	vfmadd132ps	416(%rsp), %xmm3, %xmm2 ## 16-byte Folded Reload
                                        ## xmm2 = (xmm2 * mem) + xmm3
	vmovups	6752(%rsp,%rdi,4), %xmm3
	vunpcklps	6784(%rsp,%rdi,4), %xmm3, %xmm3 ## xmm3 = xmm3[0],mem[0],xmm3[1],mem[1]
	vshufps	$36, %xmm4, %xmm3, %xmm3        ## xmm3 = xmm3[0,1],xmm4[2,0]
	vfmadd132ps	480(%rsp), %xmm2, %xmm3 ## 16-byte Folded Reload
                                        ## xmm3 = (xmm3 * mem) + xmm2
	vpermilpd	$1, %xmm3, %xmm2        ## xmm2 = xmm3[1,0]
	vaddps	%xmm2, %xmm3, %xmm2
	vmovshdup	%xmm2, %xmm3            ## xmm3 = xmm2[1,1,3,3]
	vaddss	%xmm3, %xmm2, %xmm2
	vmovss	112(%rsp), %xmm3                ## 4-byte Reload
                                        ## xmm3 = mem[0],zero,zero,zero
	vfmadd231ss	6880(%rsp,%rdi,4), %xmm3, %xmm2 ## xmm2 = (xmm3 * mem) + xmm2
	vmovss	104(%rsp), %xmm3                ## 4-byte Reload
                                        ## xmm3 = mem[0],zero,zero,zero
	vfmadd231ss	6912(%rsp,%rdi,4), %xmm3, %xmm2 ## xmm2 = (xmm3 * mem) + xmm2
	vmovss	128(%rsp), %xmm3                ## 4-byte Reload
                                        ## xmm3 = mem[0],zero,zero,zero
	vfmadd231ss	6944(%rsp,%rdi,4), %xmm3, %xmm2 ## xmm2 = (xmm3 * mem) + xmm2
	vmovss	120(%rsp), %xmm3                ## 4-byte Reload
                                        ## xmm3 = mem[0],zero,zero,zero
	vfmadd231ss	6976(%rsp,%rdi,4), %xmm3, %xmm2 ## xmm2 = (xmm3 * mem) + xmm2
	vmovups	7104(%rsp,%rdi,4), %xmm3
	vunpcklpd	7072(%rsp,%rdi,4), %xmm3, %xmm3 ## xmm3 = xmm3[0],mem[0]
	vmovups	7008(%rsp,%rdi,4), %xmm4
	vblendps	$1, %xmm2, %xmm13, %xmm2        ## xmm2 = xmm2[0],xmm13[1,2,3]
	vunpcklps	7040(%rsp,%rdi,4), %xmm4, %xmm4 ## xmm4 = xmm4[0],mem[0],xmm4[1],mem[1]
	vshufps	$36, %xmm3, %xmm4, %xmm3        ## xmm3 = xmm4[0,1],xmm3[2,0]
	vfmadd132ps	640(%rsp), %xmm2, %xmm3 ## 16-byte Folded Reload
                                        ## xmm3 = (xmm3 * mem) + xmm2
	vmovups	7232(%rsp,%rdi,4), %xmm2
	vunpcklpd	7200(%rsp,%rdi,4), %xmm2, %xmm2 ## xmm2 = xmm2[0],mem[0]
	vmovups	7136(%rsp,%rdi,4), %xmm4
	vunpcklps	7168(%rsp,%rdi,4), %xmm4, %xmm4 ## xmm4 = xmm4[0],mem[0],xmm4[1],mem[1]
	vshufps	$36, %xmm2, %xmm4, %xmm2        ## xmm2 = xmm4[0,1],xmm2[2,0]
	vfmadd132ps	912(%rsp), %xmm3, %xmm2 ## 16-byte Folded Reload
                                        ## xmm2 = (xmm2 * mem) + xmm3
	vmovups	7360(%rsp,%rdi,4), %xmm3
	vunpcklpd	7328(%rsp,%rdi,4), %xmm3, %xmm3 ## xmm3 = xmm3[0],mem[0]
	vmovups	7264(%rsp,%rdi,4), %xmm4
	vunpcklps	7296(%rsp,%rdi,4), %xmm4, %xmm4 ## xmm4 = xmm4[0],mem[0],xmm4[1],mem[1]
	vshufps	$36, %xmm3, %xmm4, %xmm3        ## xmm3 = xmm4[0,1],xmm3[2,0]
	vfmadd132ps	896(%rsp), %xmm2, %xmm3 ## 16-byte Folded Reload
                                        ## xmm3 = (xmm3 * mem) + xmm2
	vmovups	7488(%rsp,%rdi,4), %xmm2
	vunpcklpd	7456(%rsp,%rdi,4), %xmm2, %xmm2 ## xmm2 = xmm2[0],mem[0]
	vmovups	7392(%rsp,%rdi,4), %xmm4
	vunpcklps	7424(%rsp,%rdi,4), %xmm4, %xmm4 ## xmm4 = xmm4[0],mem[0],xmm4[1],mem[1]
	vshufps	$36, %xmm2, %xmm4, %xmm2        ## xmm2 = xmm4[0,1],xmm2[2,0]
	vfmadd132ps	880(%rsp), %xmm3, %xmm2 ## 16-byte Folded Reload
                                        ## xmm2 = (xmm2 * mem) + xmm3
	vmovups	7616(%rsp,%rdi,4), %xmm3
	vunpcklpd	7584(%rsp,%rdi,4), %xmm3, %xmm3 ## xmm3 = xmm3[0],mem[0]
	vmovups	7520(%rsp,%rdi,4), %xmm4
	vunpcklps	7552(%rsp,%rdi,4), %xmm4, %xmm4 ## xmm4 = xmm4[0],mem[0],xmm4[1],mem[1]
	vshufps	$36, %xmm3, %xmm4, %xmm3        ## xmm3 = xmm4[0,1],xmm3[2,0]
	vfmadd132ps	864(%rsp), %xmm2, %xmm3 ## 16-byte Folded Reload
                                        ## xmm3 = (xmm3 * mem) + xmm2
	vmovups	7744(%rsp,%rdi,4), %xmm2
	vunpcklpd	7712(%rsp,%rdi,4), %xmm2, %xmm2 ## xmm2 = xmm2[0],mem[0]
	vmovups	7648(%rsp,%rdi,4), %xmm4
	vunpcklps	7680(%rsp,%rdi,4), %xmm4, %xmm4 ## xmm4 = xmm4[0],mem[0],xmm4[1],mem[1]
	vshufps	$36, %xmm2, %xmm4, %xmm2        ## xmm2 = xmm4[0,1],xmm2[2,0]
	vfmadd132ps	848(%rsp), %xmm3, %xmm2 ## 16-byte Folded Reload
                                        ## xmm2 = (xmm2 * mem) + xmm3
	vmovups	7872(%rsp,%rdi,4), %xmm3
	vunpcklpd	7840(%rsp,%rdi,4), %xmm3, %xmm3 ## xmm3 = xmm3[0],mem[0]
	vmovups	7776(%rsp,%rdi,4), %xmm4
	vunpcklps	7808(%rsp,%rdi,4), %xmm4, %xmm4 ## xmm4 = xmm4[0],mem[0],xmm4[1],mem[1]
	vshufps	$36, %xmm3, %xmm4, %xmm3        ## xmm3 = xmm4[0,1],xmm3[2,0]
	vfmadd132ps	832(%rsp), %xmm2, %xmm3 ## 16-byte Folded Reload
                                        ## xmm3 = (xmm3 * mem) + xmm2
	vmovups	8000(%rsp,%rdi,4), %xmm2
	vunpcklpd	7968(%rsp,%rdi,4), %xmm2, %xmm2 ## xmm2 = xmm2[0],mem[0]
	vmovups	7904(%rsp,%rdi,4), %xmm4
	vunpcklps	7936(%rsp,%rdi,4), %xmm4, %xmm4 ## xmm4 = xmm4[0],mem[0],xmm4[1],mem[1]
	vshufps	$36, %xmm2, %xmm4, %xmm2        ## xmm2 = xmm4[0,1],xmm2[2,0]
	vfmadd132ps	816(%rsp), %xmm3, %xmm2 ## 16-byte Folded Reload
                                        ## xmm2 = (xmm2 * mem) + xmm3
	vmovups	8128(%rsp,%rdi,4), %xmm3
	vunpcklpd	8096(%rsp,%rdi,4), %xmm3, %xmm3 ## xmm3 = xmm3[0],mem[0]
	vmovups	8032(%rsp,%rdi,4), %xmm4
	vunpcklps	8064(%rsp,%rdi,4), %xmm4, %xmm4 ## xmm4 = xmm4[0],mem[0],xmm4[1],mem[1]
	vshufps	$36, %xmm3, %xmm4, %xmm3        ## xmm3 = xmm4[0,1],xmm3[2,0]
	vfmadd132ps	1328(%rsp), %xmm2, %xmm3 ## 16-byte Folded Reload
                                        ## xmm3 = (xmm3 * mem) + xmm2
	vpermilpd	$1, %xmm3, %xmm2        ## xmm2 = xmm3[1,0]
	vaddps	%xmm2, %xmm3, %xmm2
	vmovshdup	%xmm2, %xmm3            ## xmm3 = xmm2[1,1,3,3]
	vaddss	%xmm3, %xmm2, %xmm2
	vmovss	1696(%rsp), %xmm3               ## 4-byte Reload
                                        ## xmm3 = mem[0],zero,zero,zero
	vfmadd231ss	8160(%rsp,%rdi,4), %xmm3, %xmm2 ## xmm2 = (xmm3 * mem) + xmm2
	vmovss	1344(%rsp), %xmm3               ## 4-byte Reload
                                        ## xmm3 = mem[0],zero,zero,zero
	vfmadd231ss	8192(%rsp,%rdi,4), %xmm3, %xmm2 ## xmm2 = (xmm3 * mem) + xmm2
	vmovss	208(%rsp), %xmm3                ## 4-byte Reload
                                        ## xmm3 = mem[0],zero,zero,zero
	vfmadd231ss	8224(%rsp,%rdi,4), %xmm3, %xmm2 ## xmm2 = (xmm3 * mem) + xmm2
	vmovss	200(%rsp), %xmm3                ## 4-byte Reload
                                        ## xmm3 = mem[0],zero,zero,zero
	vfmadd231ss	8256(%rsp,%rdi,4), %xmm3, %xmm2 ## xmm2 = (xmm3 * mem) + xmm2
	vmovups	8384(%rsp,%rdi,4), %xmm3
	vunpcklpd	8352(%rsp,%rdi,4), %xmm3, %xmm3 ## xmm3 = xmm3[0],mem[0]
	vmovups	8288(%rsp,%rdi,4), %xmm4
	vunpcklps	8320(%rsp,%rdi,4), %xmm4, %xmm4 ## xmm4 = xmm4[0],mem[0],xmm4[1],mem[1]
	vshufps	$36, %xmm3, %xmm4, %xmm3        ## xmm3 = xmm4[0,1],xmm3[2,0]
	vmovups	8512(%rsp,%rdi,4), %xmm4
	vunpcklpd	8480(%rsp,%rdi,4), %xmm4, %xmm4 ## xmm4 = xmm4[0],mem[0]
	vblendps	$1, %xmm2, %xmm13, %xmm2        ## xmm2 = xmm2[0],xmm13[1,2,3]
	vfmadd132ps	688(%rsp), %xmm2, %xmm3 ## 16-byte Folded Reload
                                        ## xmm3 = (xmm3 * mem) + xmm2
	vmovups	8416(%rsp,%rdi,4), %xmm2
	vunpcklps	8448(%rsp,%rdi,4), %xmm2, %xmm2 ## xmm2 = xmm2[0],mem[0],xmm2[1],mem[1]
	vshufps	$36, %xmm4, %xmm2, %xmm2        ## xmm2 = xmm2[0,1],xmm4[2,0]
	vmovups	8640(%rsp,%rdi,4), %xmm4
	vunpcklpd	8608(%rsp,%rdi,4), %xmm4, %xmm4 ## xmm4 = xmm4[0],mem[0]
	vfmadd132ps	672(%rsp), %xmm3, %xmm2 ## 16-byte Folded Reload
                                        ## xmm2 = (xmm2 * mem) + xmm3
	vmovups	8544(%rsp,%rdi,4), %xmm3
	vunpcklps	8576(%rsp,%rdi,4), %xmm3, %xmm3 ## xmm3 = xmm3[0],mem[0],xmm3[1],mem[1]
	vshufps	$36, %xmm4, %xmm3, %xmm3        ## xmm3 = xmm3[0,1],xmm4[2,0]
	vmovups	8768(%rsp,%rdi,4), %xmm4
	vunpcklpd	8736(%rsp,%rdi,4), %xmm4, %xmm4 ## xmm4 = xmm4[0],mem[0]
	vfmadd132ps	656(%rsp), %xmm2, %xmm3 ## 16-byte Folded Reload
                                        ## xmm3 = (xmm3 * mem) + xmm2
	vmovups	8672(%rsp,%rdi,4), %xmm2
	vunpcklps	8704(%rsp,%rdi,4), %xmm2, %xmm2 ## xmm2 = xmm2[0],mem[0],xmm2[1],mem[1]
	vshufps	$36, %xmm4, %xmm2, %xmm2        ## xmm2 = xmm2[0,1],xmm4[2,0]
	vmovups	8896(%rsp,%rdi,4), %xmm4
	vunpcklpd	8864(%rsp,%rdi,4), %xmm4, %xmm4 ## xmm4 = xmm4[0],mem[0]
	vfmadd132ps	800(%rsp), %xmm3, %xmm2 ## 16-byte Folded Reload
                                        ## xmm2 = (xmm2 * mem) + xmm3
	vmovups	8800(%rsp,%rdi,4), %xmm3
	vunpcklps	8832(%rsp,%rdi,4), %xmm3, %xmm3 ## xmm3 = xmm3[0],mem[0],xmm3[1],mem[1]
	vshufps	$36, %xmm4, %xmm3, %xmm3        ## xmm3 = xmm3[0,1],xmm4[2,0]
	vmovups	9024(%rsp,%rdi,4), %xmm4
	vunpcklpd	8992(%rsp,%rdi,4), %xmm4, %xmm4 ## xmm4 = xmm4[0],mem[0]
	vfmadd132ps	784(%rsp), %xmm2, %xmm3 ## 16-byte Folded Reload
                                        ## xmm3 = (xmm3 * mem) + xmm2
	vmovups	8928(%rsp,%rdi,4), %xmm2
	vunpcklps	8960(%rsp,%rdi,4), %xmm2, %xmm2 ## xmm2 = xmm2[0],mem[0],xmm2[1],mem[1]
	vshufps	$36, %xmm4, %xmm2, %xmm2        ## xmm2 = xmm2[0,1],xmm4[2,0]
	vmovups	9152(%rsp,%rdi,4), %xmm4
	vunpcklpd	9120(%rsp,%rdi,4), %xmm4, %xmm4 ## xmm4 = xmm4[0],mem[0]
	vfmadd132ps	768(%rsp), %xmm3, %xmm2 ## 16-byte Folded Reload
                                        ## xmm2 = (xmm2 * mem) + xmm3
	vmovups	9056(%rsp,%rdi,4), %xmm3
	vunpcklps	9088(%rsp,%rdi,4), %xmm3, %xmm3 ## xmm3 = xmm3[0],mem[0],xmm3[1],mem[1]
	vshufps	$36, %xmm4, %xmm3, %xmm3        ## xmm3 = xmm3[0,1],xmm4[2,0]
	vmovups	9280(%rsp,%rdi,4), %xmm4
	vunpcklpd	9248(%rsp,%rdi,4), %xmm4, %xmm4 ## xmm4 = xmm4[0],mem[0]
	vfmadd132ps	1200(%rsp), %xmm2, %xmm3 ## 16-byte Folded Reload
                                        ## xmm3 = (xmm3 * mem) + xmm2
	vmovups	9184(%rsp,%rdi,4), %xmm2
	vunpcklps	9216(%rsp,%rdi,4), %xmm2, %xmm2 ## xmm2 = xmm2[0],mem[0],xmm2[1],mem[1]
	vshufps	$36, %xmm4, %xmm2, %xmm2        ## xmm2 = xmm2[0,1],xmm4[2,0]
	vmovups	9408(%rsp,%rdi,4), %xmm4
	vunpcklpd	9376(%rsp,%rdi,4), %xmm4, %xmm4 ## xmm4 = xmm4[0],mem[0]
	vfmadd132ps	2208(%rsp), %xmm3, %xmm2 ## 16-byte Folded Reload
                                        ## xmm2 = (xmm2 * mem) + xmm3
	vmovups	9312(%rsp,%rdi,4), %xmm3
	vunpcklps	9344(%rsp,%rdi,4), %xmm3, %xmm3 ## xmm3 = xmm3[0],mem[0],xmm3[1],mem[1]
	vshufps	$36, %xmm4, %xmm3, %xmm3        ## xmm3 = xmm3[0,1],xmm4[2,0]
	vfmadd132ps	1664(%rsp), %xmm2, %xmm3 ## 16-byte Folded Reload
                                        ## xmm3 = (xmm3 * mem) + xmm2
	vpermilpd	$1, %xmm3, %xmm2        ## xmm2 = xmm3[1,0]
	vaddps	%xmm2, %xmm3, %xmm2
	vmovshdup	%xmm2, %xmm3            ## xmm3 = xmm2[1,1,3,3]
	vaddss	%xmm3, %xmm2, %xmm2
	vmovss	1632(%rsp), %xmm3               ## 4-byte Reload
                                        ## xmm3 = mem[0],zero,zero,zero
	vfmadd231ss	9440(%rsp,%rdi,4), %xmm3, %xmm2 ## xmm2 = (xmm3 * mem) + xmm2
	vmovss	2176(%rsp), %xmm3               ## 4-byte Reload
                                        ## xmm3 = mem[0],zero,zero,zero
	vfmadd231ss	9472(%rsp,%rdi,4), %xmm3, %xmm2 ## xmm2 = (xmm3 * mem) + xmm2
	vmovss	2144(%rsp), %xmm3               ## 4-byte Reload
                                        ## xmm3 = mem[0],zero,zero,zero
	vfmadd231ss	9504(%rsp,%rdi,4), %xmm3, %xmm2 ## xmm2 = (xmm3 * mem) + xmm2
	vmovss	2112(%rsp), %xmm3               ## 4-byte Reload
                                        ## xmm3 = mem[0],zero,zero,zero
	vfmadd231ss	9536(%rsp,%rdi,4), %xmm3, %xmm2 ## xmm2 = (xmm3 * mem) + xmm2
	vmovups	9664(%rsp,%rdi,4), %xmm3
	vunpcklpd	9632(%rsp,%rdi,4), %xmm3, %xmm3 ## xmm3 = xmm3[0],mem[0]
	vmovups	9568(%rsp,%rdi,4), %xmm4
	vblendps	$1, %xmm2, %xmm13, %xmm2        ## xmm2 = xmm2[0],xmm13[1,2,3]
	vunpcklps	9600(%rsp,%rdi,4), %xmm4, %xmm4 ## xmm4 = xmm4[0],mem[0],xmm4[1],mem[1]
	vshufps	$36, %xmm3, %xmm4, %xmm3        ## xmm3 = xmm4[0,1],xmm3[2,0]
	vfmadd132ps	2080(%rsp), %xmm2, %xmm3 ## 16-byte Folded Reload
                                        ## xmm3 = (xmm3 * mem) + xmm2
	vmovups	9792(%rsp,%rdi,4), %xmm2
	vunpcklpd	9760(%rsp,%rdi,4), %xmm2, %xmm2 ## xmm2 = xmm2[0],mem[0]
	vmovups	9696(%rsp,%rdi,4), %xmm4
	vunpcklps	9728(%rsp,%rdi,4), %xmm4, %xmm4 ## xmm4 = xmm4[0],mem[0],xmm4[1],mem[1]
	vshufps	$36, %xmm2, %xmm4, %xmm2        ## xmm2 = xmm4[0,1],xmm2[2,0]
	vfmadd132ps	2048(%rsp), %xmm3, %xmm2 ## 16-byte Folded Reload
                                        ## xmm2 = (xmm2 * mem) + xmm3
	vmovups	9920(%rsp,%rdi,4), %xmm3
	vunpcklpd	9888(%rsp,%rdi,4), %xmm3, %xmm3 ## xmm3 = xmm3[0],mem[0]
	vmovups	9824(%rsp,%rdi,4), %xmm4
	vunpcklps	9856(%rsp,%rdi,4), %xmm4, %xmm4 ## xmm4 = xmm4[0],mem[0],xmm4[1],mem[1]
	vshufps	$36, %xmm3, %xmm4, %xmm3        ## xmm3 = xmm4[0,1],xmm3[2,0]
	vfmadd132ps	1600(%rsp), %xmm2, %xmm3 ## 16-byte Folded Reload
                                        ## xmm3 = (xmm3 * mem) + xmm2
	vmovups	10048(%rsp,%rdi,4), %xmm2
	vunpcklpd	10016(%rsp,%rdi,4), %xmm2, %xmm2 ## xmm2 = xmm2[0],mem[0]
	vmovups	9952(%rsp,%rdi,4), %xmm4
	vunpcklps	9984(%rsp,%rdi,4), %xmm4, %xmm4 ## xmm4 = xmm4[0],mem[0],xmm4[1],mem[1]
	vshufps	$36, %xmm2, %xmm4, %xmm2        ## xmm2 = xmm4[0,1],xmm2[2,0]
	vfmadd132ps	1568(%rsp), %xmm3, %xmm2 ## 16-byte Folded Reload
                                        ## xmm2 = (xmm2 * mem) + xmm3
	vmovups	10176(%rsp,%rdi,4), %xmm3
	vunpcklpd	10144(%rsp,%rdi,4), %xmm3, %xmm3 ## xmm3 = xmm3[0],mem[0]
	vmovups	10080(%rsp,%rdi,4), %xmm4
	vunpcklps	10112(%rsp,%rdi,4), %xmm4, %xmm4 ## xmm4 = xmm4[0],mem[0],xmm4[1],mem[1]
	vshufps	$36, %xmm3, %xmm4, %xmm3        ## xmm3 = xmm4[0,1],xmm3[2,0]
	vfmadd132ps	1184(%rsp), %xmm2, %xmm3 ## 16-byte Folded Reload
                                        ## xmm3 = (xmm3 * mem) + xmm2
	vmovups	10304(%rsp,%rdi,4), %xmm2
	vunpcklpd	10272(%rsp,%rdi,4), %xmm2, %xmm2 ## xmm2 = xmm2[0],mem[0]
	vmovups	10208(%rsp,%rdi,4), %xmm4
	vunpcklps	10240(%rsp,%rdi,4), %xmm4, %xmm4 ## xmm4 = xmm4[0],mem[0],xmm4[1],mem[1]
	vshufps	$36, %xmm2, %xmm4, %xmm2        ## xmm2 = xmm4[0,1],xmm2[2,0]
	vfmadd132ps	1824(%rsp), %xmm3, %xmm2 ## 16-byte Folded Reload
                                        ## xmm2 = (xmm2 * mem) + xmm3
	vmovups	10432(%rsp,%rdi,4), %xmm3
	vunpcklpd	10400(%rsp,%rdi,4), %xmm3, %xmm3 ## xmm3 = xmm3[0],mem[0]
	vmovups	10336(%rsp,%rdi,4), %xmm4
	vunpcklps	10368(%rsp,%rdi,4), %xmm4, %xmm4 ## xmm4 = xmm4[0],mem[0],xmm4[1],mem[1]
	vshufps	$36, %xmm3, %xmm4, %xmm3        ## xmm3 = xmm4[0,1],xmm3[2,0]
	vfmadd132ps	1792(%rsp), %xmm2, %xmm3 ## 16-byte Folded Reload
                                        ## xmm3 = (xmm3 * mem) + xmm2
	vmovups	10560(%rsp,%rdi,4), %xmm2
	vunpcklpd	10528(%rsp,%rdi,4), %xmm2, %xmm2 ## xmm2 = xmm2[0],mem[0]
	vmovups	10464(%rsp,%rdi,4), %xmm4
	vunpcklps	10496(%rsp,%rdi,4), %xmm4, %xmm4 ## xmm4 = xmm4[0],mem[0],xmm4[1],mem[1]
	vshufps	$36, %xmm2, %xmm4, %xmm2        ## xmm2 = xmm4[0,1],xmm2[2,0]
	vfmadd132ps	1760(%rsp), %xmm3, %xmm2 ## 16-byte Folded Reload
                                        ## xmm2 = (xmm2 * mem) + xmm3
	vmovups	10688(%rsp,%rdi,4), %xmm3
	vunpcklpd	10656(%rsp,%rdi,4), %xmm3, %xmm3 ## xmm3 = xmm3[0],mem[0]
	vmovups	10592(%rsp,%rdi,4), %xmm4
	vunpcklps	10624(%rsp,%rdi,4), %xmm4, %xmm4 ## xmm4 = xmm4[0],mem[0],xmm4[1],mem[1]
	vshufps	$36, %xmm3, %xmm4, %xmm3        ## xmm3 = xmm4[0,1],xmm3[2,0]
	vfmadd132ps	1728(%rsp), %xmm2, %xmm3 ## 16-byte Folded Reload
                                        ## xmm3 = (xmm3 * mem) + xmm2
	vpermilpd	$1, %xmm3, %xmm2        ## xmm2 = xmm3[1,0]
	vaddps	%xmm2, %xmm3, %xmm2
	vmovshdup	%xmm2, %xmm3            ## xmm3 = xmm2[1,1,3,3]
	vaddss	%xmm3, %xmm2, %xmm2
	vmovss	536(%rsp), %xmm3                ## 4-byte Reload
                                        ## xmm3 = mem[0],zero,zero,zero
	vfmadd231ss	10720(%rsp,%rdi,4), %xmm3, %xmm2 ## xmm2 = (xmm3 * mem) + xmm2
	vmovss	528(%rsp), %xmm3                ## 4-byte Reload
                                        ## xmm3 = mem[0],zero,zero,zero
	vfmadd231ss	10752(%rsp,%rdi,4), %xmm3, %xmm2 ## xmm2 = (xmm3 * mem) + xmm2
	vmovss	520(%rsp), %xmm3                ## 4-byte Reload
                                        ## xmm3 = mem[0],zero,zero,zero
	vfmadd231ss	10784(%rsp,%rdi,4), %xmm3, %xmm2 ## xmm2 = (xmm3 * mem) + xmm2
	vmovss	632(%rsp), %xmm3                ## 4-byte Reload
                                        ## xmm3 = mem[0],zero,zero,zero
	vfmadd231ss	10816(%rsp,%rdi,4), %xmm3, %xmm2 ## xmm2 = (xmm3 * mem) + xmm2
	vmovups	10944(%rsp,%rdi,4), %xmm3
	vunpcklpd	10912(%rsp,%rdi,4), %xmm3, %xmm3 ## xmm3 = xmm3[0],mem[0]
	vmovups	10848(%rsp,%rdi,4), %xmm4
	vunpcklps	10880(%rsp,%rdi,4), %xmm4, %xmm4 ## xmm4 = xmm4[0],mem[0],xmm4[1],mem[1]
	vshufps	$36, %xmm3, %xmm4, %xmm3        ## xmm3 = xmm4[0,1],xmm3[2,0]
	vmovups	11072(%rsp,%rdi,4), %xmm4
	vunpcklpd	11040(%rsp,%rdi,4), %xmm4, %xmm4 ## xmm4 = xmm4[0],mem[0]
	vblendps	$1, %xmm2, %xmm13, %xmm2        ## xmm2 = xmm2[0],xmm13[1,2,3]
	vfmadd132ps	1312(%rsp), %xmm2, %xmm3 ## 16-byte Folded Reload
                                        ## xmm3 = (xmm3 * mem) + xmm2
	vmovups	10976(%rsp,%rdi,4), %xmm2
	vunpcklps	11008(%rsp,%rdi,4), %xmm2, %xmm2 ## xmm2 = xmm2[0],mem[0],xmm2[1],mem[1]
	vshufps	$36, %xmm4, %xmm2, %xmm2        ## xmm2 = xmm2[0,1],xmm4[2,0]
	vmovups	11200(%rsp,%rdi,4), %xmm4
	vunpcklpd	11168(%rsp,%rdi,4), %xmm4, %xmm4 ## xmm4 = xmm4[0],mem[0]
	vfmadd132ps	1296(%rsp), %xmm3, %xmm2 ## 16-byte Folded Reload
                                        ## xmm2 = (xmm2 * mem) + xmm3
	vmovups	11104(%rsp,%rdi,4), %xmm3
	vunpcklps	11136(%rsp,%rdi,4), %xmm3, %xmm3 ## xmm3 = xmm3[0],mem[0],xmm3[1],mem[1]
	vshufps	$36, %xmm4, %xmm3, %xmm3        ## xmm3 = xmm3[0,1],xmm4[2,0]
	vmovups	11328(%rsp,%rdi,4), %xmm4
	vunpcklpd	11296(%rsp,%rdi,4), %xmm4, %xmm4 ## xmm4 = xmm4[0],mem[0]
	vfmadd132ps	1376(%rsp), %xmm2, %xmm3 ## 16-byte Folded Reload
                                        ## xmm3 = (xmm3 * mem) + xmm2
	vmovups	11232(%rsp,%rdi,4), %xmm2
	vunpcklps	11264(%rsp,%rdi,4), %xmm2, %xmm2 ## xmm2 = xmm2[0],mem[0],xmm2[1],mem[1]
	vshufps	$36, %xmm4, %xmm2, %xmm2        ## xmm2 = xmm2[0,1],xmm4[2,0]
	vmovups	11456(%rsp,%rdi,4), %xmm4
	vunpcklpd	11424(%rsp,%rdi,4), %xmm4, %xmm4 ## xmm4 = xmm4[0],mem[0]
	vfmadd132ps	2752(%rsp), %xmm3, %xmm2 ## 16-byte Folded Reload
                                        ## xmm2 = (xmm2 * mem) + xmm3
	vmovups	11360(%rsp,%rdi,4), %xmm3
	vunpcklps	11392(%rsp,%rdi,4), %xmm3, %xmm3 ## xmm3 = xmm3[0],mem[0],xmm3[1],mem[1]
	vshufps	$36, %xmm4, %xmm3, %xmm3        ## xmm3 = xmm3[0,1],xmm4[2,0]
	vmovups	11584(%rsp,%rdi,4), %xmm4
	vunpcklpd	11552(%rsp,%rdi,4), %xmm4, %xmm4 ## xmm4 = xmm4[0],mem[0]
	vfmadd132ps	2720(%rsp), %xmm2, %xmm3 ## 16-byte Folded Reload
                                        ## xmm3 = (xmm3 * mem) + xmm2
	vmovups	11488(%rsp,%rdi,4), %xmm2
	vunpcklps	11520(%rsp,%rdi,4), %xmm2, %xmm2 ## xmm2 = xmm2[0],mem[0],xmm2[1],mem[1]
	vshufps	$36, %xmm4, %xmm2, %xmm2        ## xmm2 = xmm2[0,1],xmm4[2,0]
	vmovups	11712(%rsp,%rdi,4), %xmm4
	vunpcklpd	11680(%rsp,%rdi,4), %xmm4, %xmm4 ## xmm4 = xmm4[0],mem[0]
	vfmadd132ps	2688(%rsp), %xmm3, %xmm2 ## 16-byte Folded Reload
                                        ## xmm2 = (xmm2 * mem) + xmm3
	vmovups	11616(%rsp,%rdi,4), %xmm3
	vunpcklps	11648(%rsp,%rdi,4), %xmm3, %xmm3 ## xmm3 = xmm3[0],mem[0],xmm3[1],mem[1]
	vshufps	$36, %xmm4, %xmm3, %xmm3        ## xmm3 = xmm3[0,1],xmm4[2,0]
	vmovups	11840(%rsp,%rdi,4), %xmm4
	vunpcklpd	11808(%rsp,%rdi,4), %xmm4, %xmm4 ## xmm4 = xmm4[0],mem[0]
	vfmadd132ps	2656(%rsp), %xmm2, %xmm3 ## 16-byte Folded Reload
                                        ## xmm3 = (xmm3 * mem) + xmm2
	vmovups	11744(%rsp,%rdi,4), %xmm2
	vunpcklps	11776(%rsp,%rdi,4), %xmm2, %xmm2 ## xmm2 = xmm2[0],mem[0],xmm2[1],mem[1]
	vshufps	$36, %xmm4, %xmm2, %xmm2        ## xmm2 = xmm2[0,1],xmm4[2,0]
	vmovups	11968(%rsp,%rdi,4), %xmm4
	vunpcklpd	11936(%rsp,%rdi,4), %xmm4, %xmm4 ## xmm4 = xmm4[0],mem[0]
	vfmadd132ps	2624(%rsp), %xmm3, %xmm2 ## 16-byte Folded Reload
                                        ## xmm2 = (xmm2 * mem) + xmm3
	vmovups	11872(%rsp,%rdi,4), %xmm3
	vunpcklps	11904(%rsp,%rdi,4), %xmm3, %xmm3 ## xmm3 = xmm3[0],mem[0],xmm3[1],mem[1]
	vshufps	$36, %xmm4, %xmm3, %xmm3        ## xmm3 = xmm3[0,1],xmm4[2,0]
	vfmadd213ps	%xmm2, %xmm12, %xmm3    ## xmm3 = (xmm12 * xmm3) + xmm2
	vpermilpd	$1, %xmm3, %xmm2        ## xmm2 = xmm3[1,0]
	vaddps	%xmm2, %xmm3, %xmm2
	vmovshdup	%xmm2, %xmm3            ## xmm3 = xmm2[1,1,3,3]
	vaddss	%xmm3, %xmm2, %xmm2
	vfmadd231ss	12000(%rsp,%rdi,4), %xmm8, %xmm2 ## xmm2 = (xmm8 * mem) + xmm2
	vfmadd231ss	12032(%rsp,%rdi,4), %xmm14, %xmm2 ## xmm2 = (xmm14 * mem) + xmm2
	vfmadd231ss	12064(%rsp,%rdi,4), %xmm15, %xmm2 ## xmm2 = (xmm15 * mem) + xmm2
	vfmadd231ss	12096(%rsp,%rdi,4), %xmm11, %xmm2 ## xmm2 = (xmm11 * mem) + xmm2
	vfmadd231ss	(%r8), %xmm2, %xmm9     ## xmm9 = (xmm2 * mem) + xmm9
	incq	%rdi
	addq	%rdx, %r8
	cmpq	$8, %rdi
	jne	LBB0_78
## %bb.79:                              ## %"end for conv1_stage1.s1.r54$x"
                                        ##   in Loop: Header=BB0_77 Depth=2
	vmovss	%xmm9, (%rsi,%r10,4)
	incq	%r9
	addq	$4, %r11
	cmpq	$32, %r9
	jne	LBB0_77
## %bb.80:                              ## %"end for conv1_stage1.s1.c"
                                        ##   in Loop: Header=BB0_76 Depth=1
	movq	%r14, %rcx
	incq	%rcx
	movq	512(%rsp), %rbx                 ## 8-byte Reload
	cmpq	%rbx, %rcx
	jne	LBB0_76
LBB0_81:                                ## %"end for conv1_stage1.s1.w"
	cmpl	$0, 616(%rsp)                   ## 4-byte Folded Reload
	movq	1504(%rsp), %r8                 ## 8-byte Reload
	movl	2976(%rsp), %eax                ## 4-byte Reload
	js	LBB0_159
## %bb.82:                              ## %"end for conv1_stage1.s1.w"
	cmpl	%r8d, %eax
	jg	LBB0_159
## %bb.83:                              ## %"consume conv1_stage1"
	movq	192(%rsp), %rax                 ## 8-byte Reload
	leal	3(%rax), %r14d
	sarl	$2, %r14d
	cmpl	$8, %r8d
	movq	728(%rsp), %rcx                 ## 8-byte Reload
	movq	712(%rsp), %r11                 ## 8-byte Reload
	movq	1272(%rsp), %rdi                ## 8-byte Reload
	movq	80(%rbp), %r9
	movq	32(%rbp), %r10
	jge	LBB0_136
## %bb.84:                              ## %then_bb168
	testl	%r8d, %r8d
	jle	LBB0_138
## %bb.85:                              ## %"for prediction_output.s0.n.v7.preheader"
	movl	608(%rsp), %eax                 ## 4-byte Reload
	andl	$-4, %eax
	cmpl	$-4, %eax
	movl	$-4, %ecx
	cmovgl	%eax, %ecx
	addl	$4, %ecx
	imulq	$156, %rcx, %rdx
	cmpq	$2147483647, %rdx               ## imm = 0x7FFFFFFF
	ja	LBB0_169
## %bb.86:                              ## %"for prediction_output.s0.n.v7.preheader420"
	movq	504(%rsp), %rax                 ## 8-byte Reload
	imull	140(%rsp), %eax                 ## 4-byte Folded Reload
	movq	2336(%rsp), %rbx                ## 8-byte Reload
	imull	%edi, %ebx
	addl	624(%rsp), %ebx                 ## 4-byte Folded Reload
	addl	%eax, %ebx
	vxorps	%xmm15, %xmm15, %xmm15
	vcvtsi2ssl	1112(%rsp), %xmm15, %xmm0 ## 4-byte Folded Reload
	orq	$12, %rdx
	movq	%rdx, 2328(%rsp)                ## 8-byte Spill
	movq	%rcx, %rax
	shlq	$5, %rax
	leaq	(%rax,%rax,2), %rax
	movq	%rax, 1432(%rsp)                ## 8-byte Spill
	orq	$12, %rax
	movq	%rax, 2312(%rsp)                ## 8-byte Spill
	movq	%rcx, 2320(%rsp)                ## 8-byte Spill
	shlq	$7, %rcx
	movq	%rcx, 1400(%rsp)                ## 8-byte Spill
	movl	%edi, %eax
	subl	%ebx, %eax
	movl	%eax, 1216(%rsp)                ## 4-byte Spill
	leal	(%rdi,%rdi,8), %eax
	leal	(%rdi,%rax,2), %edx
	leal	(%rax,%rax,2), %r8d
                                        ## kill: def $eax killed $eax killed $rax
	subl	%ebx, %eax
	movl	%eax, 256(%rsp)                 ## 4-byte Spill
	leal	(%rdi,%rdi), %eax
	leal	(%rax,%rax,4), %ecx
	subl	%ebx, %ecx
	movl	%ecx, 544(%rsp)                 ## 4-byte Spill
	leal	(%rdi,%rdi,4), %ecx
	leal	(%rdi,%rcx,2), %esi
	subl	%ebx, %esi
	movl	%esi, 32(%rsp)                  ## 4-byte Spill
	leal	(,%rdi,4), %esi
	leal	(%rsi,%rsi,2), %esi
	subl	%ebx, %esi
	movl	%esi, 40(%rsp)                  ## 4-byte Spill
	movl	%edi, %esi
	shll	$4, %esi
	addl	%edi, %esi
	subl	%ebx, %esi
	leal	(%rax,%rax,8), %eax
	subl	%ebx, %eax
	movl	%eax, 72(%rsp)                  ## 4-byte Spill
	subl	%ebx, %edx
	movl	%edx, 80(%rsp)                  ## 4-byte Spill
	leal	(%rdi,%rcx,4), %r15d
	leal	(%rdi,%r15), %eax
	subl	%ebx, %r15d
	subl	%ebx, %eax
	movl	%eax, 352(%rsp)                 ## 4-byte Spill
	leal	(,%rdi,8), %eax
	leal	(%rax,%rax,2), %eax
	subl	%ebx, %eax
	movl	%eax, 336(%rsp)                 ## 4-byte Spill
	leal	(%rcx,%rcx,4), %r9d
	movl	%r9d, %eax
	subl	%ebx, %eax
	movl	%eax, 240(%rsp)                 ## 4-byte Spill
	addl	%edi, %r9d
	subl	%ebx, %r9d
	movl	%r8d, %eax
	subl	%ebx, %eax
	movl	%eax, 144(%rsp)                 ## 4-byte Spill
	addl	%edi, %r8d
	leal	(%rdi,%r8), %eax
	subl	%ebx, %r8d
	subl	%ebx, %eax
	movl	%eax, 176(%rsp)                 ## 4-byte Spill
	movl	%edi, %r12d
	shll	$5, %r12d
	movl	%r12d, %eax
	leal	(%rdi,%r12), %r10d
	leal	(%r12,%rdi,2), %r11d
                                        ## kill: def $r12d killed $r12d killed $r12
	subl	%edi, %r12d
	movl	%r12d, %edx
	subl	%edi, %edx
	subl	%ebx, %edx
	subl	%ebx, %r12d
	subl	%ebx, %eax
	movl	%eax, 160(%rsp)                 ## 4-byte Spill
	subl	%ebx, %r10d
	subl	%ebx, %r11d
	movslq	%ebx, %r13
	movq	1136(%rsp), %rbx                ## 8-byte Reload
	movq	%rbx, %rax
	shlq	$4, %rax
	movq	%rax, 616(%rsp)                 ## 8-byte Spill
	cmpl	$31, 192(%rsp)                  ## 4-byte Folded Reload
	seta	224(%rsp)                       ## 1-byte Folded Spill
	cmpl	$1, 140(%rsp)                   ## 4-byte Folded Reload
	sete	%al
	leaq	(,%r13,4), %rcx
	movl	$96, %edi
	subq	%rcx, %rdi
	movq	%rdi, 1200(%rsp)                ## 8-byte Spill
	andb	224(%rsp), %al                  ## 1-byte Folded Reload
	movb	%al, 103(%rsp)                  ## 1-byte Spill
	movl	%r14d, %eax
	movq	%rax, 1328(%rsp)                ## 8-byte Spill
	movslq	%r11d, %r14
	movslq	%r10d, %rax
	movq	%rax, 208(%rsp)                 ## 8-byte Spill
	movslq	160(%rsp), %rax                 ## 4-byte Folded Reload
	movq	%rax, 912(%rsp)                 ## 8-byte Spill
	movslq	%r12d, %rax
	movq	%rax, 896(%rsp)                 ## 8-byte Spill
	movslq	%edx, %rax
	movq	%rax, 200(%rsp)                 ## 8-byte Spill
	movslq	176(%rsp), %rax                 ## 4-byte Folded Reload
	movq	%rax, 688(%rsp)                 ## 8-byte Spill
	movslq	%r8d, %rax
	movq	%rax, 880(%rsp)                 ## 8-byte Spill
	movslq	144(%rsp), %rax                 ## 4-byte Folded Reload
	movq	%rax, 864(%rsp)                 ## 8-byte Spill
	movslq	%r9d, %rax
	movq	%rax, 848(%rsp)                 ## 8-byte Spill
	movslq	240(%rsp), %r10                 ## 4-byte Folded Reload
	movslq	336(%rsp), %r8                  ## 4-byte Folded Reload
	movslq	352(%rsp), %r9                  ## 4-byte Folded Reload
	movslq	%r15d, %rax
	movq	%rax, 504(%rsp)                 ## 8-byte Spill
	movslq	80(%rsp), %rax                  ## 4-byte Folded Reload
	movq	%rax, 832(%rsp)                 ## 8-byte Spill
	movslq	72(%rsp), %rax                  ## 4-byte Folded Reload
	movq	%rax, 816(%rsp)                 ## 8-byte Spill
	movslq	%esi, %r11
	movslq	40(%rsp), %r12                  ## 4-byte Folded Reload
	movslq	32(%rsp), %rax                  ## 4-byte Folded Reload
	movq	%rax, 672(%rsp)                 ## 8-byte Spill
	movslq	544(%rsp), %rsi                 ## 4-byte Folded Reload
	movslq	256(%rsp), %rdx                 ## 4-byte Folded Reload
	movslq	1216(%rsp), %rdi                ## 4-byte Folded Reload
	movl	1504(%rsp), %eax                ## 4-byte Reload
	movq	%rax, 2296(%rsp)                ## 8-byte Spill
	vmovaps	3168(%rsp), %ymm1
	vmovaps	%ymm1, 1888(%rsp)               ## 32-byte Spill
	vmovaps	3200(%rsp), %ymm1
	vmovaps	%ymm1, 1440(%rsp)               ## 32-byte Spill
	vmovaps	3232(%rsp), %ymm1
	vmovaps	%ymm1, 1856(%rsp)               ## 32-byte Spill
	vmovaps	3264(%rsp), %ymm1
	vmovaps	%ymm1, 1472(%rsp)               ## 32-byte Spill
	movq	512(%rsp), %rax                 ## 8-byte Reload
                                        ## kill: def $eax killed $eax killed $rax def $rax
	andl	$-32, %eax
	movq	%rax, 1424(%rsp)                ## 8-byte Spill
	vmovaps	%xmm0, 2784(%rsp)               ## 16-byte Spill
	vbroadcastss	%xmm0, %ymm0
	vmovaps	%ymm0, 2976(%rsp)               ## 32-byte Spill
	movq	%r13, 2304(%rsp)                ## 8-byte Spill
	negq	%r13
	movq	%r13, 1184(%rsp)                ## 8-byte Spill
	movq	600(%rsp), %rcx                 ## 8-byte Reload
	leaq	(,%rcx,4), %rax
	leaq	(%rax,%rax,8), %r15
	shlq	$3, %rcx
	movq	%rcx, 600(%rsp)                 ## 8-byte Spill
	movq	1416(%rsp), %rcx                ## 8-byte Reload
	movq	%rcx, %rax
	shlq	$7, %rax
	movq	%rax, 608(%rsp)                 ## 8-byte Spill
	vbroadcastss	LCPI0_49(%rip), %ymm0   ## ymm0 = [2155872255,2155872255,2155872255,2155872255,2155872255,2155872255,2155872255,2155872255]
	vmovaps	%ymm0, 3136(%rsp)               ## 32-byte Spill
	movq	1400(%rsp), %rax                ## 8-byte Reload
	addq	$12, %rax
	movq	%rax, 2288(%rsp)                ## 8-byte Spill
	leaq	(,%rbx,8), %rax
	movq	%rax, 1288(%rsp)                ## 8-byte Spill
	leaq	(%rax,%rax,2), %rax
	movq	%rax, 1280(%rsp)                ## 8-byte Spill
	leaq	(,%rbx,4), %rax
	movq	%rax, 520(%rsp)                 ## 8-byte Spill
	movq	376(%rsp), %r13                 ## 8-byte Reload
	leaq	(,%r13,8), %rax
	movq	%rax, 1216(%rsp)                ## 8-byte Spill
	movq	1144(%rsp), %rax                ## 8-byte Reload
	leaq	32(%rax), %rbx
	movq	%rbx, 632(%rsp)                 ## 8-byte Spill
	leaq	64(%rax), %rax
	movq	%rax, 1312(%rsp)                ## 8-byte Spill
	movq	728(%rsp), %rax                 ## 8-byte Reload
	leaq	32(%rax), %rbx
	movq	%rbx, 2280(%rsp)                ## 8-byte Spill
	leaq	64(%rax), %rbx
	movq	%rbx, 1296(%rsp)                ## 8-byte Spill
	addq	$96, %rax
	movq	%rax, 1376(%rsp)                ## 8-byte Spill
	leaq	(,%rcx,4), %rax
	movq	%rax, 2272(%rsp)                ## 8-byte Spill
	xorl	%eax, %eax
	movq	%rax, 720(%rsp)                 ## 8-byte Spill
	movq	%r8, 656(%rsp)                  ## 8-byte Spill
	movq	%r8, 1104(%rsp)                 ## 8-byte Spill
	movq	%r9, 1120(%rsp)                 ## 8-byte Spill
	movq	%r9, 1096(%rsp)                 ## 8-byte Spill
	movq	%r10, 1128(%rsp)                ## 8-byte Spill
	movq	%r10, 1088(%rsp)                ## 8-byte Spill
	movq	%rdx, 768(%rsp)                 ## 8-byte Spill
	movq	%rdx, 1080(%rsp)                ## 8-byte Spill
	movq	%rsi, 784(%rsp)                 ## 8-byte Spill
	movq	%rsi, 1072(%rsp)                ## 8-byte Spill
	movq	200(%rsp), %rax                 ## 8-byte Reload
	movq	%rax, 1064(%rsp)                ## 8-byte Spill
	movq	912(%rsp), %rax                 ## 8-byte Reload
	movq	%rax, 1056(%rsp)                ## 8-byte Spill
	movq	896(%rsp), %rax                 ## 8-byte Reload
	movq	%rax, 1048(%rsp)                ## 8-byte Spill
	movq	%rdi, 1112(%rsp)                ## 8-byte Spill
	movq	%rdi, 1040(%rsp)                ## 8-byte Spill
	movq	504(%rsp), %rax                 ## 8-byte Reload
	movq	%rax, 1032(%rsp)                ## 8-byte Spill
	movq	%r11, 528(%rsp)                 ## 8-byte Spill
	movq	%r11, 1024(%rsp)                ## 8-byte Spill
	movq	%r12, 800(%rsp)                 ## 8-byte Spill
	movq	%r12, 1016(%rsp)                ## 8-byte Spill
	movq	688(%rsp), %rax                 ## 8-byte Reload
	movq	%rax, 1008(%rsp)                ## 8-byte Spill
	movq	672(%rsp), %rax                 ## 8-byte Reload
	movq	%rax, 1000(%rsp)                ## 8-byte Spill
	movq	%r14, 536(%rsp)                 ## 8-byte Spill
	movq	%r14, 992(%rsp)                 ## 8-byte Spill
	movq	208(%rsp), %rax                 ## 8-byte Reload
	movq	%rax, 984(%rsp)                 ## 8-byte Spill
	movq	864(%rsp), %rax                 ## 8-byte Reload
	movq	%rax, 976(%rsp)                 ## 8-byte Spill
	movq	880(%rsp), %rax                 ## 8-byte Reload
	movq	%rax, 968(%rsp)                 ## 8-byte Spill
	movq	848(%rsp), %rax                 ## 8-byte Reload
	movq	%rax, 960(%rsp)                 ## 8-byte Spill
	movq	832(%rsp), %rax                 ## 8-byte Reload
	movq	%rax, 952(%rsp)                 ## 8-byte Spill
	movq	816(%rsp), %rax                 ## 8-byte Reload
	movq	%rax, 944(%rsp)                 ## 8-byte Spill
	movq	712(%rsp), %r12                 ## 8-byte Reload
	movq	%r12, 936(%rsp)                 ## 8-byte Spill
	movq	%r15, 2264(%rsp)                ## 8-byte Spill
	.p2align	4, 0x90
LBB0_87:                                ## %"for prediction_output.s0.n.v7"
                                        ## =>This Loop Header: Depth=1
                                        ##     Child Loop BB0_90 Depth 2
                                        ##       Child Loop BB0_94 Depth 3
                                        ##     Child Loop BB0_103 Depth 2
                                        ##       Child Loop BB0_104 Depth 3
                                        ##       Child Loop BB0_107 Depth 3
                                        ##       Child Loop BB0_110 Depth 3
                                        ##     Child Loop BB0_113 Depth 2
                                        ##       Child Loop BB0_114 Depth 3
                                        ##       Child Loop BB0_116 Depth 3
                                        ##       Child Loop BB0_118 Depth 3
                                        ##       Child Loop BB0_120 Depth 3
                                        ##     Child Loop BB0_125 Depth 2
                                        ##     Child Loop BB0_132 Depth 2
	xorl	%edi, %edi
	movq	2328(%rsp), %rsi                ## 8-byte Reload
	vzeroupper
	callq	_halide_malloc
	movq	%rax, 32(%rsp)                  ## 8-byte Spill
	testq	%rax, %rax
	je	LBB0_146
## %bb.88:                              ## %"assert succeeded173"
                                        ##   in Loop: Header=BB0_87 Depth=1
	cmpl	$0, 192(%rsp)                   ## 4-byte Folded Reload
	movq	1272(%rsp), %r11                ## 8-byte Reload
	vmovaps	2432(%rsp), %ymm13              ## 32-byte Reload
	vmovdqa	2944(%rsp), %ymm14              ## 32-byte Reload
	vmovaps	3136(%rsp), %ymm15              ## 32-byte Reload
	vmovss	LCPI0_45(%rip), %xmm4           ## xmm4 = mem[0],zero,zero,zero
	vmovss	LCPI0_44(%rip), %xmm11          ## xmm11 = mem[0],zero,zero,zero
	vmovss	LCPI0_51(%rip), %xmm12          ## xmm12 = mem[0],zero,zero,zero
	vmovss	LCPI0_52(%rip), %xmm7           ## xmm7 = mem[0],zero,zero,zero
	vmovss	LCPI0_53(%rip), %xmm8           ## xmm8 = mem[0],zero,zero,zero
	vmovss	LCPI0_57(%rip), %xmm9           ## xmm9 = mem[0],zero,zero,zero
	vmovss	LCPI0_55(%rip), %xmm5           ## xmm5 = mem[0],zero,zero,zero
	vmovss	LCPI0_56(%rip), %xmm6           ## xmm6 = mem[0],zero,zero,zero
	vmovss	LCPI0_50(%rip), %xmm10          ## xmm10 = mem[0],zero,zero,zero
	jle	LBB0_96
## %bb.89:                              ## %"for normalized_schedule_features.s0.s.preheader"
                                        ##   in Loop: Header=BB0_87 Depth=1
	movq	720(%rsp), %r14                 ## 8-byte Reload
	subq	2304(%rsp), %r14                ## 8-byte Folded Reload
	xorl	%r9d, %r9d
	movq	32(%rsp), %r8                   ## 8-byte Reload
	xorl	%r10d, %r10d
	.p2align	4, 0x90
LBB0_90:                                ## %"for normalized_schedule_features.s0.s"
                                        ##   Parent Loop BB0_87 Depth=1
                                        ## =>  This Loop Header: Depth=2
                                        ##       Child Loop BB0_94 Depth 3
	cmpl	$1, %r11d
	jne	LBB0_92
## %bb.91:                              ## %vector.body935
                                        ##   in Loop: Header=BB0_90 Depth=2
	imulq	$39, %r10, %rdi
	movl	%r10d, %eax
	imull	140(%rsp), %eax                 ## 4-byte Folded Reload
	movslq	%eax, %rbx
	addq	%r14, %rbx
	vaddps	(%r12,%rbx,4), %ymm13, %ymm0
	vandps	%ymm0, %ymm15, %ymm1
	vpsrad	$22, %ymm1, %ymm2
	vpslld	$23, %ymm2, %ymm3
	vpsubd	%ymm3, %ymm1, %ymm1
	vpaddd	%ymm1, %ymm14, %ymm1
	vmovaps	2592(%rsp), %ymm12              ## 32-byte Reload
	vaddps	%ymm1, %ymm12, %ymm5
	vmulps	%ymm5, %ymm5, %ymm7
	vpsrad	$23, %ymm0, %ymm0
	vmovdqa	3040(%rsp), %ymm11              ## 32-byte Reload
	vpaddd	%ymm0, %ymm11, %ymm0
	vpaddd	%ymm0, %ymm2, %ymm0
	vcvtdq2ps	%ymm0, %ymm8
	vbroadcastss	LCPI0_50(%rip), %ymm0   ## ymm0 = [6.93147182E-1,6.93147182E-1,6.93147182E-1,6.93147182E-1,6.93147182E-1,6.93147182E-1,6.93147182E-1,6.93147182E-1]
	vbroadcastss	LCPI0_51(%rip), %ymm1   ## ymm1 = [7.64031857E-2,7.64031857E-2,7.64031857E-2,7.64031857E-2,7.64031857E-2,7.64031857E-2,7.64031857E-2,7.64031857E-2]
	vbroadcastss	LCPI0_52(%rip), %ymm3   ## ymm3 = [2.06252187E-1,2.06252187E-1,2.06252187E-1,2.06252187E-1,2.06252187E-1,2.06252187E-1,2.06252187E-1,2.06252187E-1]
	vmovaps	%ymm1, %ymm2
	vfmadd213ps	%ymm3, %ymm7, %ymm2     ## ymm2 = (ymm7 * ymm2) + ymm3
	vbroadcastss	LCPI0_53(%rip), %ymm4   ## ymm4 = [3.33204657E-1,3.33204657E-1,3.33204657E-1,3.33204657E-1,3.33204657E-1,3.33204657E-1,3.33204657E-1,3.33204657E-1]
	vfmadd213ps	%ymm4, %ymm7, %ymm2     ## ymm2 = (ymm7 * ymm2) + ymm4
	vfmadd213ps	%ymm13, %ymm7, %ymm2    ## ymm2 = (ymm7 * ymm2) + ymm13
	vmulps	%ymm2, %ymm5, %ymm9
	vbroadcastss	LCPI0_54(%rip), %ymm2   ## ymm2 = [1.62529618E-1,1.62529618E-1,1.62529618E-1,1.62529618E-1,1.62529618E-1,1.62529618E-1,1.62529618E-1,1.62529618E-1]
	vbroadcastss	LCPI0_55(%rip), %ymm5   ## ymm5 = [-2.51102597E-1,-2.51102597E-1,-2.51102597E-1,-2.51102597E-1,-2.51102597E-1,-2.51102597E-1,-2.51102597E-1,-2.51102597E-1]
	vmovaps	%ymm2, %ymm10
	vfmsub213ps	%ymm5, %ymm7, %ymm10    ## ymm10 = (ymm7 * ymm10) - ymm5
	vbroadcastss	LCPI0_56(%rip), %ymm6   ## ymm6 = [-4.99975145E-1,-4.99975145E-1,-4.99975145E-1,-4.99975145E-1,-4.99975145E-1,-4.99975145E-1,-4.99975145E-1,-4.99975145E-1]
	vfmsub213ps	%ymm6, %ymm7, %ymm10    ## ymm10 = (ymm7 * ymm10) - ymm6
	vfmsub213ps	%ymm9, %ymm7, %ymm10    ## ymm10 = (ymm7 * ymm10) - ymm9
	vfmsub231ps	%ymm8, %ymm0, %ymm10    ## ymm10 = (ymm0 * ymm8) - ymm10
	movq	32(%rsp), %rcx                  ## 8-byte Reload
	vmovups	%ymm10, (%rcx,%rdi,4)
	movq	1288(%rsp), %rax                ## 8-byte Reload
	leaq	(%rax,%rbx), %rax
	vaddps	(%r12,%rax,4), %ymm13, %ymm7
	vandps	%ymm7, %ymm15, %ymm8
	vpsrad	$22, %ymm8, %ymm9
	vpslld	$23, %ymm9, %ymm10
	vpsubd	%ymm10, %ymm8, %ymm8
	vpaddd	%ymm14, %ymm8, %ymm8
	vaddps	%ymm12, %ymm8, %ymm8
	vmulps	%ymm8, %ymm8, %ymm10
	vpsrad	$23, %ymm7, %ymm7
	vpaddd	%ymm7, %ymm11, %ymm7
	vpaddd	%ymm7, %ymm9, %ymm7
	vcvtdq2ps	%ymm7, %ymm7
	vmovaps	%ymm1, %ymm9
	vfmadd213ps	%ymm3, %ymm10, %ymm9    ## ymm9 = (ymm10 * ymm9) + ymm3
	vfmadd213ps	%ymm4, %ymm10, %ymm9    ## ymm9 = (ymm10 * ymm9) + ymm4
	vfmadd213ps	%ymm13, %ymm10, %ymm9   ## ymm9 = (ymm10 * ymm9) + ymm13
	vmulps	%ymm9, %ymm8, %ymm8
	vmovaps	%ymm2, %ymm9
	vfmsub213ps	%ymm5, %ymm10, %ymm9    ## ymm9 = (ymm10 * ymm9) - ymm5
	vfmsub213ps	%ymm6, %ymm10, %ymm9    ## ymm9 = (ymm10 * ymm9) - ymm6
	vfmsub213ps	%ymm8, %ymm10, %ymm9    ## ymm9 = (ymm10 * ymm9) - ymm8
	vfmsub231ps	%ymm7, %ymm0, %ymm9     ## ymm9 = (ymm0 * ymm7) - ymm9
	vmovups	%ymm9, 32(%rcx,%rdi,4)
	movq	616(%rsp), %rax                 ## 8-byte Reload
	leaq	(%rax,%rbx), %rax
	vaddps	(%r12,%rax,4), %ymm13, %ymm7
	vandps	%ymm7, %ymm15, %ymm8
	vpsrad	$22, %ymm8, %ymm9
	vpslld	$23, %ymm9, %ymm10
	vpsubd	%ymm10, %ymm8, %ymm8
	vpaddd	%ymm14, %ymm8, %ymm8
	vaddps	%ymm12, %ymm8, %ymm8
	vmulps	%ymm8, %ymm8, %ymm10
	vpsrad	$23, %ymm7, %ymm7
	vpaddd	%ymm7, %ymm11, %ymm7
	vpaddd	%ymm7, %ymm9, %ymm7
	vcvtdq2ps	%ymm7, %ymm7
	vmovaps	%ymm1, %ymm9
	vfmadd213ps	%ymm3, %ymm10, %ymm9    ## ymm9 = (ymm10 * ymm9) + ymm3
	vfmadd213ps	%ymm4, %ymm10, %ymm9    ## ymm9 = (ymm10 * ymm9) + ymm4
	vfmadd213ps	%ymm13, %ymm10, %ymm9   ## ymm9 = (ymm10 * ymm9) + ymm13
	vmulps	%ymm9, %ymm8, %ymm8
	vmovaps	%ymm2, %ymm9
	vfmsub213ps	%ymm5, %ymm10, %ymm9    ## ymm9 = (ymm10 * ymm9) - ymm5
	vfmsub213ps	%ymm6, %ymm10, %ymm9    ## ymm9 = (ymm10 * ymm9) - ymm6
	vfmsub213ps	%ymm8, %ymm10, %ymm9    ## ymm9 = (ymm10 * ymm9) - ymm8
	vfmsub231ps	%ymm7, %ymm0, %ymm9     ## ymm9 = (ymm0 * ymm7) - ymm9
	vmovups	%ymm9, 64(%rcx,%rdi,4)
	addq	1280(%rsp), %rbx                ## 8-byte Folded Reload
	vaddps	(%r12,%rbx,4), %ymm13, %ymm7
	vandps	%ymm7, %ymm15, %ymm8
	vpsrad	$22, %ymm8, %ymm9
	vpslld	$23, %ymm9, %ymm10
	vpsubd	%ymm10, %ymm8, %ymm8
	vpaddd	%ymm14, %ymm8, %ymm8
	vaddps	%ymm12, %ymm8, %ymm8
	vmulps	%ymm8, %ymm8, %ymm10
	vpsrad	$23, %ymm7, %ymm7
	vpaddd	%ymm7, %ymm11, %ymm7
	vpaddd	%ymm7, %ymm9, %ymm7
	vmovss	LCPI0_57(%rip), %xmm9           ## xmm9 = mem[0],zero,zero,zero
	vcvtdq2ps	%ymm7, %ymm7
	vfmadd213ps	%ymm3, %ymm10, %ymm1    ## ymm1 = (ymm10 * ymm1) + ymm3
	vfmadd213ps	%ymm4, %ymm10, %ymm1    ## ymm1 = (ymm10 * ymm1) + ymm4
	vmovss	LCPI0_45(%rip), %xmm4           ## xmm4 = mem[0],zero,zero,zero
	vfmadd213ps	%ymm13, %ymm10, %ymm1   ## ymm1 = (ymm10 * ymm1) + ymm13
	vmulps	%ymm1, %ymm8, %ymm1
	vmovss	LCPI0_53(%rip), %xmm8           ## xmm8 = mem[0],zero,zero,zero
	vfmsub213ps	%ymm5, %ymm10, %ymm2    ## ymm2 = (ymm10 * ymm2) - ymm5
	vmovss	LCPI0_44(%rip), %xmm11          ## xmm11 = mem[0],zero,zero,zero
	vfmsub213ps	%ymm6, %ymm10, %ymm2    ## ymm2 = (ymm10 * ymm2) - ymm6
	vmovss	LCPI0_51(%rip), %xmm12          ## xmm12 = mem[0],zero,zero,zero
	vfmsub213ps	%ymm1, %ymm10, %ymm2    ## ymm2 = (ymm10 * ymm2) - ymm1
	vmovss	LCPI0_50(%rip), %xmm10          ## xmm10 = mem[0],zero,zero,zero
	vmovss	LCPI0_56(%rip), %xmm6           ## xmm6 = mem[0],zero,zero,zero
	vmovss	LCPI0_55(%rip), %xmm5           ## xmm5 = mem[0],zero,zero,zero
	vfmsub231ps	%ymm7, %ymm0, %ymm2     ## ymm2 = (ymm0 * ymm7) - ymm2
	vmovss	LCPI0_52(%rip), %xmm7           ## xmm7 = mem[0],zero,zero,zero
	vmovups	%ymm2, 96(%rcx,%rdi,4)
	movl	$32, %edi
	jmp	LBB0_93
	.p2align	4, 0x90
LBB0_92:                                ##   in Loop: Header=BB0_90 Depth=2
	xorl	%edi, %edi
LBB0_93:                                ## %"for normalized_schedule_features.s0.c.preheader"
                                        ##   in Loop: Header=BB0_90 Depth=2
	movq	520(%rsp), %rdx                 ## 8-byte Reload
	movslq	%r9d, %rbx
	movq	1136(%rsp), %rax                ## 8-byte Reload
	imulq	%rdi, %rax
	addq	1184(%rsp), %rax                ## 8-byte Folded Reload
	addq	%rbx, %rax
	leaq	(%r12,%rax,4), %rbx
	.p2align	4, 0x90
LBB0_94:                                ## %"for normalized_schedule_features.s0.c"
                                        ##   Parent Loop BB0_87 Depth=1
                                        ##     Parent Loop BB0_90 Depth=2
                                        ## =>    This Inner Loop Header: Depth=3
	vaddss	(%rbx), %xmm4, %xmm0
	vmovd	%xmm0, %eax
	movl	%eax, %esi
	andl	$-2139095041, %esi              ## imm = 0x807FFFFF
	movl	%esi, %ecx
	sarl	$22, %ecx
	sarl	$23, %eax
	addl	%ecx, %eax
	addl	$-127, %eax
                                        ## kill: def $ecx killed $ecx killed $rcx def $rcx
	shll	$23, %ecx
	negl	%ecx
	addl	%esi, %ecx
	addl	$1065353216, %ecx               ## imm = 0x3F800000
	vmovd	%ecx, %xmm0
	vaddss	%xmm0, %xmm11, %xmm0
	vmulss	%xmm0, %xmm0, %xmm1
	vmovaps	%xmm12, %xmm2
	vfmadd213ss	%xmm7, %xmm1, %xmm2     ## xmm2 = (xmm1 * xmm2) + xmm7
	vfmadd213ss	%xmm8, %xmm1, %xmm2     ## xmm2 = (xmm1 * xmm2) + xmm8
	vfmadd213ss	%xmm4, %xmm1, %xmm2     ## xmm2 = (xmm1 * xmm2) + xmm4
	vmovaps	%xmm9, %xmm3
	vmulss	%xmm2, %xmm0, %xmm0
	vfmadd213ss	%xmm5, %xmm1, %xmm3     ## xmm3 = (xmm1 * xmm3) + xmm5
	vfmadd213ss	%xmm6, %xmm1, %xmm3     ## xmm3 = (xmm1 * xmm3) + xmm6
	vcvtsi2ss	%eax, %xmm13, %xmm2
	vfmadd213ss	%xmm0, %xmm1, %xmm3     ## xmm3 = (xmm1 * xmm3) + xmm0
	vfmadd231ss	%xmm10, %xmm2, %xmm3    ## xmm3 = (xmm2 * xmm10) + xmm3
	vmovss	%xmm3, (%r8,%rdi,4)
	incq	%rdi
	addq	%rdx, %rbx
	cmpq	$39, %rdi
	jne	LBB0_94
## %bb.95:                              ## %"end for normalized_schedule_features.s0.c"
                                        ##   in Loop: Header=BB0_90 Depth=2
	incq	%r10
	addq	$156, %r8
	addl	140(%rsp), %r9d                 ## 4-byte Folded Reload
	cmpq	512(%rsp), %r10                 ## 8-byte Folded Reload
	movq	376(%rsp), %r13                 ## 8-byte Reload
	jne	LBB0_90
LBB0_96:                                ## %"end for normalized_schedule_features.s0.s"
                                        ##   in Loop: Header=BB0_87 Depth=1
	movl	$2147483648, %eax               ## imm = 0x80000000
	cmpq	%rax, 1432(%rsp)                ## 8-byte Folded Reload
	jae	LBB0_148
## %bb.97:                              ## %"assert succeeded175"
                                        ##   in Loop: Header=BB0_87 Depth=1
	xorl	%edi, %edi
	movq	2312(%rsp), %rsi                ## 8-byte Reload
	vzeroupper
	callq	_halide_malloc
	movq	%rax, 256(%rsp)                 ## 8-byte Spill
	testq	%rax, %rax
	je	LBB0_149
## %bb.98:                              ## %"assert succeeded177"
                                        ##   in Loop: Header=BB0_87 Depth=1
	cmpl	$16777216, 2320(%rsp)           ## 4-byte Folded Reload
                                        ## imm = 0x1000000
	jae	LBB0_151
## %bb.99:                              ## %"assert succeeded179"
                                        ##   in Loop: Header=BB0_87 Depth=1
	xorl	%edi, %edi
	movq	2288(%rsp), %rsi                ## 8-byte Reload
	callq	_halide_malloc
	testq	%rax, %rax
	je	LBB0_152
## %bb.100:                             ## %"assert succeeded181"
                                        ##   in Loop: Header=BB0_87 Depth=1
	movq	%rax, %r12
	cmpl	$0, 192(%rsp)                   ## 4-byte Folded Reload
	movq	1216(%rsp), %rbx                ## 8-byte Reload
	vxorps	%xmm12, %xmm12, %xmm12
	jle	LBB0_134
## %bb.101:                             ## %"for head2_relu.s0.w.w.preheader"
                                        ##   in Loop: Header=BB0_87 Depth=1
	movq	1264(%rsp), %rax                ## 8-byte Reload
	vmovups	(%rax), %ymm0
	vmovups	32(%rax), %ymm1
	vmovups	64(%rax), %ymm2
	movq	32(%rsp), %rax                  ## 8-byte Reload
	addq	$472, %rax                      ## imm = 0x1D8
	xorl	%ecx, %ecx
	jmp	LBB0_103
	.p2align	4, 0x90
LBB0_102:                               ## %"for head2_relu.s0.w.v9.preheader.2"
                                        ##   in Loop: Header=BB0_103 Depth=2
	vmaxps	%ymm12, %ymm10, %ymm3
	orq	$64, %rcx
	movq	256(%rsp), %rdx                 ## 8-byte Reload
	vmovaps	%ymm3, (%rdx,%rcx)
	vmaxps	%ymm12, %ymm9, %ymm3
	vmovaps	%ymm3, 64(%r10,%rdx)
	vmaxps	%ymm12, %ymm8, %ymm3
	vmovaps	%ymm3, 64(%r9,%rdx)
	vmaxps	%ymm12, %ymm7, %ymm3
	shlq	$5, %r11
	leaq	(%r11,%r11,2), %rcx
	orq	$64, %rcx
	vmovaps	%ymm3, (%rdx,%rcx)
	movq	544(%rsp), %rcx                 ## 8-byte Reload
	incq	%rcx
	addq	$624, %rax                      ## imm = 0x270
	cmpq	1328(%rsp), %rcx                ## 8-byte Folded Reload
	je	LBB0_112
LBB0_103:                               ## %"for head2_relu.s0.w.w"
                                        ##   Parent Loop BB0_87 Depth=1
                                        ## =>  This Loop Header: Depth=2
                                        ##       Child Loop BB0_104 Depth 3
                                        ##       Child Loop BB0_107 Depth 3
                                        ##       Child Loop BB0_110 Depth 3
	leal	(,%rcx,4), %r8d
	movq	$-38, %rdx
	movq	1144(%rsp), %rsi                ## 8-byte Reload
	vmovaps	%ymm0, %ymm4
	vmovaps	%ymm0, %ymm5
	vmovaps	%ymm0, %ymm6
	vmovaps	%ymm0, %ymm7
	.p2align	4, 0x90
LBB0_104:                               ## %"for head2_conv.s1.r40$x"
                                        ##   Parent Loop BB0_87 Depth=1
                                        ##     Parent Loop BB0_103 Depth=2
                                        ## =>    This Inner Loop Header: Depth=3
	vmovups	(%rsi), %ymm11
	vbroadcastss	-320(%rax,%rdx,4), %ymm10
	vfmadd213ps	%ymm7, %ymm11, %ymm10   ## ymm10 = (ymm11 * ymm10) + ymm7
	vbroadcastss	-164(%rax,%rdx,4), %ymm9
	vfmadd213ps	%ymm6, %ymm11, %ymm9    ## ymm9 = (ymm11 * ymm9) + ymm6
	vbroadcastss	-8(%rax,%rdx,4), %ymm8
	vfmadd213ps	%ymm5, %ymm11, %ymm8    ## ymm8 = (ymm11 * ymm8) + ymm5
	vbroadcastss	148(%rax,%rdx,4), %ymm3
	vfmadd213ps	%ymm4, %ymm11, %ymm3    ## ymm3 = (ymm11 * ymm3) + ymm4
	testq	%rdx, %rdx
	je	LBB0_106
## %bb.105:                             ## %"for head2_conv.s1.r40$x.11076"
                                        ##   in Loop: Header=BB0_104 Depth=3
	vmovups	(%rsi,%r13,4), %ymm11
	vbroadcastss	-316(%rax,%rdx,4), %ymm7
	vfmadd213ps	%ymm10, %ymm11, %ymm7   ## ymm7 = (ymm11 * ymm7) + ymm10
	vbroadcastss	-160(%rax,%rdx,4), %ymm6
	vfmadd213ps	%ymm9, %ymm11, %ymm6    ## ymm6 = (ymm11 * ymm6) + ymm9
	vbroadcastss	-4(%rax,%rdx,4), %ymm5
	vfmadd213ps	%ymm8, %ymm11, %ymm5    ## ymm5 = (ymm11 * ymm5) + ymm8
	vbroadcastss	152(%rax,%rdx,4), %ymm4
	vfmadd213ps	%ymm3, %ymm11, %ymm4    ## ymm4 = (ymm11 * ymm4) + ymm3
	addq	$2, %rdx
	addq	%rbx, %rsi
	jmp	LBB0_104
	.p2align	4, 0x90
LBB0_106:                               ## %"for head2_relu.s0.w.v9.preheader"
                                        ##   in Loop: Header=BB0_103 Depth=2
	movq	%rcx, 544(%rsp)                 ## 8-byte Spill
	vmaxps	%ymm12, %ymm10, %ymm4
	leaq	(%r8,%r8,2), %rcx
	shlq	$5, %rcx
	movq	256(%rsp), %rsi                 ## 8-byte Reload
	vmovaps	%ymm4, (%rsi,%rcx)
	vmaxps	%ymm12, %ymm9, %ymm4
	movq	%r8, %rcx
	orq	$1, %rcx
	leaq	(%rcx,%rcx,2), %r10
	shlq	$5, %r10
	vmovaps	%ymm4, (%rsi,%r10)
	vmaxps	%ymm12, %ymm8, %ymm4
	movq	%r8, %rbx
	orq	$2, %rbx
	leaq	(%rbx,%rbx,2), %r9
	shlq	$5, %r9
	vmovaps	%ymm4, (%rsi,%r9)
	vmaxps	%ymm12, %ymm3, %ymm3
	movq	%r8, %r11
	orq	$3, %r11
	leaq	(%r11,%r11,2), %rdi
	shlq	$5, %rdi
	vmovaps	%ymm3, (%rsi,%rdi)
	movq	$-38, %rsi
	movq	632(%rsp), %rcx                 ## 8-byte Reload
	vmovaps	%ymm1, %ymm6
	vmovaps	%ymm1, %ymm5
	vmovaps	%ymm1, %ymm4
	vmovaps	%ymm1, %ymm3
	.p2align	4, 0x90
LBB0_107:                               ## %"for head2_conv.s1.r40$x.1"
                                        ##   Parent Loop BB0_87 Depth=1
                                        ##     Parent Loop BB0_103 Depth=2
                                        ## =>    This Inner Loop Header: Depth=3
	vmovups	(%rcx), %ymm11
	vbroadcastss	-320(%rax,%rsi,4), %ymm10
	vfmadd213ps	%ymm3, %ymm11, %ymm10   ## ymm10 = (ymm11 * ymm10) + ymm3
	vbroadcastss	-164(%rax,%rsi,4), %ymm9
	vfmadd213ps	%ymm4, %ymm11, %ymm9    ## ymm9 = (ymm11 * ymm9) + ymm4
	vbroadcastss	-8(%rax,%rsi,4), %ymm8
	vfmadd213ps	%ymm5, %ymm11, %ymm8    ## ymm8 = (ymm11 * ymm8) + ymm5
	vbroadcastss	148(%rax,%rsi,4), %ymm7
	vfmadd213ps	%ymm6, %ymm11, %ymm7    ## ymm7 = (ymm11 * ymm7) + ymm6
	testq	%rsi, %rsi
	je	LBB0_109
## %bb.108:                             ## %"for head2_conv.s1.r40$x.1.1"
                                        ##   in Loop: Header=BB0_107 Depth=3
	movq	376(%rsp), %r14                 ## 8-byte Reload
	vmovups	(%rcx,%r14,4), %ymm11
	vbroadcastss	-316(%rax,%rsi,4), %ymm3
	vfmadd213ps	%ymm10, %ymm11, %ymm3   ## ymm3 = (ymm11 * ymm3) + ymm10
	vbroadcastss	-160(%rax,%rsi,4), %ymm4
	vfmadd213ps	%ymm9, %ymm11, %ymm4    ## ymm4 = (ymm11 * ymm4) + ymm9
	vbroadcastss	-4(%rax,%rsi,4), %ymm5
	vfmadd213ps	%ymm8, %ymm11, %ymm5    ## ymm5 = (ymm11 * ymm5) + ymm8
	vbroadcastss	152(%rax,%rsi,4), %ymm6
	vfmadd213ps	%ymm7, %ymm11, %ymm6    ## ymm6 = (ymm11 * ymm6) + ymm7
	addq	$2, %rsi
	addq	1216(%rsp), %rcx                ## 8-byte Folded Reload
	jmp	LBB0_107
	.p2align	4, 0x90
LBB0_109:                               ## %"for head2_relu.s0.w.v9.preheader.1"
                                        ##   in Loop: Header=BB0_103 Depth=2
	vmaxps	%ymm12, %ymm10, %ymm3
	shlq	$5, %r8
	leaq	(%r8,%r8,2), %rcx
	movq	%rcx, %rsi
	orq	$32, %rsi
	movq	256(%rsp), %rdx                 ## 8-byte Reload
	vmovaps	%ymm3, (%rdx,%rsi)
	vmaxps	%ymm12, %ymm9, %ymm3
	vmovaps	%ymm3, 32(%r10,%rdx)
	vmaxps	%ymm12, %ymm8, %ymm3
	shlq	$5, %rbx
	leaq	(%rbx,%rbx,2), %rsi
	orq	$32, %rsi
	vmovaps	%ymm3, (%rdx,%rsi)
	vmaxps	%ymm12, %ymm7, %ymm3
	vmovaps	%ymm3, 32(%rdi,%rdx)
	movq	$-38, %rsi
	movq	1312(%rsp), %rdi                ## 8-byte Reload
	vmovaps	%ymm2, %ymm6
	vmovaps	%ymm2, %ymm5
	vmovaps	%ymm2, %ymm4
	vmovaps	%ymm2, %ymm3
	movq	376(%rsp), %r13                 ## 8-byte Reload
	movq	1216(%rsp), %rbx                ## 8-byte Reload
	.p2align	4, 0x90
LBB0_110:                               ## %"for head2_conv.s1.r40$x.2"
                                        ##   Parent Loop BB0_87 Depth=1
                                        ##     Parent Loop BB0_103 Depth=2
                                        ## =>    This Inner Loop Header: Depth=3
	vmovups	(%rdi), %ymm11
	vbroadcastss	-320(%rax,%rsi,4), %ymm10
	vfmadd213ps	%ymm3, %ymm11, %ymm10   ## ymm10 = (ymm11 * ymm10) + ymm3
	vbroadcastss	-164(%rax,%rsi,4), %ymm9
	vfmadd213ps	%ymm4, %ymm11, %ymm9    ## ymm9 = (ymm11 * ymm9) + ymm4
	vbroadcastss	-8(%rax,%rsi,4), %ymm8
	vfmadd213ps	%ymm5, %ymm11, %ymm8    ## ymm8 = (ymm11 * ymm8) + ymm5
	vbroadcastss	148(%rax,%rsi,4), %ymm7
	vfmadd213ps	%ymm6, %ymm11, %ymm7    ## ymm7 = (ymm11 * ymm7) + ymm6
	testq	%rsi, %rsi
	je	LBB0_102
## %bb.111:                             ## %"for head2_conv.s1.r40$x.2.1"
                                        ##   in Loop: Header=BB0_110 Depth=3
	vmovups	(%rdi,%r13,4), %ymm11
	vbroadcastss	-316(%rax,%rsi,4), %ymm3
	vfmadd213ps	%ymm10, %ymm11, %ymm3   ## ymm3 = (ymm11 * ymm3) + ymm10
	vbroadcastss	-160(%rax,%rsi,4), %ymm4
	vfmadd213ps	%ymm9, %ymm11, %ymm4    ## ymm4 = (ymm11 * ymm4) + ymm9
	vbroadcastss	-4(%rax,%rsi,4), %ymm5
	vfmadd213ps	%ymm8, %ymm11, %ymm5    ## ymm5 = (ymm11 * ymm5) + ymm8
	vbroadcastss	152(%rax,%rsi,4), %ymm6
	vfmadd213ps	%ymm7, %ymm11, %ymm6    ## ymm6 = (ymm11 * ymm6) + ymm7
	addq	$2, %rsi
	addq	%rbx, %rdi
	jmp	LBB0_110
	.p2align	4, 0x90
LBB0_112:                               ## %"for relu1.s0.w.w.preheader"
                                        ##   in Loop: Header=BB0_87 Depth=1
	xorl	%edi, %edi
	movq	32(%rsp), %rsi                  ## 8-byte Reload
	vzeroupper
	callq	_halide_free
	vxorps	%xmm9, %xmm9, %xmm9
	movq	256(%rsp), %rax                 ## 8-byte Reload
	addq	$292, %rax                      ## imm = 0x124
	xorl	%r8d, %r8d
	movq	728(%rsp), %r10                 ## 8-byte Reload
	movq	600(%rsp), %rdx                 ## 8-byte Reload
	movq	88(%rsp), %rsi                  ## 8-byte Reload
	movq	2280(%rsp), %r13                ## 8-byte Reload
	.p2align	4, 0x90
LBB0_113:                               ## %"for relu1.s0.w.w"
                                        ##   Parent Loop BB0_87 Depth=1
                                        ## =>  This Loop Header: Depth=2
                                        ##       Child Loop BB0_114 Depth 3
                                        ##       Child Loop BB0_116 Depth 3
                                        ##       Child Loop BB0_118 Depth 3
                                        ##       Child Loop BB0_120 Depth 3
	movq	%r8, %r14
	shlq	$7, %r14
	leal	(,%r8,4), %r11d
	movq	%r8, %rcx
	shlq	$9, %rcx
	vmovaps	(%rsi,%rcx), %ymm3
	vmovaps	128(%rsi,%rcx), %ymm2
	vmovaps	256(%rsi,%rcx), %ymm1
	vmovaps	384(%rsi,%rcx), %ymm0
	movq	$-96, %rsi
	movq	%r10, %rbx
	.p2align	4, 0x90
LBB0_114:                               ## %"for conv1_stage2.s1.r63$x"
                                        ##   Parent Loop BB0_87 Depth=1
                                        ##     Parent Loop BB0_113 Depth=2
                                        ## =>    This Inner Loop Header: Depth=3
	movq	216(%rsp), %rdi                 ## 8-byte Reload
	vmovups	(%rbx,%rdi), %ymm4
	vbroadcastss	-196(%rax,%rsi), %ymm5
	vfmadd213ps	%ymm3, %ymm4, %ymm5     ## ymm5 = (ymm4 * ymm5) + ymm3
	vbroadcastss	-100(%rax,%rsi), %ymm6
	vfmadd213ps	%ymm2, %ymm4, %ymm6     ## ymm6 = (ymm4 * ymm6) + ymm2
	vbroadcastss	-4(%rax,%rsi), %ymm7
	vfmadd213ps	%ymm1, %ymm4, %ymm7     ## ymm7 = (ymm4 * ymm7) + ymm1
	vbroadcastss	92(%rax,%rsi), %ymm8
	vfmadd213ps	%ymm0, %ymm4, %ymm8     ## ymm8 = (ymm4 * ymm8) + ymm0
	vmovups	(%rbx,%r15), %ymm4
	vbroadcastss	-192(%rax,%rsi), %ymm3
	vfmadd213ps	%ymm5, %ymm4, %ymm3     ## ymm3 = (ymm4 * ymm3) + ymm5
	vbroadcastss	-96(%rax,%rsi), %ymm2
	vfmadd213ps	%ymm6, %ymm4, %ymm2     ## ymm2 = (ymm4 * ymm2) + ymm6
	vbroadcastss	(%rax,%rsi), %ymm1
	vfmadd213ps	%ymm7, %ymm4, %ymm1     ## ymm1 = (ymm4 * ymm1) + ymm7
	vbroadcastss	96(%rax,%rsi), %ymm0
	vfmadd213ps	%ymm8, %ymm4, %ymm0     ## ymm0 = (ymm4 * ymm0) + ymm8
	addq	%rdx, %rbx
	addq	$8, %rsi
	jne	LBB0_114
## %bb.115:                             ## %"for relu1.s0.w.v12.preheader"
                                        ##   in Loop: Header=BB0_113 Depth=2
	vmaxps	%ymm9, %ymm3, %ymm3
	movq	%r11, %r9
	shlq	$5, %r9
	shlq	$7, %r11
	vmovaps	%ymm3, (%r12,%r11)
	vmaxps	%ymm9, %ymm2, %ymm2
	movq	%r11, %rsi
	orq	$128, %rsi
	vmovaps	%ymm2, (%r12,%rsi)
	vmaxps	%ymm9, %ymm1, %ymm1
	movq	%r11, %rsi
	orq	$256, %rsi                      ## imm = 0x100
	vmovaps	%ymm1, (%r12,%rsi)
	vmaxps	%ymm9, %ymm0, %ymm0
	movq	%r11, %rsi
	orq	$384, %rsi                      ## imm = 0x180
	vmovaps	%ymm0, (%r12,%rsi)
	movq	88(%rsp), %rsi                  ## 8-byte Reload
	vmovaps	32(%rsi,%rcx), %ymm3
	vmovaps	160(%rsi,%rcx), %ymm2
	vmovaps	288(%rsi,%rcx), %ymm1
	vmovaps	416(%rsi,%rcx), %ymm0
	movq	$-96, %rbx
	movq	%r13, %rsi
	.p2align	4, 0x90
LBB0_116:                               ## %"for conv1_stage2.s1.r63$x.1"
                                        ##   Parent Loop BB0_87 Depth=1
                                        ##     Parent Loop BB0_113 Depth=2
                                        ## =>    This Inner Loop Header: Depth=3
	movq	216(%rsp), %rdi                 ## 8-byte Reload
	vmovups	(%rsi,%rdi), %ymm4
	vbroadcastss	-196(%rax,%rbx), %ymm5
	vfmadd213ps	%ymm3, %ymm4, %ymm5     ## ymm5 = (ymm4 * ymm5) + ymm3
	vbroadcastss	-100(%rax,%rbx), %ymm6
	vfmadd213ps	%ymm2, %ymm4, %ymm6     ## ymm6 = (ymm4 * ymm6) + ymm2
	vbroadcastss	-4(%rax,%rbx), %ymm7
	vfmadd213ps	%ymm1, %ymm4, %ymm7     ## ymm7 = (ymm4 * ymm7) + ymm1
	vbroadcastss	92(%rax,%rbx), %ymm8
	vfmadd213ps	%ymm0, %ymm4, %ymm8     ## ymm8 = (ymm4 * ymm8) + ymm0
	vmovups	(%rsi,%r15), %ymm4
	vbroadcastss	-192(%rax,%rbx), %ymm3
	vfmadd213ps	%ymm5, %ymm4, %ymm3     ## ymm3 = (ymm4 * ymm3) + ymm5
	vbroadcastss	-96(%rax,%rbx), %ymm2
	vfmadd213ps	%ymm6, %ymm4, %ymm2     ## ymm2 = (ymm4 * ymm2) + ymm6
	vbroadcastss	(%rax,%rbx), %ymm1
	vfmadd213ps	%ymm7, %ymm4, %ymm1     ## ymm1 = (ymm4 * ymm1) + ymm7
	vbroadcastss	96(%rax,%rbx), %ymm0
	vfmadd213ps	%ymm8, %ymm4, %ymm0     ## ymm0 = (ymm4 * ymm0) + ymm8
	addq	%rdx, %rsi
	addq	$8, %rbx
	jne	LBB0_116
## %bb.117:                             ## %"for relu1.s0.w.v12.preheader.1"
                                        ##   in Loop: Header=BB0_113 Depth=2
	vmaxps	%ymm9, %ymm3, %ymm3
	vmovaps	%ymm3, 32(%r12,%r11)
	vmaxps	%ymm9, %ymm2, %ymm2
	movq	%r11, %rsi
	orq	$160, %rsi
	vmovaps	%ymm2, (%r12,%rsi)
	vmaxps	%ymm9, %ymm1, %ymm1
	movq	%r11, %rsi
	orq	$288, %rsi                      ## imm = 0x120
	vmovaps	%ymm1, (%r12,%rsi)
	vmaxps	%ymm9, %ymm0, %ymm0
	movq	%r11, %rsi
	orq	$416, %rsi                      ## imm = 0x1A0
	vmovaps	%ymm0, (%r12,%rsi)
	movq	88(%rsp), %rsi                  ## 8-byte Reload
	vmovaps	64(%rsi,%rcx), %ymm3
	vmovaps	192(%rsi,%rcx), %ymm2
	vmovaps	320(%rsi,%rcx), %ymm1
	vmovaps	448(%rsi,%rcx), %ymm0
	movq	$-96, %rcx
	movq	1296(%rsp), %rbx                ## 8-byte Reload
	.p2align	4, 0x90
LBB0_118:                               ## %"for conv1_stage2.s1.r63$x.2"
                                        ##   Parent Loop BB0_87 Depth=1
                                        ##     Parent Loop BB0_113 Depth=2
                                        ## =>    This Inner Loop Header: Depth=3
	movq	216(%rsp), %rdi                 ## 8-byte Reload
	vmovups	(%rbx,%rdi), %ymm4
	vbroadcastss	-196(%rax,%rcx), %ymm5
	vfmadd213ps	%ymm3, %ymm4, %ymm5     ## ymm5 = (ymm4 * ymm5) + ymm3
	vbroadcastss	-100(%rax,%rcx), %ymm6
	vfmadd213ps	%ymm2, %ymm4, %ymm6     ## ymm6 = (ymm4 * ymm6) + ymm2
	vbroadcastss	-4(%rax,%rcx), %ymm7
	vfmadd213ps	%ymm1, %ymm4, %ymm7     ## ymm7 = (ymm4 * ymm7) + ymm1
	vbroadcastss	92(%rax,%rcx), %ymm8
	vfmadd213ps	%ymm0, %ymm4, %ymm8     ## ymm8 = (ymm4 * ymm8) + ymm0
	vmovups	(%rbx,%r15), %ymm4
	vbroadcastss	-192(%rax,%rcx), %ymm3
	vfmadd213ps	%ymm5, %ymm4, %ymm3     ## ymm3 = (ymm4 * ymm3) + ymm5
	vbroadcastss	-96(%rax,%rcx), %ymm2
	vfmadd213ps	%ymm6, %ymm4, %ymm2     ## ymm2 = (ymm4 * ymm2) + ymm6
	vbroadcastss	(%rax,%rcx), %ymm1
	vfmadd213ps	%ymm7, %ymm4, %ymm1     ## ymm1 = (ymm4 * ymm1) + ymm7
	vbroadcastss	96(%rax,%rcx), %ymm0
	vfmadd213ps	%ymm8, %ymm4, %ymm0     ## ymm0 = (ymm4 * ymm0) + ymm8
	addq	%rdx, %rbx
	addq	$8, %rcx
	jne	LBB0_118
## %bb.119:                             ## %"for relu1.s0.w.v12.preheader.2"
                                        ##   in Loop: Header=BB0_113 Depth=2
	vmaxps	%ymm9, %ymm3, %ymm3
	vmovaps	%ymm3, 64(%r12,%r11)
	vmaxps	%ymm9, %ymm2, %ymm2
	movq	%r11, %rcx
	orq	$192, %rcx
	vmovaps	%ymm2, (%r12,%rcx)
	vmaxps	%ymm9, %ymm1, %ymm1
	movq	%r11, %rcx
	orq	$320, %rcx                      ## imm = 0x140
	vmovaps	%ymm1, (%r12,%rcx)
	vmaxps	%ymm9, %ymm0, %ymm0
	movq	%r11, %rcx
	orq	$448, %rcx                      ## imm = 0x1C0
	vmovaps	%ymm0, (%r12,%rcx)
	shlq	$2, %r14
	orq	$96, %r14
	vmovaps	(%rsi,%r14), %ymm7
	vmovaps	128(%rsi,%r14), %ymm6
	vmovaps	256(%rsi,%r14), %ymm8
	vmovaps	384(%rsi,%r14), %ymm5
	movq	$-96, %rcx
	movq	1376(%rsp), %rdi                ## 8-byte Reload
	.p2align	4, 0x90
LBB0_120:                               ## %"for conv1_stage2.s1.r63$x.3"
                                        ##   Parent Loop BB0_87 Depth=1
                                        ##     Parent Loop BB0_113 Depth=2
                                        ## =>    This Inner Loop Header: Depth=3
	movq	216(%rsp), %rbx                 ## 8-byte Reload
	vmovups	(%rdi,%rbx), %ymm0
	vbroadcastss	-196(%rax,%rcx), %ymm1
	vfmadd213ps	%ymm7, %ymm0, %ymm1     ## ymm1 = (ymm0 * ymm1) + ymm7
	vbroadcastss	-100(%rax,%rcx), %ymm2
	vfmadd213ps	%ymm6, %ymm0, %ymm2     ## ymm2 = (ymm0 * ymm2) + ymm6
	vbroadcastss	-4(%rax,%rcx), %ymm3
	vfmadd213ps	%ymm8, %ymm0, %ymm3     ## ymm3 = (ymm0 * ymm3) + ymm8
	vbroadcastss	92(%rax,%rcx), %ymm4
	vfmadd213ps	%ymm5, %ymm0, %ymm4     ## ymm4 = (ymm0 * ymm4) + ymm5
	vmovups	(%rdi,%r15), %ymm0
	vbroadcastss	-192(%rax,%rcx), %ymm7
	vfmadd213ps	%ymm1, %ymm0, %ymm7     ## ymm7 = (ymm0 * ymm7) + ymm1
	vbroadcastss	-96(%rax,%rcx), %ymm6
	vfmadd213ps	%ymm2, %ymm0, %ymm6     ## ymm6 = (ymm0 * ymm6) + ymm2
	vbroadcastss	(%rax,%rcx), %ymm8
	vfmadd213ps	%ymm3, %ymm0, %ymm8     ## ymm8 = (ymm0 * ymm8) + ymm3
	vbroadcastss	96(%rax,%rcx), %ymm5
	vfmadd213ps	%ymm4, %ymm0, %ymm5     ## ymm5 = (ymm0 * ymm5) + ymm4
	addq	%rdx, %rdi
	addq	$8, %rcx
	jne	LBB0_120
## %bb.121:                             ## %"for relu1.s0.w.v12.preheader.3"
                                        ##   in Loop: Header=BB0_113 Depth=2
	vmaxps	%ymm9, %ymm7, %ymm0
	shlq	$2, %r9
	orq	$96, %r9
	vmovaps	%ymm0, (%r12,%r9)
	vmaxps	%ymm9, %ymm6, %ymm0
	movq	%r11, %rcx
	orq	$224, %rcx
	vmovaps	%ymm0, (%r12,%rcx)
	vmaxps	%ymm9, %ymm8, %ymm0
	movq	%r11, %rcx
	orq	$352, %rcx                      ## imm = 0x160
	vmovaps	%ymm0, (%r12,%rcx)
	vmaxps	%ymm9, %ymm5, %ymm0
	orq	$480, %r11                      ## imm = 0x1E0
	vmovaps	%ymm0, (%r12,%r11)
	incq	%r8
	addq	$384, %rax                      ## imm = 0x180
	cmpq	1328(%rsp), %r8                 ## 8-byte Folded Reload
	jne	LBB0_113
## %bb.122:                             ## %call_destructor.exit317
                                        ##   in Loop: Header=BB0_87 Depth=1
	vmovaps	%ymm8, 1856(%rsp)               ## 32-byte Spill
	vmovaps	%ymm7, 1888(%rsp)               ## 32-byte Spill
	vmovaps	%ymm6, 1440(%rsp)               ## 32-byte Spill
	vmovaps	%ymm5, 1472(%rsp)               ## 32-byte Spill
	xorl	%edi, %edi
	movq	256(%rsp), %rsi                 ## 8-byte Reload
	vzeroupper
	callq	_halide_free
	cmpl	$0, 192(%rsp)                   ## 4-byte Folded Reload
	jle	LBB0_135
## %bb.123:                             ## %"for f1.s1.r79$x.preheader"
                                        ##   in Loop: Header=BB0_87 Depth=1
	cmpb	$0, 103(%rsp)                   ## 1-byte Folded Reload
	je	LBB0_129
## %bb.124:                             ## %vector.body822.preheader
                                        ##   in Loop: Header=BB0_87 Depth=1
	vxorps	%xmm0, %xmm0, %xmm0
	vmovaps	%ymm0, 1824(%rsp)               ## 32-byte Spill
	movq	936(%rsp), %rax                 ## 8-byte Reload
	movq	%rax, 640(%rsp)                 ## 8-byte Spill
	movq	1424(%rsp), %rcx                ## 8-byte Reload
	movq	%rcx, 624(%rsp)                 ## 8-byte Spill
	vmovdqa	LCPI0_59(%rip), %ymm1           ## ymm1 = [0,1,2,3]
	vmovdqa	LCPI0_58(%rip), %ymm13          ## ymm13 = [4,5,6,7]
	vxorps	%xmm2, %xmm2, %xmm2
	vmovaps	%ymm2, 1792(%rsp)               ## 32-byte Spill
	vxorps	%xmm2, %xmm2, %xmm2
	vmovaps	%ymm2, 1760(%rsp)               ## 32-byte Spill
	vxorps	%xmm2, %xmm2, %xmm2
	vmovaps	%ymm2, 1728(%rsp)               ## 32-byte Spill
	.p2align	4, 0x90
LBB0_125:                               ## %vector.body822
                                        ##   Parent Loop BB0_87 Depth=1
                                        ## =>  This Inner Loop Header: Depth=2
	vpbroadcastq	LCPI0_60(%rip), %ymm7   ## ymm7 = [8,8,8,8]
	vpbroadcastq	LCPI0_61(%rip), %ymm8   ## ymm8 = [16,16,16,16]
	vbroadcastsd	LCPI0_62(%rip), %ymm2   ## ymm2 = [24,24,24,24]
	vmovaps	%ymm2, 736(%rsp)                ## 32-byte Spill
	vmovdqa	%ymm1, %ymm6
	vxorps	%xmm4, %xmm4, %xmm4
	vpcmpeqq	%ymm4, %ymm1, %ymm9
	vpsllq	$5, %ymm13, %ymm0
	vpsllq	$5, %ymm1, %ymm1
	vmovq	%xmm1, %r8
	vpextrq	$1, %xmm1, %rax
	vextracti128	$1, %ymm1, %xmm1
	vmovq	%xmm0, %rcx
	vpextrq	$1, %xmm0, %rdx
	vmovss	68(%r12,%rcx,4), %xmm2          ## xmm2 = mem[0],zero,zero,zero
	vinsertps	$16, 68(%r12,%rdx,4), %xmm2, %xmm2 ## xmm2 = xmm2[0],mem[0],xmm2[2,3]
	vmovq	%xmm1, %rsi
	vextracti128	$1, %ymm0, %xmm3
	vmovss	68(%r12,%r8,4), %xmm0           ## xmm0 = mem[0],zero,zero,zero
	vpextrq	$1, %xmm1, %rbx
	vinsertps	$16, 68(%r12,%rax,4), %xmm0, %xmm0 ## xmm0 = xmm0[0],mem[0],xmm0[2,3]
	vpextrq	$1, %xmm3, %rdi
	vmovq	%xmm3, %r9
	vmovss	72(%r12,%rcx,4), %xmm1          ## xmm1 = mem[0],zero,zero,zero
	vinsertps	$32, 68(%r12,%r9,4), %xmm2, %xmm2 ## xmm2 = xmm2[0,1],mem[0],xmm2[3]
	vinsertps	$16, 72(%r12,%rdx,4), %xmm1, %xmm1 ## xmm1 = xmm1[0],mem[0],xmm1[2,3]
	vinsertps	$32, 68(%r12,%rsi,4), %xmm0, %xmm3 ## xmm3 = xmm0[0,1],mem[0],xmm0[3]
	vinsertps	$32, 72(%r12,%r9,4), %xmm1, %xmm1 ## xmm1 = xmm1[0,1],mem[0],xmm1[3]
	vinsertps	$48, 68(%r12,%rdi,4), %xmm2, %xmm0 ## xmm0 = xmm2[0,1,2],mem[0]
	vmovaps	%xmm0, 256(%rsp)                ## 16-byte Spill
	vpcmpeqq	%ymm4, %ymm13, %ymm4
	vmovss	72(%r12,%r8,4), %xmm5           ## xmm5 = mem[0],zero,zero,zero
	vinsertps	$48, 68(%r12,%rbx,4), %xmm3, %xmm14 ## xmm14 = xmm3[0,1,2],mem[0]
	vinsertps	$16, 72(%r12,%rax,4), %xmm5, %xmm3 ## xmm3 = xmm5[0],mem[0],xmm5[2,3]
	vinsertps	$48, 72(%r12,%rdi,4), %xmm1, %xmm1 ## xmm1 = xmm1[0,1,2],mem[0]
	vinsertps	$32, 72(%r12,%rsi,4), %xmm3, %xmm3 ## xmm3 = xmm3[0,1],mem[0],xmm3[3]
	vinsertps	$48, 72(%r12,%rbx,4), %xmm3, %xmm3 ## xmm3 = xmm3[0,1,2],mem[0]
	vmovss	80(%r12,%rcx,4), %xmm5          ## xmm5 = mem[0],zero,zero,zero
	vinsertps	$16, 80(%r12,%rdx,4), %xmm5, %xmm5 ## xmm5 = xmm5[0],mem[0],xmm5[2,3]
	vpaddq	%ymm7, %ymm13, %ymm11
	vperm2i128	$49, %ymm4, %ymm9, %ymm10 ## ymm10 = ymm9[2,3],ymm4[2,3]
	movq	%r9, %r10
	vinsertps	$32, 80(%r12,%r9,4), %xmm5, %xmm5 ## xmm5 = xmm5[0,1],mem[0],xmm5[3]
	vmovdqa	%ymm6, %ymm0
	vmovdqa	%ymm6, 2208(%rsp)               ## 32-byte Spill
	vpaddq	%ymm7, %ymm6, %ymm7
	vinserti128	$1, %xmm4, %ymm9, %ymm9
	movq	%rdi, 384(%rsp)                 ## 8-byte Spill
	vinsertps	$48, 80(%r12,%rdi,4), %xmm5, %xmm6 ## xmm6 = xmm5[0,1,2],mem[0]
	vpaddq	%ymm8, %ymm13, %ymm12
	vmovdqa	%ymm13, 2752(%rsp)              ## 32-byte Spill
	vmovss	80(%r12,%r8,4), %xmm5           ## xmm5 = mem[0],zero,zero,zero
	vinsertps	$16, 80(%r12,%rax,4), %xmm5, %xmm4 ## xmm4 = xmm5[0],mem[0],xmm5[2,3]
	vpaddq	%ymm0, %ymm8, %ymm2
	vpackssdw	%ymm10, %ymm9, %ymm5
	vinsertps	$32, 80(%r12,%rsi,4), %xmm4, %xmm4 ## xmm4 = xmm4[0,1],mem[0],xmm4[3]
	vpxor	%xmm0, %xmm0, %xmm0
	vpcmpeqq	%ymm0, %ymm7, %ymm8
	vpcmpeqq	%ymm0, %ymm11, %ymm9
	movq	%rbx, 40(%rsp)                  ## 8-byte Spill
	vinsertps	$48, 80(%r12,%rbx,4), %xmm4, %xmm10 ## xmm10 = xmm4[0,1,2],mem[0]
	vperm2i128	$49, %ymm9, %ymm8, %ymm0 ## ymm0 = ymm8[2,3],ymm9[2,3]
	vmovdqa	%ymm0, 544(%rsp)                ## 32-byte Spill
	vmovss	84(%r12,%rcx,4), %xmm0          ## xmm0 = mem[0],zero,zero,zero
	vinsertps	$16, 84(%r12,%rdx,4), %xmm0, %xmm0 ## xmm0 = xmm0[0],mem[0],xmm0[2,3]
	vinserti128	$1, %xmm9, %ymm8, %ymm15
	vmovdqa	736(%rsp), %ymm8                ## 32-byte Reload
	vpaddq	%ymm8, %ymm13, %ymm4
	vmovdqa	%ymm4, 1152(%rsp)               ## 32-byte Spill
	vinsertps	$32, 84(%r12,%r9,4), %xmm0, %xmm0 ## xmm0 = xmm0[0,1],mem[0],xmm0[3]
	vinsertf128	$1, 256(%rsp), %ymm14, %ymm14 ## 16-byte Folded Reload
	vpxor	%xmm13, %xmm13, %xmm13
	vpcmpeqq	%ymm13, %ymm2, %ymm9
	vinsertps	$48, 84(%r12,%rdi,4), %xmm0, %xmm0 ## xmm0 = xmm0[0,1,2],mem[0]
	vinsertf128	$1, %xmm1, %ymm3, %ymm3
	vmovss	84(%r12,%r8,4), %xmm1           ## xmm1 = mem[0],zero,zero,zero
	movq	%r8, 288(%rsp)                  ## 8-byte Spill
	vinsertps	$16, 84(%r12,%rax,4), %xmm1, %xmm1 ## xmm1 = xmm1[0],mem[0],xmm1[2,3]
	vinsertf128	$1, %xmm6, %ymm10, %ymm4
	vpcmpeqq	%ymm13, %ymm12, %ymm10
	movq	%rsi, 400(%rsp)                 ## 8-byte Spill
	vinsertps	$32, 84(%r12,%rsi,4), %xmm1, %xmm6 ## xmm6 = xmm1[0,1],mem[0],xmm1[3]
	vpsllq	$5, %ymm11, %ymm1
	vinsertps	$48, 84(%r12,%rbx,4), %xmm6, %xmm6 ## xmm6 = xmm6[0,1,2],mem[0]
	vinsertf128	$1, %xmm0, %ymm6, %ymm0
	vperm2i128	$49, %ymm10, %ymm9, %ymm11 ## ymm11 = ymm9[2,3],ymm10[2,3]
	vpsllq	$5, %ymm7, %ymm7
	vpsllq	$5, %ymm12, %ymm6
	vpsllq	$5, %ymm2, %ymm13
	vmovq	%xmm7, %r9
	movq	%r9, 32(%rsp)                   ## 8-byte Spill
	vpextrq	$1, %xmm7, %rdi
	vblendvps	%ymm5, %ymm14, %ymm3, %ymm2
	vmovaps	%ymm2, 1696(%rsp)               ## 32-byte Spill
	vmovss	(%r12,%rcx,4), %xmm3            ## xmm3 = mem[0],zero,zero,zero
	movq	%rcx, 176(%rsp)                 ## 8-byte Spill
	vinsertps	$16, (%r12,%rdx,4), %xmm3, %xmm3 ## xmm3 = xmm3[0],mem[0],xmm3[2,3]
	movq	%rdx, 160(%rsp)                 ## 8-byte Spill
	vextracti128	$1, %ymm7, %xmm7
	vinsertps	$32, (%r12,%r10,4), %xmm3, %xmm2 ## xmm2 = xmm3[0,1],mem[0],xmm3[3]
	vmovaps	%xmm2, 1664(%rsp)               ## 16-byte Spill
	movq	%r10, 320(%rsp)                 ## 8-byte Spill
	vblendvps	%ymm5, %ymm4, %ymm0, %ymm0
	vmovaps	%ymm0, 2368(%rsp)               ## 32-byte Spill
	vmovss	(%r12,%r8,4), %xmm0             ## xmm0 = mem[0],zero,zero,zero
	movq	%rax, 144(%rsp)                 ## 8-byte Spill
	vinsertps	$16, (%r12,%rax,4), %xmm0, %xmm0 ## xmm0 = xmm0[0],mem[0],xmm0[2,3]
	vmovq	%xmm1, %r8
	vextracti128	$1, %ymm1, %xmm5
	vinsertps	$32, (%r12,%rsi,4), %xmm0, %xmm14 ## xmm14 = xmm0[0,1],mem[0],xmm0[3]
	vpackssdw	544(%rsp), %ymm15, %ymm15 ## 32-byte Folded Reload
	vmovss	4(%r12,%rcx,4), %xmm0           ## xmm0 = mem[0],zero,zero,zero
	vinsertps	$16, 4(%r12,%rdx,4), %xmm0, %xmm0 ## xmm0 = xmm0[0],mem[0],xmm0[2,3]
	vmovq	%xmm7, %r13
	vinsertps	$32, 4(%r12,%r10,4), %xmm0, %xmm12 ## xmm12 = xmm0[0,1],mem[0],xmm0[3]
	vpextrq	$1, %xmm7, 80(%rsp)             ## 8-byte Folded Spill
	vextracti128	$1, %ymm13, %xmm4
	vpextrq	$1, %xmm1, %rax
	vextracti128	$1, %ymm6, %xmm1
	vmovq	%xmm5, %rbx
	vpextrq	$1, %xmm5, %rcx
	vmovss	68(%r12,%r8,4), %xmm5           ## xmm5 = mem[0],zero,zero,zero
	vinsertps	$16, 68(%r12,%rax,4), %xmm5, %xmm5 ## xmm5 = xmm5[0],mem[0],xmm5[2,3]
	movq	%rax, %rsi
	movq	%rax, 544(%rsp)                 ## 8-byte Spill
	vpextrq	$1, %xmm13, %r11
	vmovq	%xmm13, %rdx
	movq	%rdx, 48(%rsp)                  ## 8-byte Spill
	vmovss	68(%r12,%r9,4), %xmm7           ## xmm7 = mem[0],zero,zero,zero
	vinsertps	$32, 68(%r12,%rbx,4), %xmm5, %xmm13 ## xmm13 = xmm5[0,1],mem[0],xmm5[3]
	movq	%rbx, %r9
	movq	%rbx, 336(%rsp)                 ## 8-byte Spill
	movq	%rdi, 256(%rsp)                 ## 8-byte Spill
	vinsertps	$16, 68(%r12,%rdi,4), %xmm7, %xmm7 ## xmm7 = xmm7[0],mem[0],xmm7[2,3]
	vpextrq	$1, %xmm6, %rax
	vmovq	%xmm6, %rbx
	vmovss	68(%r12,%rbx,4), %xmm6          ## xmm6 = mem[0],zero,zero,zero
	movq	%rbx, %r14
	movq	%rbx, 224(%rsp)                 ## 8-byte Spill
	vinsertps	$32, 68(%r12,%r13,4), %xmm7, %xmm7 ## xmm7 = xmm7[0,1],mem[0],xmm7[3]
	movq	%r13, 304(%rsp)                 ## 8-byte Spill
	vinsertps	$16, 68(%r12,%rax,4), %xmm6, %xmm6 ## xmm6 = xmm6[0],mem[0],xmm6[2,3]
	movq	%rax, %r10
	movq	%rax, 56(%rsp)                  ## 8-byte Spill
	vpextrq	$1, %xmm1, %rbx
	vmovq	%xmm1, %rax
	vmovss	68(%r12,%rdx,4), %xmm1          ## xmm1 = mem[0],zero,zero,zero
	vinsertps	$32, 68(%r12,%rax,4), %xmm6, %xmm6 ## xmm6 = xmm6[0,1],mem[0],xmm6[3]
	movq	%rax, %r15
	movq	%rax, 72(%rsp)                  ## 8-byte Spill
	vinsertps	$16, 68(%r12,%r11,4), %xmm1, %xmm1 ## xmm1 = xmm1[0],mem[0],xmm1[2,3]
	movq	%r11, %rdx
	movq	%r11, 480(%rsp)                 ## 8-byte Spill
	vpextrq	$1, %xmm4, %r11
	vmovq	%xmm4, %rax
	vmovss	72(%r12,%r8,4), %xmm4           ## xmm4 = mem[0],zero,zero,zero
	movq	%r8, 352(%rsp)                  ## 8-byte Spill
	vinsertps	$32, 68(%r12,%rax,4), %xmm1, %xmm1 ## xmm1 = xmm1[0,1],mem[0],xmm1[3]
	movq	%rax, 64(%rsp)                  ## 8-byte Spill
	vinsertps	$16, 72(%r12,%rsi,4), %xmm4, %xmm4 ## xmm4 = xmm4[0],mem[0],xmm4[2,3]
	vinsertps	$32, 72(%r12,%r9,4), %xmm4, %xmm4 ## xmm4 = xmm4[0,1],mem[0],xmm4[3]
	vinserti128	$1, %xmm10, %ymm9, %ymm9
	movq	32(%rsp), %rsi                  ## 8-byte Reload
	vmovss	72(%r12,%rsi,4), %xmm5          ## xmm5 = mem[0],zero,zero,zero
	vinsertps	$48, 68(%r12,%rcx,4), %xmm13, %xmm10 ## xmm10 = xmm13[0,1,2],mem[0]
	vinsertps	$16, 72(%r12,%rdi,4), %xmm5, %xmm5 ## xmm5 = xmm5[0],mem[0],xmm5[2,3]
	movq	80(%rsp), %rsi                  ## 8-byte Reload
	vinsertps	$48, 68(%r12,%rsi,4), %xmm7, %xmm7 ## xmm7 = xmm7[0,1,2],mem[0]
	vinsertps	$32, 72(%r12,%r13,4), %xmm5, %xmm5 ## xmm5 = xmm5[0,1],mem[0],xmm5[3]
	vinsertps	$48, 68(%r12,%rbx,4), %xmm6, %xmm13 ## xmm13 = xmm6[0,1,2],mem[0]
	vpackssdw	%ymm11, %ymm9, %ymm9
	vmovss	72(%r12,%r14,4), %xmm6          ## xmm6 = mem[0],zero,zero,zero
	vinsertps	$48, 68(%r12,%r11,4), %xmm1, %xmm1 ## xmm1 = xmm1[0,1,2],mem[0]
	vinsertps	$16, 72(%r12,%r10,4), %xmm6, %xmm6 ## xmm6 = xmm6[0],mem[0],xmm6[2,3]
	vinsertps	$48, 72(%r12,%rcx,4), %xmm4, %xmm4 ## xmm4 = xmm4[0,1,2],mem[0]
	movq	%rcx, %r14
	vinsertps	$32, 72(%r12,%r15,4), %xmm6, %xmm6 ## xmm6 = xmm6[0,1],mem[0],xmm6[3]
	vinsertps	$48, 72(%r12,%rsi,4), %xmm5, %xmm5 ## xmm5 = xmm5[0,1,2],mem[0]
	vpaddq	2208(%rsp), %ymm8, %ymm11       ## 32-byte Folded Reload
	vinsertps	$48, 72(%r12,%rbx,4), %xmm6, %xmm6 ## xmm6 = xmm6[0,1,2],mem[0]
	vinsertf128	$1, %xmm10, %ymm7, %ymm7
	movq	48(%rsp), %r10                  ## 8-byte Reload
	vmovss	72(%r12,%r10,4), %xmm2          ## xmm2 = mem[0],zero,zero,zero
	vinsertps	$16, 72(%r12,%rdx,4), %xmm2, %xmm2 ## xmm2 = xmm2[0],mem[0],xmm2[2,3]
	vinsertf128	$1, %xmm13, %ymm1, %ymm1
	vxorps	%xmm0, %xmm0, %xmm0
	vpcmpeqq	%ymm0, %ymm11, %ymm10
	vinsertps	$32, 72(%r12,%rax,4), %xmm2, %xmm2 ## xmm2 = xmm2[0,1],mem[0],xmm2[3]
	vinsertf128	$1, %xmm4, %ymm5, %ymm4
	vmovdqa	1152(%rsp), %ymm3               ## 32-byte Reload
	vpcmpeqq	%ymm0, %ymm3, %ymm5
	vxorps	%xmm13, %xmm13, %xmm13
	vinsertps	$48, 72(%r12,%r11,4), %xmm2, %xmm2 ## xmm2 = xmm2[0,1,2],mem[0]
	movq	%r11, %r13
	vinsertf128	$1, %xmm6, %ymm2, %ymm2
	movq	288(%rsp), %rsi                 ## 8-byte Reload
	vmovss	4(%r12,%rsi,4), %xmm6           ## xmm6 = mem[0],zero,zero,zero
	movq	144(%rsp), %rdx                 ## 8-byte Reload
	vinsertps	$16, 4(%r12,%rdx,4), %xmm6, %xmm6 ## xmm6 = xmm6[0],mem[0],xmm6[2,3]
	vpsllq	$5, %ymm3, %ymm8
	vpsllq	$5, %ymm11, %ymm11
	vblendvps	%ymm15, %ymm7, %ymm4, %ymm0
	vmovaps	%ymm0, 1984(%rsp)               ## 32-byte Spill
	vextracti128	$1, %ymm11, %xmm7
	vmovq	%xmm11, %r9
	movq	%r9, 112(%rsp)                  ## 8-byte Spill
	vpextrq	$1, %xmm11, %r11
	vextracti128	$1, %ymm8, %xmm4
	vmovq	%xmm8, %rcx
	movq	%rcx, 464(%rsp)                 ## 8-byte Spill
	vpextrq	$1, %xmm8, %r15
	movq	%r15, 736(%rsp)                 ## 8-byte Spill
	vmovss	68(%r12,%rcx,4), %xmm3          ## xmm3 = mem[0],zero,zero,zero
	vinsertps	$16, 68(%r12,%r15,4), %xmm3, %xmm3 ## xmm3 = xmm3[0],mem[0],xmm3[2,3]
	vpextrq	$1, %xmm4, %rdx
	vmovq	%xmm4, %rsi
	movq	%rsi, 448(%rsp)                 ## 8-byte Spill
	vmovss	68(%r12,%r9,4), %xmm4           ## xmm4 = mem[0],zero,zero,zero
	vinsertps	$32, 68(%r12,%rsi,4), %xmm3, %xmm3 ## xmm3 = xmm3[0,1],mem[0],xmm3[3]
	vinsertps	$16, 68(%r12,%r11,4), %xmm4, %xmm4 ## xmm4 = xmm4[0],mem[0],xmm4[2,3]
	movq	%r11, 1152(%rsp)                ## 8-byte Spill
	vpextrq	$1, %xmm7, %rax
	vmovq	%xmm7, %rdi
	vmovss	72(%r12,%rcx,4), %xmm7          ## xmm7 = mem[0],zero,zero,zero
	movq	%rdi, 104(%rsp)                 ## 8-byte Spill
	vinsertps	$32, 68(%r12,%rdi,4), %xmm4, %xmm4 ## xmm4 = xmm4[0,1],mem[0],xmm4[3]
	vinsertps	$16, 72(%r12,%r15,4), %xmm7, %xmm7 ## xmm7 = xmm7[0],mem[0],xmm7[2,3]
	movq	%rdx, 120(%rsp)                 ## 8-byte Spill
	vinsertps	$48, 68(%r12,%rdx,4), %xmm3, %xmm3 ## xmm3 = xmm3[0,1,2],mem[0]
	vinsertps	$32, 72(%r12,%rsi,4), %xmm7, %xmm7 ## xmm7 = xmm7[0,1],mem[0],xmm7[3]
	movq	%rax, 128(%rsp)                 ## 8-byte Spill
	vinsertps	$48, 68(%r12,%rax,4), %xmm4, %xmm4 ## xmm4 = xmm4[0,1,2],mem[0]
	vinsertps	$48, 72(%r12,%rdx,4), %xmm7, %xmm7 ## xmm7 = xmm7[0,1,2],mem[0]
	vblendvps	%ymm9, %ymm1, %ymm2, %ymm0
	vmovaps	%ymm0, 1344(%rsp)               ## 32-byte Spill
	vmovss	72(%r12,%r9,4), %xmm1           ## xmm1 = mem[0],zero,zero,zero
	vinsertps	$16, 72(%r12,%r11,4), %xmm1, %xmm1 ## xmm1 = xmm1[0],mem[0],xmm1[2,3]
	vperm2i128	$49, %ymm5, %ymm10, %ymm2 ## ymm2 = ymm10[2,3],ymm5[2,3]
	vinserti128	$1, %xmm5, %ymm10, %ymm5
	vinsertps	$32, 72(%r12,%rdi,4), %xmm1, %xmm1 ## xmm1 = xmm1[0,1],mem[0],xmm1[3]
	vmovss	80(%r12,%r8,4), %xmm0           ## xmm0 = mem[0],zero,zero,zero
	movq	544(%rsp), %r11                 ## 8-byte Reload
	vinsertps	$16, 80(%r12,%r11,4), %xmm0, %xmm0 ## xmm0 = xmm0[0],mem[0],xmm0[2,3]
	vinsertps	$48, 72(%r12,%rax,4), %xmm1, %xmm8 ## xmm8 = xmm1[0,1,2],mem[0]
	vpackssdw	%ymm2, %ymm5, %ymm1
	movq	336(%rsp), %rax                 ## 8-byte Reload
	vinsertps	$32, 80(%r12,%rax,4), %xmm0, %xmm0 ## xmm0 = xmm0[0,1],mem[0],xmm0[3]
	movq	32(%rsp), %rdi                  ## 8-byte Reload
	vmovss	80(%r12,%rdi,4), %xmm2          ## xmm2 = mem[0],zero,zero,zero
	movq	256(%rsp), %rsi                 ## 8-byte Reload
	vinsertps	$16, 80(%r12,%rsi,4), %xmm2, %xmm2 ## xmm2 = xmm2[0],mem[0],xmm2[2,3]
	movq	%r14, %r8
	movq	%r14, 240(%rsp)                 ## 8-byte Spill
	vinsertps	$48, 80(%r12,%r14,4), %xmm0, %xmm0 ## xmm0 = xmm0[0,1,2],mem[0]
	vinsertf128	$1, %xmm3, %ymm4, %ymm3
	movq	304(%rsp), %r9                  ## 8-byte Reload
	vinsertps	$32, 80(%r12,%r9,4), %xmm2, %xmm2 ## xmm2 = xmm2[0,1],mem[0],xmm2[3]
	movq	224(%rsp), %rdx                 ## 8-byte Reload
	vmovss	80(%r12,%rdx,4), %xmm4          ## xmm4 = mem[0],zero,zero,zero
	movq	56(%rsp), %r14                  ## 8-byte Reload
	vinsertps	$16, 80(%r12,%r14,4), %xmm4, %xmm4 ## xmm4 = xmm4[0],mem[0],xmm4[2,3]
	movq	80(%rsp), %r15                  ## 8-byte Reload
	vinsertps	$48, 80(%r12,%r15,4), %xmm2, %xmm2 ## xmm2 = xmm2[0,1,2],mem[0]
	vinsertf128	$1, %xmm7, %ymm8, %ymm5
	movq	72(%rsp), %rax                  ## 8-byte Reload
	vinsertps	$32, 80(%r12,%rax,4), %xmm4, %xmm4 ## xmm4 = xmm4[0,1],mem[0],xmm4[3]
	movq	%r10, %rcx
	vmovss	80(%r12,%r10,4), %xmm7          ## xmm7 = mem[0],zero,zero,zero
	movq	480(%rsp), %rsi                 ## 8-byte Reload
	vinsertps	$16, 80(%r12,%rsi,4), %xmm7, %xmm7 ## xmm7 = xmm7[0],mem[0],xmm7[2,3]
	movq	%rbx, 432(%rsp)                 ## 8-byte Spill
	vinsertps	$48, 80(%r12,%rbx,4), %xmm4, %xmm4 ## xmm4 = xmm4[0,1,2],mem[0]
	movq	64(%rsp), %rax                  ## 8-byte Reload
	vinsertps	$32, 80(%r12,%rax,4), %xmm7, %xmm7 ## xmm7 = xmm7[0,1],mem[0],xmm7[3]
	movq	%r13, %r10
	movq	%r13, 416(%rsp)                 ## 8-byte Spill
	vinsertps	$48, 80(%r12,%r13,4), %xmm7, %xmm7 ## xmm7 = xmm7[0,1,2],mem[0]
	vinsertf128	$1, %xmm0, %ymm2, %ymm0
	vinsertf128	$1, %xmm4, %ymm7, %ymm2
	vblendvps	%ymm1, %ymm3, %ymm5, %ymm3
	vmovaps	%ymm3, 1568(%rsp)               ## 32-byte Spill
	movq	352(%rsp), %r13                 ## 8-byte Reload
	vmovss	84(%r12,%r13,4), %xmm3          ## xmm3 = mem[0],zero,zero,zero
	vmovss	84(%r12,%rdi,4), %xmm4          ## xmm4 = mem[0],zero,zero,zero
	vinsertps	$16, 84(%r12,%r11,4), %xmm3, %xmm3 ## xmm3 = xmm3[0],mem[0],xmm3[2,3]
	movq	256(%rsp), %rdi                 ## 8-byte Reload
	vinsertps	$16, 84(%r12,%rdi,4), %xmm4, %xmm4 ## xmm4 = xmm4[0],mem[0],xmm4[2,3]
	movq	336(%rsp), %rdi                 ## 8-byte Reload
	vinsertps	$32, 84(%r12,%rdi,4), %xmm3, %xmm3 ## xmm3 = xmm3[0,1],mem[0],xmm3[3]
	vinsertps	$32, 84(%r12,%r9,4), %xmm4, %xmm4 ## xmm4 = xmm4[0,1],mem[0],xmm4[3]
	vinsertps	$48, 84(%r12,%r8,4), %xmm3, %xmm3 ## xmm3 = xmm3[0,1,2],mem[0]
	vinsertps	$48, 84(%r12,%r15,4), %xmm4, %xmm4 ## xmm4 = xmm4[0,1,2],mem[0]
	vinsertf128	$1, %xmm3, %ymm4, %ymm3
	vmovss	84(%r12,%rdx,4), %xmm4          ## xmm4 = mem[0],zero,zero,zero
	vinsertps	$16, 84(%r12,%r14,4), %xmm4, %xmm4 ## xmm4 = xmm4[0],mem[0],xmm4[2,3]
	vblendvps	%ymm15, %ymm0, %ymm3, %ymm0
	vmovaps	%ymm0, 2016(%rsp)               ## 32-byte Spill
	movq	464(%rsp), %r11                 ## 8-byte Reload
	vmovss	80(%r12,%r11,4), %xmm0          ## xmm0 = mem[0],zero,zero,zero
	vmovss	84(%r12,%rcx,4), %xmm3          ## xmm3 = mem[0],zero,zero,zero
	movq	72(%rsp), %rcx                  ## 8-byte Reload
	vinsertps	$32, 84(%r12,%rcx,4), %xmm4, %xmm4 ## xmm4 = xmm4[0,1],mem[0],xmm4[3]
	vinsertps	$16, 84(%r12,%rsi,4), %xmm3, %xmm3 ## xmm3 = xmm3[0],mem[0],xmm3[2,3]
	vinsertps	$48, 84(%r12,%rbx,4), %xmm4, %xmm4 ## xmm4 = xmm4[0,1,2],mem[0]
	vinsertps	$32, 84(%r12,%rax,4), %xmm3, %xmm3 ## xmm3 = xmm3[0,1],mem[0],xmm3[3]
	vinsertps	$48, 84(%r12,%r10,4), %xmm3, %xmm3 ## xmm3 = xmm3[0,1,2],mem[0]
	vinsertf128	$1, %xmm4, %ymm3, %ymm3
	movq	112(%rsp), %rdx                 ## 8-byte Reload
	vmovss	80(%r12,%rdx,4), %xmm4          ## xmm4 = mem[0],zero,zero,zero
	movq	736(%rsp), %rax                 ## 8-byte Reload
	vinsertps	$16, 80(%r12,%rax,4), %xmm0, %xmm0 ## xmm0 = xmm0[0],mem[0],xmm0[2,3]
	movq	1152(%rsp), %rdi                ## 8-byte Reload
	vinsertps	$16, 80(%r12,%rdi,4), %xmm4, %xmm4 ## xmm4 = xmm4[0],mem[0],xmm4[2,3]
	movq	448(%rsp), %rsi                 ## 8-byte Reload
	vinsertps	$32, 80(%r12,%rsi,4), %xmm0, %xmm0 ## xmm0 = xmm0[0,1],mem[0],xmm0[3]
	movq	104(%rsp), %rbx                 ## 8-byte Reload
	vinsertps	$32, 80(%r12,%rbx,4), %xmm4, %xmm4 ## xmm4 = xmm4[0,1],mem[0],xmm4[3]
	vblendvps	%ymm9, %ymm2, %ymm3, %ymm2
	vmovaps	%ymm2, 1600(%rsp)               ## 32-byte Spill
	movq	288(%rsp), %r8                  ## 8-byte Reload
	vmovss	8(%r12,%r8,4), %xmm2            ## xmm2 = mem[0],zero,zero,zero
	vmovss	84(%r12,%r11,4), %xmm3          ## xmm3 = mem[0],zero,zero,zero
	movq	120(%rsp), %rcx                 ## 8-byte Reload
	vinsertps	$48, 80(%r12,%rcx,4), %xmm0, %xmm0 ## xmm0 = xmm0[0,1,2],mem[0]
	vinsertps	$16, 84(%r12,%rax,4), %xmm3, %xmm3 ## xmm3 = xmm3[0],mem[0],xmm3[2,3]
	movq	128(%rsp), %rax                 ## 8-byte Reload
	vinsertps	$48, 80(%r12,%rax,4), %xmm4, %xmm4 ## xmm4 = xmm4[0,1,2],mem[0]
	vinsertf128	$1, %xmm0, %ymm4, %ymm0
	vmovss	84(%r12,%rdx,4), %xmm4          ## xmm4 = mem[0],zero,zero,zero
	vinsertps	$32, 84(%r12,%rsi,4), %xmm3, %xmm3 ## xmm3 = xmm3[0,1],mem[0],xmm3[3]
	vinsertps	$16, 84(%r12,%rdi,4), %xmm4, %xmm4 ## xmm4 = xmm4[0],mem[0],xmm4[2,3]
	movq	640(%rsp), %r14                 ## 8-byte Reload
	vinsertps	$48, 84(%r12,%rcx,4), %xmm3, %xmm3 ## xmm3 = xmm3[0,1,2],mem[0]
	vinsertps	$32, 84(%r12,%rbx,4), %xmm4, %xmm4 ## xmm4 = xmm4[0,1],mem[0],xmm4[3]
	vinsertps	$48, 84(%r12,%rax,4), %xmm4, %xmm4 ## xmm4 = xmm4[0,1,2],mem[0]
	vinsertf128	$1, %xmm3, %ymm4, %ymm3
	movq	176(%rsp), %rax                 ## 8-byte Reload
	vmovss	8(%r12,%rax,4), %xmm4           ## xmm4 = mem[0],zero,zero,zero
	movq	400(%rsp), %rdx                 ## 8-byte Reload
	vinsertps	$32, 4(%r12,%rdx,4), %xmm6, %xmm6 ## xmm6 = xmm6[0,1],mem[0],xmm6[3]
	movq	160(%rsp), %rcx                 ## 8-byte Reload
	vinsertps	$16, 8(%r12,%rcx,4), %xmm4, %xmm4 ## xmm4 = xmm4[0],mem[0],xmm4[2,3]
	movq	384(%rsp), %rbx                 ## 8-byte Reload
	vmovaps	1664(%rsp), %xmm5               ## 16-byte Reload
	vinsertps	$48, (%r12,%rbx,4), %xmm5, %xmm7 ## xmm7 = xmm5[0,1,2],mem[0]
	movq	320(%rsp), %r15                 ## 8-byte Reload
	vinsertps	$32, 8(%r12,%r15,4), %xmm4, %xmm4 ## xmm4 = xmm4[0,1],mem[0],xmm4[3]
	movq	40(%rsp), %rdi                  ## 8-byte Reload
	vinsertps	$48, (%r12,%rdi,4), %xmm14, %xmm8 ## xmm8 = xmm14[0,1,2],mem[0]
	movq	144(%rsp), %rsi                 ## 8-byte Reload
	vinsertps	$16, 8(%r12,%rsi,4), %xmm2, %xmm2 ## xmm2 = xmm2[0],mem[0],xmm2[2,3]
	vinsertps	$48, 4(%r12,%rbx,4), %xmm12, %xmm5 ## xmm5 = xmm12[0,1,2],mem[0]
	vinsertps	$32, 8(%r12,%rdx,4), %xmm2, %xmm2 ## xmm2 = xmm2[0,1],mem[0],xmm2[3]
	vblendvps	%ymm1, %ymm0, %ymm3, %ymm0
	vmovaps	%ymm0, 2400(%rsp)               ## 32-byte Spill
	vmovss	12(%r12,%rax,4), %xmm0          ## xmm0 = mem[0],zero,zero,zero
	vinsertps	$48, 4(%r12,%rdi,4), %xmm6, %xmm1 ## xmm1 = xmm6[0,1,2],mem[0]
	vinsertps	$16, 12(%r12,%rcx,4), %xmm0, %xmm0 ## xmm0 = xmm0[0],mem[0],xmm0[2,3]
	vinsertps	$48, 8(%r12,%rbx,4), %xmm4, %xmm4 ## xmm4 = xmm4[0,1,2],mem[0]
	vinsertps	$32, 12(%r12,%r15,4), %xmm0, %xmm3 ## xmm3 = xmm0[0,1],mem[0],xmm0[3]
	vinsertps	$48, 8(%r12,%rdi,4), %xmm2, %xmm2 ## xmm2 = xmm2[0,1,2],mem[0]
	movq	656(%rsp), %rax                 ## 8-byte Reload
	vmovups	(%r14,%rax,4), %ymm10
	movq	%rax, %r13
	vinsertps	$48, 12(%r12,%rbx,4), %xmm3, %xmm6 ## xmm6 = xmm3[0,1,2],mem[0]
	vinsertf128	$1, %xmm7, %ymm8, %ymm7
	vmovss	12(%r12,%r8,4), %xmm3           ## xmm3 = mem[0],zero,zero,zero
	vinsertps	$16, 12(%r12,%rsi,4), %xmm3, %xmm0 ## xmm0 = xmm3[0],mem[0],xmm3[2,3]
	vinsertf128	$1, %xmm5, %ymm1, %ymm1
	movq	1128(%rsp), %r15                ## 8-byte Reload
	vmovups	(%r14,%r15,4), %ymm9
	vinsertps	$32, 12(%r12,%rdx,4), %xmm0, %xmm0 ## xmm0 = xmm0[0,1],mem[0],xmm0[3]
	vmulps	%ymm1, %ymm9, %ymm1
	vinsertps	$48, 12(%r12,%rdi,4), %xmm0, %xmm0 ## xmm0 = xmm0[0,1,2],mem[0]
	movq	1120(%rsp), %r11                ## 8-byte Reload
	vmulps	(%r14,%r11,4), %ymm10, %ymm5
	vmovaps	%ymm10, 1504(%rsp)              ## 32-byte Spill
	vfmadd231ps	%ymm7, %ymm5, %ymm1     ## ymm1 = (ymm5 * ymm7) + ymm1
	movq	352(%rsp), %rcx                 ## 8-byte Reload
	vmovss	(%r12,%rcx,4), %xmm7            ## xmm7 = mem[0],zero,zero,zero
	movq	544(%rsp), %rsi                 ## 8-byte Reload
	vinsertps	$16, (%r12,%rsi,4), %xmm7, %xmm7 ## xmm7 = xmm7[0],mem[0],xmm7[2,3]
	vinsertf128	$1, %xmm4, %ymm2, %ymm2
	movq	32(%rsp), %rdi                  ## 8-byte Reload
	vmovss	(%r12,%rdi,4), %xmm4            ## xmm4 = mem[0],zero,zero,zero
	movq	256(%rsp), %r10                 ## 8-byte Reload
	vinsertps	$16, (%r12,%r10,4), %xmm4, %xmm4 ## xmm4 = xmm4[0],mem[0],xmm4[2,3]
	vinsertf128	$1, %xmm6, %ymm0, %ymm0
	vmulps	%ymm0, %ymm9, %ymm0
	vmovaps	%ymm9, 1536(%rsp)               ## 32-byte Spill
	movq	528(%rsp), %rax                 ## 8-byte Reload
	vcmpeqps	(%r14,%rax,4), %ymm13, %ymm6
	vfmadd231ps	%ymm2, %ymm5, %ymm0     ## ymm0 = (ymm5 * ymm2) + ymm0
	vmovss	4(%r12,%rcx,4), %xmm2           ## xmm2 = mem[0],zero,zero,zero
	movq	336(%rsp), %rdx                 ## 8-byte Reload
	vinsertps	$32, (%r12,%rdx,4), %xmm7, %xmm5 ## xmm5 = xmm7[0,1],mem[0],xmm7[3]
	vinsertps	$16, 4(%r12,%rsi,4), %xmm2, %xmm2 ## xmm2 = xmm2[0],mem[0],xmm2[2,3]
	vblendvps	%ymm6, %ymm1, %ymm0, %ymm0
	vmovaps	%ymm0, 2656(%rsp)               ## 32-byte Spill
	vmovss	4(%r12,%rdi,4), %xmm0           ## xmm0 = mem[0],zero,zero,zero
	vinsertps	$32, (%r12,%r9,4), %xmm4, %xmm1 ## xmm1 = xmm4[0,1],mem[0],xmm4[3]
	vinsertps	$16, 4(%r12,%r10,4), %xmm0, %xmm0 ## xmm0 = xmm0[0],mem[0],xmm0[2,3]
	vinsertps	$32, 4(%r12,%rdx,4), %xmm2, %xmm2 ## xmm2 = xmm2[0,1],mem[0],xmm2[3]
	vinsertps	$32, 4(%r12,%r9,4), %xmm0, %xmm0 ## xmm0 = xmm0[0,1],mem[0],xmm0[3]
	movq	240(%rsp), %rbx                 ## 8-byte Reload
	vinsertps	$48, (%r12,%rbx,4), %xmm5, %xmm4 ## xmm4 = xmm5[0,1,2],mem[0]
	vmovss	8(%r12,%rcx,4), %xmm5           ## xmm5 = mem[0],zero,zero,zero
	movq	80(%rsp), %rax                  ## 8-byte Reload
	vinsertps	$48, (%r12,%rax,4), %xmm1, %xmm1 ## xmm1 = xmm1[0,1,2],mem[0]
	vinsertps	$16, 8(%r12,%rsi,4), %xmm5, %xmm5 ## xmm5 = xmm5[0],mem[0],xmm5[2,3]
	vinsertps	$48, 4(%r12,%rbx,4), %xmm2, %xmm2 ## xmm2 = xmm2[0,1,2],mem[0]
	vinsertps	$32, 8(%r12,%rdx,4), %xmm5, %xmm5 ## xmm5 = xmm5[0,1],mem[0],xmm5[3]
	vmovss	8(%r12,%rdi,4), %xmm6           ## xmm6 = mem[0],zero,zero,zero
	vinsertps	$16, 8(%r12,%r10,4), %xmm6, %xmm6 ## xmm6 = xmm6[0],mem[0],xmm6[2,3]
	vinsertps	$48, 4(%r12,%rax,4), %xmm0, %xmm0 ## xmm0 = xmm0[0,1,2],mem[0]
	vinsertps	$32, 8(%r12,%r9,4), %xmm6, %xmm6 ## xmm6 = xmm6[0,1],mem[0],xmm6[3]
	vinsertps	$48, 8(%r12,%rbx,4), %xmm5, %xmm5 ## xmm5 = xmm5[0,1,2],mem[0]
	vinsertf128	$1, %xmm4, %ymm1, %ymm1
	vinsertps	$48, 8(%r12,%rax,4), %xmm6, %xmm4 ## xmm4 = xmm6[0,1,2],mem[0]
	vinsertf128	$1, %xmm2, %ymm0, %ymm0
	vmovss	12(%r12,%rcx,4), %xmm2          ## xmm2 = mem[0],zero,zero,zero
	vinsertps	$16, 12(%r12,%rsi,4), %xmm2, %xmm2 ## xmm2 = xmm2[0],mem[0],xmm2[2,3]
	vmovups	32(%r14,%r15,4), %ymm6
	vmovaps	%ymm6, 2176(%rsp)               ## 32-byte Spill
	vmulps	%ymm0, %ymm6, %ymm11
	vmovups	32(%r14,%r13,4), %ymm0
	vmovaps	%ymm0, 2048(%rsp)               ## 32-byte Spill
	vmulps	32(%r14,%r11,4), %ymm0, %ymm0
	vfmadd231ps	%ymm1, %ymm0, %ymm11    ## ymm11 = (ymm0 * ymm1) + ymm11
	vinsertps	$32, 12(%r12,%rdx,4), %xmm2, %xmm1 ## xmm1 = xmm2[0,1],mem[0],xmm2[3]
	vmovss	12(%r12,%rdi,4), %xmm2          ## xmm2 = mem[0],zero,zero,zero
	vinsertps	$16, 12(%r12,%r10,4), %xmm2, %xmm2 ## xmm2 = xmm2[0],mem[0],xmm2[2,3]
	vinsertps	$48, 12(%r12,%rbx,4), %xmm1, %xmm1 ## xmm1 = xmm1[0,1,2],mem[0]
	vinsertps	$32, 12(%r12,%r9,4), %xmm2, %xmm2 ## xmm2 = xmm2[0,1],mem[0],xmm2[3]
	vinsertps	$48, 12(%r12,%rax,4), %xmm2, %xmm2 ## xmm2 = xmm2[0,1,2],mem[0]
	vinsertf128	$1, %xmm5, %ymm4, %ymm4
	vinsertf128	$1, %xmm1, %ymm2, %ymm1
	vmulps	%ymm1, %ymm6, %ymm7
	vfmadd231ps	%ymm4, %ymm0, %ymm7     ## ymm7 = (ymm0 * ymm4) + ymm7
	movq	224(%rsp), %rdx                 ## 8-byte Reload
	vmovss	(%r12,%rdx,4), %xmm0            ## xmm0 = mem[0],zero,zero,zero
	movq	56(%rsp), %rcx                  ## 8-byte Reload
	vinsertps	$16, (%r12,%rcx,4), %xmm0, %xmm0 ## xmm0 = xmm0[0],mem[0],xmm0[2,3]
	movq	48(%rsp), %rax                  ## 8-byte Reload
	vmovss	(%r12,%rax,4), %xmm1            ## xmm1 = mem[0],zero,zero,zero
	movq	480(%rsp), %r8                  ## 8-byte Reload
	vinsertps	$16, (%r12,%r8,4), %xmm1, %xmm1 ## xmm1 = xmm1[0],mem[0],xmm1[2,3]
	movq	72(%rsp), %rbx                  ## 8-byte Reload
	vinsertps	$32, (%r12,%rbx,4), %xmm0, %xmm0 ## xmm0 = xmm0[0,1],mem[0],xmm0[3]
	movq	64(%rsp), %r9                   ## 8-byte Reload
	vinsertps	$32, (%r12,%r9,4), %xmm1, %xmm1 ## xmm1 = xmm1[0,1],mem[0],xmm1[3]
	movq	432(%rsp), %rsi                 ## 8-byte Reload
	vinsertps	$48, (%r12,%rsi,4), %xmm0, %xmm0 ## xmm0 = xmm0[0,1,2],mem[0]
	movq	464(%rsp), %rdi                 ## 8-byte Reload
	vmovss	(%r12,%rdi,4), %xmm2            ## xmm2 = mem[0],zero,zero,zero
	movq	736(%rsp), %r13                 ## 8-byte Reload
	vinsertps	$16, (%r12,%r13,4), %xmm2, %xmm2 ## xmm2 = xmm2[0],mem[0],xmm2[2,3]
	vmovss	4(%r12,%rdx,4), %xmm4           ## xmm4 = mem[0],zero,zero,zero
	vinsertps	$16, 4(%r12,%rcx,4), %xmm4, %xmm4 ## xmm4 = xmm4[0],mem[0],xmm4[2,3]
	movq	416(%rsp), %r10                 ## 8-byte Reload
	vinsertps	$48, (%r12,%r10,4), %xmm1, %xmm1 ## xmm1 = xmm1[0,1,2],mem[0]
	vinsertf128	$1, %xmm0, %ymm1, %ymm0
	vinsertps	$32, 4(%r12,%rbx,4), %xmm4, %xmm1 ## xmm1 = xmm4[0,1],mem[0],xmm4[3]
	vmovss	4(%r12,%rax,4), %xmm4           ## xmm4 = mem[0],zero,zero,zero
	vinsertps	$16, 4(%r12,%r8,4), %xmm4, %xmm4 ## xmm4 = xmm4[0],mem[0],xmm4[2,3]
	vinsertps	$48, 4(%r12,%rsi,4), %xmm1, %xmm1 ## xmm1 = xmm1[0,1,2],mem[0]
	vinsertps	$32, 4(%r12,%r9,4), %xmm4, %xmm4 ## xmm4 = xmm4[0,1],mem[0],xmm4[3]
	vinsertps	$48, 4(%r12,%r10,4), %xmm4, %xmm4 ## xmm4 = xmm4[0,1,2],mem[0]
	vinsertf128	$1, %xmm1, %ymm4, %ymm1
	vmovss	8(%r12,%rdx,4), %xmm4           ## xmm4 = mem[0],zero,zero,zero
	vinsertps	$16, 8(%r12,%rcx,4), %xmm4, %xmm4 ## xmm4 = xmm4[0],mem[0],xmm4[2,3]
	vinsertps	$32, 8(%r12,%rbx,4), %xmm4, %xmm4 ## xmm4 = xmm4[0,1],mem[0],xmm4[3]
	vinsertps	$48, 8(%r12,%rsi,4), %xmm4, %xmm4 ## xmm4 = xmm4[0,1,2],mem[0]
	vmovss	8(%r12,%rax,4), %xmm5           ## xmm5 = mem[0],zero,zero,zero
	vinsertps	$16, 8(%r12,%r8,4), %xmm5, %xmm5 ## xmm5 = xmm5[0],mem[0],xmm5[2,3]
	vmovups	64(%r14,%r15,4), %ymm3
	vmulps	%ymm1, %ymm3, %ymm1
	vmovaps	%ymm3, %ymm6
	vmovaps	%ymm3, 1632(%rsp)               ## 32-byte Spill
	movq	656(%rsp), %rdi                 ## 8-byte Reload
	vmovups	64(%r14,%rdi,4), %ymm3
	vmovaps	%ymm3, 2080(%rsp)               ## 32-byte Spill
	vmulps	64(%r14,%r11,4), %ymm3, %ymm8
	vfmadd231ps	%ymm0, %ymm8, %ymm1     ## ymm1 = (ymm8 * ymm0) + ymm1
	vinsertps	$32, 8(%r12,%r9,4), %xmm5, %xmm0 ## xmm0 = xmm5[0,1],mem[0],xmm5[3]
	vmovss	12(%r12,%rdx,4), %xmm5          ## xmm5 = mem[0],zero,zero,zero
	vinsertps	$16, 12(%r12,%rcx,4), %xmm5, %xmm5 ## xmm5 = xmm5[0],mem[0],xmm5[2,3]
	vinsertps	$48, 8(%r12,%r10,4), %xmm0, %xmm0 ## xmm0 = xmm0[0,1,2],mem[0]
	vinsertf128	$1, %xmm4, %ymm0, %ymm0
	vinsertps	$32, 12(%r12,%rbx,4), %xmm5, %xmm4 ## xmm4 = xmm5[0,1],mem[0],xmm5[3]
	vmovss	12(%r12,%rax,4), %xmm5          ## xmm5 = mem[0],zero,zero,zero
	vinsertps	$16, 12(%r12,%r8,4), %xmm5, %xmm5 ## xmm5 = xmm5[0],mem[0],xmm5[2,3]
	vinsertps	$48, 12(%r12,%rsi,4), %xmm4, %xmm4 ## xmm4 = xmm4[0,1,2],mem[0]
	vinsertps	$32, 12(%r12,%r9,4), %xmm5, %xmm5 ## xmm5 = xmm5[0,1],mem[0],xmm5[3]
	vinsertps	$48, 12(%r12,%r10,4), %xmm5, %xmm5 ## xmm5 = xmm5[0,1,2],mem[0]
	vinsertf128	$1, %xmm4, %ymm5, %ymm4
	movq	448(%rsp), %r8                  ## 8-byte Reload
	vinsertps	$32, (%r12,%r8,4), %xmm2, %xmm2 ## xmm2 = xmm2[0,1],mem[0],xmm2[3]
	movq	112(%rsp), %r10                 ## 8-byte Reload
	vmovss	(%r12,%r10,4), %xmm5            ## xmm5 = mem[0],zero,zero,zero
	movq	1152(%rsp), %r9                 ## 8-byte Reload
	vinsertps	$16, (%r12,%r9,4), %xmm5, %xmm5 ## xmm5 = xmm5[0],mem[0],xmm5[2,3]
	movq	120(%rsp), %rax                 ## 8-byte Reload
	vinsertps	$48, (%r12,%rax,4), %xmm2, %xmm2 ## xmm2 = xmm2[0,1,2],mem[0]
	movq	104(%rsp), %rcx                 ## 8-byte Reload
	vinsertps	$32, (%r12,%rcx,4), %xmm5, %xmm5 ## xmm5 = xmm5[0,1],mem[0],xmm5[3]
	movq	128(%rsp), %rsi                 ## 8-byte Reload
	vinsertps	$48, (%r12,%rsi,4), %xmm5, %xmm5 ## xmm5 = xmm5[0,1,2],mem[0]
	movq	464(%rsp), %rdi                 ## 8-byte Reload
	vmovss	4(%r12,%rdi,4), %xmm3           ## xmm3 = mem[0],zero,zero,zero
	vinsertps	$16, 4(%r12,%r13,4), %xmm3, %xmm3 ## xmm3 = xmm3[0],mem[0],xmm3[2,3]
	vmulps	%ymm4, %ymm6, %ymm4
	vfmadd231ps	%ymm0, %ymm8, %ymm4     ## ymm4 = (ymm8 * ymm0) + ymm4
	vinsertps	$32, 4(%r12,%r8,4), %xmm3, %xmm0 ## xmm0 = xmm3[0,1],mem[0],xmm3[3]
	vmovss	4(%r12,%r10,4), %xmm3           ## xmm3 = mem[0],zero,zero,zero
	vinsertps	$16, 4(%r12,%r9,4), %xmm3, %xmm3 ## xmm3 = xmm3[0],mem[0],xmm3[2,3]
	vinsertps	$48, 4(%r12,%rax,4), %xmm0, %xmm0 ## xmm0 = xmm0[0,1,2],mem[0]
	vinsertf128	$1, %xmm2, %ymm5, %ymm2
	vinsertps	$32, 4(%r12,%rcx,4), %xmm3, %xmm3 ## xmm3 = xmm3[0,1],mem[0],xmm3[3]
	vmovss	8(%r12,%rdi,4), %xmm5           ## xmm5 = mem[0],zero,zero,zero
	vinsertps	$16, 8(%r12,%r13,4), %xmm5, %xmm5 ## xmm5 = xmm5[0],mem[0],xmm5[2,3]
	vinsertps	$48, 4(%r12,%rsi,4), %xmm3, %xmm3 ## xmm3 = xmm3[0,1,2],mem[0]
	vinsertf128	$1, %xmm0, %ymm3, %ymm0
	vinsertps	$32, 8(%r12,%r8,4), %xmm5, %xmm3 ## xmm3 = xmm5[0,1],mem[0],xmm5[3]
	vmovss	8(%r12,%r10,4), %xmm5           ## xmm5 = mem[0],zero,zero,zero
	vinsertps	$16, 8(%r12,%r9,4), %xmm5, %xmm5 ## xmm5 = xmm5[0],mem[0],xmm5[2,3]
	vinsertps	$48, 8(%r12,%rax,4), %xmm3, %xmm3 ## xmm3 = xmm3[0,1,2],mem[0]
	vinsertps	$32, 8(%r12,%rcx,4), %xmm5, %xmm5 ## xmm5 = xmm5[0,1],mem[0],xmm5[3]
	vinsertps	$48, 8(%r12,%rsi,4), %xmm5, %xmm5 ## xmm5 = xmm5[0,1,2],mem[0]
	vmovss	12(%r12,%rdi,4), %xmm6          ## xmm6 = mem[0],zero,zero,zero
	vinsertps	$16, 12(%r12,%r13,4), %xmm6, %xmm6 ## xmm6 = xmm6[0],mem[0],xmm6[2,3]
	vmovups	96(%r14,%r15,4), %ymm14
	vmovaps	%ymm14, 1664(%rsp)              ## 32-byte Spill
	vmulps	%ymm0, %ymm14, %ymm0
	movq	656(%rsp), %rdx                 ## 8-byte Reload
	vmovups	96(%r14,%rdx,4), %ymm8
	vmovaps	%ymm8, 2112(%rsp)               ## 32-byte Spill
	vmulps	96(%r14,%r11,4), %ymm8, %ymm8
	vfmadd231ps	%ymm2, %ymm8, %ymm0     ## ymm0 = (ymm8 * ymm2) + ymm0
	vinsertps	$32, 12(%r12,%r8,4), %xmm6, %xmm2 ## xmm2 = xmm6[0,1],mem[0],xmm6[3]
	vmovss	12(%r12,%r10,4), %xmm6          ## xmm6 = mem[0],zero,zero,zero
	vinsertps	$16, 12(%r12,%r9,4), %xmm6, %xmm6 ## xmm6 = xmm6[0],mem[0],xmm6[2,3]
	vinsertps	$48, 12(%r12,%rax,4), %xmm2, %xmm2 ## xmm2 = xmm2[0,1,2],mem[0]
	vinsertps	$32, 12(%r12,%rcx,4), %xmm6, %xmm6 ## xmm6 = xmm6[0,1],mem[0],xmm6[3]
	vinsertps	$48, 12(%r12,%rsi,4), %xmm6, %xmm6 ## xmm6 = xmm6[0,1,2],mem[0]
	vinsertf128	$1, %xmm3, %ymm5, %ymm3
	vinsertf128	$1, %xmm2, %ymm6, %ymm2
	vmulps	%ymm2, %ymm14, %ymm2
	vfmadd231ps	%ymm3, %ymm8, %ymm2     ## ymm2 = (ymm8 * ymm3) + ymm2
	vxorps	%xmm5, %xmm5, %xmm5
	movq	528(%rsp), %rax                 ## 8-byte Reload
	vcmpeqps	32(%r14,%rax,4), %ymm5, %ymm3
	vblendvps	%ymm3, %ymm11, %ymm7, %ymm3
	vmovaps	%ymm3, 2528(%rsp)               ## 32-byte Spill
	vcmpeqps	64(%r14,%rax,4), %ymm5, %ymm3
	vxorps	%xmm5, %xmm5, %xmm5
	vblendvps	%ymm3, %ymm1, %ymm4, %ymm1
	vmovaps	%ymm1, 2624(%rsp)               ## 32-byte Spill
	vcmpeqps	96(%r14,%rax,4), %ymm5, %ymm1
	vblendvps	%ymm1, %ymm0, %ymm2, %ymm0
	vmovaps	%ymm0, 2560(%rsp)               ## 32-byte Spill
	movq	176(%rsp), %rax                 ## 8-byte Reload
	vmovss	64(%r12,%rax,4), %xmm0          ## xmm0 = mem[0],zero,zero,zero
	movq	160(%rsp), %rax                 ## 8-byte Reload
	vinsertps	$16, 64(%r12,%rax,4), %xmm0, %xmm0 ## xmm0 = xmm0[0],mem[0],xmm0[2,3]
	movq	288(%rsp), %rax                 ## 8-byte Reload
	vmovss	64(%r12,%rax,4), %xmm1          ## xmm1 = mem[0],zero,zero,zero
	movq	144(%rsp), %r10                 ## 8-byte Reload
	vinsertps	$16, 64(%r12,%r10,4), %xmm1, %xmm1 ## xmm1 = xmm1[0],mem[0],xmm1[2,3]
	movq	320(%rsp), %rbx                 ## 8-byte Reload
	vinsertps	$32, 64(%r12,%rbx,4), %xmm0, %xmm0 ## xmm0 = xmm0[0,1],mem[0],xmm0[3]
	movq	400(%rsp), %r15                 ## 8-byte Reload
	vinsertps	$32, 64(%r12,%r15,4), %xmm1, %xmm1 ## xmm1 = xmm1[0,1],mem[0],xmm1[3]
	movq	384(%rsp), %rcx                 ## 8-byte Reload
	vinsertps	$48, 64(%r12,%rcx,4), %xmm0, %xmm0 ## xmm0 = xmm0[0,1,2],mem[0]
	movq	40(%rsp), %rcx                  ## 8-byte Reload
	vinsertps	$48, 64(%r12,%rcx,4), %xmm1, %xmm1 ## xmm1 = xmm1[0,1,2],mem[0]
	vinsertf128	$1, %xmm0, %ymm1, %ymm0
	movq	768(%rsp), %rcx                 ## 8-byte Reload
	vmovups	(%r14,%rcx,4), %ymm1
	vmovaps	2432(%rsp), %ymm13              ## 32-byte Reload
	vcmpltps	%ymm1, %ymm13, %ymm2
	vmovaps	%ymm1, %ymm11
	vmovaps	1696(%rsp), %ymm1               ## 32-byte Reload
	vblendvps	%ymm2, %ymm0, %ymm1, %ymm0
	vmovaps	%ymm2, %ymm6
	movq	352(%rsp), %rdi                 ## 8-byte Reload
	vmovss	64(%r12,%rdi,4), %xmm1          ## xmm1 = mem[0],zero,zero,zero
	movq	544(%rsp), %rsi                 ## 8-byte Reload
	vinsertps	$16, 64(%r12,%rsi,4), %xmm1, %xmm1 ## xmm1 = xmm1[0],mem[0],xmm1[2,3]
	movq	32(%rsp), %rdx                  ## 8-byte Reload
	vmovss	64(%r12,%rdx,4), %xmm2          ## xmm2 = mem[0],zero,zero,zero
	movq	256(%rsp), %r13                 ## 8-byte Reload
	vinsertps	$16, 64(%r12,%r13,4), %xmm2, %xmm2 ## xmm2 = xmm2[0],mem[0],xmm2[2,3]
	movq	336(%rsp), %rdx                 ## 8-byte Reload
	vinsertps	$32, 64(%r12,%rdx,4), %xmm1, %xmm1 ## xmm1 = xmm1[0,1],mem[0],xmm1[3]
	movq	304(%rsp), %r9                  ## 8-byte Reload
	vinsertps	$32, 64(%r12,%r9,4), %xmm2, %xmm2 ## xmm2 = xmm2[0,1],mem[0],xmm2[3]
	movq	240(%rsp), %r11                 ## 8-byte Reload
	vinsertps	$48, 64(%r12,%r11,4), %xmm1, %xmm1 ## xmm1 = xmm1[0,1,2],mem[0]
	movq	80(%rsp), %r8                   ## 8-byte Reload
	vinsertps	$48, 64(%r12,%r8,4), %xmm2, %xmm2 ## xmm2 = xmm2[0,1,2],mem[0]
	vinsertf128	$1, %xmm1, %ymm2, %ymm1
	vmovups	32(%r14,%rcx,4), %ymm8
	vcmpltps	%ymm8, %ymm13, %ymm2
	vmovaps	1984(%rsp), %ymm3               ## 32-byte Reload
	vblendvps	%ymm2, %ymm1, %ymm3, %ymm1
	vmovaps	%ymm2, %ymm5
	vmovaps	%ymm2, 2144(%rsp)               ## 32-byte Spill
	movq	176(%rsp), %rcx                 ## 8-byte Reload
	vmovss	76(%r12,%rcx,4), %xmm2          ## xmm2 = mem[0],zero,zero,zero
	movq	160(%rsp), %rcx                 ## 8-byte Reload
	vinsertps	$16, 76(%r12,%rcx,4), %xmm2, %xmm2 ## xmm2 = xmm2[0],mem[0],xmm2[2,3]
	vmovss	76(%r12,%rax,4), %xmm3          ## xmm3 = mem[0],zero,zero,zero
	vinsertps	$16, 76(%r12,%r10,4), %xmm3, %xmm3 ## xmm3 = xmm3[0],mem[0],xmm3[2,3]
	vinsertps	$32, 76(%r12,%rbx,4), %xmm2, %xmm2 ## xmm2 = xmm2[0,1],mem[0],xmm2[3]
	movq	%rbx, %r10
	vinsertps	$32, 76(%r12,%r15,4), %xmm3, %xmm3 ## xmm3 = xmm3[0,1],mem[0],xmm3[3]
	movq	384(%rsp), %rbx                 ## 8-byte Reload
	vinsertps	$48, 76(%r12,%rbx,4), %xmm2, %xmm2 ## xmm2 = xmm2[0,1,2],mem[0]
	movq	40(%rsp), %rax                  ## 8-byte Reload
	vinsertps	$48, 76(%r12,%rax,4), %xmm3, %xmm3 ## xmm3 = xmm3[0,1,2],mem[0]
	movq	688(%rsp), %rax                 ## 8-byte Reload
	vmulps	(%r14,%rax,4), %ymm11, %ymm4
	vmulps	%ymm0, %ymm4, %ymm0
	vmovss	76(%r12,%rdi,4), %xmm4          ## xmm4 = mem[0],zero,zero,zero
	vinsertps	$16, 76(%r12,%rsi,4), %xmm4, %xmm4 ## xmm4 = xmm4[0],mem[0],xmm4[2,3]
	vinsertf128	$1, %xmm2, %ymm3, %ymm2
	vinsertps	$32, 76(%r12,%rdx,4), %xmm4, %xmm3 ## xmm3 = xmm4[0,1],mem[0],xmm4[3]
	movq	%rdx, %rsi
	movq	32(%rsp), %rdx                  ## 8-byte Reload
	vmovss	76(%r12,%rdx,4), %xmm4          ## xmm4 = mem[0],zero,zero,zero
	vinsertps	$16, 76(%r12,%r13,4), %xmm4, %xmm4 ## xmm4 = xmm4[0],mem[0],xmm4[2,3]
	vinsertps	$48, 76(%r12,%r11,4), %xmm3, %xmm3 ## xmm3 = xmm3[0,1,2],mem[0]
	vinsertps	$32, 76(%r12,%r9,4), %xmm4, %xmm4 ## xmm4 = xmm4[0,1],mem[0],xmm4[3]
	vinsertps	$48, 76(%r12,%r8,4), %xmm4, %xmm4 ## xmm4 = xmm4[0,1,2],mem[0]
	vinsertf128	$1, %xmm3, %ymm4, %ymm3
	vmovaps	2368(%rsp), %ymm4               ## 32-byte Reload
	vblendvps	%ymm6, %ymm2, %ymm4, %ymm15
	vmovaps	%ymm6, %ymm12
	vmovaps	%ymm6, 2688(%rsp)               ## 32-byte Spill
	movq	200(%rsp), %rcx                 ## 8-byte Reload
	vmovups	(%r14,%rcx,4), %ymm2
	vmaxps	%ymm13, %ymm2, %ymm2
	vdivps	%ymm2, %ymm0, %ymm0
	vmovaps	2016(%rsp), %ymm4               ## 32-byte Reload
	vblendvps	%ymm5, %ymm3, %ymm4, %ymm14
	vmulps	32(%r14,%rax,4), %ymm8, %ymm3
	vmulps	%ymm1, %ymm3, %ymm3
	movq	672(%rsp), %rax                 ## 8-byte Reload
	vfmadd132ps	(%r14,%rax,4), %ymm0, %ymm15 ## ymm15 = (ymm15 * mem) + ymm0
	vmovups	32(%r14,%rcx,4), %ymm0
	vmaxps	%ymm13, %ymm0, %ymm1
	vdivps	%ymm1, %ymm3, %ymm0
	vfmadd132ps	32(%r14,%rax,4), %ymm0, %ymm14 ## ymm14 = (ymm14 * mem) + ymm0
	movq	176(%rsp), %rax                 ## 8-byte Reload
	vmovss	92(%r12,%rax,4), %xmm0          ## xmm0 = mem[0],zero,zero,zero
	movq	160(%rsp), %rcx                 ## 8-byte Reload
	vinsertps	$16, 92(%r12,%rcx,4), %xmm0, %xmm0 ## xmm0 = xmm0[0],mem[0],xmm0[2,3]
	vinsertps	$32, 92(%r12,%r10,4), %xmm0, %xmm0 ## xmm0 = xmm0[0,1],mem[0],xmm0[3]
	movq	288(%rsp), %r13                 ## 8-byte Reload
	vmovss	92(%r12,%r13,4), %xmm3          ## xmm3 = mem[0],zero,zero,zero
	movq	144(%rsp), %r15                 ## 8-byte Reload
	vinsertps	$16, 92(%r12,%r15,4), %xmm3, %xmm3 ## xmm3 = xmm3[0],mem[0],xmm3[2,3]
	vmovss	88(%r12,%rax,4), %xmm4          ## xmm4 = mem[0],zero,zero,zero
	vinsertps	$16, 88(%r12,%rcx,4), %xmm4, %xmm4 ## xmm4 = xmm4[0],mem[0],xmm4[2,3]
	vinsertps	$48, 92(%r12,%rbx,4), %xmm0, %xmm0 ## xmm0 = xmm0[0,1,2],mem[0]
	vinsertps	$32, 88(%r12,%r10,4), %xmm4, %xmm4 ## xmm4 = xmm4[0,1],mem[0],xmm4[3]
	vmovss	88(%r12,%r13,4), %xmm5          ## xmm5 = mem[0],zero,zero,zero
	vinsertps	$16, 88(%r12,%r15,4), %xmm5, %xmm5 ## xmm5 = xmm5[0],mem[0],xmm5[2,3]
	vinsertps	$48, 88(%r12,%rbx,4), %xmm4, %xmm4 ## xmm4 = xmm4[0,1,2],mem[0]
	movq	400(%rsp), %rcx                 ## 8-byte Reload
	vinsertps	$32, 88(%r12,%rcx,4), %xmm5, %xmm5 ## xmm5 = xmm5[0,1],mem[0],xmm5[3]
	movq	40(%rsp), %rax                  ## 8-byte Reload
	vinsertps	$48, 88(%r12,%rax,4), %xmm5, %xmm5 ## xmm5 = xmm5[0,1,2],mem[0]
	vinsertf128	$1, %xmm4, %ymm5, %ymm4
	vinsertps	$32, 92(%r12,%rcx,4), %xmm3, %xmm3 ## xmm3 = xmm3[0,1],mem[0],xmm3[3]
	vaddps	%ymm9, %ymm10, %ymm5
	vmulps	%ymm4, %ymm5, %ymm4
	vinsertps	$48, 92(%r12,%rax,4), %xmm3, %xmm3 ## xmm3 = xmm3[0,1,2],mem[0]
	vmovss	92(%r12,%rdi,4), %xmm5          ## xmm5 = mem[0],zero,zero,zero
	movq	544(%rsp), %rax                 ## 8-byte Reload
	vinsertps	$16, 92(%r12,%rax,4), %xmm5, %xmm5 ## xmm5 = xmm5[0],mem[0],xmm5[2,3]
	movq	%rsi, %r10
	vinsertps	$32, 92(%r12,%rsi,4), %xmm5, %xmm5 ## xmm5 = xmm5[0,1],mem[0],xmm5[3]
	vdivps	%ymm2, %ymm4, %ymm6
	vbroadcastss	LCPI0_63(%rip), %ymm4   ## ymm4 = [4.096E+3,4.096E+3,4.096E+3,4.096E+3,4.096E+3,4.096E+3,4.096E+3,4.096E+3]
	vdivps	%ymm2, %ymm4, %ymm7
	vmovaps	%ymm4, %ymm9
	vmovss	92(%r12,%rdx,4), %xmm2          ## xmm2 = mem[0],zero,zero,zero
	movq	256(%rsp), %rsi                 ## 8-byte Reload
	vinsertps	$16, 92(%r12,%rsi,4), %xmm2, %xmm4 ## xmm4 = xmm2[0],mem[0],xmm2[2,3]
	vinsertps	$48, 92(%r12,%r11,4), %xmm5, %xmm2 ## xmm2 = xmm5[0,1,2],mem[0]
	vinsertps	$32, 92(%r12,%r9,4), %xmm4, %xmm4 ## xmm4 = xmm4[0,1],mem[0],xmm4[3]
	vinsertps	$48, 92(%r12,%r8,4), %xmm4, %xmm4 ## xmm4 = xmm4[0,1,2],mem[0]
	vmovss	88(%r12,%rdi,4), %xmm5          ## xmm5 = mem[0],zero,zero,zero
	vinsertps	$16, 88(%r12,%rax,4), %xmm5, %xmm5 ## xmm5 = xmm5[0],mem[0],xmm5[2,3]
	vmovaps	%ymm11, 2720(%rsp)              ## 32-byte Spill
	vminps	%ymm11, %ymm7, %ymm7
	vinsertps	$32, 88(%r12,%r10,4), %xmm5, %xmm5 ## xmm5 = xmm5[0,1],mem[0],xmm5[3]
	movq	800(%rsp), %rax                 ## 8-byte Reload
	vmulps	(%r14,%rax,4), %ymm7, %ymm7
	vinsertf128	$1, %xmm0, %ymm3, %ymm0
	vinsertps	$48, 88(%r12,%r11,4), %xmm5, %xmm5 ## xmm5 = xmm5[0,1,2],mem[0]
	vmovss	88(%r12,%rdx,4), %xmm3          ## xmm3 = mem[0],zero,zero,zero
	vinsertps	$16, 88(%r12,%rsi,4), %xmm3, %xmm3 ## xmm3 = xmm3[0],mem[0],xmm3[2,3]
	vandps	%ymm6, %ymm12, %ymm6
	movq	1200(%rsp), %rcx                ## 8-byte Reload
	vmovups	-96(%r14,%rcx), %ymm12
	vfmadd213ps	%ymm6, %ymm12, %ymm15   ## ymm15 = (ymm12 * ymm15) + ymm6
	movq	784(%rsp), %rdx                 ## 8-byte Reload
	vmulps	(%r14,%rdx,4), %ymm11, %ymm6
	vmovaps	%ymm6, 1952(%rsp)               ## 32-byte Spill
	vmulps	%ymm6, %ymm7, %ymm6
	vfmadd231ps	%ymm6, %ymm0, %ymm15    ## ymm15 = (ymm0 * ymm6) + ymm15
	vmovaps	%ymm15, 2368(%rsp)              ## 32-byte Spill
	vinsertps	$32, 88(%r12,%r9,4), %xmm3, %xmm6 ## xmm6 = xmm3[0,1],mem[0],xmm3[3]
	movq	176(%rsp), %rsi                 ## 8-byte Reload
	vmovss	20(%r12,%rsi,4), %xmm0          ## xmm0 = mem[0],zero,zero,zero
	movq	160(%rsp), %rdi                 ## 8-byte Reload
	vinsertps	$16, 20(%r12,%rdi,4), %xmm0, %xmm0 ## xmm0 = xmm0[0],mem[0],xmm0[2,3]
	vinsertps	$48, 88(%r12,%r8,4), %xmm6, %xmm3 ## xmm3 = xmm6[0,1,2],mem[0]
	vinsertf128	$1, %xmm5, %ymm3, %ymm3
	movq	320(%rsp), %r9                  ## 8-byte Reload
	vinsertps	$32, 20(%r12,%r9,4), %xmm0, %xmm0 ## xmm0 = xmm0[0,1],mem[0],xmm0[3]
	vmovaps	2048(%rsp), %ymm5               ## 32-byte Reload
	vaddps	2176(%rsp), %ymm5, %ymm5        ## 32-byte Folded Reload
	vmulps	%ymm3, %ymm5, %ymm3
	vdivps	%ymm1, %ymm3, %ymm3
	vinsertf128	$1, %xmm2, %ymm4, %ymm2
	vdivps	%ymm1, %ymm9, %ymm1
	vmovaps	%ymm8, 2496(%rsp)               ## 32-byte Spill
	vminps	%ymm8, %ymm1, %ymm1
	movq	%rbx, %r8
	vinsertps	$48, 20(%r12,%rbx,4), %xmm0, %xmm0 ## xmm0 = xmm0[0,1,2],mem[0]
	vmovss	20(%r12,%r13,4), %xmm4          ## xmm4 = mem[0],zero,zero,zero
	movq	144(%rsp), %r11                 ## 8-byte Reload
	vinsertps	$16, 20(%r12,%r11,4), %xmm4, %xmm4 ## xmm4 = xmm4[0],mem[0],xmm4[2,3]
	vmulps	32(%r14,%rax,4), %ymm1, %ymm1
	vandps	2144(%rsp), %ymm3, %ymm3        ## 32-byte Folded Reload
	movq	400(%rsp), %rbx                 ## 8-byte Reload
	vinsertps	$32, 20(%r12,%rbx,4), %xmm4, %xmm4 ## xmm4 = xmm4[0,1],mem[0],xmm4[3]
	vmovups	-64(%r14,%rcx), %ymm11
	vfmadd213ps	%ymm3, %ymm11, %ymm14   ## ymm14 = (ymm11 * ymm14) + ymm3
	vmulps	32(%r14,%rdx,4), %ymm8, %ymm3
	vmovaps	%ymm3, 1920(%rsp)               ## 32-byte Spill
	vmulps	%ymm3, %ymm1, %ymm1
	vfmadd231ps	%ymm1, %ymm2, %ymm14    ## ymm14 = (ymm2 * ymm1) + ymm14
	vmovaps	%ymm14, 1696(%rsp)              ## 32-byte Spill
	movq	40(%rsp), %r15                  ## 8-byte Reload
	vinsertps	$48, 20(%r12,%r15,4), %xmm4, %xmm1 ## xmm1 = xmm4[0,1,2],mem[0]
	vinsertf128	$1, %xmm0, %ymm1, %ymm2
	vmovss	24(%r12,%rsi,4), %xmm0          ## xmm0 = mem[0],zero,zero,zero
	vinsertps	$16, 24(%r12,%rdi,4), %xmm0, %xmm0 ## xmm0 = xmm0[0],mem[0],xmm0[2,3]
	vmovss	24(%r12,%r13,4), %xmm1          ## xmm1 = mem[0],zero,zero,zero
	vinsertps	$16, 24(%r12,%r11,4), %xmm1, %xmm1 ## xmm1 = xmm1[0],mem[0],xmm1[2,3]
	movq	%r11, %rcx
	vinsertps	$32, 24(%r12,%r9,4), %xmm0, %xmm0 ## xmm0 = xmm0[0,1],mem[0],xmm0[3]
	vinsertps	$32, 24(%r12,%rbx,4), %xmm1, %xmm1 ## xmm1 = xmm1[0,1],mem[0],xmm1[3]
	vinsertps	$48, 24(%r12,%r8,4), %xmm0, %xmm0 ## xmm0 = xmm0[0,1,2],mem[0]
	vinsertps	$48, 24(%r12,%r15,4), %xmm1, %xmm1 ## xmm1 = xmm1[0,1,2],mem[0]
	vinsertf128	$1, %xmm0, %ymm1, %ymm0
	vmovss	28(%r12,%rsi,4), %xmm1          ## xmm1 = mem[0],zero,zero,zero
	vinsertps	$16, 28(%r12,%rdi,4), %xmm1, %xmm3 ## xmm3 = xmm1[0],mem[0],xmm1[2,3]
	vmovaps	%ymm12, 2464(%rsp)              ## 32-byte Spill
	movq	816(%rsp), %rax                 ## 8-byte Reload
	vmulps	(%r14,%rax,4), %ymm12, %ymm1
	vmulps	%ymm1, %ymm0, %ymm10
	vinsertps	$32, 28(%r12,%r9,4), %xmm3, %xmm0 ## xmm0 = xmm3[0,1],mem[0],xmm3[3]
	vmovss	28(%r12,%r13,4), %xmm3          ## xmm3 = mem[0],zero,zero,zero
	vinsertps	$16, 28(%r12,%r11,4), %xmm3, %xmm3 ## xmm3 = xmm3[0],mem[0],xmm3[2,3]
	vinsertps	$48, 28(%r12,%r8,4), %xmm0, %xmm0 ## xmm0 = xmm0[0,1,2],mem[0]
	vinsertps	$32, 28(%r12,%rbx,4), %xmm3, %xmm3 ## xmm3 = xmm3[0,1],mem[0],xmm3[3]
	movq	832(%rsp), %rax                 ## 8-byte Reload
	vmulps	(%r14,%rax,4), %ymm12, %ymm4
	vfmadd231ps	%ymm4, %ymm2, %ymm10    ## ymm10 = (ymm2 * ymm4) + ymm10
	vinsertps	$48, 28(%r12,%r15,4), %xmm3, %xmm2 ## xmm2 = xmm3[0,1,2],mem[0]
	vinsertf128	$1, %xmm0, %ymm2, %ymm0
	vmovss	32(%r12,%rsi,4), %xmm2          ## xmm2 = mem[0],zero,zero,zero
	vinsertps	$16, 32(%r12,%rdi,4), %xmm2, %xmm2 ## xmm2 = xmm2[0],mem[0],xmm2[2,3]
	movq	%rdi, %rsi
	vmovss	32(%r12,%r13,4), %xmm3          ## xmm3 = mem[0],zero,zero,zero
	movq	%r13, %rdx
	vinsertps	$16, 32(%r12,%r11,4), %xmm3, %xmm3 ## xmm3 = xmm3[0],mem[0],xmm3[2,3]
	vinsertps	$32, 32(%r12,%r9,4), %xmm2, %xmm2 ## xmm2 = xmm2[0,1],mem[0],xmm2[3]
	vinsertps	$32, 32(%r12,%rbx,4), %xmm3, %xmm3 ## xmm3 = xmm3[0,1],mem[0],xmm3[3]
	vinsertps	$48, 32(%r12,%r8,4), %xmm2, %xmm2 ## xmm2 = xmm2[0,1,2],mem[0]
	vinsertps	$48, 32(%r12,%r15,4), %xmm3, %xmm3 ## xmm3 = xmm3[0,1,2],mem[0]
	vmovaps	1504(%rsp), %ymm1               ## 32-byte Reload
	movq	848(%rsp), %rax                 ## 8-byte Reload
	movq	%r14, %rcx
	movq	%r14, 640(%rsp)                 ## 8-byte Spill
	vmulps	(%r14,%rax,4), %ymm1, %ymm4
	vfmadd231ps	%ymm4, %ymm0, %ymm10    ## ymm10 = (ymm0 * ymm4) + ymm10
	movq	224(%rsp), %r9                  ## 8-byte Reload
	vmovss	64(%r12,%r9,4), %xmm0           ## xmm0 = mem[0],zero,zero,zero
	movq	56(%rsp), %r11                  ## 8-byte Reload
	vinsertps	$16, 64(%r12,%r11,4), %xmm0, %xmm0 ## xmm0 = xmm0[0],mem[0],xmm0[2,3]
	vinsertf128	$1, %xmm2, %ymm3, %ymm2
	movq	72(%rsp), %r8                   ## 8-byte Reload
	vinsertps	$32, 64(%r12,%r8,4), %xmm0, %xmm0 ## xmm0 = xmm0[0,1],mem[0],xmm0[3]
	movq	48(%rsp), %rbx                  ## 8-byte Reload
	vmovss	64(%r12,%rbx,4), %xmm3          ## xmm3 = mem[0],zero,zero,zero
	movq	480(%rsp), %r13                 ## 8-byte Reload
	vinsertps	$16, 64(%r12,%r13,4), %xmm3, %xmm3 ## xmm3 = xmm3[0],mem[0],xmm3[2,3]
	movq	432(%rsp), %rdi                 ## 8-byte Reload
	vinsertps	$48, 64(%r12,%rdi,4), %xmm0, %xmm0 ## xmm0 = xmm0[0,1,2],mem[0]
	movq	64(%rsp), %rax                  ## 8-byte Reload
	vinsertps	$32, 64(%r12,%rax,4), %xmm3, %xmm3 ## xmm3 = xmm3[0,1],mem[0],xmm3[3]
	vmovaps	1536(%rsp), %ymm8               ## 32-byte Reload
	movq	880(%rsp), %rax                 ## 8-byte Reload
	vmulps	(%rcx,%rax,4), %ymm8, %ymm4
	vfmadd231ps	%ymm4, %ymm2, %ymm10    ## ymm10 = (ymm2 * ymm4) + ymm10
	movq	416(%rsp), %r14                 ## 8-byte Reload
	vinsertps	$48, 64(%r12,%r14,4), %xmm3, %xmm2 ## xmm2 = xmm3[0,1,2],mem[0]
	vinsertf128	$1, %xmm0, %ymm2, %ymm0
	movq	768(%rsp), %r10                 ## 8-byte Reload
	vmovups	64(%rcx,%r10,4), %ymm14
	vcmpltps	%ymm14, %ymm13, %ymm3
	vmovaps	1344(%rsp), %ymm2               ## 32-byte Reload
	vblendvps	%ymm3, %ymm0, %ymm2, %ymm2
	vmovaps	%ymm3, %ymm12
	movq	176(%rsp), %rcx                 ## 8-byte Reload
	vmovss	36(%r12,%rcx,4), %xmm0          ## xmm0 = mem[0],zero,zero,zero
	movq	%rsi, %rax
	vinsertps	$16, 36(%r12,%rsi,4), %xmm0, %xmm0 ## xmm0 = xmm0[0],mem[0],xmm0[2,3]
	vmovss	36(%r12,%rdx,4), %xmm3          ## xmm3 = mem[0],zero,zero,zero
	movq	144(%rsp), %r10                 ## 8-byte Reload
	vinsertps	$16, 36(%r12,%r10,4), %xmm3, %xmm3 ## xmm3 = xmm3[0],mem[0],xmm3[2,3]
	movq	320(%rsp), %rsi                 ## 8-byte Reload
	vinsertps	$32, 36(%r12,%rsi,4), %xmm0, %xmm0 ## xmm0 = xmm0[0,1],mem[0],xmm0[3]
	movq	400(%rsp), %rdx                 ## 8-byte Reload
	vinsertps	$32, 36(%r12,%rdx,4), %xmm3, %xmm3 ## xmm3 = xmm3[0,1],mem[0],xmm3[3]
	movq	384(%rsp), %rdx                 ## 8-byte Reload
	vinsertps	$48, 36(%r12,%rdx,4), %xmm0, %xmm0 ## xmm0 = xmm0[0,1,2],mem[0]
	movq	40(%rsp), %r15                  ## 8-byte Reload
	vinsertps	$48, 36(%r12,%r15,4), %xmm3, %xmm3 ## xmm3 = xmm3[0,1,2],mem[0]
	vinsertf128	$1, %xmm0, %ymm3, %ymm0
	vmovss	40(%r12,%rcx,4), %xmm3          ## xmm3 = mem[0],zero,zero,zero
	vinsertps	$16, 40(%r12,%rax,4), %xmm3, %xmm3 ## xmm3 = xmm3[0],mem[0],xmm3[2,3]
	movq	864(%rsp), %rax                 ## 8-byte Reload
	movq	640(%rsp), %r15                 ## 8-byte Reload
	vmulps	(%r15,%rax,4), %ymm1, %ymm4
	vfmadd231ps	%ymm4, %ymm0, %ymm10    ## ymm10 = (ymm0 * ymm4) + ymm10
	vinsertps	$32, 40(%r12,%rsi,4), %xmm3, %xmm0 ## xmm0 = xmm3[0,1],mem[0],xmm3[3]
	movq	288(%rsp), %rax                 ## 8-byte Reload
	vmovss	40(%r12,%rax,4), %xmm3          ## xmm3 = mem[0],zero,zero,zero
	vinsertps	$16, 40(%r12,%r10,4), %xmm3, %xmm3 ## xmm3 = xmm3[0],mem[0],xmm3[2,3]
	vinsertps	$48, 40(%r12,%rdx,4), %xmm0, %xmm0 ## xmm0 = xmm0[0,1,2],mem[0]
	movq	%rdx, %r10
	movq	400(%rsp), %rcx                 ## 8-byte Reload
	vinsertps	$32, 40(%r12,%rcx,4), %xmm3, %xmm3 ## xmm3 = xmm3[0,1],mem[0],xmm3[3]
	movq	40(%rsp), %rdx                  ## 8-byte Reload
	vinsertps	$48, 40(%r12,%rdx,4), %xmm3, %xmm3 ## xmm3 = xmm3[0,1,2],mem[0]
	vinsertf128	$1, %xmm0, %ymm3, %ymm0
	vmovss	76(%r12,%r9,4), %xmm3           ## xmm3 = mem[0],zero,zero,zero
	vinsertps	$16, 76(%r12,%r11,4), %xmm3, %xmm3 ## xmm3 = xmm3[0],mem[0],xmm3[2,3]
	vmovss	76(%r12,%rbx,4), %xmm4          ## xmm4 = mem[0],zero,zero,zero
	vinsertps	$16, 76(%r12,%r13,4), %xmm4, %xmm4 ## xmm4 = xmm4[0],mem[0],xmm4[2,3]
	movq	176(%rsp), %rax                 ## 8-byte Reload
	vmovss	44(%r12,%rax,4), %xmm5          ## xmm5 = mem[0],zero,zero,zero
	movq	160(%rsp), %rax                 ## 8-byte Reload
	vinsertps	$16, 44(%r12,%rax,4), %xmm5, %xmm5 ## xmm5 = xmm5[0],mem[0],xmm5[2,3]
	vinsertps	$32, 76(%r12,%r8,4), %xmm3, %xmm3 ## xmm3 = xmm3[0,1],mem[0],xmm3[3]
	vinsertps	$32, 44(%r12,%rsi,4), %xmm5, %xmm5 ## xmm5 = xmm5[0,1],mem[0],xmm5[3]
	movq	896(%rsp), %rax                 ## 8-byte Reload
	movq	%r15, %rsi
	vmovups	(%r15,%rax,4), %ymm6
	vmulps	%ymm6, %ymm8, %ymm7
	vfmadd231ps	%ymm7, %ymm0, %ymm10    ## ymm10 = (ymm0 * ymm7) + ymm10
	vinsertps	$48, 44(%r12,%r10,4), %xmm5, %xmm0 ## xmm0 = xmm5[0,1,2],mem[0]
	movq	288(%rsp), %r10                 ## 8-byte Reload
	vmovss	44(%r12,%r10,4), %xmm5          ## xmm5 = mem[0],zero,zero,zero
	movq	144(%rsp), %rax                 ## 8-byte Reload
	vinsertps	$16, 44(%r12,%rax,4), %xmm5, %xmm5 ## xmm5 = xmm5[0],mem[0],xmm5[2,3]
	movq	64(%rsp), %rax                  ## 8-byte Reload
	vinsertps	$32, 76(%r12,%rax,4), %xmm4, %xmm4 ## xmm4 = xmm4[0,1],mem[0],xmm4[3]
	vinsertps	$32, 44(%r12,%rcx,4), %xmm5, %xmm5 ## xmm5 = xmm5[0,1],mem[0],xmm5[3]
	vinsertps	$48, 76(%r12,%rdi,4), %xmm3, %xmm3 ## xmm3 = xmm3[0,1,2],mem[0]
	vinsertps	$48, 44(%r12,%rdx,4), %xmm5, %xmm5 ## xmm5 = xmm5[0,1,2],mem[0]
	vinsertps	$48, 76(%r12,%r14,4), %xmm4, %xmm4 ## xmm4 = xmm4[0,1,2],mem[0]
	vinsertf128	$1, %xmm3, %ymm4, %ymm3
	vinsertf128	$1, %xmm0, %ymm5, %ymm0
	vmovaps	1600(%rsp), %ymm4               ## 32-byte Reload
	vmovaps	%ymm12, %ymm15
	vmovaps	%ymm12, 1984(%rsp)              ## 32-byte Spill
	vblendvps	%ymm12, %ymm3, %ymm4, %ymm12
	vmulps	%ymm6, %ymm1, %ymm3
	vmovss	88(%r12,%r9,4), %xmm4           ## xmm4 = mem[0],zero,zero,zero
	vinsertps	$16, 88(%r12,%r11,4), %xmm4, %xmm4 ## xmm4 = xmm4[0],mem[0],xmm4[2,3]
	vfmadd231ps	%ymm3, %ymm0, %ymm10    ## ymm10 = (ymm0 * ymm3) + ymm10
	vinsertps	$32, 88(%r12,%r8,4), %xmm4, %xmm0 ## xmm0 = xmm4[0,1],mem[0],xmm4[3]
	vmovss	88(%r12,%rbx,4), %xmm3          ## xmm3 = mem[0],zero,zero,zero
	vinsertps	$16, 88(%r12,%r13,4), %xmm3, %xmm3 ## xmm3 = xmm3[0],mem[0],xmm3[2,3]
	vinsertps	$48, 88(%r12,%rdi,4), %xmm0, %xmm0 ## xmm0 = xmm0[0,1,2],mem[0]
	vinsertps	$32, 88(%r12,%rax,4), %xmm3, %xmm3 ## xmm3 = xmm3[0,1],mem[0],xmm3[3]
	vinsertps	$48, 88(%r12,%r14,4), %xmm3, %xmm3 ## xmm3 = xmm3[0,1,2],mem[0]
	vinsertf128	$1, %xmm0, %ymm3, %ymm0
	vmovss	92(%r12,%r9,4), %xmm3           ## xmm3 = mem[0],zero,zero,zero
	vinsertps	$16, 92(%r12,%r11,4), %xmm3, %xmm3 ## xmm3 = xmm3[0],mem[0],xmm3[2,3]
	vinsertps	$32, 92(%r12,%r8,4), %xmm3, %xmm3 ## xmm3 = xmm3[0,1],mem[0],xmm3[3]
	vinsertps	$48, 92(%r12,%rdi,4), %xmm3, %xmm3 ## xmm3 = xmm3[0,1,2],mem[0]
	vmovss	92(%r12,%rbx,4), %xmm4          ## xmm4 = mem[0],zero,zero,zero
	vinsertps	$16, 92(%r12,%r13,4), %xmm4, %xmm4 ## xmm4 = xmm4[0],mem[0],xmm4[2,3]
	vmovaps	2080(%rsp), %ymm5               ## 32-byte Reload
	vaddps	1632(%rsp), %ymm5, %ymm6        ## 32-byte Folded Reload
	vmulps	%ymm0, %ymm6, %ymm0
	vinsertps	$32, 92(%r12,%rax,4), %xmm4, %xmm4 ## xmm4 = xmm4[0,1],mem[0],xmm4[3]
	movq	200(%rsp), %rax                 ## 8-byte Reload
	movq	%r15, %rdx
	vmovups	64(%r15,%rax,4), %ymm6
	vmaxps	%ymm13, %ymm6, %ymm6
	vdivps	%ymm6, %ymm0, %ymm0
	vinsertps	$48, 92(%r12,%r14,4), %xmm4, %xmm4 ## xmm4 = xmm4[0,1,2],mem[0]
	movq	688(%rsp), %rax                 ## 8-byte Reload
	vmulps	64(%r15,%rax,4), %ymm14, %ymm7
	vmulps	%ymm2, %ymm7, %ymm2
	vdivps	%ymm6, %ymm2, %ymm2
	vinsertf128	$1, %xmm3, %ymm4, %ymm3
	vdivps	%ymm6, %ymm9, %ymm4
	vmovaps	%ymm9, %ymm5
	movq	672(%rsp), %rax                 ## 8-byte Reload
	vfmadd132ps	64(%r15,%rax,4), %ymm2, %ymm12 ## ymm12 = (ymm12 * mem) + ymm2
	vandps	%ymm0, %ymm15, %ymm0
	movq	1200(%rsp), %rax                ## 8-byte Reload
	vmovups	-32(%r15,%rax), %ymm2
	vmovaps	%ymm2, 1600(%rsp)               ## 32-byte Spill
	vfmadd213ps	%ymm0, %ymm2, %ymm12    ## ymm12 = (ymm2 * ymm12) + ymm0
	vmovaps	%ymm14, 2016(%rsp)              ## 32-byte Spill
	vminps	%ymm14, %ymm4, %ymm0
	movq	800(%rsp), %rax                 ## 8-byte Reload
	vmulps	64(%r15,%rax,4), %ymm0, %ymm0
	movq	784(%rsp), %rax                 ## 8-byte Reload
	vmulps	64(%r15,%rax,4), %ymm14, %ymm2
	movq	%r15, %r14
	vmovaps	%ymm2, 2336(%rsp)               ## 32-byte Spill
	vmulps	%ymm2, %ymm0, %ymm0
	vfmadd231ps	%ymm0, %ymm3, %ymm12    ## ymm12 = (ymm3 * ymm0) + ymm12
	vmovaps	%ymm12, 1344(%rsp)              ## 32-byte Spill
	movq	176(%rsp), %rsi                 ## 8-byte Reload
	vmovss	48(%r12,%rsi,4), %xmm0          ## xmm0 = mem[0],zero,zero,zero
	movq	160(%rsp), %rdx                 ## 8-byte Reload
	vinsertps	$16, 48(%r12,%rdx,4), %xmm0, %xmm0 ## xmm0 = xmm0[0],mem[0],xmm0[2,3]
	movq	%r10, %r9
	vmovss	48(%r12,%r10,4), %xmm2          ## xmm2 = mem[0],zero,zero,zero
	movq	144(%rsp), %r8                  ## 8-byte Reload
	vinsertps	$16, 48(%r12,%r8,4), %xmm2, %xmm2 ## xmm2 = xmm2[0],mem[0],xmm2[2,3]
	movq	320(%rsp), %rax                 ## 8-byte Reload
	vinsertps	$32, 48(%r12,%rax,4), %xmm0, %xmm0 ## xmm0 = xmm0[0,1],mem[0],xmm0[3]
	movq	%rcx, %r10
	vinsertps	$32, 48(%r12,%rcx,4), %xmm2, %xmm2 ## xmm2 = xmm2[0,1],mem[0],xmm2[3]
	movq	384(%rsp), %rdi                 ## 8-byte Reload
	vinsertps	$48, 48(%r12,%rdi,4), %xmm0, %xmm0 ## xmm0 = xmm0[0,1,2],mem[0]
	movq	40(%rsp), %rbx                  ## 8-byte Reload
	vinsertps	$48, 48(%r12,%rbx,4), %xmm2, %xmm2 ## xmm2 = xmm2[0,1,2],mem[0]
	vinsertf128	$1, %xmm0, %ymm2, %ymm0
	vmovss	52(%r12,%rsi,4), %xmm2          ## xmm2 = mem[0],zero,zero,zero
	vinsertps	$16, 52(%r12,%rdx,4), %xmm2, %xmm2 ## xmm2 = xmm2[0],mem[0],xmm2[2,3]
	movq	912(%rsp), %rcx                 ## 8-byte Reload
	vmovups	(%r15,%rcx,4), %ymm3
	vmulps	%ymm3, %ymm8, %ymm4
	vfmadd231ps	%ymm4, %ymm0, %ymm10    ## ymm10 = (ymm0 * ymm4) + ymm10
	vinsertps	$32, 52(%r12,%rax,4), %xmm2, %xmm0 ## xmm0 = xmm2[0,1],mem[0],xmm2[3]
	vmovss	52(%r12,%r9,4), %xmm2           ## xmm2 = mem[0],zero,zero,zero
	vinsertps	$16, 52(%r12,%r8,4), %xmm2, %xmm2 ## xmm2 = xmm2[0],mem[0],xmm2[2,3]
	vinsertps	$48, 52(%r12,%rdi,4), %xmm0, %xmm0 ## xmm0 = xmm0[0,1,2],mem[0]
	vinsertps	$32, 52(%r12,%r10,4), %xmm2, %xmm2 ## xmm2 = xmm2[0,1],mem[0],xmm2[3]
	vinsertps	$48, 52(%r12,%rbx,4), %xmm2, %xmm2 ## xmm2 = xmm2[0,1,2],mem[0]
	vinsertf128	$1, %xmm0, %ymm2, %ymm0
	vmovss	56(%r12,%rsi,4), %xmm2          ## xmm2 = mem[0],zero,zero,zero
	vinsertps	$16, 56(%r12,%rdx,4), %xmm2, %xmm2 ## xmm2 = xmm2[0],mem[0],xmm2[2,3]
	vmulps	%ymm3, %ymm1, %ymm3
	vinsertps	$32, 56(%r12,%rax,4), %xmm2, %xmm2 ## xmm2 = xmm2[0,1],mem[0],xmm2[3]
	vmovss	56(%r12,%r9,4), %xmm4           ## xmm4 = mem[0],zero,zero,zero
	vinsertps	$16, 56(%r12,%r8,4), %xmm4, %xmm4 ## xmm4 = xmm4[0],mem[0],xmm4[2,3]
	vinsertps	$48, 56(%r12,%rdi,4), %xmm2, %xmm2 ## xmm2 = xmm2[0,1,2],mem[0]
	vinsertps	$32, 56(%r12,%r10,4), %xmm4, %xmm4 ## xmm4 = xmm4[0,1],mem[0],xmm4[3]
	vinsertps	$48, 56(%r12,%rbx,4), %xmm4, %xmm4 ## xmm4 = xmm4[0,1,2],mem[0]
	vfmadd231ps	%ymm3, %ymm0, %ymm10    ## ymm10 = (ymm0 * ymm3) + ymm10
	movq	352(%rsp), %r8                  ## 8-byte Reload
	vmovss	20(%r12,%r8,4), %xmm0           ## xmm0 = mem[0],zero,zero,zero
	movq	544(%rsp), %r9                  ## 8-byte Reload
	vinsertps	$16, 20(%r12,%r9,4), %xmm0, %xmm0 ## xmm0 = xmm0[0],mem[0],xmm0[2,3]
	vinsertf128	$1, %xmm2, %ymm4, %ymm2
	movq	336(%rsp), %r13                 ## 8-byte Reload
	vinsertps	$32, 20(%r12,%r13,4), %xmm0, %xmm0 ## xmm0 = xmm0[0,1],mem[0],xmm0[3]
	movq	32(%rsp), %r11                  ## 8-byte Reload
	vmovss	20(%r12,%r11,4), %xmm3          ## xmm3 = mem[0],zero,zero,zero
	movq	256(%rsp), %rbx                 ## 8-byte Reload
	vinsertps	$16, 20(%r12,%rbx,4), %xmm3, %xmm4 ## xmm4 = xmm3[0],mem[0],xmm3[2,3]
	movq	240(%rsp), %rax                 ## 8-byte Reload
	vinsertps	$48, 20(%r12,%rax,4), %xmm0, %xmm0 ## xmm0 = xmm0[0,1,2],mem[0]
	vmovaps	1952(%rsp), %ymm1               ## 32-byte Reload
	vmaxps	%ymm13, %ymm1, %ymm15
	movq	304(%rsp), %rsi                 ## 8-byte Reload
	vinsertps	$32, 20(%r12,%rsi,4), %xmm4, %xmm4 ## xmm4 = xmm4[0,1],mem[0],xmm4[3]
	vmovss	24(%r12,%r8,4), %xmm6           ## xmm6 = mem[0],zero,zero,zero
	vinsertps	$16, 24(%r12,%r9,4), %xmm6, %xmm6 ## xmm6 = xmm6[0],mem[0],xmm6[2,3]
	movq	208(%rsp), %rdx                 ## 8-byte Reload
	vmulps	(%r15,%rdx,4), %ymm15, %ymm7
	vfmadd231ps	%ymm7, %ymm2, %ymm10    ## ymm10 = (ymm2 * ymm7) + ymm10
	vinsertps	$32, 24(%r12,%r13,4), %xmm6, %xmm2 ## xmm2 = xmm6[0,1],mem[0],xmm6[3]
	vmovss	24(%r12,%r11,4), %xmm6          ## xmm6 = mem[0],zero,zero,zero
	vinsertps	$16, 24(%r12,%rbx,4), %xmm6, %xmm6 ## xmm6 = xmm6[0],mem[0],xmm6[2,3]
	movq	80(%rsp), %r15                  ## 8-byte Reload
	vinsertps	$48, 20(%r12,%r15,4), %xmm4, %xmm4 ## xmm4 = xmm4[0,1,2],mem[0]
	vinsertps	$32, 24(%r12,%rsi,4), %xmm6, %xmm6 ## xmm6 = xmm6[0,1],mem[0],xmm6[3]
	movq	%rsi, %rdx
	vinsertps	$48, 24(%r12,%rax,4), %xmm2, %xmm2 ## xmm2 = xmm2[0,1,2],mem[0]
	vinsertf128	$1, %xmm0, %ymm4, %ymm4
	vinsertps	$48, 24(%r12,%r15,4), %xmm6, %xmm0 ## xmm0 = xmm6[0,1,2],mem[0]
	vinsertf128	$1, %xmm2, %ymm0, %ymm0
	vmovaps	%ymm11, 3008(%rsp)              ## 32-byte Spill
	movq	816(%rsp), %rcx                 ## 8-byte Reload
	vmulps	32(%r14,%rcx,4), %ymm11, %ymm2
	vmulps	%ymm2, %ymm0, %ymm8
	vmovss	28(%r12,%r8,4), %xmm2           ## xmm2 = mem[0],zero,zero,zero
	movq	%r9, %rdi
	vinsertps	$16, 28(%r12,%r9,4), %xmm2, %xmm2 ## xmm2 = xmm2[0],mem[0],xmm2[2,3]
	movq	832(%rsp), %rcx                 ## 8-byte Reload
	vmulps	32(%r14,%rcx,4), %ymm11, %ymm6
	vfmadd231ps	%ymm6, %ymm4, %ymm8     ## ymm8 = (ymm4 * ymm6) + ymm8
	vinsertps	$32, 28(%r12,%r13,4), %xmm2, %xmm2 ## xmm2 = xmm2[0,1],mem[0],xmm2[3]
	vmovss	28(%r12,%r11,4), %xmm4          ## xmm4 = mem[0],zero,zero,zero
	vinsertps	$16, 28(%r12,%rbx,4), %xmm4, %xmm4 ## xmm4 = xmm4[0],mem[0],xmm4[2,3]
	vinsertps	$48, 28(%r12,%rax,4), %xmm2, %xmm2 ## xmm2 = xmm2[0,1,2],mem[0]
	vinsertps	$32, 28(%r12,%rsi,4), %xmm4, %xmm4 ## xmm4 = xmm4[0,1],mem[0],xmm4[3]
	vinsertps	$48, 28(%r12,%r15,4), %xmm4, %xmm4 ## xmm4 = xmm4[0,1,2],mem[0]
	vinsertf128	$1, %xmm2, %ymm4, %ymm2
	movq	464(%rsp), %rsi                 ## 8-byte Reload
	vmovss	92(%r12,%rsi,4), %xmm4          ## xmm4 = mem[0],zero,zero,zero
	movq	736(%rsp), %rcx                 ## 8-byte Reload
	vinsertps	$16, 92(%r12,%rcx,4), %xmm4, %xmm4 ## xmm4 = xmm4[0],mem[0],xmm4[2,3]
	movq	448(%rsp), %r11                 ## 8-byte Reload
	vinsertps	$32, 92(%r12,%r11,4), %xmm4, %xmm4 ## xmm4 = xmm4[0,1],mem[0],xmm4[3]
	movq	120(%rsp), %r10                 ## 8-byte Reload
	vinsertps	$48, 92(%r12,%r10,4), %xmm4, %xmm4 ## xmm4 = xmm4[0,1,2],mem[0]
	movq	112(%rsp), %rdx                 ## 8-byte Reload
	vmovss	92(%r12,%rdx,4), %xmm6          ## xmm6 = mem[0],zero,zero,zero
	movq	1152(%rsp), %r9                 ## 8-byte Reload
	vinsertps	$16, 92(%r12,%r9,4), %xmm6, %xmm6 ## xmm6 = xmm6[0],mem[0],xmm6[2,3]
	vmovss	32(%r12,%r8,4), %xmm7           ## xmm7 = mem[0],zero,zero,zero
	vinsertps	$16, 32(%r12,%rdi,4), %xmm7, %xmm7 ## xmm7 = xmm7[0],mem[0],xmm7[2,3]
	vmovaps	2048(%rsp), %ymm11              ## 32-byte Reload
	movq	848(%rsp), %rax                 ## 8-byte Reload
	vmulps	32(%r14,%rax,4), %ymm11, %ymm9
	vfmadd231ps	%ymm9, %ymm2, %ymm8     ## ymm8 = (ymm2 * ymm9) + ymm8
	vinsertps	$32, 32(%r12,%r13,4), %xmm7, %xmm2 ## xmm2 = xmm7[0,1],mem[0],xmm7[3]
	movq	32(%rsp), %r13                  ## 8-byte Reload
	vmovss	32(%r12,%r13,4), %xmm7          ## xmm7 = mem[0],zero,zero,zero
	vinsertps	$16, 32(%r12,%rbx,4), %xmm7, %xmm7 ## xmm7 = xmm7[0],mem[0],xmm7[2,3]
	movq	240(%rsp), %rax                 ## 8-byte Reload
	vinsertps	$48, 32(%r12,%rax,4), %xmm2, %xmm2 ## xmm2 = xmm2[0,1,2],mem[0]
	movq	304(%rsp), %rax                 ## 8-byte Reload
	vinsertps	$32, 32(%r12,%rax,4), %xmm7, %xmm7 ## xmm7 = xmm7[0,1],mem[0],xmm7[3]
	vinsertps	$48, 32(%r12,%r15,4), %xmm7, %xmm7 ## xmm7 = xmm7[0,1,2],mem[0]
	vinsertf128	$1, %xmm2, %ymm7, %ymm2
	movq	104(%rsp), %r8                  ## 8-byte Reload
	vinsertps	$32, 92(%r12,%r8,4), %xmm6, %xmm6 ## xmm6 = xmm6[0,1],mem[0],xmm6[3]
	vmovss	88(%r12,%rsi,4), %xmm7          ## xmm7 = mem[0],zero,zero,zero
	vinsertps	$16, 88(%r12,%rcx,4), %xmm7, %xmm7 ## xmm7 = xmm7[0],mem[0],xmm7[2,3]
	vmovaps	2176(%rsp), %ymm14              ## 32-byte Reload
	movq	880(%rsp), %rax                 ## 8-byte Reload
	vmulps	32(%r14,%rax,4), %ymm14, %ymm9
	vfmadd231ps	%ymm9, %ymm2, %ymm8     ## ymm8 = (ymm2 * ymm9) + ymm8
	vinsertps	$32, 88(%r12,%r11,4), %xmm7, %xmm2 ## xmm2 = xmm7[0,1],mem[0],xmm7[3]
	vmovss	88(%r12,%rdx,4), %xmm7          ## xmm7 = mem[0],zero,zero,zero
	vinsertps	$16, 88(%r12,%r9,4), %xmm7, %xmm7 ## xmm7 = xmm7[0],mem[0],xmm7[2,3]
	movq	128(%rsp), %rdi                 ## 8-byte Reload
	vinsertps	$48, 92(%r12,%rdi,4), %xmm6, %xmm6 ## xmm6 = xmm6[0,1,2],mem[0]
	vinsertps	$32, 88(%r12,%r8,4), %xmm7, %xmm7 ## xmm7 = xmm7[0,1],mem[0],xmm7[3]
	vinsertps	$48, 88(%r12,%r10,4), %xmm2, %xmm2 ## xmm2 = xmm2[0,1,2],mem[0]
	vinsertf128	$1, %xmm4, %ymm6, %ymm9
	vinsertps	$48, 88(%r12,%rdi,4), %xmm7, %xmm4 ## xmm4 = xmm7[0,1,2],mem[0]
	vinsertf128	$1, %xmm2, %ymm4, %ymm1
	vmovss	64(%r12,%rsi,4), %xmm4          ## xmm4 = mem[0],zero,zero,zero
	vinsertps	$16, 64(%r12,%rcx,4), %xmm4, %xmm4 ## xmm4 = xmm4[0],mem[0],xmm4[2,3]
	vmovss	64(%r12,%rdx,4), %xmm6          ## xmm6 = mem[0],zero,zero,zero
	vinsertps	$16, 64(%r12,%r9,4), %xmm6, %xmm6 ## xmm6 = xmm6[0],mem[0],xmm6[2,3]
	vinsertps	$32, 64(%r12,%r11,4), %xmm4, %xmm4 ## xmm4 = xmm4[0,1],mem[0],xmm4[3]
	vinsertps	$32, 64(%r12,%r8,4), %xmm6, %xmm6 ## xmm6 = xmm6[0,1],mem[0],xmm6[3]
	vinsertps	$48, 64(%r12,%r10,4), %xmm4, %xmm4 ## xmm4 = xmm4[0,1,2],mem[0]
	vinsertps	$48, 64(%r12,%rdi,4), %xmm6, %xmm6 ## xmm6 = xmm6[0,1,2],mem[0]
	vinsertf128	$1, %xmm4, %ymm6, %ymm4
	movq	768(%rsp), %rbx                 ## 8-byte Reload
	vmovups	96(%r14,%rbx,4), %ymm2
	vcmpltps	%ymm2, %ymm13, %ymm3
	vmovaps	1568(%rsp), %ymm0               ## 32-byte Reload
	vblendvps	%ymm3, %ymm4, %ymm0, %ymm4
	vmovaps	%ymm3, %ymm0
	movq	200(%rsp), %rbx                 ## 8-byte Reload
	vmovups	96(%r14,%rbx,4), %ymm6
	vmaxps	%ymm13, %ymm6, %ymm6
	vdivps	%ymm6, %ymm5, %ymm3
	vmovss	76(%r12,%rsi,4), %xmm7          ## xmm7 = mem[0],zero,zero,zero
	vinsertps	$16, 76(%r12,%rcx,4), %xmm7, %xmm7 ## xmm7 = xmm7[0],mem[0],xmm7[2,3]
	vmovaps	2112(%rsp), %ymm12              ## 32-byte Reload
	vaddps	1664(%rsp), %ymm12, %ymm13      ## 32-byte Folded Reload
	vmulps	%ymm1, %ymm13, %ymm1
	vinsertps	$32, 76(%r12,%r11,4), %xmm7, %xmm7 ## xmm7 = xmm7[0,1],mem[0],xmm7[3]
	vmovss	76(%r12,%rdx,4), %xmm5          ## xmm5 = mem[0],zero,zero,zero
	vinsertps	$16, 76(%r12,%r9,4), %xmm5, %xmm5 ## xmm5 = xmm5[0],mem[0],xmm5[2,3]
	vinsertps	$48, 76(%r12,%r10,4), %xmm7, %xmm7 ## xmm7 = xmm7[0,1,2],mem[0]
	vinsertps	$32, 76(%r12,%r8,4), %xmm5, %xmm5 ## xmm5 = xmm5[0,1],mem[0],xmm5[3]
	vinsertps	$48, 76(%r12,%rdi,4), %xmm5, %xmm5 ## xmm5 = xmm5[0,1,2],mem[0]
	vinsertf128	$1, %xmm7, %ymm5, %ymm5
	vdivps	%ymm6, %ymm1, %ymm7
	vmovaps	2400(%rsp), %ymm1               ## 32-byte Reload
	vmovaps	%ymm0, 1536(%rsp)               ## 32-byte Spill
	vblendvps	%ymm0, %ymm5, %ymm1, %ymm13
	movq	688(%rsp), %rbx                 ## 8-byte Reload
	vmovaps	%ymm2, %ymm1
	vmovaps	%ymm2, 1952(%rsp)               ## 32-byte Spill
	vmulps	96(%r14,%rbx,4), %ymm2, %ymm5
	vmulps	%ymm4, %ymm5, %ymm4
	vdivps	%ymm6, %ymm4, %ymm4
	movq	672(%rsp), %rbx                 ## 8-byte Reload
	vfmadd132ps	96(%r14,%rbx,4), %ymm4, %ymm13 ## ymm13 = (ymm13 * mem) + ymm4
	vandps	%ymm7, %ymm0, %ymm2
	movq	1200(%rsp), %rax                ## 8-byte Reload
	vmovups	(%r14,%rax), %ymm0
	vmovaps	%ymm0, 1568(%rsp)               ## 32-byte Spill
	vfmadd213ps	%ymm2, %ymm0, %ymm13    ## ymm13 = (ymm0 * ymm13) + ymm2
	vminps	%ymm1, %ymm3, %ymm2
	movq	800(%rsp), %rax                 ## 8-byte Reload
	vmulps	96(%r14,%rax,4), %ymm2, %ymm2
	movq	784(%rsp), %rax                 ## 8-byte Reload
	vmulps	96(%r14,%rax,4), %ymm1, %ymm0
	vmovaps	%ymm0, 2400(%rsp)               ## 32-byte Spill
	vmulps	%ymm0, %ymm2, %ymm2
	vfmadd231ps	%ymm2, %ymm9, %ymm13    ## ymm13 = (ymm9 * ymm2) + ymm13
	movq	352(%rsp), %rax                 ## 8-byte Reload
	vmovss	36(%r12,%rax,4), %xmm2          ## xmm2 = mem[0],zero,zero,zero
	movq	544(%rsp), %rcx                 ## 8-byte Reload
	vinsertps	$16, 36(%r12,%rcx,4), %xmm2, %xmm2 ## xmm2 = xmm2[0],mem[0],xmm2[2,3]
	movq	%r13, %rbx
	vmovss	36(%r12,%r13,4), %xmm3          ## xmm3 = mem[0],zero,zero,zero
	movq	256(%rsp), %r9                  ## 8-byte Reload
	vinsertps	$16, 36(%r12,%r9,4), %xmm3, %xmm3 ## xmm3 = xmm3[0],mem[0],xmm3[2,3]
	movq	336(%rsp), %rdx                 ## 8-byte Reload
	vinsertps	$32, 36(%r12,%rdx,4), %xmm2, %xmm2 ## xmm2 = xmm2[0,1],mem[0],xmm2[3]
	movq	304(%rsp), %rdi                 ## 8-byte Reload
	vinsertps	$32, 36(%r12,%rdi,4), %xmm3, %xmm3 ## xmm3 = xmm3[0,1],mem[0],xmm3[3]
	movq	240(%rsp), %r8                  ## 8-byte Reload
	vinsertps	$48, 36(%r12,%r8,4), %xmm2, %xmm2 ## xmm2 = xmm2[0,1,2],mem[0]
	vinsertps	$48, 36(%r12,%r15,4), %xmm3, %xmm3 ## xmm3 = xmm3[0,1,2],mem[0]
	vinsertf128	$1, %xmm2, %ymm3, %ymm2
	vmovss	48(%r12,%rax,4), %xmm3          ## xmm3 = mem[0],zero,zero,zero
	vinsertps	$16, 48(%r12,%rcx,4), %xmm3, %xmm3 ## xmm3 = xmm3[0],mem[0],xmm3[2,3]
	vinsertps	$32, 48(%r12,%rdx,4), %xmm3, %xmm3 ## xmm3 = xmm3[0,1],mem[0],xmm3[3]
	vmovss	48(%r12,%r13,4), %xmm5          ## xmm5 = mem[0],zero,zero,zero
	vinsertps	$16, 48(%r12,%r9,4), %xmm5, %xmm5 ## xmm5 = xmm5[0],mem[0],xmm5[2,3]
	vmovss	40(%r12,%rax,4), %xmm7          ## xmm7 = mem[0],zero,zero,zero
	vinsertps	$16, 40(%r12,%rcx,4), %xmm7, %xmm7 ## xmm7 = xmm7[0],mem[0],xmm7[2,3]
	movq	864(%rsp), %rsi                 ## 8-byte Reload
	vmulps	32(%r14,%rsi,4), %ymm11, %ymm9
	vfmadd231ps	%ymm9, %ymm2, %ymm8     ## ymm8 = (ymm2 * ymm9) + ymm8
	vinsertps	$32, 40(%r12,%rdx,4), %xmm7, %xmm2 ## xmm2 = xmm7[0,1],mem[0],xmm7[3]
	vmovss	40(%r12,%r13,4), %xmm7          ## xmm7 = mem[0],zero,zero,zero
	vinsertps	$16, 40(%r12,%r9,4), %xmm7, %xmm7 ## xmm7 = xmm7[0],mem[0],xmm7[2,3]
	vinsertps	$48, 40(%r12,%r8,4), %xmm2, %xmm2 ## xmm2 = xmm2[0,1,2],mem[0]
	vinsertps	$32, 40(%r12,%rdi,4), %xmm7, %xmm7 ## xmm7 = xmm7[0,1],mem[0],xmm7[3]
	vinsertps	$48, 40(%r12,%r15,4), %xmm7, %xmm7 ## xmm7 = xmm7[0,1,2],mem[0]
	vinsertf128	$1, %xmm2, %ymm7, %ymm2
	vinsertps	$32, 48(%r12,%rdi,4), %xmm5, %xmm5 ## xmm5 = xmm5[0,1],mem[0],xmm5[3]
	vmovss	44(%r12,%rax,4), %xmm7          ## xmm7 = mem[0],zero,zero,zero
	vinsertps	$16, 44(%r12,%rcx,4), %xmm7, %xmm7 ## xmm7 = xmm7[0],mem[0],xmm7[2,3]
	movq	896(%rsp), %rax                 ## 8-byte Reload
	vmovups	32(%r14,%rax,4), %ymm9
	vmovaps	%ymm14, %ymm4
	vmulps	%ymm9, %ymm14, %ymm14
	vfmadd231ps	%ymm14, %ymm2, %ymm8    ## ymm8 = (ymm2 * ymm14) + ymm8
	vinsertps	$32, 44(%r12,%rdx,4), %xmm7, %xmm2 ## xmm2 = xmm7[0,1],mem[0],xmm7[3]
	vmovss	44(%r12,%r13,4), %xmm7          ## xmm7 = mem[0],zero,zero,zero
	vinsertps	$16, 44(%r12,%r9,4), %xmm7, %xmm7 ## xmm7 = xmm7[0],mem[0],xmm7[2,3]
	vinsertps	$48, 44(%r12,%r8,4), %xmm2, %xmm2 ## xmm2 = xmm2[0,1,2],mem[0]
	vinsertps	$32, 44(%r12,%rdi,4), %xmm7, %xmm7 ## xmm7 = xmm7[0,1],mem[0],xmm7[3]
	vinsertps	$48, 44(%r12,%r15,4), %xmm7, %xmm7 ## xmm7 = xmm7[0,1,2],mem[0]
	vinsertf128	$1, %xmm2, %ymm7, %ymm2
	vmulps	%ymm9, %ymm11, %ymm7
	vinsertps	$48, 48(%r12,%r8,4), %xmm3, %xmm3 ## xmm3 = xmm3[0,1,2],mem[0]
	vfmadd231ps	%ymm7, %ymm2, %ymm8     ## ymm8 = (ymm2 * ymm7) + ymm8
	vinsertps	$48, 48(%r12,%r15,4), %xmm5, %xmm2 ## xmm2 = xmm5[0,1,2],mem[0]
	movq	%r15, %r13
	vinsertf128	$1, %xmm3, %ymm2, %ymm2
	movq	224(%rsp), %rdx                 ## 8-byte Reload
	vmovss	20(%r12,%rdx,4), %xmm3          ## xmm3 = mem[0],zero,zero,zero
	movq	56(%rsp), %r8                   ## 8-byte Reload
	vinsertps	$16, 20(%r12,%r8,4), %xmm3, %xmm3 ## xmm3 = xmm3[0],mem[0],xmm3[2,3]
	movq	912(%rsp), %rax                 ## 8-byte Reload
	vmovups	32(%r14,%rax,4), %ymm5
	vmulps	%ymm5, %ymm4, %ymm7
	movq	72(%rsp), %r11                  ## 8-byte Reload
	vinsertps	$32, 20(%r12,%r11,4), %xmm3, %xmm3 ## xmm3 = xmm3[0,1],mem[0],xmm3[3]
	movq	48(%rsp), %rax                  ## 8-byte Reload
	vmovss	20(%r12,%rax,4), %xmm6          ## xmm6 = mem[0],zero,zero,zero
	movq	480(%rsp), %r15                 ## 8-byte Reload
	vinsertps	$16, 20(%r12,%r15,4), %xmm6, %xmm6 ## xmm6 = xmm6[0],mem[0],xmm6[2,3]
	movq	432(%rsp), %rcx                 ## 8-byte Reload
	vinsertps	$48, 20(%r12,%rcx,4), %xmm3, %xmm3 ## xmm3 = xmm3[0,1,2],mem[0]
	vfmadd231ps	%ymm7, %ymm2, %ymm8     ## ymm8 = (ymm2 * ymm7) + ymm8
	movq	64(%rsp), %r10                  ## 8-byte Reload
	vinsertps	$32, 20(%r12,%r10,4), %xmm6, %xmm2 ## xmm2 = xmm6[0,1],mem[0],xmm6[3]
	vmovss	24(%r12,%rdx,4), %xmm6          ## xmm6 = mem[0],zero,zero,zero
	movq	%rdx, %rsi
	vinsertps	$16, 24(%r12,%r8,4), %xmm6, %xmm6 ## xmm6 = xmm6[0],mem[0],xmm6[2,3]
	movq	416(%rsp), %r9                  ## 8-byte Reload
	vinsertps	$48, 20(%r12,%r9,4), %xmm2, %xmm2 ## xmm2 = xmm2[0,1,2],mem[0]
	vinsertf128	$1, %xmm3, %ymm2, %ymm2
	vinsertps	$32, 24(%r12,%r11,4), %xmm6, %xmm3 ## xmm3 = xmm6[0,1],mem[0],xmm6[3]
	vmovss	24(%r12,%rax,4), %xmm6          ## xmm6 = mem[0],zero,zero,zero
	vinsertps	$16, 24(%r12,%r15,4), %xmm6, %xmm6 ## xmm6 = xmm6[0],mem[0],xmm6[2,3]
	vinsertps	$48, 24(%r12,%rcx,4), %xmm3, %xmm3 ## xmm3 = xmm3[0,1,2],mem[0]
	vinsertps	$32, 24(%r12,%r10,4), %xmm6, %xmm6 ## xmm6 = xmm6[0,1],mem[0],xmm6[3]
	vinsertps	$48, 24(%r12,%r9,4), %xmm6, %xmm6 ## xmm6 = xmm6[0,1,2],mem[0]
	vinsertf128	$1, %xmm3, %ymm6, %ymm3
	vmovaps	1600(%rsp), %ymm0               ## 32-byte Reload
	movq	816(%rsp), %rdx                 ## 8-byte Reload
	vmulps	64(%r14,%rdx,4), %ymm0, %ymm6
	vmulps	%ymm6, %ymm3, %ymm9
	vmovss	32(%r12,%rsi,4), %xmm3          ## xmm3 = mem[0],zero,zero,zero
	vinsertps	$16, 32(%r12,%r8,4), %xmm3, %xmm3 ## xmm3 = xmm3[0],mem[0],xmm3[2,3]
	vmovss	28(%r12,%rsi,4), %xmm6          ## xmm6 = mem[0],zero,zero,zero
	vinsertps	$16, 28(%r12,%r8,4), %xmm6, %xmm6 ## xmm6 = xmm6[0],mem[0],xmm6[2,3]
	movq	832(%rsp), %rbx                 ## 8-byte Reload
	vmulps	64(%r14,%rbx,4), %ymm0, %ymm7
	vfmadd231ps	%ymm7, %ymm2, %ymm9     ## ymm9 = (ymm2 * ymm7) + ymm9
	movq	%r11, %rbx
	vinsertps	$32, 28(%r12,%r11,4), %xmm6, %xmm2 ## xmm2 = xmm6[0,1],mem[0],xmm6[3]
	vmovss	28(%r12,%rax,4), %xmm6          ## xmm6 = mem[0],zero,zero,zero
	vinsertps	$16, 28(%r12,%r15,4), %xmm6, %xmm6 ## xmm6 = xmm6[0],mem[0],xmm6[2,3]
	vinsertps	$48, 28(%r12,%rcx,4), %xmm2, %xmm2 ## xmm2 = xmm2[0,1,2],mem[0]
	vinsertps	$32, 28(%r12,%r10,4), %xmm6, %xmm6 ## xmm6 = xmm6[0,1],mem[0],xmm6[3]
	vinsertps	$48, 28(%r12,%r9,4), %xmm6, %xmm6 ## xmm6 = xmm6[0,1,2],mem[0]
	vinsertf128	$1, %xmm2, %ymm6, %ymm2
	vinsertps	$32, 32(%r12,%r11,4), %xmm3, %xmm3 ## xmm3 = xmm3[0,1],mem[0],xmm3[3]
	vmovss	32(%r12,%rax,4), %xmm6          ## xmm6 = mem[0],zero,zero,zero
	vinsertps	$16, 32(%r12,%r15,4), %xmm6, %xmm6 ## xmm6 = xmm6[0],mem[0],xmm6[2,3]
	vinsertps	$48, 32(%r12,%rcx,4), %xmm3, %xmm3 ## xmm3 = xmm3[0,1,2],mem[0]
	vinsertps	$32, 32(%r12,%r10,4), %xmm6, %xmm6 ## xmm6 = xmm6[0,1],mem[0],xmm6[3]
	vmovaps	2080(%rsp), %ymm1               ## 32-byte Reload
	movq	848(%rsp), %r11                 ## 8-byte Reload
	vmulps	64(%r14,%r11,4), %ymm1, %ymm7
	vfmadd231ps	%ymm7, %ymm2, %ymm9     ## ymm9 = (ymm2 * ymm7) + ymm9
	vinsertps	$48, 32(%r12,%r9,4), %xmm6, %xmm2 ## xmm2 = xmm6[0,1,2],mem[0]
	vinsertf128	$1, %xmm3, %ymm2, %ymm2
	movq	352(%rsp), %rdx                 ## 8-byte Reload
	vmovss	52(%r12,%rdx,4), %xmm3          ## xmm3 = mem[0],zero,zero,zero
	movq	544(%rsp), %rdx                 ## 8-byte Reload
	vinsertps	$16, 52(%r12,%rdx,4), %xmm3, %xmm3 ## xmm3 = xmm3[0],mem[0],xmm3[2,3]
	movq	32(%rsp), %rdx                  ## 8-byte Reload
	vmovss	52(%r12,%rdx,4), %xmm6          ## xmm6 = mem[0],zero,zero,zero
	movq	256(%rsp), %rdx                 ## 8-byte Reload
	vinsertps	$16, 52(%r12,%rdx,4), %xmm6, %xmm6 ## xmm6 = xmm6[0],mem[0],xmm6[2,3]
	movq	336(%rsp), %rdx                 ## 8-byte Reload
	vinsertps	$32, 52(%r12,%rdx,4), %xmm3, %xmm3 ## xmm3 = xmm3[0,1],mem[0],xmm3[3]
	vinsertps	$32, 52(%r12,%rdi,4), %xmm6, %xmm6 ## xmm6 = xmm6[0,1],mem[0],xmm6[3]
	movq	240(%rsp), %rdx                 ## 8-byte Reload
	vinsertps	$48, 52(%r12,%rdx,4), %xmm3, %xmm3 ## xmm3 = xmm3[0,1,2],mem[0]
	vinsertps	$48, 52(%r12,%r13,4), %xmm6, %xmm6 ## xmm6 = xmm6[0,1,2],mem[0]
	vmovaps	1632(%rsp), %ymm4               ## 32-byte Reload
	movq	880(%rsp), %rdi                 ## 8-byte Reload
	vmulps	64(%r14,%rdi,4), %ymm4, %ymm7
	vfmadd231ps	%ymm7, %ymm2, %ymm9     ## ymm9 = (ymm2 * ymm7) + ymm9
	vinsertf128	$1, %xmm3, %ymm6, %ymm2
	vmulps	%ymm5, %ymm11, %ymm3
	vfmadd231ps	%ymm3, %ymm2, %ymm8     ## ymm8 = (ymm2 * ymm3) + ymm8
	vmovss	36(%r12,%rsi,4), %xmm2          ## xmm2 = mem[0],zero,zero,zero
	vinsertps	$16, 36(%r12,%r8,4), %xmm2, %xmm2 ## xmm2 = xmm2[0],mem[0],xmm2[2,3]
	vmovss	36(%r12,%rax,4), %xmm3          ## xmm3 = mem[0],zero,zero,zero
	vinsertps	$16, 36(%r12,%r15,4), %xmm3, %xmm3 ## xmm3 = xmm3[0],mem[0],xmm3[2,3]
	vinsertps	$32, 36(%r12,%rbx,4), %xmm2, %xmm2 ## xmm2 = xmm2[0,1],mem[0],xmm2[3]
	vinsertps	$32, 36(%r12,%r10,4), %xmm3, %xmm3 ## xmm3 = xmm3[0,1],mem[0],xmm3[3]
	vinsertps	$48, 36(%r12,%rcx,4), %xmm2, %xmm2 ## xmm2 = xmm2[0,1,2],mem[0]
	vinsertps	$48, 36(%r12,%r9,4), %xmm3, %xmm3 ## xmm3 = xmm3[0,1,2],mem[0]
	vinsertf128	$1, %xmm2, %ymm3, %ymm2
	vmovss	48(%r12,%rsi,4), %xmm3          ## xmm3 = mem[0],zero,zero,zero
	vinsertps	$16, 48(%r12,%r8,4), %xmm3, %xmm3 ## xmm3 = xmm3[0],mem[0],xmm3[2,3]
	vinsertps	$32, 48(%r12,%rbx,4), %xmm3, %xmm3 ## xmm3 = xmm3[0,1],mem[0],xmm3[3]
	vmovss	48(%r12,%rax,4), %xmm5          ## xmm5 = mem[0],zero,zero,zero
	vinsertps	$16, 48(%r12,%r15,4), %xmm5, %xmm5 ## xmm5 = xmm5[0],mem[0],xmm5[2,3]
	vmovss	40(%r12,%rsi,4), %xmm6          ## xmm6 = mem[0],zero,zero,zero
	vinsertps	$16, 40(%r12,%r8,4), %xmm6, %xmm6 ## xmm6 = xmm6[0],mem[0],xmm6[2,3]
	movq	864(%rsp), %rdx                 ## 8-byte Reload
	vmulps	64(%r14,%rdx,4), %ymm1, %ymm7
	vfmadd231ps	%ymm7, %ymm2, %ymm9     ## ymm9 = (ymm2 * ymm7) + ymm9
	vinsertps	$32, 40(%r12,%rbx,4), %xmm6, %xmm2 ## xmm2 = xmm6[0,1],mem[0],xmm6[3]
	vmovss	40(%r12,%rax,4), %xmm6          ## xmm6 = mem[0],zero,zero,zero
	vinsertps	$16, 40(%r12,%r15,4), %xmm6, %xmm6 ## xmm6 = xmm6[0],mem[0],xmm6[2,3]
	vinsertps	$48, 40(%r12,%rcx,4), %xmm2, %xmm2 ## xmm2 = xmm2[0,1,2],mem[0]
	vinsertps	$32, 40(%r12,%r10,4), %xmm6, %xmm6 ## xmm6 = xmm6[0,1],mem[0],xmm6[3]
	vinsertps	$48, 40(%r12,%r9,4), %xmm6, %xmm6 ## xmm6 = xmm6[0,1,2],mem[0]
	vinsertf128	$1, %xmm2, %ymm6, %ymm2
	vinsertps	$32, 48(%r12,%r10,4), %xmm5, %xmm5 ## xmm5 = xmm5[0,1],mem[0],xmm5[3]
	vmovss	44(%r12,%rsi,4), %xmm6          ## xmm6 = mem[0],zero,zero,zero
	vinsertps	$16, 44(%r12,%r8,4), %xmm6, %xmm6 ## xmm6 = xmm6[0],mem[0],xmm6[2,3]
	movq	896(%rsp), %rdx                 ## 8-byte Reload
	vmovups	64(%r14,%rdx,4), %ymm7
	vmulps	%ymm7, %ymm4, %ymm14
	vfmadd231ps	%ymm14, %ymm2, %ymm9    ## ymm9 = (ymm2 * ymm14) + ymm9
	vinsertps	$32, 44(%r12,%rbx,4), %xmm6, %xmm2 ## xmm2 = xmm6[0,1],mem[0],xmm6[3]
	vmovss	44(%r12,%rax,4), %xmm6          ## xmm6 = mem[0],zero,zero,zero
	vinsertps	$16, 44(%r12,%r15,4), %xmm6, %xmm6 ## xmm6 = xmm6[0],mem[0],xmm6[2,3]
	movq	%r15, %r13
	vinsertps	$48, 44(%r12,%rcx,4), %xmm2, %xmm2 ## xmm2 = xmm2[0,1,2],mem[0]
	vinsertps	$32, 44(%r12,%r10,4), %xmm6, %xmm6 ## xmm6 = xmm6[0,1],mem[0],xmm6[3]
	movq	%r10, %r15
	movq	%r9, %r11
	vinsertps	$48, 44(%r12,%r9,4), %xmm6, %xmm6 ## xmm6 = xmm6[0,1,2],mem[0]
	vinsertf128	$1, %xmm2, %ymm6, %ymm2
	vmulps	%ymm7, %ymm1, %ymm6
	vinsertps	$48, 48(%r12,%rcx,4), %xmm3, %xmm3 ## xmm3 = xmm3[0,1,2],mem[0]
	vfmadd231ps	%ymm6, %ymm2, %ymm9     ## ymm9 = (ymm2 * ymm6) + ymm9
	vinsertps	$48, 48(%r12,%r9,4), %xmm5, %xmm2 ## xmm2 = xmm5[0,1,2],mem[0]
	vinsertf128	$1, %xmm3, %ymm2, %ymm2
	movq	464(%rsp), %r9                  ## 8-byte Reload
	vmovss	20(%r12,%r9,4), %xmm3           ## xmm3 = mem[0],zero,zero,zero
	movq	736(%rsp), %rdx                 ## 8-byte Reload
	vinsertps	$16, 20(%r12,%rdx,4), %xmm3, %xmm3 ## xmm3 = xmm3[0],mem[0],xmm3[2,3]
	movq	912(%rsp), %rax                 ## 8-byte Reload
	vmovups	64(%r14,%rax,4), %ymm5
	vmulps	%ymm5, %ymm4, %ymm6
	movq	448(%rsp), %rax                 ## 8-byte Reload
	vinsertps	$32, 20(%r12,%rax,4), %xmm3, %xmm3 ## xmm3 = xmm3[0,1],mem[0],xmm3[3]
	movq	112(%rsp), %r8                  ## 8-byte Reload
	vmovss	20(%r12,%r8,4), %xmm7           ## xmm7 = mem[0],zero,zero,zero
	movq	1152(%rsp), %rcx                ## 8-byte Reload
	vinsertps	$16, 20(%r12,%rcx,4), %xmm7, %xmm7 ## xmm7 = xmm7[0],mem[0],xmm7[2,3]
	movq	120(%rsp), %rdi                 ## 8-byte Reload
	vinsertps	$48, 20(%r12,%rdi,4), %xmm3, %xmm3 ## xmm3 = xmm3[0,1,2],mem[0]
	vfmadd231ps	%ymm6, %ymm2, %ymm9     ## ymm9 = (ymm2 * ymm6) + ymm9
	movq	104(%rsp), %rbx                 ## 8-byte Reload
	vinsertps	$32, 20(%r12,%rbx,4), %xmm7, %xmm2 ## xmm2 = xmm7[0,1],mem[0],xmm7[3]
	vmovss	24(%r12,%r9,4), %xmm6           ## xmm6 = mem[0],zero,zero,zero
	vinsertps	$16, 24(%r12,%rdx,4), %xmm6, %xmm6 ## xmm6 = xmm6[0],mem[0],xmm6[2,3]
	movq	128(%rsp), %rsi                 ## 8-byte Reload
	vinsertps	$48, 20(%r12,%rsi,4), %xmm2, %xmm2 ## xmm2 = xmm2[0,1,2],mem[0]
	vinsertf128	$1, %xmm3, %ymm2, %ymm2
	vinsertps	$32, 24(%r12,%rax,4), %xmm6, %xmm3 ## xmm3 = xmm6[0,1],mem[0],xmm6[3]
	movq	%rax, %r10
	vmovss	24(%r12,%r8,4), %xmm6           ## xmm6 = mem[0],zero,zero,zero
	vinsertps	$16, 24(%r12,%rcx,4), %xmm6, %xmm6 ## xmm6 = xmm6[0],mem[0],xmm6[2,3]
	movq	%rcx, %rdx
	vinsertps	$48, 24(%r12,%rdi,4), %xmm3, %xmm3 ## xmm3 = xmm3[0,1,2],mem[0]
	vinsertps	$32, 24(%r12,%rbx,4), %xmm6, %xmm6 ## xmm6 = xmm6[0,1],mem[0],xmm6[3]
	vinsertps	$48, 24(%r12,%rsi,4), %xmm6, %xmm6 ## xmm6 = xmm6[0,1,2],mem[0]
	vinsertf128	$1, %xmm3, %ymm6, %ymm3
	movq	816(%rsp), %rax                 ## 8-byte Reload
	vmovaps	1568(%rsp), %ymm0               ## 32-byte Reload
	vmulps	96(%r14,%rax,4), %ymm0, %ymm6
	vmulps	%ymm6, %ymm3, %ymm14
	movq	224(%rsp), %rax                 ## 8-byte Reload
	vmovss	52(%r12,%rax,4), %xmm3          ## xmm3 = mem[0],zero,zero,zero
	movq	56(%rsp), %rax                  ## 8-byte Reload
	vinsertps	$16, 52(%r12,%rax,4), %xmm3, %xmm3 ## xmm3 = xmm3[0],mem[0],xmm3[2,3]
	movq	48(%rsp), %rax                  ## 8-byte Reload
	vmovss	52(%r12,%rax,4), %xmm6          ## xmm6 = mem[0],zero,zero,zero
	vinsertps	$16, 52(%r12,%r13,4), %xmm6, %xmm6 ## xmm6 = xmm6[0],mem[0],xmm6[2,3]
	movq	72(%rsp), %rax                  ## 8-byte Reload
	vinsertps	$32, 52(%r12,%rax,4), %xmm3, %xmm3 ## xmm3 = xmm3[0,1],mem[0],xmm3[3]
	vinsertps	$32, 52(%r12,%r15,4), %xmm6, %xmm6 ## xmm6 = xmm6[0,1],mem[0],xmm6[3]
	movq	432(%rsp), %rax                 ## 8-byte Reload
	vinsertps	$48, 52(%r12,%rax,4), %xmm3, %xmm3 ## xmm3 = xmm3[0,1,2],mem[0]
	vinsertps	$48, 52(%r12,%r11,4), %xmm6, %xmm6 ## xmm6 = xmm6[0,1,2],mem[0]
	movq	832(%rsp), %rax                 ## 8-byte Reload
	vmulps	96(%r14,%rax,4), %ymm0, %ymm7
	vfmadd231ps	%ymm7, %ymm2, %ymm14    ## ymm14 = (ymm2 * ymm7) + ymm14
	vinsertf128	$1, %xmm3, %ymm6, %ymm2
	vmulps	%ymm5, %ymm1, %ymm3
	vfmadd231ps	%ymm3, %ymm2, %ymm9     ## ymm9 = (ymm2 * ymm3) + ymm9
	vmovss	28(%r12,%r9,4), %xmm2           ## xmm2 = mem[0],zero,zero,zero
	movq	736(%rsp), %rcx                 ## 8-byte Reload
	vinsertps	$16, 28(%r12,%rcx,4), %xmm2, %xmm2 ## xmm2 = xmm2[0],mem[0],xmm2[2,3]
	vmovss	28(%r12,%r8,4), %xmm3           ## xmm3 = mem[0],zero,zero,zero
	vinsertps	$16, 28(%r12,%rdx,4), %xmm3, %xmm3 ## xmm3 = xmm3[0],mem[0],xmm3[2,3]
	vinsertps	$32, 28(%r12,%r10,4), %xmm2, %xmm2 ## xmm2 = xmm2[0,1],mem[0],xmm2[3]
	vinsertps	$32, 28(%r12,%rbx,4), %xmm3, %xmm3 ## xmm3 = xmm3[0,1],mem[0],xmm3[3]
	vinsertps	$48, 28(%r12,%rdi,4), %xmm2, %xmm2 ## xmm2 = xmm2[0,1,2],mem[0]
	vinsertps	$48, 28(%r12,%rsi,4), %xmm3, %xmm3 ## xmm3 = xmm3[0,1,2],mem[0]
	vinsertf128	$1, %xmm2, %ymm3, %ymm2
	movq	176(%rsp), %rax                 ## 8-byte Reload
	vmovss	60(%r12,%rax,4), %xmm3          ## xmm3 = mem[0],zero,zero,zero
	movq	160(%rsp), %rax                 ## 8-byte Reload
	vinsertps	$16, 60(%r12,%rax,4), %xmm3, %xmm3 ## xmm3 = xmm3[0],mem[0],xmm3[2,3]
	movq	848(%rsp), %rax                 ## 8-byte Reload
	vmulps	96(%r14,%rax,4), %ymm12, %ymm5
	vfmadd231ps	%ymm5, %ymm2, %ymm14    ## ymm14 = (ymm2 * ymm5) + ymm14
	movq	320(%rsp), %rax                 ## 8-byte Reload
	vinsertps	$32, 60(%r12,%rax,4), %xmm3, %xmm2 ## xmm2 = xmm3[0,1],mem[0],xmm3[3]
	movq	288(%rsp), %rax                 ## 8-byte Reload
	vmovss	60(%r12,%rax,4), %xmm3          ## xmm3 = mem[0],zero,zero,zero
	movq	144(%rsp), %rax                 ## 8-byte Reload
	vinsertps	$16, 60(%r12,%rax,4), %xmm3, %xmm3 ## xmm3 = xmm3[0],mem[0],xmm3[2,3]
	movq	384(%rsp), %rax                 ## 8-byte Reload
	vinsertps	$48, 60(%r12,%rax,4), %xmm2, %xmm2 ## xmm2 = xmm2[0,1,2],mem[0]
	movq	400(%rsp), %rax                 ## 8-byte Reload
	vinsertps	$32, 60(%r12,%rax,4), %xmm3, %xmm3 ## xmm3 = xmm3[0,1],mem[0],xmm3[3]
	movq	40(%rsp), %rax                  ## 8-byte Reload
	vinsertps	$48, 60(%r12,%rax,4), %xmm3, %xmm3 ## xmm3 = xmm3[0,1,2],mem[0]
	vinsertf128	$1, %xmm2, %ymm3, %ymm2
	movq	536(%rsp), %rax                 ## 8-byte Reload
	vmulps	(%r14,%rax,4), %ymm15, %ymm3
	vfmadd231ps	%ymm3, %ymm2, %ymm10    ## ymm10 = (ymm2 * ymm3) + ymm10
	vmovaps	2432(%rsp), %ymm1               ## 32-byte Reload
	vmovaps	1920(%rsp), %ymm0               ## 32-byte Reload
	vmaxps	%ymm1, %ymm0, %ymm4
	vmovaps	2976(%rsp), %ymm12              ## 32-byte Reload
	vdivps	%ymm12, %ymm15, %ymm5
	vdivps	%ymm12, %ymm4, %ymm6
	vroundps	$10, %ymm5, %ymm3
	vmulps	2656(%rsp), %ymm3, %ymm7        ## 32-byte Folded Reload
	vroundps	$10, %ymm6, %ymm3
	vmulps	2528(%rsp), %ymm3, %ymm3        ## 32-byte Folded Reload
	vmovaps	2336(%rsp), %ymm0               ## 32-byte Reload
	vmaxps	%ymm1, %ymm0, %ymm11
	vmovaps	2400(%rsp), %ymm0               ## 32-byte Reload
	vmaxps	%ymm1, %ymm0, %ymm2
	vmovaps	%ymm2, 1632(%rsp)               ## 32-byte Spill
	vdivps	%ymm12, %ymm11, %ymm0
	vdivps	%ymm12, %ymm2, %ymm2
	vroundps	$10, %ymm0, %ymm15
	vmulps	2624(%rsp), %ymm15, %ymm15      ## 32-byte Folded Reload
	vroundps	$10, %ymm2, %ymm12
	vmulps	2560(%rsp), %ymm12, %ymm12      ## 32-byte Folded Reload
	vmaxps	%ymm1, %ymm5, %ymm5
	vdivps	%ymm5, %ymm7, %ymm5
	vmaxps	%ymm1, %ymm6, %ymm6
	vdivps	%ymm6, %ymm3, %ymm3
	vmaxps	%ymm1, %ymm0, %ymm0
	vdivps	%ymm0, %ymm15, %ymm0
	vmaxps	%ymm1, %ymm2, %ymm1
	vdivps	%ymm1, %ymm12, %ymm1
	vbroadcastss	LCPI0_64(%rip), %ymm6   ## ymm6 = [2.0E+0,2.0E+0,2.0E+0,2.0E+0,2.0E+0,2.0E+0,2.0E+0,2.0E+0]
	vmovaps	2368(%rsp), %ymm15              ## 32-byte Reload
	vfmadd213ps	%ymm5, %ymm6, %ymm15    ## ymm15 = (ymm6 * ymm15) + ymm5
	vmovaps	1696(%rsp), %ymm2               ## 32-byte Reload
	vfmadd213ps	%ymm3, %ymm6, %ymm2     ## ymm2 = (ymm6 * ymm2) + ymm3
	vmovaps	%ymm2, 1696(%rsp)               ## 32-byte Spill
	vmovaps	1344(%rsp), %ymm2               ## 32-byte Reload
	vfmadd213ps	%ymm0, %ymm6, %ymm2     ## ymm2 = (ymm6 * ymm2) + ymm0
	vmovaps	%ymm2, 1344(%rsp)               ## 32-byte Spill
	vfmadd213ps	%ymm1, %ymm6, %ymm13    ## ymm13 = (ymm6 * ymm13) + ymm1
	vmovss	32(%r12,%r9,4), %xmm0           ## xmm0 = mem[0],zero,zero,zero
	vinsertps	$16, 32(%r12,%rcx,4), %xmm0, %xmm0 ## xmm0 = xmm0[0],mem[0],xmm0[2,3]
	movq	%rcx, %r13
	vmovss	32(%r12,%r8,4), %xmm1           ## xmm1 = mem[0],zero,zero,zero
	vinsertps	$16, 32(%r12,%rdx,4), %xmm1, %xmm1 ## xmm1 = xmm1[0],mem[0],xmm1[2,3]
	vinsertps	$32, 32(%r12,%r10,4), %xmm0, %xmm0 ## xmm0 = xmm0[0,1],mem[0],xmm0[3]
	vinsertps	$32, 32(%r12,%rbx,4), %xmm1, %xmm1 ## xmm1 = xmm1[0,1],mem[0],xmm1[3]
	vinsertps	$48, 32(%r12,%rdi,4), %xmm0, %xmm0 ## xmm0 = xmm0[0,1,2],mem[0]
	vinsertps	$48, 32(%r12,%rsi,4), %xmm1, %xmm1 ## xmm1 = xmm1[0,1,2],mem[0]
	vinsertf128	$1, %xmm0, %ymm1, %ymm0
	movq	352(%rsp), %rax                 ## 8-byte Reload
	vmovss	56(%r12,%rax,4), %xmm1          ## xmm1 = mem[0],zero,zero,zero
	movq	544(%rsp), %rax                 ## 8-byte Reload
	vinsertps	$16, 56(%r12,%rax,4), %xmm1, %xmm1 ## xmm1 = xmm1[0],mem[0],xmm1[2,3]
	vmovaps	1664(%rsp), %ymm7               ## 32-byte Reload
	movq	880(%rsp), %rax                 ## 8-byte Reload
	vmulps	96(%r14,%rax,4), %ymm7, %ymm3
	vfmadd231ps	%ymm3, %ymm0, %ymm14    ## ymm14 = (ymm0 * ymm3) + ymm14
	movq	336(%rsp), %r15                 ## 8-byte Reload
	vinsertps	$32, 56(%r12,%r15,4), %xmm1, %xmm0 ## xmm0 = xmm1[0,1],mem[0],xmm1[3]
	movq	32(%rsp), %r11                  ## 8-byte Reload
	vmovss	56(%r12,%r11,4), %xmm1          ## xmm1 = mem[0],zero,zero,zero
	movq	256(%rsp), %rax                 ## 8-byte Reload
	vinsertps	$16, 56(%r12,%rax,4), %xmm1, %xmm1 ## xmm1 = xmm1[0],mem[0],xmm1[2,3]
	movq	240(%rsp), %rax                 ## 8-byte Reload
	vinsertps	$48, 56(%r12,%rax,4), %xmm0, %xmm0 ## xmm0 = xmm0[0,1,2],mem[0]
	movq	304(%rsp), %rax                 ## 8-byte Reload
	vinsertps	$32, 56(%r12,%rax,4), %xmm1, %xmm1 ## xmm1 = xmm1[0,1],mem[0],xmm1[3]
	movq	80(%rsp), %rax                  ## 8-byte Reload
	vinsertps	$48, 56(%r12,%rax,4), %xmm1, %xmm1 ## xmm1 = xmm1[0,1,2],mem[0]
	vinsertf128	$1, %xmm0, %ymm1, %ymm0
	movq	208(%rsp), %rcx                 ## 8-byte Reload
	vmulps	32(%r14,%rcx,4), %ymm4, %ymm1
	vfmadd231ps	%ymm1, %ymm0, %ymm8     ## ymm8 = (ymm0 * ymm1) + ymm8
	vmovss	36(%r12,%r9,4), %xmm0           ## xmm0 = mem[0],zero,zero,zero
	movq	%r13, %rax
	vinsertps	$16, 36(%r12,%r13,4), %xmm0, %xmm0 ## xmm0 = xmm0[0],mem[0],xmm0[2,3]
	vmovss	36(%r12,%r8,4), %xmm1           ## xmm1 = mem[0],zero,zero,zero
	vinsertps	$16, 36(%r12,%rdx,4), %xmm1, %xmm1 ## xmm1 = xmm1[0],mem[0],xmm1[2,3]
	vinsertps	$32, 36(%r12,%r10,4), %xmm0, %xmm0 ## xmm0 = xmm0[0,1],mem[0],xmm0[3]
	vinsertps	$32, 36(%r12,%rbx,4), %xmm1, %xmm1 ## xmm1 = xmm1[0,1],mem[0],xmm1[3]
	vinsertps	$48, 36(%r12,%rdi,4), %xmm0, %xmm0 ## xmm0 = xmm0[0,1,2],mem[0]
	vinsertps	$48, 36(%r12,%rsi,4), %xmm1, %xmm1 ## xmm1 = xmm1[0,1,2],mem[0]
	vinsertf128	$1, %xmm0, %ymm1, %ymm0
	movq	224(%rsp), %r13                 ## 8-byte Reload
	vmovss	56(%r12,%r13,4), %xmm1          ## xmm1 = mem[0],zero,zero,zero
	movq	56(%rsp), %rdx                  ## 8-byte Reload
	vinsertps	$16, 56(%r12,%rdx,4), %xmm1, %xmm1 ## xmm1 = xmm1[0],mem[0],xmm1[2,3]
	vmovaps	2112(%rsp), %ymm6               ## 32-byte Reload
	movq	864(%rsp), %rdx                 ## 8-byte Reload
	vmulps	96(%r14,%rdx,4), %ymm6, %ymm3
	vfmadd231ps	%ymm3, %ymm0, %ymm14    ## ymm14 = (ymm0 * ymm3) + ymm14
	movq	72(%rsp), %rdx                  ## 8-byte Reload
	vinsertps	$32, 56(%r12,%rdx,4), %xmm1, %xmm0 ## xmm0 = xmm1[0,1],mem[0],xmm1[3]
	movq	48(%rsp), %rdx                  ## 8-byte Reload
	vmovss	56(%r12,%rdx,4), %xmm1          ## xmm1 = mem[0],zero,zero,zero
	movq	480(%rsp), %rdx                 ## 8-byte Reload
	vinsertps	$16, 56(%r12,%rdx,4), %xmm1, %xmm1 ## xmm1 = xmm1[0],mem[0],xmm1[2,3]
	movq	432(%rsp), %rdx                 ## 8-byte Reload
	vinsertps	$48, 56(%r12,%rdx,4), %xmm0, %xmm0 ## xmm0 = xmm0[0,1,2],mem[0]
	movq	64(%rsp), %rdx                  ## 8-byte Reload
	vinsertps	$32, 56(%r12,%rdx,4), %xmm1, %xmm1 ## xmm1 = xmm1[0,1],mem[0],xmm1[3]
	movq	416(%rsp), %rdx                 ## 8-byte Reload
	vinsertps	$48, 56(%r12,%rdx,4), %xmm1, %xmm1 ## xmm1 = xmm1[0,1,2],mem[0]
	vinsertf128	$1, %xmm0, %ymm1, %ymm0
	vmulps	64(%r14,%rcx,4), %ymm11, %ymm1
	vfmadd231ps	%ymm1, %ymm0, %ymm9     ## ymm9 = (ymm0 * ymm1) + ymm9
	vmovss	40(%r12,%r9,4), %xmm0           ## xmm0 = mem[0],zero,zero,zero
	vinsertps	$16, 40(%r12,%rax,4), %xmm0, %xmm0 ## xmm0 = xmm0[0],mem[0],xmm0[2,3]
	vmovss	40(%r12,%r8,4), %xmm1           ## xmm1 = mem[0],zero,zero,zero
	movq	1152(%rsp), %rcx                ## 8-byte Reload
	vinsertps	$16, 40(%r12,%rcx,4), %xmm1, %xmm1 ## xmm1 = xmm1[0],mem[0],xmm1[2,3]
	vinsertps	$32, 40(%r12,%r10,4), %xmm0, %xmm0 ## xmm0 = xmm0[0,1],mem[0],xmm0[3]
	vinsertps	$32, 40(%r12,%rbx,4), %xmm1, %xmm1 ## xmm1 = xmm1[0,1],mem[0],xmm1[3]
	vinsertps	$48, 40(%r12,%rdi,4), %xmm0, %xmm0 ## xmm0 = xmm0[0,1,2],mem[0]
	vinsertps	$48, 40(%r12,%rsi,4), %xmm1, %xmm1 ## xmm1 = xmm1[0,1,2],mem[0]
	vinsertf128	$1, %xmm0, %ymm1, %ymm0
	vmovss	44(%r12,%r9,4), %xmm1           ## xmm1 = mem[0],zero,zero,zero
	vinsertps	$16, 44(%r12,%rax,4), %xmm1, %xmm1 ## xmm1 = xmm1[0],mem[0],xmm1[2,3]
	movq	896(%rsp), %rdx                 ## 8-byte Reload
	vmovups	96(%r14,%rdx,4), %ymm3
	vmulps	%ymm3, %ymm7, %ymm5
	vmovaps	%ymm7, %ymm12
	vfmadd231ps	%ymm5, %ymm0, %ymm14    ## ymm14 = (ymm0 * ymm5) + ymm14
	vinsertps	$32, 44(%r12,%r10,4), %xmm1, %xmm0 ## xmm0 = xmm1[0,1],mem[0],xmm1[3]
	vmovss	44(%r12,%r8,4), %xmm1           ## xmm1 = mem[0],zero,zero,zero
	vinsertps	$16, 44(%r12,%rcx,4), %xmm1, %xmm1 ## xmm1 = xmm1[0],mem[0],xmm1[2,3]
	vinsertps	$48, 44(%r12,%rdi,4), %xmm0, %xmm0 ## xmm0 = xmm0[0,1,2],mem[0]
	vinsertps	$32, 44(%r12,%rbx,4), %xmm1, %xmm1 ## xmm1 = xmm1[0,1],mem[0],xmm1[3]
	vinsertps	$48, 44(%r12,%rsi,4), %xmm1, %xmm1 ## xmm1 = xmm1[0,1,2],mem[0]
	vinsertf128	$1, %xmm0, %ymm1, %ymm0
	vmovss	52(%r12,%r9,4), %xmm1           ## xmm1 = mem[0],zero,zero,zero
	vinsertps	$16, 52(%r12,%rax,4), %xmm1, %xmm1 ## xmm1 = xmm1[0],mem[0],xmm1[2,3]
	vmulps	%ymm3, %ymm6, %ymm3
	vmovaps	%ymm6, %ymm7
	vmovss	48(%r12,%r9,4), %xmm5           ## xmm5 = mem[0],zero,zero,zero
	vinsertps	$16, 48(%r12,%rax,4), %xmm5, %xmm5 ## xmm5 = xmm5[0],mem[0],xmm5[2,3]
	vfmadd231ps	%ymm3, %ymm0, %ymm14    ## ymm14 = (ymm0 * ymm3) + ymm14
	vinsertps	$32, 48(%r12,%r10,4), %xmm5, %xmm0 ## xmm0 = xmm5[0,1],mem[0],xmm5[3]
	vmovss	48(%r12,%r8,4), %xmm3           ## xmm3 = mem[0],zero,zero,zero
	vinsertps	$16, 48(%r12,%rcx,4), %xmm3, %xmm3 ## xmm3 = xmm3[0],mem[0],xmm3[2,3]
	vinsertps	$48, 48(%r12,%rdi,4), %xmm0, %xmm0 ## xmm0 = xmm0[0,1,2],mem[0]
	vinsertps	$32, 48(%r12,%rbx,4), %xmm3, %xmm3 ## xmm3 = xmm3[0,1],mem[0],xmm3[3]
	vinsertps	$48, 48(%r12,%rsi,4), %xmm3, %xmm3 ## xmm3 = xmm3[0,1,2],mem[0]
	vinsertf128	$1, %xmm0, %ymm3, %ymm0
	vinsertps	$32, 52(%r12,%r10,4), %xmm1, %xmm1 ## xmm1 = xmm1[0,1],mem[0],xmm1[3]
	vmovss	52(%r12,%r8,4), %xmm3           ## xmm3 = mem[0],zero,zero,zero
	movq	%r8, %r10
	vinsertps	$16, 52(%r12,%rcx,4), %xmm3, %xmm3 ## xmm3 = xmm3[0],mem[0],xmm3[2,3]
	movq	%rcx, %r8
	vinsertps	$32, 52(%r12,%rbx,4), %xmm3, %xmm3 ## xmm3 = xmm3[0,1],mem[0],xmm3[3]
	movq	912(%rsp), %rdx                 ## 8-byte Reload
	vmovups	96(%r14,%rdx,4), %ymm5
	vmulps	%ymm5, %ymm12, %ymm6
	vinsertps	$48, 52(%r12,%rdi,4), %xmm1, %xmm1 ## xmm1 = xmm1[0,1,2],mem[0]
	movq	%rdi, %rax
	vfmadd231ps	%ymm6, %ymm0, %ymm14    ## ymm14 = (ymm0 * ymm6) + ymm14
	vinsertps	$48, 52(%r12,%rsi,4), %xmm3, %xmm0 ## xmm0 = xmm3[0,1,2],mem[0]
	movq	%rsi, %r13
	vinsertf128	$1, %xmm1, %ymm0, %ymm0
	vmulps	%ymm5, %ymm7, %ymm1
	movq	352(%rsp), %rcx                 ## 8-byte Reload
	vmovss	60(%r12,%rcx,4), %xmm3          ## xmm3 = mem[0],zero,zero,zero
	movq	544(%rsp), %rcx                 ## 8-byte Reload
	vinsertps	$16, 60(%r12,%rcx,4), %xmm3, %xmm3 ## xmm3 = xmm3[0],mem[0],xmm3[2,3]
	vfmadd231ps	%ymm1, %ymm0, %ymm14    ## ymm14 = (ymm0 * ymm1) + ymm14
	vinsertps	$32, 60(%r12,%r15,4), %xmm3, %xmm0 ## xmm0 = xmm3[0,1],mem[0],xmm3[3]
	vmovss	60(%r12,%r11,4), %xmm1          ## xmm1 = mem[0],zero,zero,zero
	movq	256(%rsp), %rcx                 ## 8-byte Reload
	vinsertps	$16, 60(%r12,%rcx,4), %xmm1, %xmm1 ## xmm1 = xmm1[0],mem[0],xmm1[2,3]
	movq	240(%rsp), %rcx                 ## 8-byte Reload
	vinsertps	$48, 60(%r12,%rcx,4), %xmm0, %xmm0 ## xmm0 = xmm0[0,1,2],mem[0]
	movq	304(%rsp), %rcx                 ## 8-byte Reload
	vinsertps	$32, 60(%r12,%rcx,4), %xmm1, %xmm1 ## xmm1 = xmm1[0,1],mem[0],xmm1[3]
	movq	80(%rsp), %rcx                  ## 8-byte Reload
	vinsertps	$48, 60(%r12,%rcx,4), %xmm1, %xmm1 ## xmm1 = xmm1[0,1,2],mem[0]
	vinsertf128	$1, %xmm0, %ymm1, %ymm0
	movq	536(%rsp), %rcx                 ## 8-byte Reload
	vmulps	32(%r14,%rcx,4), %ymm4, %ymm1
	vfmadd231ps	%ymm1, %ymm0, %ymm8     ## ymm8 = (ymm0 * ymm1) + ymm8
	movq	%r9, %rdi
	vmovss	56(%r12,%r9,4), %xmm0           ## xmm0 = mem[0],zero,zero,zero
	movq	736(%rsp), %rsi                 ## 8-byte Reload
	vinsertps	$16, 56(%r12,%rsi,4), %xmm0, %xmm0 ## xmm0 = xmm0[0],mem[0],xmm0[2,3]
	vmovss	56(%r12,%r10,4), %xmm1          ## xmm1 = mem[0],zero,zero,zero
	vinsertps	$16, 56(%r12,%r8,4), %xmm1, %xmm1 ## xmm1 = xmm1[0],mem[0],xmm1[2,3]
	movq	%r8, %r15
	movq	448(%rsp), %rdx                 ## 8-byte Reload
	vinsertps	$32, 56(%r12,%rdx,4), %xmm0, %xmm0 ## xmm0 = xmm0[0,1],mem[0],xmm0[3]
	vinsertps	$32, 56(%r12,%rbx,4), %xmm1, %xmm1 ## xmm1 = xmm1[0,1],mem[0],xmm1[3]
	vinsertps	$48, 56(%r12,%rax,4), %xmm0, %xmm0 ## xmm0 = xmm0[0,1,2],mem[0]
	movq	%rax, %r8
	vinsertps	$48, 56(%r12,%r13,4), %xmm1, %xmm1 ## xmm1 = xmm1[0,1,2],mem[0]
	vinsertf128	$1, %xmm0, %ymm1, %ymm0
	movq	224(%rsp), %rax                 ## 8-byte Reload
	vmovss	60(%r12,%rax,4), %xmm1          ## xmm1 = mem[0],zero,zero,zero
	movq	56(%rsp), %r11                  ## 8-byte Reload
	vinsertps	$16, 60(%r12,%r11,4), %xmm1, %xmm1 ## xmm1 = xmm1[0],mem[0],xmm1[2,3]
	vmovaps	1632(%rsp), %ymm3               ## 32-byte Reload
	movq	208(%rsp), %rax                 ## 8-byte Reload
	vmulps	96(%r14,%rax,4), %ymm3, %ymm2
	vfmadd231ps	%ymm2, %ymm0, %ymm14    ## ymm14 = (ymm0 * ymm2) + ymm14
	movq	72(%rsp), %rax                  ## 8-byte Reload
	vinsertps	$32, 60(%r12,%rax,4), %xmm1, %xmm0 ## xmm0 = xmm1[0,1],mem[0],xmm1[3]
	movq	48(%rsp), %rbx                  ## 8-byte Reload
	vmovss	60(%r12,%rbx,4), %xmm1          ## xmm1 = mem[0],zero,zero,zero
	movq	480(%rsp), %rax                 ## 8-byte Reload
	vinsertps	$16, 60(%r12,%rax,4), %xmm1, %xmm1 ## xmm1 = xmm1[0],mem[0],xmm1[2,3]
	movq	432(%rsp), %r9                  ## 8-byte Reload
	vinsertps	$48, 60(%r12,%r9,4), %xmm0, %xmm0 ## xmm0 = xmm0[0,1,2],mem[0]
	movq	64(%rsp), %rax                  ## 8-byte Reload
	vinsertps	$32, 60(%r12,%rax,4), %xmm1, %xmm1 ## xmm1 = xmm1[0,1],mem[0],xmm1[3]
	movq	416(%rsp), %rax                 ## 8-byte Reload
	vinsertps	$48, 60(%r12,%rax,4), %xmm1, %xmm1 ## xmm1 = xmm1[0,1,2],mem[0]
	vinsertf128	$1, %xmm0, %ymm1, %ymm0
	vmulps	64(%r14,%rcx,4), %ymm11, %ymm1
	vfmadd231ps	%ymm1, %ymm0, %ymm9     ## ymm9 = (ymm0 * ymm1) + ymm9
	vmovss	60(%r12,%rdi,4), %xmm0          ## xmm0 = mem[0],zero,zero,zero
	vinsertps	$16, 60(%r12,%rsi,4), %xmm0, %xmm0 ## xmm0 = xmm0[0],mem[0],xmm0[2,3]
	vmovss	60(%r12,%r10,4), %xmm1          ## xmm1 = mem[0],zero,zero,zero
	vinsertps	$16, 60(%r12,%r15,4), %xmm1, %xmm1 ## xmm1 = xmm1[0],mem[0],xmm1[2,3]
	vinsertps	$32, 60(%r12,%rdx,4), %xmm0, %xmm0 ## xmm0 = xmm0[0,1],mem[0],xmm0[3]
	movq	104(%rsp), %r15                 ## 8-byte Reload
	vinsertps	$32, 60(%r12,%r15,4), %xmm1, %xmm1 ## xmm1 = xmm1[0,1],mem[0],xmm1[3]
	vinsertps	$48, 60(%r12,%r8,4), %xmm0, %xmm0 ## xmm0 = xmm0[0,1,2],mem[0]
	movq	%r8, %rdx
	vinsertps	$48, 60(%r12,%r13,4), %xmm1, %xmm1 ## xmm1 = xmm1[0,1,2],mem[0]
	movq	%r13, %r10
	vinsertf128	$1, %xmm0, %ymm1, %ymm0
	movq	176(%rsp), %rdi                 ## 8-byte Reload
	vmovss	96(%r12,%rdi,4), %xmm1          ## xmm1 = mem[0],zero,zero,zero
	movq	160(%rsp), %rsi                 ## 8-byte Reload
	vinsertps	$16, 96(%r12,%rsi,4), %xmm1, %xmm1 ## xmm1 = xmm1[0],mem[0],xmm1[2,3]
	vmulps	96(%r14,%rcx,4), %ymm3, %ymm2
	vfmadd231ps	%ymm2, %ymm0, %ymm14    ## ymm14 = (ymm0 * ymm2) + ymm14
	movq	320(%rsp), %rcx                 ## 8-byte Reload
	vinsertps	$32, 96(%r12,%rcx,4), %xmm1, %xmm0 ## xmm0 = xmm1[0,1],mem[0],xmm1[3]
	movq	288(%rsp), %rcx                 ## 8-byte Reload
	vmovss	96(%r12,%rcx,4), %xmm1          ## xmm1 = mem[0],zero,zero,zero
	movq	144(%rsp), %r13                 ## 8-byte Reload
	vinsertps	$16, 96(%r12,%r13,4), %xmm1, %xmm1 ## xmm1 = xmm1[0],mem[0],xmm1[2,3]
	movq	384(%rsp), %rcx                 ## 8-byte Reload
	vinsertps	$48, 96(%r12,%rcx,4), %xmm0, %xmm0 ## xmm0 = xmm0[0,1,2],mem[0]
	movq	400(%rsp), %r8                  ## 8-byte Reload
	vinsertps	$32, 96(%r12,%r8,4), %xmm1, %xmm1 ## xmm1 = xmm1[0,1],mem[0],xmm1[3]
	movq	40(%rsp), %rcx                  ## 8-byte Reload
	vinsertps	$48, 96(%r12,%rcx,4), %xmm1, %xmm1 ## xmm1 = xmm1[0,1,2],mem[0]
	vinsertf128	$1, %xmm0, %ymm1, %ymm0
	movq	352(%rsp), %rsi                 ## 8-byte Reload
	vmovss	96(%r12,%rsi,4), %xmm1          ## xmm1 = mem[0],zero,zero,zero
	movq	544(%rsp), %rsi                 ## 8-byte Reload
	vinsertps	$16, 96(%r12,%rsi,4), %xmm1, %xmm1 ## xmm1 = xmm1[0],mem[0],xmm1[2,3]
	vaddps	%ymm10, %ymm15, %ymm4
	movq	336(%rsp), %rsi                 ## 8-byte Reload
	vinsertps	$32, 96(%r12,%rsi,4), %xmm1, %xmm1 ## xmm1 = xmm1[0,1],mem[0],xmm1[3]
	movq	32(%rsp), %rsi                  ## 8-byte Reload
	vmovss	96(%r12,%rsi,4), %xmm2          ## xmm2 = mem[0],zero,zero,zero
	movq	256(%rsp), %rcx                 ## 8-byte Reload
	vinsertps	$16, 96(%r12,%rcx,4), %xmm2, %xmm2 ## xmm2 = xmm2[0],mem[0],xmm2[2,3]
	movq	240(%rsp), %rsi                 ## 8-byte Reload
	vinsertps	$48, 96(%r12,%rsi,4), %xmm1, %xmm1 ## xmm1 = xmm1[0,1,2],mem[0]
	movq	304(%rsp), %rsi                 ## 8-byte Reload
	vinsertps	$32, 96(%r12,%rsi,4), %xmm2, %xmm2 ## xmm2 = xmm2[0,1],mem[0],xmm2[3]
	movq	80(%rsp), %rcx                  ## 8-byte Reload
	vinsertps	$48, 96(%r12,%rcx,4), %xmm2, %xmm2 ## xmm2 = xmm2[0,1,2],mem[0]
	vfmadd231ps	2464(%rsp), %ymm0, %ymm4 ## 32-byte Folded Reload
                                        ## ymm4 = (ymm0 * mem) + ymm4
	vinsertf128	$1, %xmm1, %ymm2, %ymm0
	vaddps	1696(%rsp), %ymm8, %ymm15       ## 32-byte Folded Reload
	vfmadd231ps	3008(%rsp), %ymm0, %ymm15 ## 32-byte Folded Reload
                                        ## ymm15 = (ymm0 * mem) + ymm15
	movq	224(%rsp), %rcx                 ## 8-byte Reload
	vmovss	96(%r12,%rcx,4), %xmm0          ## xmm0 = mem[0],zero,zero,zero
	vinsertps	$16, 96(%r12,%r11,4), %xmm0, %xmm0 ## xmm0 = xmm0[0],mem[0],xmm0[2,3]
	vmovss	96(%r12,%rbx,4), %xmm1          ## xmm1 = mem[0],zero,zero,zero
	movq	480(%rsp), %rbx                 ## 8-byte Reload
	vinsertps	$16, 96(%r12,%rbx,4), %xmm1, %xmm1 ## xmm1 = xmm1[0],mem[0],xmm1[2,3]
	movq	72(%rsp), %rcx                  ## 8-byte Reload
	vinsertps	$32, 96(%r12,%rcx,4), %xmm0, %xmm0 ## xmm0 = xmm0[0,1],mem[0],xmm0[3]
	movq	64(%rsp), %r11                  ## 8-byte Reload
	vinsertps	$32, 96(%r12,%r11,4), %xmm1, %xmm1 ## xmm1 = xmm1[0,1],mem[0],xmm1[3]
	vinsertps	$48, 96(%r12,%r9,4), %xmm0, %xmm0 ## xmm0 = xmm0[0,1,2],mem[0]
	vinsertps	$48, 96(%r12,%rax,4), %xmm1, %xmm1 ## xmm1 = xmm1[0,1,2],mem[0]
	vinsertf128	$1, %xmm0, %ymm1, %ymm0
	vaddps	1344(%rsp), %ymm9, %ymm12       ## 32-byte Folded Reload
	movq	464(%rsp), %rax                 ## 8-byte Reload
	vmovss	96(%r12,%rax,4), %xmm2          ## xmm2 = mem[0],zero,zero,zero
	movq	736(%rsp), %rax                 ## 8-byte Reload
	vinsertps	$16, 96(%r12,%rax,4), %xmm2, %xmm2 ## xmm2 = xmm2[0],mem[0],xmm2[2,3]
	movq	112(%rsp), %rax                 ## 8-byte Reload
	vmovss	96(%r12,%rax,4), %xmm5          ## xmm5 = mem[0],zero,zero,zero
	movq	1152(%rsp), %rax                ## 8-byte Reload
	vinsertps	$16, 96(%r12,%rax,4), %xmm5, %xmm5 ## xmm5 = xmm5[0],mem[0],xmm5[2,3]
	movq	448(%rsp), %rax                 ## 8-byte Reload
	vinsertps	$32, 96(%r12,%rax,4), %xmm2, %xmm2 ## xmm2 = xmm2[0,1],mem[0],xmm2[3]
	vinsertps	$32, 96(%r12,%r15,4), %xmm5, %xmm5 ## xmm5 = xmm5[0,1],mem[0],xmm5[3]
	vinsertps	$48, 96(%r12,%rdx,4), %xmm2, %xmm2 ## xmm2 = xmm2[0,1,2],mem[0]
	vfmadd231ps	1600(%rsp), %ymm0, %ymm12 ## 32-byte Folded Reload
                                        ## ymm12 = (ymm0 * mem) + ymm12
	vinsertps	$48, 96(%r12,%r10,4), %xmm5, %xmm0 ## xmm0 = xmm5[0,1,2],mem[0]
	vinsertf128	$1, %xmm2, %ymm0, %ymm5
	vmovss	108(%r12,%rdi,4), %xmm2         ## xmm2 = mem[0],zero,zero,zero
	vaddps	%ymm14, %ymm13, %ymm11
	vmovss	104(%r12,%rdi,4), %xmm6         ## xmm6 = mem[0],zero,zero,zero
	vfmadd231ps	1568(%rsp), %ymm5, %ymm11 ## 32-byte Folded Reload
                                        ## ymm11 = (ymm5 * mem) + ymm11
	vmovss	100(%r12,%rdi,4), %xmm5         ## xmm5 = mem[0],zero,zero,zero
	movq	288(%rsp), %rdi                 ## 8-byte Reload
	vmovss	104(%r12,%rdi,4), %xmm7         ## xmm7 = mem[0],zero,zero,zero
	movq	160(%rsp), %r9                  ## 8-byte Reload
	vinsertps	$16, 104(%r12,%r9,4), %xmm6, %xmm6 ## xmm6 = xmm6[0],mem[0],xmm6[2,3]
	vinsertps	$16, 104(%r12,%r13,4), %xmm7, %xmm7 ## xmm7 = xmm7[0],mem[0],xmm7[2,3]
	movq	320(%rsp), %rax                 ## 8-byte Reload
	vinsertps	$32, 104(%r12,%rax,4), %xmm6, %xmm6 ## xmm6 = xmm6[0,1],mem[0],xmm6[3]
	movq	%r8, %rsi
	vinsertps	$32, 104(%r12,%r8,4), %xmm7, %xmm7 ## xmm7 = xmm7[0,1],mem[0],xmm7[3]
	movq	384(%rsp), %rdx                 ## 8-byte Reload
	vinsertps	$48, 104(%r12,%rdx,4), %xmm6, %xmm6 ## xmm6 = xmm6[0,1,2],mem[0]
	vinsertps	$16, 100(%r12,%r9,4), %xmm5, %xmm5 ## xmm5 = xmm5[0],mem[0],xmm5[2,3]
	movq	40(%rsp), %rcx                  ## 8-byte Reload
	vinsertps	$48, 104(%r12,%rcx,4), %xmm7, %xmm7 ## xmm7 = xmm7[0,1,2],mem[0]
	vinsertf128	$1, %xmm6, %ymm7, %ymm7
	vmovss	100(%r12,%rdi,4), %xmm6         ## xmm6 = mem[0],zero,zero,zero
	movq	%rdi, %r8
	vinsertps	$32, 100(%r12,%rax,4), %xmm5, %xmm5 ## xmm5 = xmm5[0,1],mem[0],xmm5[3]
	movq	%rax, %rdi
	vinsertps	$16, 100(%r12,%r13,4), %xmm6, %xmm6 ## xmm6 = xmm6[0],mem[0],xmm6[2,3]
	vinsertps	$48, 100(%r12,%rdx,4), %xmm5, %xmm5 ## xmm5 = xmm5[0,1,2],mem[0]
	vinsertps	$32, 100(%r12,%rsi,4), %xmm6, %xmm6 ## xmm6 = xmm6[0,1],mem[0],xmm6[3]
	vinsertps	$48, 100(%r12,%rcx,4), %xmm6, %xmm6 ## xmm6 = xmm6[0,1,2],mem[0]
	vinsertf128	$1, %xmm5, %ymm6, %ymm6
	vmovss	108(%r12,%r8,4), %xmm5          ## xmm5 = mem[0],zero,zero,zero
	vinsertps	$16, 108(%r12,%r9,4), %xmm2, %xmm2 ## xmm2 = xmm2[0],mem[0],xmm2[2,3]
	vinsertps	$16, 108(%r12,%r13,4), %xmm5, %xmm5 ## xmm5 = xmm5[0],mem[0],xmm5[2,3]
	vinsertps	$32, 108(%r12,%rax,4), %xmm2, %xmm2 ## xmm2 = xmm2[0,1],mem[0],xmm2[3]
	vinsertps	$32, 108(%r12,%rsi,4), %xmm5, %xmm5 ## xmm5 = xmm5[0,1],mem[0],xmm5[3]
	vinsertps	$48, 108(%r12,%rdx,4), %xmm2, %xmm0 ## xmm0 = xmm2[0,1,2],mem[0]
	vmovaps	%xmm0, 144(%rsp)                ## 16-byte Spill
	vinsertps	$48, 108(%r12,%rcx,4), %xmm5, %xmm10 ## xmm10 = xmm5[0,1,2],mem[0]
	vandps	2688(%rsp), %ymm6, %ymm6        ## 32-byte Folded Reload
	movq	1112(%rsp), %r15                ## 8-byte Reload
	vmovups	(%r14,%r15,4), %ymm8
	vfmadd213ps	%ymm4, %ymm8, %ymm6     ## ymm6 = (ymm8 * ymm6) + ymm4
	movq	352(%rsp), %rax                 ## 8-byte Reload
	vmovss	108(%r12,%rax,4), %xmm4         ## xmm4 = mem[0],zero,zero,zero
	movq	544(%rsp), %rcx                 ## 8-byte Reload
	vinsertps	$16, 108(%r12,%rcx,4), %xmm4, %xmm2 ## xmm2 = xmm4[0],mem[0],xmm4[2,3]
	vmovaps	2592(%rsp), %ymm9               ## 32-byte Reload
	vaddps	2720(%rsp), %ymm9, %ymm4        ## 32-byte Folded Reload
	vmovss	104(%r12,%rax,4), %xmm5         ## xmm5 = mem[0],zero,zero,zero
	vinsertps	$16, 104(%r12,%rcx,4), %xmm5, %xmm5 ## xmm5 = xmm5[0],mem[0],xmm5[2,3]
	vmulps	%ymm4, %ymm8, %ymm4
	vmovss	100(%r12,%rax,4), %xmm0         ## xmm0 = mem[0],zero,zero,zero
	vinsertps	$16, 100(%r12,%rcx,4), %xmm0, %xmm0 ## xmm0 = xmm0[0],mem[0],xmm0[2,3]
	vfmadd231ps	%ymm4, %ymm7, %ymm6     ## ymm6 = (ymm7 * ymm4) + ymm6
	movq	32(%rsp), %rsi                  ## 8-byte Reload
	vmovss	100(%r12,%rsi,4), %xmm4         ## xmm4 = mem[0],zero,zero,zero
	movq	336(%rsp), %rdx                 ## 8-byte Reload
	vinsertps	$32, 100(%r12,%rdx,4), %xmm0, %xmm0 ## xmm0 = xmm0[0,1],mem[0],xmm0[3]
	movq	256(%rsp), %rax                 ## 8-byte Reload
	vinsertps	$16, 100(%r12,%rax,4), %xmm4, %xmm4 ## xmm4 = xmm4[0],mem[0],xmm4[2,3]
	movq	240(%rsp), %rdi                 ## 8-byte Reload
	vinsertps	$48, 100(%r12,%rdi,4), %xmm0, %xmm0 ## xmm0 = xmm0[0,1,2],mem[0]
	movq	304(%rsp), %rcx                 ## 8-byte Reload
	vinsertps	$32, 100(%r12,%rcx,4), %xmm4, %xmm4 ## xmm4 = xmm4[0,1],mem[0],xmm4[3]
	movq	80(%rsp), %r8                   ## 8-byte Reload
	vinsertps	$48, 100(%r12,%r8,4), %xmm4, %xmm4 ## xmm4 = xmm4[0,1,2],mem[0]
	vinsertf128	$1, %xmm0, %ymm4, %ymm0
	vmovss	108(%r12,%rsi,4), %xmm4         ## xmm4 = mem[0],zero,zero,zero
	vinsertps	$16, 108(%r12,%rax,4), %xmm4, %xmm13 ## xmm13 = xmm4[0],mem[0],xmm4[2,3]
	vmovss	104(%r12,%rsi,4), %xmm4         ## xmm4 = mem[0],zero,zero,zero
	vinsertps	$16, 104(%r12,%rax,4), %xmm4, %xmm1 ## xmm1 = xmm4[0],mem[0],xmm4[2,3]
	movq	224(%rsp), %rax                 ## 8-byte Reload
	vmovss	108(%r12,%rax,4), %xmm4         ## xmm4 = mem[0],zero,zero,zero
	movq	56(%rsp), %rsi                  ## 8-byte Reload
	vinsertps	$16, 108(%r12,%rsi,4), %xmm4, %xmm3 ## xmm3 = xmm4[0],mem[0],xmm4[2,3]
	vinsertps	$32, 104(%r12,%rdx,4), %xmm5, %xmm5 ## xmm5 = xmm5[0,1],mem[0],xmm5[3]
	vandps	2144(%rsp), %ymm0, %ymm4        ## 32-byte Folded Reload
	vinsertps	$32, 104(%r12,%rcx,4), %xmm1, %xmm0 ## xmm0 = xmm1[0,1],mem[0],xmm1[3]
	vmovups	32(%r14,%r15,4), %ymm1
	vfmadd213ps	%ymm15, %ymm1, %ymm4    ## ymm4 = (ymm1 * ymm4) + ymm15
	vaddps	2496(%rsp), %ymm9, %ymm8        ## 32-byte Folded Reload
	vinsertps	$48, 104(%r12,%rdi,4), %xmm5, %xmm5 ## xmm5 = xmm5[0,1,2],mem[0]
	movq	%rdi, %r13
	vmulps	%ymm1, %ymm8, %ymm1
	vinsertps	$48, 104(%r12,%r8,4), %xmm0, %xmm0 ## xmm0 = xmm0[0,1,2],mem[0]
	vinsertf128	$1, %xmm5, %ymm0, %ymm0
	vmovss	104(%r12,%rax,4), %xmm5         ## xmm5 = mem[0],zero,zero,zero
	vinsertps	$16, 104(%r12,%rsi,4), %xmm5, %xmm5 ## xmm5 = xmm5[0],mem[0],xmm5[2,3]
	vmovss	100(%r12,%rax,4), %xmm7         ## xmm7 = mem[0],zero,zero,zero
	vinsertps	$16, 100(%r12,%rsi,4), %xmm7, %xmm7 ## xmm7 = xmm7[0],mem[0],xmm7[2,3]
	vinsertps	$32, 108(%r12,%rdx,4), %xmm2, %xmm8 ## xmm8 = xmm2[0,1],mem[0],xmm2[3]
	vfmadd231ps	%ymm1, %ymm0, %ymm4     ## ymm4 = (ymm0 * ymm1) + ymm4
	movq	48(%rsp), %r9                   ## 8-byte Reload
	vmovss	104(%r12,%r9,4), %xmm0          ## xmm0 = mem[0],zero,zero,zero
	movq	72(%rsp), %rdi                  ## 8-byte Reload
	vinsertps	$32, 104(%r12,%rdi,4), %xmm5, %xmm1 ## xmm1 = xmm5[0,1],mem[0],xmm5[3]
	movq	%rbx, %r8
	vinsertps	$16, 104(%r12,%rbx,4), %xmm0, %xmm0 ## xmm0 = xmm0[0],mem[0],xmm0[2,3]
	movq	432(%rsp), %rsi                 ## 8-byte Reload
	vinsertps	$48, 104(%r12,%rsi,4), %xmm1, %xmm1 ## xmm1 = xmm1[0,1,2],mem[0]
	movq	%r11, %r10
	vinsertps	$32, 104(%r12,%r11,4), %xmm0, %xmm0 ## xmm0 = xmm0[0,1],mem[0],xmm0[3]
	movq	416(%rsp), %rax                 ## 8-byte Reload
	vinsertps	$48, 104(%r12,%rax,4), %xmm0, %xmm0 ## xmm0 = xmm0[0,1,2],mem[0]
	vinsertf128	$1, %xmm1, %ymm0, %ymm0
	vmovss	100(%r12,%r9,4), %xmm1          ## xmm1 = mem[0],zero,zero,zero
	vinsertps	$32, 100(%r12,%rdi,4), %xmm7, %xmm5 ## xmm5 = xmm7[0,1],mem[0],xmm7[3]
	vinsertps	$16, 100(%r12,%rbx,4), %xmm1, %xmm1 ## xmm1 = xmm1[0],mem[0],xmm1[2,3]
	movq	%rbx, %r11
	vinsertps	$48, 100(%r12,%rsi,4), %xmm5, %xmm5 ## xmm5 = xmm5[0,1,2],mem[0]
	movq	%rsi, %r8
	vinsertps	$32, 100(%r12,%r10,4), %xmm1, %xmm1 ## xmm1 = xmm1[0,1],mem[0],xmm1[3]
	vinsertps	$48, 100(%r12,%rax,4), %xmm1, %xmm1 ## xmm1 = xmm1[0,1,2],mem[0]
	vinsertf128	$1, %xmm5, %ymm1, %ymm1
	vmovss	108(%r12,%r9,4), %xmm5          ## xmm5 = mem[0],zero,zero,zero
	vinsertps	$32, 108(%r12,%rcx,4), %xmm13, %xmm15 ## xmm15 = xmm13[0,1],mem[0],xmm13[3]
	vinsertps	$16, 108(%r12,%rbx,4), %xmm5, %xmm5 ## xmm5 = xmm5[0],mem[0],xmm5[2,3]
	vinsertps	$32, 108(%r12,%rdi,4), %xmm3, %xmm13 ## xmm13 = xmm3[0,1],mem[0],xmm3[3]
	vandps	1984(%rsp), %ymm1, %ymm1        ## 32-byte Folded Reload
	movq	464(%rsp), %rcx                 ## 8-byte Reload
	vmovss	104(%r12,%rcx,4), %xmm2         ## xmm2 = mem[0],zero,zero,zero
	movq	736(%rsp), %r11                 ## 8-byte Reload
	vinsertps	$16, 104(%r12,%r11,4), %xmm2, %xmm2 ## xmm2 = xmm2[0],mem[0],xmm2[2,3]
	movq	112(%rsp), %rdx                 ## 8-byte Reload
	vmovss	104(%r12,%rdx,4), %xmm3         ## xmm3 = mem[0],zero,zero,zero
	movq	1152(%rsp), %r9                 ## 8-byte Reload
	vinsertps	$16, 104(%r12,%r9,4), %xmm3, %xmm3 ## xmm3 = xmm3[0],mem[0],xmm3[2,3]
	vmovups	64(%r14,%r15,4), %ymm14
	vfmadd213ps	%ymm12, %ymm14, %ymm1   ## ymm1 = (ymm14 * ymm1) + ymm12
	vaddps	2016(%rsp), %ymm9, %ymm12       ## 32-byte Folded Reload
	movq	448(%rsp), %rdi                 ## 8-byte Reload
	vinsertps	$32, 104(%r12,%rdi,4), %xmm2, %xmm2 ## xmm2 = xmm2[0,1],mem[0],xmm2[3]
	vmulps	%ymm14, %ymm12, %ymm12
	movq	104(%rsp), %rbx                 ## 8-byte Reload
	vinsertps	$32, 104(%r12,%rbx,4), %xmm3, %xmm3 ## xmm3 = xmm3[0,1],mem[0],xmm3[3]
	vmovss	100(%r12,%rcx,4), %xmm7         ## xmm7 = mem[0],zero,zero,zero
	vinsertps	$16, 100(%r12,%r11,4), %xmm7, %xmm7 ## xmm7 = xmm7[0],mem[0],xmm7[2,3]
	vinsertps	$32, 100(%r12,%rdi,4), %xmm7, %xmm7 ## xmm7 = xmm7[0,1],mem[0],xmm7[3]
	vfmadd231ps	%ymm12, %ymm0, %ymm1    ## ymm1 = (ymm0 * ymm12) + ymm1
	vmovss	100(%r12,%rdx,4), %xmm0         ## xmm0 = mem[0],zero,zero,zero
	movq	120(%rsp), %rsi                 ## 8-byte Reload
	vinsertps	$48, 104(%r12,%rsi,4), %xmm2, %xmm2 ## xmm2 = xmm2[0,1,2],mem[0]
	vinsertps	$16, 100(%r12,%r9,4), %xmm0, %xmm0 ## xmm0 = xmm0[0],mem[0],xmm0[2,3]
	movq	128(%rsp), %rax                 ## 8-byte Reload
	vinsertps	$48, 104(%r12,%rax,4), %xmm3, %xmm3 ## xmm3 = xmm3[0,1,2],mem[0]
	vinsertps	$32, 100(%r12,%rbx,4), %xmm0, %xmm0 ## xmm0 = xmm0[0,1],mem[0],xmm0[3]
	vinsertps	$48, 100(%r12,%rsi,4), %xmm7, %xmm7 ## xmm7 = xmm7[0,1,2],mem[0]
	vinsertf128	$1, %xmm2, %ymm3, %ymm2
	vinsertps	$48, 100(%r12,%rax,4), %xmm0, %xmm0 ## xmm0 = xmm0[0,1,2],mem[0]
	vinsertf128	$1, %xmm7, %ymm0, %ymm0
	vandps	1536(%rsp), %ymm0, %ymm0        ## 32-byte Folded Reload
	vmovups	96(%r14,%r15,4), %ymm3
	vfmadd213ps	%ymm11, %ymm3, %ymm0    ## ymm0 = (ymm3 * ymm0) + ymm11
	vaddps	1952(%rsp), %ymm9, %ymm7        ## 32-byte Folded Reload
	vmulps	%ymm3, %ymm7, %ymm3
	vfmadd231ps	%ymm3, %ymm2, %ymm0     ## ymm0 = (ymm2 * ymm3) + ymm0
	vinsertps	$32, 108(%r12,%r10,4), %xmm5, %xmm2 ## xmm2 = xmm5[0,1],mem[0],xmm5[3]
	vinsertf128	$1, 144(%rsp), %ymm10, %ymm9 ## 16-byte Folded Reload
	vmovss	108(%r12,%rcx,4), %xmm5         ## xmm5 = mem[0],zero,zero,zero
	vinsertps	$48, 108(%r12,%r13,4), %xmm8, %xmm7 ## xmm7 = xmm8[0,1,2],mem[0]
	vinsertps	$16, 108(%r12,%r11,4), %xmm5, %xmm5 ## xmm5 = xmm5[0],mem[0],xmm5[2,3]
	movq	80(%rsp), %rcx                  ## 8-byte Reload
	vinsertps	$48, 108(%r12,%rcx,4), %xmm15, %xmm8 ## xmm8 = xmm15[0,1,2],mem[0]
	vinsertps	$32, 108(%r12,%rdi,4), %xmm5, %xmm5 ## xmm5 = xmm5[0,1],mem[0],xmm5[3]
	vinsertps	$48, 108(%r12,%r8,4), %xmm13, %xmm3 ## xmm3 = xmm13[0,1,2],mem[0]
	vinsertf128	$1, %xmm7, %ymm8, %ymm8
	vmovss	108(%r12,%rdx,4), %xmm7         ## xmm7 = mem[0],zero,zero,zero
	movq	416(%rsp), %rcx                 ## 8-byte Reload
	vinsertps	$48, 108(%r12,%rcx,4), %xmm2, %xmm2 ## xmm2 = xmm2[0,1,2],mem[0]
	vinsertps	$16, 108(%r12,%r9,4), %xmm7, %xmm7 ## xmm7 = xmm7[0],mem[0],xmm7[2,3]
	vinsertps	$48, 108(%r12,%rsi,4), %xmm5, %xmm5 ## xmm5 = xmm5[0,1,2],mem[0]
	vinsertps	$32, 108(%r12,%rbx,4), %xmm7, %xmm7 ## xmm7 = xmm7[0,1],mem[0],xmm7[3]
	vinsertps	$48, 108(%r12,%rax,4), %xmm7, %xmm7 ## xmm7 = xmm7[0,1,2],mem[0]
	vinsertf128	$1, %xmm3, %ymm2, %ymm2
	vinsertf128	$1, %xmm5, %ymm7, %ymm3
	movq	504(%rsp), %rax                 ## 8-byte Reload
	vfmadd231ps	(%r14,%rax,4), %ymm9, %ymm6 ## ymm6 = (ymm9 * mem) + ymm6
	vfmadd231ps	32(%r14,%rax,4), %ymm8, %ymm4 ## ymm4 = (ymm8 * mem) + ymm4
	vfmadd231ps	64(%r14,%rax,4), %ymm2, %ymm1 ## ymm1 = (ymm2 * mem) + ymm1
	vfmadd231ps	96(%r14,%rax,4), %ymm3, %ymm0 ## ymm0 = (ymm3 * mem) + ymm0
	vbroadcastss	LCPI0_65(%rip), %ymm2   ## ymm2 = [9.99999971E-10,9.99999971E-10,9.99999971E-10,9.99999971E-10,9.99999971E-10,9.99999971E-10,9.99999971E-10,9.99999971E-10]
	vmovaps	1824(%rsp), %ymm3               ## 32-byte Reload
	vfmadd231ps	%ymm6, %ymm2, %ymm3     ## ymm3 = (ymm2 * ymm6) + ymm3
	vmovaps	%ymm3, 1824(%rsp)               ## 32-byte Spill
	vmovaps	1792(%rsp), %ymm3               ## 32-byte Reload
	vfmadd231ps	%ymm4, %ymm2, %ymm3     ## ymm3 = (ymm2 * ymm4) + ymm3
	vmovaps	%ymm3, 1792(%rsp)               ## 32-byte Spill
	vmovaps	1760(%rsp), %ymm3               ## 32-byte Reload
	vfmadd231ps	%ymm1, %ymm2, %ymm3     ## ymm3 = (ymm2 * ymm1) + ymm3
	vmovaps	%ymm3, 1760(%rsp)               ## 32-byte Spill
	vmovaps	1728(%rsp), %ymm1               ## 32-byte Reload
	vfmadd231ps	%ymm0, %ymm2, %ymm1     ## ymm1 = (ymm2 * ymm0) + ymm1
	vmovaps	%ymm1, 1728(%rsp)               ## 32-byte Spill
	vpbroadcastq	LCPI0_66(%rip), %ymm0   ## ymm0 = [32,32,32,32]
	vmovdqa	2208(%rsp), %ymm1               ## 32-byte Reload
	vpaddq	%ymm0, %ymm1, %ymm1
	vmovdqa	2752(%rsp), %ymm13              ## 32-byte Reload
	vpaddq	%ymm0, %ymm13, %ymm13
	addq	608(%rsp), %r14                 ## 8-byte Folded Reload
	movq	%r14, 640(%rsp)                 ## 8-byte Spill
	addq	$-32, 624(%rsp)                 ## 8-byte Folded Spill
	jne	LBB0_125
## %bb.126:                             ## %middle.block820
                                        ##   in Loop: Header=BB0_87 Depth=1
	vmovaps	1792(%rsp), %ymm0               ## 32-byte Reload
	vaddps	1824(%rsp), %ymm0, %ymm0        ## 32-byte Folded Reload
	vaddps	1760(%rsp), %ymm0, %ymm0        ## 32-byte Folded Reload
	vaddps	1728(%rsp), %ymm0, %ymm0        ## 32-byte Folded Reload
	vextractf128	$1, %ymm0, %xmm1
	vaddps	%xmm1, %xmm0, %xmm0
	vpermilpd	$1, %xmm0, %xmm1        ## xmm1 = xmm0[1,0]
	vaddps	%xmm1, %xmm0, %xmm0
	vmovshdup	%xmm0, %xmm1            ## xmm1 = xmm0[1,1,3,3]
	vaddss	%xmm1, %xmm0, %xmm0
	vmovss	%xmm0, 256(%rsp)                ## 4-byte Spill
	movq	1424(%rsp), %rcx                ## 8-byte Reload
	movq	%rcx, %rax
	cmpq	512(%rsp), %rcx                 ## 8-byte Folded Reload
	vmovaps	2784(%rsp), %xmm11              ## 16-byte Reload
	vmovss	LCPI0_45(%rip), %xmm4           ## xmm4 = mem[0],zero,zero,zero
	vmovss	LCPI0_63(%rip), %xmm13          ## xmm13 = mem[0],zero,zero,zero
	vmovss	LCPI0_64(%rip), %xmm14          ## xmm14 = mem[0],zero,zero,zero
	vmovss	LCPI0_65(%rip), %xmm15          ## xmm15 = mem[0],zero,zero,zero
	movq	2264(%rsp), %r15                ## 8-byte Reload
	jne	LBB0_130
LBB0_127:                               ##   in Loop: Header=BB0_87 Depth=1
	movq	376(%rsp), %r13                 ## 8-byte Reload
LBB0_128:                               ## %call_destructor.exit318
                                        ##   in Loop: Header=BB0_87 Depth=1
	xorl	%edi, %edi
	movq	%r12, %rsi
	vzeroupper
	callq	_halide_free
	movq	1256(%rsp), %rax                ## 8-byte Reload
	movq	720(%rsp), %rcx                 ## 8-byte Reload
	vmovss	256(%rsp), %xmm0                ## 4-byte Reload
                                        ## xmm0 = mem[0],zero,zero,zero
	vmovss	%xmm0, (%rax,%rcx,4)
	incq	%rcx
	incq	1184(%rsp)                      ## 8-byte Folded Spill
	addq	$4, 936(%rsp)                   ## 8-byte Folded Spill
	incq	944(%rsp)                       ## 8-byte Folded Spill
	incq	952(%rsp)                       ## 8-byte Folded Spill
	incq	960(%rsp)                       ## 8-byte Folded Spill
	incq	968(%rsp)                       ## 8-byte Folded Spill
	incq	976(%rsp)                       ## 8-byte Folded Spill
	incq	984(%rsp)                       ## 8-byte Folded Spill
	incq	992(%rsp)                       ## 8-byte Folded Spill
	incq	1000(%rsp)                      ## 8-byte Folded Spill
	incq	1008(%rsp)                      ## 8-byte Folded Spill
	incq	1016(%rsp)                      ## 8-byte Folded Spill
	incq	1024(%rsp)                      ## 8-byte Folded Spill
	incq	1032(%rsp)                      ## 8-byte Folded Spill
	incq	1040(%rsp)                      ## 8-byte Folded Spill
	incq	1048(%rsp)                      ## 8-byte Folded Spill
	incq	1056(%rsp)                      ## 8-byte Folded Spill
	incq	1064(%rsp)                      ## 8-byte Folded Spill
	incq	1072(%rsp)                      ## 8-byte Folded Spill
	incq	1080(%rsp)                      ## 8-byte Folded Spill
	incq	1088(%rsp)                      ## 8-byte Folded Spill
	incq	1096(%rsp)                      ## 8-byte Folded Spill
	incq	1104(%rsp)                      ## 8-byte Folded Spill
	movq	%rcx, 720(%rsp)                 ## 8-byte Spill
	cmpq	2296(%rsp), %rcx                ## 8-byte Folded Reload
	movq	712(%rsp), %r12                 ## 8-byte Reload
	jne	LBB0_87
	jmp	LBB0_137
	.p2align	4, 0x90
LBB0_129:                               ##   in Loop: Header=BB0_87 Depth=1
	vxorps	%xmm0, %xmm0, %xmm0
	vmovss	%xmm0, 256(%rsp)                ## 4-byte Spill
	xorl	%eax, %eax
	vmovaps	2784(%rsp), %xmm11              ## 16-byte Reload
	vmovss	LCPI0_45(%rip), %xmm4           ## xmm4 = mem[0],zero,zero,zero
	vmovss	LCPI0_63(%rip), %xmm13          ## xmm13 = mem[0],zero,zero,zero
	vmovss	LCPI0_64(%rip), %xmm14          ## xmm14 = mem[0],zero,zero,zero
	vmovss	LCPI0_65(%rip), %xmm15          ## xmm15 = mem[0],zero,zero,zero
LBB0_130:                               ## %"for f1.s1.r79$x.preheader980"
                                        ##   in Loop: Header=BB0_87 Depth=1
	movq	%rax, %rcx
	shlq	$7, %rcx
	addq	%r12, %rcx
	addq	$108, %rcx
	movq	1416(%rsp), %rdx                ## 8-byte Reload
	imulq	%rax, %rdx
	movq	944(%rsp), %rsi                 ## 8-byte Reload
	leaq	(%rsi,%rdx), %r14
	movq	952(%rsp), %rsi                 ## 8-byte Reload
	addq	%rdx, %rsi
	movq	%rsi, 544(%rsp)                 ## 8-byte Spill
	movq	960(%rsp), %rsi                 ## 8-byte Reload
	leaq	(%rsi,%rdx), %rsi
	movq	%rsi, 32(%rsp)                  ## 8-byte Spill
	movq	968(%rsp), %rsi                 ## 8-byte Reload
	leaq	(%rsi,%rdx), %rsi
	movq	%rsi, 40(%rsp)                  ## 8-byte Spill
	movq	976(%rsp), %rsi                 ## 8-byte Reload
	leaq	(%rsi,%rdx), %rsi
	movq	%rsi, 80(%rsp)                  ## 8-byte Spill
	movq	984(%rsp), %rsi                 ## 8-byte Reload
	leaq	(%rsi,%rdx), %rsi
	movq	%rsi, 72(%rsp)                  ## 8-byte Spill
	movq	992(%rsp), %rsi                 ## 8-byte Reload
	leaq	(%rsi,%rdx), %rsi
	movq	%rsi, 352(%rsp)                 ## 8-byte Spill
	movq	1000(%rsp), %rsi                ## 8-byte Reload
	leaq	(%rsi,%rdx), %rsi
	movq	%rsi, 336(%rsp)                 ## 8-byte Spill
	movq	1008(%rsp), %rsi                ## 8-byte Reload
	leaq	(%rsi,%rdx), %rsi
	movq	%rsi, 240(%rsp)                 ## 8-byte Spill
	movq	1016(%rsp), %rsi                ## 8-byte Reload
	leaq	(%rsi,%rdx), %rsi
	movq	%rsi, 144(%rsp)                 ## 8-byte Spill
	movq	1024(%rsp), %rsi                ## 8-byte Reload
	leaq	(%rsi,%rdx), %rsi
	movq	%rsi, 176(%rsp)                 ## 8-byte Spill
	movq	1032(%rsp), %rsi                ## 8-byte Reload
	leaq	(%rsi,%rdx), %rsi
	movq	%rsi, 160(%rsp)                 ## 8-byte Spill
	movq	1040(%rsp), %rsi                ## 8-byte Reload
	leaq	(%rsi,%rdx), %rsi
	movq	%rsi, 224(%rsp)                 ## 8-byte Spill
	movq	1048(%rsp), %rsi                ## 8-byte Reload
	leaq	(%rsi,%rdx), %rsi
	movq	%rsi, 304(%rsp)                 ## 8-byte Spill
	movq	1056(%rsp), %rsi                ## 8-byte Reload
	leaq	(%rsi,%rdx), %rsi
	movq	%rsi, 288(%rsp)                 ## 8-byte Spill
	movq	1064(%rsp), %rsi                ## 8-byte Reload
	leaq	(%rsi,%rdx), %rsi
	movq	%rsi, 736(%rsp)                 ## 8-byte Spill
	movq	1072(%rsp), %rsi                ## 8-byte Reload
	leaq	(%rsi,%rdx), %rsi
	movq	%rsi, 64(%rsp)                  ## 8-byte Spill
	movq	1080(%rsp), %rsi                ## 8-byte Reload
	leaq	(%rsi,%rdx), %rsi
	movq	%rsi, 56(%rsp)                  ## 8-byte Spill
	movq	1088(%rsp), %rsi                ## 8-byte Reload
	addq	%rdx, %rsi
	movq	%rsi, 48(%rsp)                  ## 8-byte Spill
	movq	1096(%rsp), %rsi                ## 8-byte Reload
	addq	%rdx, %rsi
	movq	%rsi, 320(%rsp)                 ## 8-byte Spill
	movq	1104(%rsp), %rsi                ## 8-byte Reload
	leaq	(%rsi,%rdx), %r13
	addq	1184(%rsp), %rdx                ## 8-byte Folded Reload
	movq	712(%rsp), %r8                  ## 8-byte Reload
	movq	2272(%rsp), %rsi                ## 8-byte Reload
	jmp	LBB0_132
	.p2align	4, 0x90
LBB0_131:                               ## %"for f1.s1.r79$x"
                                        ##   in Loop: Header=BB0_132 Depth=2
	cmovbq	%rdi, %r10
	movq	240(%rsp), %rdi                 ## 8-byte Reload
	vmulss	(%r8,%rdi,4), %xmm0, %xmm1
	vmulss	(%r10), %xmm1, %xmm1
	cmovbq	%r11, %r9
	vdivss	%xmm7, %xmm1, %xmm1
	movq	336(%rsp), %rdi                 ## 8-byte Reload
	vmovss	(%r8,%rdi,4), %xmm4             ## xmm4 = mem[0],zero,zero,zero
	vfmadd132ss	(%r9), %xmm1, %xmm4     ## xmm4 = (xmm4 * mem) + xmm1
	vmovss	(%r8,%rdx,4), %xmm1             ## xmm1 = mem[0],zero,zero,zero
	vfmadd213ss	%xmm5, %xmm1, %xmm4     ## xmm4 = (xmm1 * xmm4) + xmm5
	vfmadd132ss	-16(%rcx), %xmm4, %xmm6 ## xmm6 = (xmm6 * mem) + xmm4
	vfmadd231ss	%xmm14, %xmm6, %xmm3    ## xmm3 = (xmm6 * xmm14) + xmm3
	vinsertps	$16, %xmm9, %xmm12, %xmm2 ## xmm2 = xmm12[0],xmm9[0],xmm12[2,3]
	vshufps	$4, %xmm8, %xmm2, %xmm4         ## xmm4 = xmm2[0,1],xmm8[0,0]
	vmovddup	%xmm2, %xmm2                    ## xmm2 = xmm2[0,0]
	vinsertf128	$1, %xmm4, %ymm2, %ymm2
	movq	40(%rsp), %rdi                  ## 8-byte Reload
	vmovss	(%r8,%rdi,4), %xmm4             ## xmm4 = mem[0],zero,zero,zero
	movq	80(%rsp), %rdi                  ## 8-byte Reload
	vinsertps	$16, (%r8,%rdi,4), %xmm4, %xmm4 ## xmm4 = xmm4[0],mem[0],xmm4[2,3]
	movq	304(%rsp), %rdi                 ## 8-byte Reload
	vmovss	(%r8,%rdi,4), %xmm5             ## xmm5 = mem[0],zero,zero,zero
	vshufps	$4, %xmm5, %xmm4, %xmm4         ## xmm4 = xmm4[0,1],xmm5[0,0]
	movq	288(%rsp), %rdi                 ## 8-byte Reload
	vbroadcastss	(%r8,%rdi,4), %xmm5
	movq	72(%rsp), %rdi                  ## 8-byte Reload
	vinsertps	$32, (%r8,%rdi,4), %xmm5, %xmm5 ## xmm5 = xmm5[0,1],mem[0],xmm5[3]
	movq	352(%rsp), %rdi                 ## 8-byte Reload
	vinsertps	$48, (%r8,%rdi,4), %xmm5, %xmm5 ## xmm5 = xmm5[0,1,2],mem[0]
	vinsertf128	$1, %xmm5, %ymm4, %ymm4
	vmulps	%ymm4, %ymm2, %ymm2
	vmulps	-76(%rcx), %ymm2, %ymm2
	vextractf128	$1, %ymm2, %xmm4
	vaddps	%xmm4, %xmm2, %xmm2
	vpermilpd	$1, %xmm2, %xmm4        ## xmm4 = xmm2[1,0]
	vaddps	%xmm4, %xmm2, %xmm2
	vmovshdup	%xmm2, %xmm4            ## xmm4 = xmm2[1,1,3,3]
	vaddss	%xmm4, %xmm2, %xmm2
	movq	32(%rsp), %rdi                  ## 8-byte Reload
	vmulss	(%r8,%rdi,4), %xmm9, %xmm4
	vfmadd231ss	-80(%rcx), %xmm4, %xmm2 ## xmm2 = (xmm4 * mem) + xmm2
	movq	544(%rsp), %rdi                 ## 8-byte Reload
	vmulss	(%r8,%rdi,4), %xmm1, %xmm4
	vfmadd231ss	-88(%rcx), %xmm4, %xmm2 ## xmm2 = (xmm4 * mem) + xmm2
	vmulss	(%r8,%r14,4), %xmm1, %xmm4
	vfmadd231ss	-84(%rcx), %xmm4, %xmm2 ## xmm2 = (xmm4 * mem) + xmm2
	vaddss	%xmm2, %xmm3, %xmm2
	vfmadd231ss	-12(%rcx), %xmm1, %xmm2 ## xmm2 = (xmm1 * mem) + xmm2
	vmovss	LCPI0_45(%rip), %xmm4           ## xmm4 = mem[0],zero,zero,zero
	vcmpltss	%xmm0, %xmm4, %xmm1
	vmovss	-8(%rcx), %xmm3                 ## xmm3 = mem[0],zero,zero,zero
	vandps	%xmm3, %xmm1, %xmm1
	movq	224(%rsp), %rdi                 ## 8-byte Reload
	vmovss	(%r8,%rdi,4), %xmm3             ## xmm3 = mem[0],zero,zero,zero
	vfmadd213ss	%xmm2, %xmm3, %xmm1     ## xmm1 = (xmm3 * xmm1) + xmm2
	vaddss	LCPI0_44(%rip), %xmm0, %xmm0
	vmulss	%xmm3, %xmm0, %xmm0
	vfmadd231ss	-4(%rcx), %xmm0, %xmm1  ## xmm1 = (xmm0 * mem) + xmm1
	movq	160(%rsp), %rdi                 ## 8-byte Reload
	vmovss	(%r8,%rdi,4), %xmm0             ## xmm0 = mem[0],zero,zero,zero
	vfmadd231ss	(%rcx), %xmm0, %xmm1    ## xmm1 = (xmm0 * mem) + xmm1
	vmovss	256(%rsp), %xmm0                ## 4-byte Reload
                                        ## xmm0 = mem[0],zero,zero,zero
	vfmadd231ss	%xmm15, %xmm1, %xmm0    ## xmm0 = (xmm1 * xmm15) + xmm0
	vmovss	%xmm0, 256(%rsp)                ## 4-byte Spill
	incq	%rax
	subq	$-128, %rcx
	addq	%rsi, %r8
	cmpq	%rax, 512(%rsp)                 ## 8-byte Folded Reload
	je	LBB0_127
LBB0_132:                               ## %"for f1.s1.r79$x"
                                        ##   Parent Loop BB0_87 Depth=1
                                        ## =>  This Inner Loop Header: Depth=2
	vmovss	(%r8,%r13,4), %xmm9             ## xmm9 = mem[0],zero,zero,zero
	movq	48(%rsp), %rdi                  ## 8-byte Reload
	vmovss	(%r8,%rdi,4), %xmm12            ## xmm12 = mem[0],zero,zero,zero
	movq	56(%rsp), %rdi                  ## 8-byte Reload
	vmovss	(%r8,%rdi,4), %xmm0             ## xmm0 = mem[0],zero,zero,zero
	movq	64(%rsp), %rdi                  ## 8-byte Reload
	vmulss	(%r8,%rdi,4), %xmm0, %xmm10
	vmaxss	%xmm4, %xmm10, %xmm8
	vdivss	%xmm11, %xmm8, %xmm3
	movq	736(%rsp), %rdi                 ## 8-byte Reload
	vmovss	(%r8,%rdi,4), %xmm7             ## xmm7 = mem[0],zero,zero,zero
	leaq	-40(%rcx), %rdi
	leaq	-36(%rcx), %r10
	leaq	-28(%rcx), %rbx
	leaq	-24(%rcx), %r9
	testq	%rax, %rax
	cmoveq	%rdi, %r10
	cmoveq	%rbx, %r9
	leaq	-44(%rcx), %rdi
	leaq	-32(%rcx), %r11
	movq	320(%rsp), %rbx                 ## 8-byte Reload
	vmulss	(%r8,%rbx,4), %xmm9, %xmm5
	vmovaps	%xmm4, %xmm2
	vmulss	-104(%rcx), %xmm12, %xmm4
	vfmadd231ss	-108(%rcx), %xmm5, %xmm4 ## xmm4 = (xmm5 * mem) + xmm4
	vmulss	-96(%rcx), %xmm12, %xmm1
	vfmadd231ss	-100(%rcx), %xmm5, %xmm1 ## xmm1 = (xmm5 * mem) + xmm1
	vucomiss	%xmm0, %xmm2
	vxorps	%xmm5, %xmm5, %xmm5
	movq	176(%rsp), %rbx                 ## 8-byte Reload
	vcmpeqss	(%r8,%rbx,4), %xmm5, %xmm6
	vmaxss	%xmm2, %xmm7, %xmm7
	vblendvps	%xmm6, %xmm4, %xmm1, %xmm1
	vroundss	$10, %xmm3, %xmm3, %xmm4
	vmaxss	%xmm2, %xmm3, %xmm3
	vmulss	%xmm1, %xmm4, %xmm1
	vdivss	%xmm3, %xmm1, %xmm3
	vdivss	%xmm7, %xmm13, %xmm1
	vminss	%xmm0, %xmm1, %xmm1
	movq	144(%rsp), %rbx                 ## 8-byte Reload
	vmulss	(%r8,%rbx,4), %xmm1, %xmm1
	vmulss	%xmm1, %xmm10, %xmm6
	jae	LBB0_131
## %bb.133:                             ##   in Loop: Header=BB0_132 Depth=2
	vaddss	%xmm12, %xmm9, %xmm1
	vmulss	-20(%rcx), %xmm1, %xmm1
	vdivss	%xmm7, %xmm1, %xmm5
	jmp	LBB0_131
LBB0_134:                               ## %call_destructor.exit317.thread
                                        ##   in Loop: Header=BB0_87 Depth=1
	xorl	%edi, %edi
	movq	32(%rsp), %rsi                  ## 8-byte Reload
	vzeroupper
	callq	_halide_free
	xorl	%edi, %edi
	movq	256(%rsp), %rsi                 ## 8-byte Reload
	callq	_halide_free
	vxorps	%xmm0, %xmm0, %xmm0
	vmovss	%xmm0, 256(%rsp)                ## 4-byte Spill
	jmp	LBB0_128
LBB0_135:                               ##   in Loop: Header=BB0_87 Depth=1
	vxorps	%xmm0, %xmm0, %xmm0
	vmovss	%xmm0, 256(%rsp)                ## 4-byte Spill
	jmp	LBB0_127
LBB0_136:                               ## %next_bb169
	movq	2336(%rsp), %rbx                ## 8-byte Reload
	imull	%edi, %ebx
	addl	624(%rsp), %ebx                 ## 4-byte Folded Reload
	movl	140(%rsp), %esi                 ## 4-byte Reload
	movq	504(%rsp), %rdx                 ## 8-byte Reload
	imull	%esi, %edx
	addl	%edx, %ebx
	vxorps	%xmm15, %xmm15, %xmm15
	vcvtsi2ssl	1112(%rsp), %xmm15, %xmm0 ## 4-byte Folded Reload
	movl	%r8d, 2808(%rsp)
	movl	1128(%rsp), %edx                ## 4-byte Reload
	movl	%edx, 2812(%rsp)
	movl	1120(%rsp), %edx                ## 4-byte Reload
	movl	%edx, 2816(%rsp)
	movl	%eax, 2820(%rsp)
	movl	%edi, 2824(%rsp)
	movl	%esi, 2828(%rsp)
	movl	608(%rsp), %eax                 ## 4-byte Reload
	andl	$-4, %eax
	movl	%eax, 2832(%rsp)
	movl	%ebx, 2836(%rsp)
	movl	%r14d, 2840(%rsp)
	vmovss	%xmm0, 2844(%rsp)
	movq	88(%rsp), %rax                  ## 8-byte Reload
	movq	%rax, 2848(%rsp)
	movq	$0, 2856(%rsp)
	movq	%rcx, 2864(%rsp)
	movq	40(%rbp), %rax
	movq	%rax, 2872(%rsp)
	movq	1264(%rsp), %rax                ## 8-byte Reload
	movq	%rax, 2880(%rsp)
	movq	%r10, 2888(%rsp)
	movq	1144(%rsp), %rax                ## 8-byte Reload
	movq	%rax, 2896(%rsp)
	movq	24(%rbp), %rax
	movq	%rax, 2904(%rsp)
	movq	1256(%rsp), %rax                ## 8-byte Reload
	movq	%rax, 2912(%rsp)
	movq	%r9, 2920(%rsp)
	movq	%r11, 2928(%rsp)
	movq	3008(%rsp), %rax                ## 8-byte Reload
	movq	%rax, 2936(%rsp)
	leal	7(%r8), %ecx
	sarl	$3, %ecx
	leaq	_cost_model.par_for.prediction_output.s0.n.v7(%rip), %rsi
	xorl	%r12d, %r12d
	leaq	2808(%rsp), %r8
	xorl	%edi, %edi
	xorl	%edx, %edx
	vzeroupper
	callq	_halide_do_par_for
	movq	88(%rsp), %rsi                  ## 8-byte Reload
	testl	%eax, %eax
	je	LBB0_138
	jmp	LBB0_160
LBB0_137:                               ## %call_destructor.exit314.loopexit
	vmovaps	1856(%rsp), %ymm0               ## 32-byte Reload
	vmovaps	%ymm0, 3232(%rsp)
	vmovaps	1888(%rsp), %ymm0               ## 32-byte Reload
	vmovaps	%ymm0, 3168(%rsp)
	vmovaps	1440(%rsp), %ymm0               ## 32-byte Reload
	vmovaps	%ymm0, 3200(%rsp)
	vmovaps	1472(%rsp), %ymm0               ## 32-byte Reload
	vmovaps	%ymm0, 3264(%rsp)
	movq	88(%rsp), %rsi                  ## 8-byte Reload
LBB0_138:                               ## %call_destructor.exit314
	xorl	%r12d, %r12d
	xorl	%edi, %edi
	vzeroupper
	callq	_halide_free
	movq	1408(%rsp), %rax                ## 8-byte Reload
	movl	$0, (%rax)
	xorl	%esi, %esi
	xorl	%r14d, %r14d
LBB0_139:                               ## %call_destructor.exit.thread
	testl	%r14d, %r14d
	setne	%bl
	testb	%bl, %bl
	je	LBB0_142
LBB0_140:                               ## %call_destructor.exit280
	testq	%r12, %r12
	je	LBB0_142
## %bb.141:
	xorl	%edi, %edi
	movq	%rsi, %r15
	movq	%r12, %rsi
	vzeroupper
	callq	_halide_free
	movq	%r15, %rsi
LBB0_142:                               ## %call_destructor.exit281
	testq	%rsi, %rsi
	sete	%al
	xorb	$1, %bl
	orb	%al, %bl
	jne	LBB0_144
## %bb.143:
	xorl	%edi, %edi
	vzeroupper
	callq	_halide_free
LBB0_144:                               ## %call_destructor.exit282
	movl	%r14d, %eax
LBB0_145:                               ## %call_destructor.exit282
	leaq	-40(%rbp), %rsp
	popq	%rbx
	popq	%r12
	popq	%r13
	popq	%r14
	popq	%r15
	popq	%rbp
	vzeroupper
	retq
LBB0_44:                                ## %after_bb34
	movq	40(%r12), %rax
	vxorpd	%xmm0, %xmm0, %xmm0
	vmovupd	%xmm0, (%r12)
	movq	1504(%rsp), %rdi                ## 8-byte Reload
	imull	$39, %edi, %r13d
	movq	$0, 16(%r12)
	movabsq	$12884975618, %rdx              ## imm = 0x300012002
	movq	%rdx, 32(%r12)
	movl	$0, (%rax)
	movl	%edi, 4(%rax)
	movq	$1, 8(%rax)
	movq	40(%r12), %rax
	movabsq	$167503724544, %rdx             ## imm = 0x2700000000
	movq	%rdx, 16(%rax)
	movl	%edi, 24(%rax)
	movq	48(%rbp), %rdi
	movq	24(%rbp), %rdx
	movl	$0, 28(%rax)
	movq	40(%r12), %rax
	movl	$0, 32(%rax)
	movq	192(%rsp), %rcx                 ## 8-byte Reload
	movl	%ecx, 36(%rax)
	movl	%r13d, 40(%rax)
	movl	$0, 44(%rax)
	movq	88(%rbp), %rax
	movq	$0, 24(%r12)
	cmpq	$0, 16(%r12)
	jne	LBB0_32
	jmp	LBB0_35
LBB0_146:                               ## %"assert failed172"
	vmovaps	1856(%rsp), %ymm0               ## 32-byte Reload
	vmovaps	%ymm0, 3232(%rsp)
	vmovaps	1888(%rsp), %ymm0               ## 32-byte Reload
	vmovaps	%ymm0, 3168(%rsp)
	vmovaps	1440(%rsp), %ymm0               ## 32-byte Reload
	vmovaps	%ymm0, 3200(%rsp)
	vmovaps	1472(%rsp), %ymm0               ## 32-byte Reload
	vmovaps	%ymm0, 3264(%rsp)
	xorl	%r12d, %r12d
	xorl	%edi, %edi
	vzeroupper
	callq	_halide_error_out_of_memory
LBB0_147:                               ## %call_destructor.exit.thread
	movl	%eax, %r14d
	movq	88(%rsp), %rsi                  ## 8-byte Reload
	jmp	LBB0_139
LBB0_148:                               ## %"assert failed174"
	vmovaps	1856(%rsp), %ymm0               ## 32-byte Reload
	vmovaps	%ymm0, 3232(%rsp)
	vmovaps	1888(%rsp), %ymm0               ## 32-byte Reload
	vmovaps	%ymm0, 3168(%rsp)
	vmovaps	1440(%rsp), %ymm0               ## 32-byte Reload
	vmovaps	%ymm0, 3200(%rsp)
	vmovaps	1472(%rsp), %ymm0               ## 32-byte Reload
	vmovaps	%ymm0, 3264(%rsp)
	leaq	l_str.62(%rip), %rsi
	movl	$2147483647, %ecx               ## imm = 0x7FFFFFFF
	xorl	%edi, %edi
	movq	1432(%rsp), %rdx                ## 8-byte Reload
	vzeroupper
	callq	_halide_error_buffer_allocation_too_large
	jmp	LBB0_150
LBB0_149:                               ## %"assert failed176"
	vmovaps	1856(%rsp), %ymm0               ## 32-byte Reload
	vmovaps	%ymm0, 3232(%rsp)
	vmovaps	1888(%rsp), %ymm0               ## 32-byte Reload
	vmovaps	%ymm0, 3168(%rsp)
	vmovaps	1440(%rsp), %ymm0               ## 32-byte Reload
	vmovaps	%ymm0, 3200(%rsp)
	vmovaps	1472(%rsp), %ymm0               ## 32-byte Reload
	vmovaps	%ymm0, 3264(%rsp)
	xorl	%edi, %edi
	vzeroupper
	callq	_halide_error_out_of_memory
LBB0_150:                               ## %call_destructor.exit.thread
	movl	%eax, %r14d
	movq	88(%rsp), %rsi                  ## 8-byte Reload
	movq	32(%rsp), %r12                  ## 8-byte Reload
	jmp	LBB0_139
LBB0_151:                               ## %"assert failed178"
	vmovaps	1856(%rsp), %ymm0               ## 32-byte Reload
	vmovaps	%ymm0, 3232(%rsp)
	vmovaps	1888(%rsp), %ymm0               ## 32-byte Reload
	vmovaps	%ymm0, 3168(%rsp)
	vmovaps	1440(%rsp), %ymm0               ## 32-byte Reload
	vmovaps	%ymm0, 3200(%rsp)
	vmovaps	1472(%rsp), %ymm0               ## 32-byte Reload
	vmovaps	%ymm0, 3264(%rsp)
	leaq	l_str.63(%rip), %rsi
	movl	$2147483647, %ecx               ## imm = 0x7FFFFFFF
	xorl	%edi, %edi
	movq	1400(%rsp), %rdx                ## 8-byte Reload
	vzeroupper
	callq	_halide_error_buffer_allocation_too_large
	jmp	LBB0_153
LBB0_152:                               ## %"assert failed180"
	vmovaps	1856(%rsp), %ymm0               ## 32-byte Reload
	vmovaps	%ymm0, 3232(%rsp)
	vmovaps	1888(%rsp), %ymm0               ## 32-byte Reload
	vmovaps	%ymm0, 3168(%rsp)
	vmovaps	1440(%rsp), %ymm0               ## 32-byte Reload
	vmovaps	%ymm0, 3200(%rsp)
	vmovaps	1472(%rsp), %ymm0               ## 32-byte Reload
	vmovaps	%ymm0, 3264(%rsp)
	xorl	%edi, %edi
	vzeroupper
	callq	_halide_error_out_of_memory
LBB0_153:                               ## %call_destructor.exit
	movl	%eax, %r14d
	testl	%r14d, %r14d
	je	LBB0_156
## %bb.155:
	xorl	%edi, %edi
	movq	256(%rsp), %rsi                 ## 8-byte Reload
	callq	_halide_free
	movb	$1, %bl
	movq	88(%rsp), %rsi                  ## 8-byte Reload
	movq	32(%rsp), %r12                  ## 8-byte Reload
	testb	%bl, %bl
	jne	LBB0_140
	jmp	LBB0_142
LBB0_156:
	xorl	%r14d, %r14d
	jmp	LBB0_144
LBB0_157:                               ## %entry
	leaq	LJTI0_0(%rip), %rcx
	movslq	(%rcx,%rax,4), %rax
	addq	%rcx, %rax
	jmpq	*%rax
LBB0_158:                               ## %assert_failed
	leaq	l_str(%rip), %rsi
	jmp	LBB0_181
LBB0_159:                               ## %"assert failed165"
	decl	%r8d
	decl	%eax
	movl	%eax, (%rsp)
	leaq	l_str.60(%rip), %rsi
	leaq	l_str.3(%rip), %rdx
	xorl	%r12d, %r12d
	xorl	%edi, %edi
	xorl	%ecx, %ecx
                                        ## kill: def $r8d killed $r8d killed $r8
	movq	616(%rsp), %r9                  ## 8-byte Reload
                                        ## kill: def $r9d killed $r9d killed $r9
	vzeroupper
	callq	_halide_error_explicit_bounds_too_small
	movq	88(%rsp), %rsi                  ## 8-byte Reload
LBB0_160:                               ## %call_destructor.exit.thread
	movl	%eax, %r14d
	jmp	LBB0_139
LBB0_161:                               ## %then_bb38
	movl	944(%rsp), %r13d                ## 4-byte Reload
	movl	952(%rsp), %r12d                ## 4-byte Reload
	movl	960(%rsp), %r15d                ## 4-byte Reload
	movl	968(%rsp), %r14d                ## 4-byte Reload
	movl	976(%rsp), %r11d                ## 4-byte Reload
	movl	984(%rsp), %r10d                ## 4-byte Reload
	movl	992(%rsp), %r8d                 ## 4-byte Reload
	movl	1000(%rsp), %edi                ## 4-byte Reload
	movl	1008(%rsp), %ebx                ## 4-byte Reload
	movl	1016(%rsp), %r9d                ## 4-byte Reload
	movl	1024(%rsp), %esi                ## 4-byte Reload
	movl	1032(%rsp), %edx                ## 4-byte Reload
	leaq	LJTI0_1(%rip), %rcx
	movslq	(%rcx,%rax,4), %rax
	addq	%rcx, %rax
	jmpq	*%rax
LBB0_162:                               ## %assert_failed41
	leaq	l_str.13(%rip), %rsi
	xorl	%edi, %edi
	movl	688(%rsp), %edx                 ## 4-byte Reload
	jmp	LBB0_259
LBB0_163:                               ## %no_errors_bb40
	leaq	LJTI0_2(%rip), %rcx
	movslq	(%rcx,%rax,4), %rax
	addq	%rcx, %rax
	jmpq	*%rax
LBB0_164:                               ## %assert_failed105
	leaq	l_str.34(%rip), %rsi
	leaq	l_str.25(%rip), %rcx
	xorl	%edi, %edi
	movq	144(%rsp), %rdx                 ## 8-byte Reload
	jmp	LBB0_303
LBB0_165:                               ## %no_errors_bb104
	movq	%r14, 216(%rsp)                 ## 8-byte Spill
	movq	800(%rsp), %r13                 ## 8-byte Reload
	movq	672(%rsp), %r12                 ## 8-byte Reload
	movq	688(%rsp), %r15                 ## 8-byte Reload
	movq	200(%rsp), %r14                 ## 8-byte Reload
	movq	32(%rsp), %r11                  ## 8-byte Reload
	movq	208(%rsp), %r10                 ## 8-byte Reload
	movq	1344(%rsp), %r9                 ## 8-byte Reload
	movq	640(%rsp), %r8                  ## 8-byte Reload
	movq	120(%rsp), %rbx                 ## 8-byte Reload
	movq	128(%rsp), %rdx                 ## 8-byte Reload
	leaq	LJTI0_3(%rip), %rcx
	movslq	(%rcx,%rax,4), %rax
	addq	%rcx, %rax
	jmpq	*%rax
LBB0_166:                               ## %assert_failed126
	leaq	l_str.10(%rip), %rsi
	movl	$2147483647, %ecx               ## imm = 0x7FFFFFFF
	xorl	%edi, %edi
	movq	%r14, %rdx
	vzeroupper
	callq	_halide_error_buffer_allocation_too_large
	jmp	LBB0_145
LBB0_167:                               ## %"assert failed161"
	addl	$3, %r12d
	movl	%r12d, %eax
	sarl	$31, %eax
	andnl	%r12d, %eax, %edx
	andl	$-4, %edx
	shlq	$7, %rdx
	leaq	l_str.59(%rip), %rsi
	xorl	%r12d, %r12d
	movl	$2147483647, %ecx               ## imm = 0x7FFFFFFF
	xorl	%edi, %edi
	vzeroupper
	callq	_halide_error_buffer_allocation_too_large
	jmp	LBB0_171
LBB0_168:                               ## %"assert failed163"
	xorl	%r12d, %r12d
	xorl	%edi, %edi
	callq	_halide_error_out_of_memory
	jmp	LBB0_171
LBB0_169:                               ## %"assert failed170"
	leaq	l_str.61(%rip), %rsi
	xorl	%r12d, %r12d
	movl	$2147483647, %ecx               ## imm = 0x7FFFFFFF
	xorl	%edi, %edi
	vzeroupper
	callq	_halide_error_buffer_allocation_too_large
	jmp	LBB0_147
LBB0_170:                               ## %"assert failed"
	addl	$23, %ecx
	movl	%ecx, (%rsp)
	leaq	l_str.12(%rip), %rsi
	xorl	%r12d, %r12d
	xorl	%edi, %edi
	movl	$1, %edx
	xorl	%ecx, %ecx
	movl	$31, %r8d
	callq	_halide_error_constraints_make_required_region_smaller
LBB0_171:                               ## %call_destructor.exit.thread
	movl	%eax, %r14d
	xorl	%esi, %esi
	jmp	LBB0_139
LBB0_172:                               ## %assert_failed1
	leaq	l_str.3(%rip), %rsi
	jmp	LBB0_181
LBB0_173:                               ## %assert_failed2
	leaq	l_str.4(%rip), %rsi
	jmp	LBB0_181
LBB0_174:                               ## %assert_failed3
	leaq	l_str.5(%rip), %rsi
	jmp	LBB0_181
LBB0_175:                               ## %assert_failed4
	leaq	l_str.6(%rip), %rsi
	jmp	LBB0_181
LBB0_176:                               ## %assert_failed5
	leaq	l_str.7(%rip), %rsi
	jmp	LBB0_181
LBB0_177:                               ## %assert_failed6
	leaq	l_str.8(%rip), %rsi
	jmp	LBB0_181
LBB0_178:                               ## %assert_failed7
	leaq	l_str.9(%rip), %rsi
	jmp	LBB0_181
LBB0_179:                               ## %assert_failed8
	leaq	l_str.10(%rip), %rsi
	jmp	LBB0_181
LBB0_180:                               ## %assert_failed9
	leaq	l_str.11(%rip), %rsi
LBB0_181:                               ## %assert_failed
	xorl	%edi, %edi
	vzeroupper
	callq	_halide_error_buffer_argument_is_null
	jmp	LBB0_145
LBB0_182:                               ## %assert_failed106
	leaq	l_str.35(%rip), %rsi
	leaq	l_str.36(%rip), %rcx
	xorl	%edi, %edi
	movq	176(%rsp), %rdx                 ## 8-byte Reload
	jmp	LBB0_186
LBB0_183:                               ## %assert_failed107
	leaq	l_str.37(%rip), %rsi
	leaq	l_str.23(%rip), %rcx
	xorl	%edi, %edi
	movl	1344(%rsp), %edx                ## 4-byte Reload
	jmp	LBB0_307
LBB0_184:                               ## %assert_failed108
	leaq	l_str.38(%rip), %rsi
	leaq	l_str.25(%rip), %rcx
	xorl	%edi, %edi
	movq	160(%rsp), %rdx                 ## 8-byte Reload
	jmp	LBB0_303
LBB0_185:                               ## %assert_failed109
	leaq	l_str.39(%rip), %rsi
	leaq	l_str.36(%rip), %rcx
	xorl	%edi, %edi
	movq	32(%rsp), %rdx                  ## 8-byte Reload
LBB0_186:                               ## %assert_failed106
                                        ## kill: def $edx killed $edx killed $rdx
	movl	$8, %r8d
	vzeroupper
	callq	_halide_error_constraint_violated
	jmp	LBB0_145
LBB0_187:                               ## %assert_failed110
	leaq	l_str.40(%rip), %rsi
	leaq	l_str.25(%rip), %rcx
	xorl	%edi, %edi
	movq	224(%rsp), %rdx                 ## 8-byte Reload
	jmp	LBB0_303
LBB0_188:                               ## %assert_failed111
	leaq	l_str.41(%rip), %rsi
	leaq	l_str.42(%rip), %rcx
	xorl	%edi, %edi
	movq	288(%rsp), %rdx                 ## 8-byte Reload
                                        ## kill: def $edx killed $edx killed $rdx
	movl	$40, %r8d
	vzeroupper
	callq	_halide_error_constraint_violated
	jmp	LBB0_145
LBB0_189:                               ## %assert_failed112
	leaq	l_str.43(%rip), %rsi
	leaq	l_str.25(%rip), %rcx
	xorl	%edi, %edi
	movq	304(%rsp), %rdx                 ## 8-byte Reload
	jmp	LBB0_303
LBB0_190:                               ## %assert_failed113
	leaq	l_str.44(%rip), %rsi
	leaq	l_str.45(%rip), %rcx
	xorl	%edi, %edi
	movq	736(%rsp), %rdx                 ## 8-byte Reload
                                        ## kill: def $edx killed $edx killed $rdx
	movl	$7, %r8d
	vzeroupper
	callq	_halide_error_constraint_violated
	jmp	LBB0_145
LBB0_191:                               ## %assert_failed114
	leaq	l_str.46(%rip), %rsi
	leaq	l_str.23(%rip), %rcx
	xorl	%edi, %edi
	movl	208(%rsp), %edx                 ## 4-byte Reload
	jmp	LBB0_307
LBB0_192:                               ## %assert_failed115
	leaq	l_str.47(%rip), %rsi
	leaq	l_str.25(%rip), %rcx
	xorl	%edi, %edi
	movq	64(%rsp), %rdx                  ## 8-byte Reload
	jmp	LBB0_303
LBB0_193:                               ## %assert_failed116
	leaq	l_str.48(%rip), %rsi
	leaq	l_str.49(%rip), %rcx
	xorl	%edi, %edi
	movq	56(%rsp), %rdx                  ## 8-byte Reload
	jmp	LBB0_197
LBB0_194:                               ## %assert_failed117
	leaq	l_str.50(%rip), %rsi
	leaq	l_str.23(%rip), %rcx
	xorl	%edi, %edi
	movl	672(%rsp), %edx                 ## 4-byte Reload
	jmp	LBB0_307
LBB0_195:                               ## %assert_failed118
	leaq	l_str.51(%rip), %rsi
	leaq	l_str.25(%rip), %rcx
	xorl	%edi, %edi
	movq	48(%rsp), %rdx                  ## 8-byte Reload
	jmp	LBB0_303
LBB0_196:                               ## %assert_failed119
	leaq	l_str.52(%rip), %rsi
	leaq	l_str.49(%rip), %rcx
	xorl	%edi, %edi
	movq	384(%rsp), %rdx                 ## 8-byte Reload
LBB0_197:                               ## %assert_failed116
                                        ## kill: def $edx killed $edx killed $rdx
	movl	$24, %r8d
	vzeroupper
	callq	_halide_error_constraint_violated
	jmp	LBB0_145
LBB0_198:                               ## %assert_failed120
	leaq	l_str.53(%rip), %rsi
	leaq	l_str.25(%rip), %rcx
	xorl	%edi, %edi
	movq	320(%rsp), %rdx                 ## 8-byte Reload
	jmp	LBB0_303
LBB0_199:                               ## %assert_failed121
	leaq	l_str.54(%rip), %rsi
	leaq	l_str.55(%rip), %rcx
	xorl	%edi, %edi
	movq	1152(%rsp), %rdx                ## 8-byte Reload
                                        ## kill: def $edx killed $edx killed $rdx
	movl	$39, %r8d
	vzeroupper
	callq	_halide_error_constraint_violated
	jmp	LBB0_145
LBB0_200:                               ## %assert_failed122
	leaq	l_str.56(%rip), %rsi
	leaq	l_str.23(%rip), %rcx
	xorl	%edi, %edi
	movl	1664(%rsp), %edx                ## 4-byte Reload
	jmp	LBB0_307
LBB0_201:                               ## %assert_failed123
	leaq	l_str.57(%rip), %rsi
	leaq	l_str.23(%rip), %rcx
	xorl	%edi, %edi
	movl	2080(%rsp), %edx                ## 4-byte Reload
	jmp	LBB0_307
LBB0_202:                               ## %assert_failed124
	leaq	l_str.58(%rip), %rsi
	leaq	l_str.23(%rip), %rcx
	xorl	%edi, %edi
	movl	2048(%rsp), %edx                ## 4-byte Reload
	jmp	LBB0_307
LBB0_203:                               ## %assert_failed127
	leaq	l_str.8(%rip), %rsi
	movl	$2147483647, %ecx               ## imm = 0x7FFFFFFF
	xorl	%edi, %edi
	movq	%r15, %rdx
	vzeroupper
	callq	_halide_error_buffer_allocation_too_large
	jmp	LBB0_145
LBB0_204:                               ## %assert_failed128
	leaq	l_str.8(%rip), %rsi
	movl	$2147483647, %ecx               ## imm = 0x7FFFFFFF
	xorl	%edi, %edi
	movq	%r12, %rdx
	vzeroupper
	callq	_halide_error_buffer_allocation_too_large
	jmp	LBB0_145
LBB0_205:                               ## %assert_failed129
	leaq	l_str.6(%rip), %rsi
	movl	$2147483647, %ecx               ## imm = 0x7FFFFFFF
	xorl	%edi, %edi
	movq	%r13, %rdx
	vzeroupper
	callq	_halide_error_buffer_allocation_too_large
	jmp	LBB0_145
LBB0_206:                               ## %assert_failed130
	leaq	l_str.4(%rip), %rsi
	movl	$2147483647, %ecx               ## imm = 0x7FFFFFFF
	xorl	%edi, %edi
	movq	%rbx, %rdx
	vzeroupper
	callq	_halide_error_buffer_allocation_too_large
	jmp	LBB0_145
LBB0_207:                               ## %assert_failed131
	leaq	l_str.4(%rip), %rsi
	movl	$2147483647, %ecx               ## imm = 0x7FFFFFFF
	xorl	%edi, %edi
	movq	656(%rsp), %rdx                 ## 8-byte Reload
	vzeroupper
	callq	_halide_error_buffer_allocation_too_large
	jmp	LBB0_145
LBB0_208:                               ## %assert_failed132
	leaq	l_str.4(%rip), %rsi
	movl	$2147483647, %ecx               ## imm = 0x7FFFFFFF
	xorl	%edi, %edi
	vzeroupper
	callq	_halide_error_buffer_extents_too_large
	jmp	LBB0_145
LBB0_209:                               ## %assert_failed133
	leaq	l_str.4(%rip), %rsi
	movl	$2147483647, %ecx               ## imm = 0x7FFFFFFF
	xorl	%edi, %edi
	movq	784(%rsp), %rdx                 ## 8-byte Reload
	vzeroupper
	callq	_halide_error_buffer_allocation_too_large
	jmp	LBB0_145
LBB0_210:                               ## %assert_failed134
	leaq	l_str.4(%rip), %rsi
	movl	$2147483647, %ecx               ## imm = 0x7FFFFFFF
	xorl	%edi, %edi
	movq	%r8, %rdx
	vzeroupper
	callq	_halide_error_buffer_extents_too_large
	jmp	LBB0_145
LBB0_211:                               ## %assert_failed135
	movslq	240(%rsp), %rdx                 ## 4-byte Folded Reload
	leaq	l_str.3(%rip), %rsi
	movl	$2147483647, %ecx               ## imm = 0x7FFFFFFF
	xorl	%edi, %edi
	vzeroupper
	callq	_halide_error_buffer_allocation_too_large
	jmp	LBB0_145
LBB0_212:                               ## %assert_failed136
	leaq	l_str(%rip), %rsi
	movl	$2147483647, %ecx               ## imm = 0x7FFFFFFF
	xorl	%edi, %edi
	movq	%r9, %rdx
	vzeroupper
	callq	_halide_error_buffer_allocation_too_large
	jmp	LBB0_145
LBB0_213:                               ## %assert_failed137
	leaq	l_str(%rip), %rsi
	movl	$2147483647, %ecx               ## imm = 0x7FFFFFFF
	xorl	%edi, %edi
	movq	768(%rsp), %rdx                 ## 8-byte Reload
	vzeroupper
	callq	_halide_error_buffer_allocation_too_large
	jmp	LBB0_145
LBB0_214:                               ## %assert_failed138
	leaq	l_str(%rip), %rsi
	movl	$2147483647, %ecx               ## imm = 0x7FFFFFFF
	xorl	%edi, %edi
	movq	%r10, %rdx
	vzeroupper
	callq	_halide_error_buffer_extents_too_large
	jmp	LBB0_145
LBB0_215:                               ## %assert_failed139
	leaq	l_str(%rip), %rsi
	movl	$2147483647, %ecx               ## imm = 0x7FFFFFFF
	xorl	%edi, %edi
	movq	216(%rsp), %rdx                 ## 8-byte Reload
	vzeroupper
	callq	_halide_error_buffer_allocation_too_large
	jmp	LBB0_145
LBB0_216:                               ## %assert_failed140
	leaq	l_str(%rip), %rsi
	movl	$2147483647, %ecx               ## imm = 0x7FFFFFFF
	xorl	%edi, %edi
	movq	%r11, %rdx
	vzeroupper
	callq	_halide_error_buffer_extents_too_large
	jmp	LBB0_145
LBB0_217:                               ## %assert_failed141
	leaq	l_str.13(%rip), %rsi
	jmp	LBB0_227
LBB0_218:                               ## %assert_failed142
	leaq	l_str.12(%rip), %rsi
	jmp	LBB0_227
LBB0_219:                               ## %assert_failed143
	leaq	l_str.14(%rip), %rsi
	jmp	LBB0_227
LBB0_220:                               ## %assert_failed144
	leaq	l_str.15(%rip), %rsi
	jmp	LBB0_227
LBB0_221:                               ## %assert_failed145
	leaq	l_str.16(%rip), %rsi
	jmp	LBB0_227
LBB0_222:                               ## %assert_failed146
	leaq	l_str.17(%rip), %rsi
	jmp	LBB0_227
LBB0_223:                               ## %assert_failed147
	leaq	l_str.18(%rip), %rsi
	jmp	LBB0_227
LBB0_224:                               ## %assert_failed148
	leaq	l_str.19(%rip), %rsi
	jmp	LBB0_227
LBB0_225:                               ## %assert_failed149
	leaq	l_str.20(%rip), %rsi
	jmp	LBB0_227
LBB0_226:                               ## %assert_failed150
	leaq	l_str.21(%rip), %rsi
LBB0_227:                               ## %assert_failed141
	xorl	%edi, %edi
	vzeroupper
	callq	_halide_error_device_dirty_with_no_device_support
	jmp	LBB0_145
LBB0_228:                               ## %assert_failed151
	leaq	l_str.13(%rip), %rsi
	jmp	LBB0_238
LBB0_229:                               ## %assert_failed152
	leaq	l_str.12(%rip), %rsi
	jmp	LBB0_238
LBB0_230:                               ## %assert_failed153
	leaq	l_str.14(%rip), %rsi
	jmp	LBB0_238
LBB0_231:                               ## %assert_failed154
	leaq	l_str.15(%rip), %rsi
	jmp	LBB0_238
LBB0_232:                               ## %assert_failed155
	leaq	l_str.16(%rip), %rsi
	jmp	LBB0_238
LBB0_233:                               ## %assert_failed156
	leaq	l_str.17(%rip), %rsi
	jmp	LBB0_238
LBB0_234:                               ## %assert_failed157
	leaq	l_str.18(%rip), %rsi
	jmp	LBB0_238
LBB0_235:                               ## %assert_failed158
	leaq	l_str.19(%rip), %rsi
	jmp	LBB0_238
LBB0_236:                               ## %assert_failed159
	leaq	l_str.20(%rip), %rsi
	jmp	LBB0_238
LBB0_237:                               ## %assert_failed160
	leaq	l_str.21(%rip), %rsi
LBB0_238:                               ## %assert_failed151
	xorl	%edi, %edi
	vzeroupper
	callq	_halide_error_host_is_null
	jmp	LBB0_145
LBB0_239:                               ## %assert_failed42
	leaq	l_str.13(%rip), %rsi
	xorl	%edi, %edi
	movl	200(%rsp), %edx                 ## 4-byte Reload
	jmp	LBB0_257
LBB0_240:                               ## %assert_failed43
	leaq	l_str.12(%rip), %rsi
	xorl	%edi, %edi
	movl	784(%rsp), %edx                 ## 4-byte Reload
	jmp	LBB0_259
LBB0_241:                               ## %assert_failed44
	leaq	l_str.12(%rip), %rsi
	xorl	%edi, %edi
	movl	768(%rsp), %edx                 ## 4-byte Reload
	jmp	LBB0_250
LBB0_242:                               ## %assert_failed45
	leaq	l_str.14(%rip), %rsi
	xorl	%edi, %edi
	movl	2208(%rsp), %edx                ## 4-byte Reload
	jmp	LBB0_259
LBB0_243:                               ## %assert_failed46
	leaq	l_str.14(%rip), %rsi
	xorl	%edi, %edi
	movl	1632(%rsp), %edx                ## 4-byte Reload
	jmp	LBB0_257
LBB0_244:                               ## %assert_failed47
	leaq	l_str.15(%rip), %rsi
	xorl	%edi, %edi
	movl	2144(%rsp), %edx                ## 4-byte Reload
	jmp	LBB0_259
LBB0_245:                               ## %assert_failed48
	leaq	l_str.15(%rip), %rsi
	xorl	%edi, %edi
	movl	2112(%rsp), %edx                ## 4-byte Reload
	jmp	LBB0_261
LBB0_246:                               ## %assert_failed49
	leaq	l_str.16(%rip), %rsi
	xorl	%edi, %edi
	movl	1600(%rsp), %edx                ## 4-byte Reload
	jmp	LBB0_259
LBB0_247:                               ## %assert_failed50
	leaq	l_str.16(%rip), %rsi
	xorl	%edi, %edi
	movl	1568(%rsp), %edx                ## 4-byte Reload
	jmp	LBB0_257
LBB0_248:                               ## %assert_failed51
	leaq	l_str.17(%rip), %rsi
	xorl	%edi, %edi
	movl	1184(%rsp), %edx                ## 4-byte Reload
	jmp	LBB0_259
LBB0_249:                               ## %assert_failed52
	leaq	l_str.17(%rip), %rsi
	xorl	%edi, %edi
	movl	1824(%rsp), %edx                ## 4-byte Reload
LBB0_250:                               ## %assert_failed44
	movl	$2, %ecx
	vzeroupper
	callq	_halide_error_bad_dimensions
	jmp	LBB0_145
LBB0_251:                               ## %assert_failed53
	leaq	l_str.18(%rip), %rsi
	xorl	%edi, %edi
	movl	1792(%rsp), %edx                ## 4-byte Reload
	jmp	LBB0_259
LBB0_252:                               ## %assert_failed54
	leaq	l_str.18(%rip), %rsi
	xorl	%edi, %edi
	movl	1760(%rsp), %edx                ## 4-byte Reload
	xorl	%ecx, %ecx
	vzeroupper
	callq	_halide_error_bad_dimensions
	jmp	LBB0_145
LBB0_253:                               ## %assert_failed55
	leaq	l_str.19(%rip), %rsi
	xorl	%edi, %edi
	movl	1728(%rsp), %edx                ## 4-byte Reload
	jmp	LBB0_259
LBB0_254:                               ## %assert_failed56
	leaq	l_str.19(%rip), %rsi
	xorl	%edi, %edi
	movl	536(%rsp), %edx                 ## 4-byte Reload
	jmp	LBB0_261
LBB0_255:                               ## %assert_failed57
	leaq	l_str.20(%rip), %rsi
	xorl	%edi, %edi
	movl	528(%rsp), %edx                 ## 4-byte Reload
	jmp	LBB0_259
LBB0_256:                               ## %assert_failed58
	leaq	l_str.20(%rip), %rsi
	xorl	%edi, %edi
	movl	520(%rsp), %edx                 ## 4-byte Reload
LBB0_257:                               ## %assert_failed42
	movl	$1, %ecx
	vzeroupper
	callq	_halide_error_bad_dimensions
	jmp	LBB0_145
LBB0_258:                               ## %assert_failed59
	leaq	l_str.21(%rip), %rsi
	xorl	%edi, %edi
	movl	1312(%rsp), %edx                ## 4-byte Reload
LBB0_259:                               ## %assert_failed41
	movl	$73730, %ecx                    ## imm = 0x12002
	vzeroupper
	callq	_halide_error_bad_type
	jmp	LBB0_145
LBB0_260:                               ## %assert_failed60
	leaq	l_str.21(%rip), %rsi
	xorl	%edi, %edi
	movl	1296(%rsp), %edx                ## 4-byte Reload
LBB0_261:                               ## %assert_failed48
	movl	$3, %ecx
	vzeroupper
	callq	_halide_error_bad_dimensions
	jmp	LBB0_145
LBB0_262:                               ## %assert_failed61
	decl	%edx
	movl	%edx, (%rsp)
	leaq	l_str.13(%rip), %rsi
	xorl	%edi, %edi
	xorl	%edx, %edx
	xorl	%ecx, %ecx
	movl	$31, %r8d
	movq	400(%rsp), %r9                  ## 8-byte Reload
                                        ## kill: def $r9d killed $r9d killed $r9
	vzeroupper
	callq	_halide_error_access_out_of_bounds
	jmp	LBB0_145
LBB0_263:                               ## %assert_failed62
	leaq	l_str.13(%rip), %rsi
	xorl	%edi, %edi
	xorl	%edx, %edx
	movq	464(%rsp), %rcx                 ## 8-byte Reload
                                        ## kill: def $ecx killed $ecx killed $rcx
	vzeroupper
	callq	_halide_error_buffer_extents_negative
	jmp	LBB0_145
LBB0_264:                               ## %assert_failed63
	decl	%esi
	movl	%esi, (%rsp)
	leaq	l_str.12(%rip), %rsi
	xorl	%edi, %edi
	xorl	%edx, %edx
	xorl	%ecx, %ecx
	movl	$31, %r8d
	movq	656(%rsp), %r9                  ## 8-byte Reload
                                        ## kill: def $r9d killed $r9d killed $r9
	vzeroupper
	callq	_halide_error_access_out_of_bounds
	jmp	LBB0_145
LBB0_265:                               ## %assert_failed64
	leaq	l_str.12(%rip), %rsi
	xorl	%edi, %edi
	xorl	%edx, %edx
	movq	432(%rsp), %rcx                 ## 8-byte Reload
                                        ## kill: def $ecx killed $ecx killed $rcx
	vzeroupper
	callq	_halide_error_buffer_extents_negative
	jmp	LBB0_145
LBB0_266:                               ## %assert_failed65
	movq	640(%rsp), %r8                  ## 8-byte Reload
	addl	$23, %r8d
	movl	%r9d, %edi
	decl	%edi
	movl	%edi, (%rsp)
	leaq	l_str.12(%rip), %rsi
	xorl	%edi, %edi
	movl	$1, %edx
	movl	632(%rsp), %ecx                 ## 4-byte Reload
                                        ## kill: def $r8d killed $r8d killed $r8
	movq	448(%rsp), %r9                  ## 8-byte Reload
                                        ## kill: def $r9d killed $r9d killed $r9
	vzeroupper
	callq	_halide_error_access_out_of_bounds
	jmp	LBB0_145
LBB0_267:                               ## %assert_failed66
	leaq	l_str.12(%rip), %rsi
	xorl	%edi, %edi
	movl	$1, %edx
	movq	416(%rsp), %rcx                 ## 8-byte Reload
                                        ## kill: def $ecx killed $ecx killed $rcx
	vzeroupper
	callq	_halide_error_buffer_extents_negative
	jmp	LBB0_145
LBB0_268:                               ## %assert_failed67
	decl	%ebx
	movl	%ebx, (%rsp)
	leaq	l_str.14(%rip), %rsi
	xorl	%edi, %edi
	xorl	%edx, %edx
	xorl	%ecx, %ecx
	movl	$7, %r8d
	movq	144(%rsp), %r9                  ## 8-byte Reload
                                        ## kill: def $r9d killed $r9d killed $r9
	vzeroupper
	callq	_halide_error_access_out_of_bounds
	jmp	LBB0_145
LBB0_269:                               ## %assert_failed68
	leaq	l_str.14(%rip), %rsi
	xorl	%edi, %edi
	xorl	%edx, %edx
	movq	176(%rsp), %rcx                 ## 8-byte Reload
                                        ## kill: def $ecx killed $ecx killed $rcx
	vzeroupper
	callq	_halide_error_buffer_extents_negative
	jmp	LBB0_145
LBB0_270:                               ## %assert_failed69
	decl	%edi
	movl	%edi, (%rsp)
	leaq	l_str.15(%rip), %rsi
	xorl	%edi, %edi
	xorl	%edx, %edx
	xorl	%ecx, %ecx
	movl	$7, %r8d
	movq	160(%rsp), %r9                  ## 8-byte Reload
                                        ## kill: def $r9d killed $r9d killed $r9
	vzeroupper
	callq	_halide_error_access_out_of_bounds
	jmp	LBB0_145
LBB0_271:                               ## %assert_failed70
	leaq	l_str.15(%rip), %rsi
	xorl	%edi, %edi
	xorl	%edx, %edx
	movq	32(%rsp), %rcx                  ## 8-byte Reload
                                        ## kill: def $ecx killed $ecx killed $rcx
	vzeroupper
	callq	_halide_error_buffer_extents_negative
	jmp	LBB0_145
LBB0_272:                               ## %assert_failed71
	decl	%r8d
	movl	%r8d, (%rsp)
	leaq	l_str.15(%rip), %rsi
	xorl	%edi, %edi
	movl	$1, %edx
	xorl	%ecx, %ecx
	movl	$39, %r8d
	movq	224(%rsp), %r9                  ## 8-byte Reload
                                        ## kill: def $r9d killed $r9d killed $r9
	vzeroupper
	callq	_halide_error_access_out_of_bounds
	jmp	LBB0_145
LBB0_273:                               ## %assert_failed72
	leaq	l_str.15(%rip), %rsi
	xorl	%edi, %edi
	movl	$1, %edx
	movq	288(%rsp), %rcx                 ## 8-byte Reload
                                        ## kill: def $ecx killed $ecx killed $rcx
	vzeroupper
	callq	_halide_error_buffer_extents_negative
	jmp	LBB0_145
LBB0_274:                               ## %assert_failed73
	decl	%r10d
	movl	%r10d, (%rsp)
	leaq	l_str.15(%rip), %rsi
	xorl	%edi, %edi
	movl	$2, %edx
	xorl	%ecx, %ecx
	movl	$6, %r8d
	movq	304(%rsp), %r9                  ## 8-byte Reload
                                        ## kill: def $r9d killed $r9d killed $r9
	vzeroupper
	callq	_halide_error_access_out_of_bounds
	jmp	LBB0_145
LBB0_275:                               ## %assert_failed74
	leaq	l_str.15(%rip), %rsi
	xorl	%edi, %edi
	movl	$2, %edx
	movq	736(%rsp), %rcx                 ## 8-byte Reload
                                        ## kill: def $ecx killed $ecx killed $rcx
	vzeroupper
	callq	_halide_error_buffer_extents_negative
	jmp	LBB0_145
LBB0_276:                               ## %assert_failed75
	decl	%r11d
	movl	%r11d, (%rsp)
	leaq	l_str.16(%rip), %rsi
	xorl	%edi, %edi
	xorl	%edx, %edx
	xorl	%ecx, %ecx
	movl	$23, %r8d
	movq	64(%rsp), %r9                   ## 8-byte Reload
                                        ## kill: def $r9d killed $r9d killed $r9
	vzeroupper
	callq	_halide_error_access_out_of_bounds
	jmp	LBB0_145
LBB0_277:                               ## %assert_failed76
	leaq	l_str.16(%rip), %rsi
	xorl	%edi, %edi
	xorl	%edx, %edx
	movq	56(%rsp), %rcx                  ## 8-byte Reload
                                        ## kill: def $ecx killed $ecx killed $rcx
	vzeroupper
	callq	_halide_error_buffer_extents_negative
	jmp	LBB0_145
LBB0_278:                               ## %assert_failed77
	decl	%r14d
	movl	%r14d, (%rsp)
	leaq	l_str.17(%rip), %rsi
	xorl	%edi, %edi
	xorl	%edx, %edx
	xorl	%ecx, %ecx
	movl	$23, %r8d
	movq	48(%rsp), %r9                   ## 8-byte Reload
                                        ## kill: def $r9d killed $r9d killed $r9
	vzeroupper
	callq	_halide_error_access_out_of_bounds
	jmp	LBB0_145
LBB0_279:                               ## %assert_failed78
	leaq	l_str.17(%rip), %rsi
	xorl	%edi, %edi
	xorl	%edx, %edx
	movq	384(%rsp), %rcx                 ## 8-byte Reload
                                        ## kill: def $ecx killed $ecx killed $rcx
	vzeroupper
	callq	_halide_error_buffer_extents_negative
	jmp	LBB0_145
LBB0_280:                               ## %assert_failed79
	decl	%r15d
	movl	%r15d, (%rsp)
	leaq	l_str.17(%rip), %rsi
	xorl	%edi, %edi
	movl	$1, %edx
	xorl	%ecx, %ecx
	movl	$38, %r8d
	movq	320(%rsp), %r9                  ## 8-byte Reload
                                        ## kill: def $r9d killed $r9d killed $r9
	vzeroupper
	callq	_halide_error_access_out_of_bounds
	jmp	LBB0_145
LBB0_281:                               ## %assert_failed80
	leaq	l_str.17(%rip), %rsi
	xorl	%edi, %edi
	movl	$1, %edx
	movq	1152(%rsp), %rcx                ## 8-byte Reload
                                        ## kill: def $ecx killed $ecx killed $rcx
	vzeroupper
	callq	_halide_error_buffer_extents_negative
	jmp	LBB0_145
LBB0_282:                               ## %assert_failed81
	decl	%r12d
	movl	%r12d, (%rsp)
	leaq	l_str.19(%rip), %rsi
	xorl	%edi, %edi
	xorl	%edx, %edx
	xorl	%ecx, %ecx
	movl	$39, %r8d
	movq	2560(%rsp), %r9                 ## 8-byte Reload
                                        ## kill: def $r9d killed $r9d killed $r9
	vzeroupper
	callq	_halide_error_access_out_of_bounds
	jmp	LBB0_145
LBB0_283:                               ## %assert_failed82
	leaq	l_str.19(%rip), %rsi
	xorl	%edi, %edi
	xorl	%edx, %edx
	movq	256(%rsp), %rcx                 ## 8-byte Reload
                                        ## kill: def $ecx killed $ecx killed $rcx
	vzeroupper
	callq	_halide_error_buffer_extents_negative
	jmp	LBB0_145
LBB0_284:                               ## %assert_failed83
	decl	%r13d
	movl	%r13d, (%rsp)
	leaq	l_str.19(%rip), %rsi
	xorl	%edi, %edi
	movl	$1, %edx
	xorl	%ecx, %ecx
	movl	$6, %r8d
	movq	40(%rsp), %r9                   ## 8-byte Reload
                                        ## kill: def $r9d killed $r9d killed $r9
	vzeroupper
	callq	_halide_error_access_out_of_bounds
	jmp	LBB0_145
LBB0_285:                               ## %assert_failed84
	leaq	l_str.19(%rip), %rsi
	xorl	%edi, %edi
	movl	$1, %edx
	movq	112(%rsp), %rcx                 ## 8-byte Reload
                                        ## kill: def $ecx killed $ecx killed $rcx
	vzeroupper
	callq	_halide_error_buffer_extents_negative
	jmp	LBB0_145
LBB0_286:                               ## %assert_failed85
	movq	192(%rsp), %r8                  ## 8-byte Reload
	decl	%r8d
	movl	936(%rsp), %eax                 ## 4-byte Reload
	decl	%eax
	movl	%eax, (%rsp)
	leaq	l_str.19(%rip), %rsi
	xorl	%edi, %edi
	movl	$2, %edx
	xorl	%ecx, %ecx
                                        ## kill: def $r8d killed $r8d killed $r8
	movq	2528(%rsp), %r9                 ## 8-byte Reload
                                        ## kill: def $r9d killed $r9d killed $r9
	vzeroupper
	callq	_halide_error_access_out_of_bounds
	jmp	LBB0_145
LBB0_287:                               ## %assert_failed86
	leaq	l_str.19(%rip), %rsi
	xorl	%edi, %edi
	movl	$2, %edx
	movq	480(%rsp), %rcx                 ## 8-byte Reload
                                        ## kill: def $ecx killed $ecx killed $rcx
	vzeroupper
	callq	_halide_error_buffer_extents_negative
	jmp	LBB0_145
LBB0_288:                               ## %assert_failed87
	movq	1504(%rsp), %r8                 ## 8-byte Reload
	decl	%r8d
	movl	2976(%rsp), %eax                ## 4-byte Reload
	decl	%eax
	movl	%eax, (%rsp)
	leaq	l_str.20(%rip), %rsi
	xorl	%edi, %edi
	xorl	%edx, %edx
	xorl	%ecx, %ecx
                                        ## kill: def $r8d killed $r8d killed $r8
	movq	616(%rsp), %r9                  ## 8-byte Reload
                                        ## kill: def $r9d killed $r9d killed $r9
	vzeroupper
	callq	_halide_error_access_out_of_bounds
	jmp	LBB0_145
LBB0_289:                               ## %assert_failed88
	leaq	l_str.20(%rip), %rsi
	xorl	%edi, %edi
	xorl	%edx, %edx
	movq	240(%rsp), %rcx                 ## 8-byte Reload
                                        ## kill: def $ecx killed $ecx killed $rcx
	vzeroupper
	callq	_halide_error_buffer_extents_negative
	jmp	LBB0_145
LBB0_290:                               ## %assert_failed89
	movq	1504(%rsp), %r8                 ## 8-byte Reload
	decl	%r8d
	movl	600(%rsp), %eax                 ## 4-byte Reload
	decl	%eax
	movl	%eax, (%rsp)
	leaq	l_str.21(%rip), %rsi
	xorl	%edi, %edi
	xorl	%edx, %edx
	xorl	%ecx, %ecx
                                        ## kill: def $r8d killed $r8d killed $r8
	movq	624(%rsp), %r9                  ## 8-byte Reload
                                        ## kill: def $r9d killed $r9d killed $r9
	vzeroupper
	callq	_halide_error_access_out_of_bounds
	jmp	LBB0_145
LBB0_291:                               ## %assert_failed90
	leaq	l_str.21(%rip), %rsi
	xorl	%edi, %edi
	xorl	%edx, %edx
	movq	1216(%rsp), %rcx                ## 8-byte Reload
                                        ## kill: def $ecx killed $ecx killed $rcx
	vzeroupper
	callq	_halide_error_buffer_extents_negative
	jmp	LBB0_145
LBB0_292:                               ## %assert_failed91
	movl	1472(%rsp), %eax                ## 4-byte Reload
	decl	%eax
	movl	%eax, (%rsp)
	leaq	l_str.21(%rip), %rsi
	xorl	%edi, %edi
	movl	$1, %edx
	xorl	%ecx, %ecx
	movl	$38, %r8d
	movq	2336(%rsp), %r9                 ## 8-byte Reload
                                        ## kill: def $r9d killed $r9d killed $r9
	vzeroupper
	callq	_halide_error_access_out_of_bounds
	jmp	LBB0_145
LBB0_293:                               ## %assert_failed92
	leaq	l_str.21(%rip), %rsi
	xorl	%edi, %edi
	movl	$1, %edx
	movq	128(%rsp), %rcx                 ## 8-byte Reload
                                        ## kill: def $ecx killed $ecx killed $rcx
	vzeroupper
	callq	_halide_error_buffer_extents_negative
	jmp	LBB0_145
LBB0_294:                               ## %assert_failed93
	movq	192(%rsp), %r8                  ## 8-byte Reload
	decl	%r8d
	movl	1440(%rsp), %eax                ## 4-byte Reload
	decl	%eax
	movl	%eax, (%rsp)
	leaq	l_str.21(%rip), %rsi
	xorl	%edi, %edi
	movl	$2, %edx
	xorl	%ecx, %ecx
                                        ## kill: def $r8d killed $r8d killed $r8
	movq	504(%rsp), %r9                  ## 8-byte Reload
                                        ## kill: def $r9d killed $r9d killed $r9
	vzeroupper
	callq	_halide_error_access_out_of_bounds
	jmp	LBB0_145
LBB0_295:                               ## %assert_failed94
	leaq	l_str.21(%rip), %rsi
	xorl	%edi, %edi
	movl	$2, %edx
	movq	104(%rsp), %rcx                 ## 8-byte Reload
                                        ## kill: def $ecx killed $ecx killed $rcx
	vzeroupper
	callq	_halide_error_buffer_extents_negative
	jmp	LBB0_145
LBB0_296:                               ## %assert_failed95
	leaq	l_str.22(%rip), %rsi
	leaq	l_str.23(%rip), %rcx
	xorl	%edi, %edi
	movl	800(%rsp), %edx                 ## 4-byte Reload
	jmp	LBB0_307
LBB0_297:                               ## %assert_failed96
	leaq	l_str.24(%rip), %rsi
	leaq	l_str.25(%rip), %rcx
	xorl	%edi, %edi
	movq	400(%rsp), %rdx                 ## 8-byte Reload
	jmp	LBB0_303
LBB0_298:                               ## %assert_failed97
	leaq	l_str.26(%rip), %rsi
	leaq	l_str.27(%rip), %rcx
	xorl	%edi, %edi
	movq	464(%rsp), %rdx                 ## 8-byte Reload
	jmp	LBB0_305
LBB0_299:                               ## %assert_failed98
	leaq	l_str.28(%rip), %rsi
	leaq	l_str.23(%rip), %rcx
	xorl	%edi, %edi
	movl	1200(%rsp), %edx                ## 4-byte Reload
	jmp	LBB0_307
LBB0_300:                               ## %assert_failed99
	leaq	l_str.29(%rip), %rsi
	leaq	l_str.25(%rip), %rcx
	xorl	%edi, %edi
	movq	656(%rsp), %rdx                 ## 8-byte Reload
	jmp	LBB0_303
LBB0_301:                               ## %assert_failed100
	leaq	l_str.30(%rip), %rsi
	leaq	l_str.27(%rip), %rcx
	xorl	%edi, %edi
	movq	432(%rsp), %rdx                 ## 8-byte Reload
	jmp	LBB0_305
LBB0_302:                               ## %assert_failed101
	leaq	l_str.31(%rip), %rsi
	leaq	l_str.25(%rip), %rcx
	xorl	%edi, %edi
	movq	448(%rsp), %rdx                 ## 8-byte Reload
LBB0_303:                               ## %assert_failed96
                                        ## kill: def $edx killed $edx killed $rdx
	xorl	%r8d, %r8d
	vzeroupper
	callq	_halide_error_constraint_violated
	jmp	LBB0_145
LBB0_304:                               ## %assert_failed102
	leaq	l_str.32(%rip), %rsi
	leaq	l_str.27(%rip), %rcx
	xorl	%edi, %edi
	movq	416(%rsp), %rdx                 ## 8-byte Reload
LBB0_305:                               ## %assert_failed97
                                        ## kill: def $edx killed $edx killed $rdx
	movl	$32, %r8d
	vzeroupper
	callq	_halide_error_constraint_violated
	jmp	LBB0_145
LBB0_306:                               ## %assert_failed103
	leaq	l_str.33(%rip), %rsi
	leaq	l_str.23(%rip), %rcx
	xorl	%edi, %edi
	movl	2176(%rsp), %edx                ## 4-byte Reload
LBB0_307:                               ## %assert_failed95
	movl	$1, %r8d
	vzeroupper
	callq	_halide_error_constraint_violated
	jmp	LBB0_145
	.cfi_endproc
	.p2align	2, 0x90
	.data_region jt32
.set L0_0_set_158, LBB0_158-LJTI0_0
.set L0_0_set_172, LBB0_172-LJTI0_0
.set L0_0_set_173, LBB0_173-LJTI0_0
.set L0_0_set_174, LBB0_174-LJTI0_0
.set L0_0_set_175, LBB0_175-LJTI0_0
.set L0_0_set_176, LBB0_176-LJTI0_0
.set L0_0_set_177, LBB0_177-LJTI0_0
.set L0_0_set_178, LBB0_178-LJTI0_0
.set L0_0_set_179, LBB0_179-LJTI0_0
.set L0_0_set_180, LBB0_180-LJTI0_0
LJTI0_0:
	.long	L0_0_set_158
	.long	L0_0_set_172
	.long	L0_0_set_173
	.long	L0_0_set_174
	.long	L0_0_set_175
	.long	L0_0_set_176
	.long	L0_0_set_177
	.long	L0_0_set_178
	.long	L0_0_set_179
	.long	L0_0_set_180
.set L0_1_set_162, LBB0_162-LJTI0_1
.set L0_1_set_239, LBB0_239-LJTI0_1
.set L0_1_set_240, LBB0_240-LJTI0_1
.set L0_1_set_241, LBB0_241-LJTI0_1
.set L0_1_set_242, LBB0_242-LJTI0_1
.set L0_1_set_243, LBB0_243-LJTI0_1
.set L0_1_set_244, LBB0_244-LJTI0_1
.set L0_1_set_245, LBB0_245-LJTI0_1
.set L0_1_set_246, LBB0_246-LJTI0_1
.set L0_1_set_247, LBB0_247-LJTI0_1
.set L0_1_set_248, LBB0_248-LJTI0_1
.set L0_1_set_249, LBB0_249-LJTI0_1
.set L0_1_set_251, LBB0_251-LJTI0_1
.set L0_1_set_252, LBB0_252-LJTI0_1
.set L0_1_set_253, LBB0_253-LJTI0_1
.set L0_1_set_254, LBB0_254-LJTI0_1
.set L0_1_set_255, LBB0_255-LJTI0_1
.set L0_1_set_256, LBB0_256-LJTI0_1
.set L0_1_set_258, LBB0_258-LJTI0_1
.set L0_1_set_260, LBB0_260-LJTI0_1
.set L0_1_set_262, LBB0_262-LJTI0_1
.set L0_1_set_263, LBB0_263-LJTI0_1
.set L0_1_set_264, LBB0_264-LJTI0_1
.set L0_1_set_265, LBB0_265-LJTI0_1
.set L0_1_set_266, LBB0_266-LJTI0_1
.set L0_1_set_267, LBB0_267-LJTI0_1
.set L0_1_set_268, LBB0_268-LJTI0_1
.set L0_1_set_269, LBB0_269-LJTI0_1
.set L0_1_set_270, LBB0_270-LJTI0_1
.set L0_1_set_271, LBB0_271-LJTI0_1
.set L0_1_set_272, LBB0_272-LJTI0_1
.set L0_1_set_273, LBB0_273-LJTI0_1
.set L0_1_set_274, LBB0_274-LJTI0_1
.set L0_1_set_275, LBB0_275-LJTI0_1
.set L0_1_set_276, LBB0_276-LJTI0_1
.set L0_1_set_277, LBB0_277-LJTI0_1
.set L0_1_set_278, LBB0_278-LJTI0_1
.set L0_1_set_279, LBB0_279-LJTI0_1
.set L0_1_set_280, LBB0_280-LJTI0_1
.set L0_1_set_281, LBB0_281-LJTI0_1
.set L0_1_set_282, LBB0_282-LJTI0_1
.set L0_1_set_283, LBB0_283-LJTI0_1
.set L0_1_set_284, LBB0_284-LJTI0_1
.set L0_1_set_285, LBB0_285-LJTI0_1
.set L0_1_set_286, LBB0_286-LJTI0_1
.set L0_1_set_287, LBB0_287-LJTI0_1
.set L0_1_set_288, LBB0_288-LJTI0_1
.set L0_1_set_289, LBB0_289-LJTI0_1
.set L0_1_set_290, LBB0_290-LJTI0_1
.set L0_1_set_291, LBB0_291-LJTI0_1
.set L0_1_set_292, LBB0_292-LJTI0_1
.set L0_1_set_293, LBB0_293-LJTI0_1
.set L0_1_set_294, LBB0_294-LJTI0_1
.set L0_1_set_295, LBB0_295-LJTI0_1
.set L0_1_set_296, LBB0_296-LJTI0_1
.set L0_1_set_297, LBB0_297-LJTI0_1
.set L0_1_set_298, LBB0_298-LJTI0_1
.set L0_1_set_299, LBB0_299-LJTI0_1
.set L0_1_set_300, LBB0_300-LJTI0_1
.set L0_1_set_301, LBB0_301-LJTI0_1
.set L0_1_set_302, LBB0_302-LJTI0_1
.set L0_1_set_304, LBB0_304-LJTI0_1
.set L0_1_set_306, LBB0_306-LJTI0_1
LJTI0_1:
	.long	L0_1_set_162
	.long	L0_1_set_239
	.long	L0_1_set_240
	.long	L0_1_set_241
	.long	L0_1_set_242
	.long	L0_1_set_243
	.long	L0_1_set_244
	.long	L0_1_set_245
	.long	L0_1_set_246
	.long	L0_1_set_247
	.long	L0_1_set_248
	.long	L0_1_set_249
	.long	L0_1_set_251
	.long	L0_1_set_252
	.long	L0_1_set_253
	.long	L0_1_set_254
	.long	L0_1_set_255
	.long	L0_1_set_256
	.long	L0_1_set_258
	.long	L0_1_set_260
	.long	L0_1_set_262
	.long	L0_1_set_263
	.long	L0_1_set_264
	.long	L0_1_set_265
	.long	L0_1_set_266
	.long	L0_1_set_267
	.long	L0_1_set_268
	.long	L0_1_set_269
	.long	L0_1_set_270
	.long	L0_1_set_271
	.long	L0_1_set_272
	.long	L0_1_set_273
	.long	L0_1_set_274
	.long	L0_1_set_275
	.long	L0_1_set_276
	.long	L0_1_set_277
	.long	L0_1_set_278
	.long	L0_1_set_279
	.long	L0_1_set_280
	.long	L0_1_set_281
	.long	L0_1_set_282
	.long	L0_1_set_283
	.long	L0_1_set_284
	.long	L0_1_set_285
	.long	L0_1_set_286
	.long	L0_1_set_287
	.long	L0_1_set_288
	.long	L0_1_set_289
	.long	L0_1_set_290
	.long	L0_1_set_291
	.long	L0_1_set_292
	.long	L0_1_set_293
	.long	L0_1_set_294
	.long	L0_1_set_295
	.long	L0_1_set_296
	.long	L0_1_set_297
	.long	L0_1_set_298
	.long	L0_1_set_299
	.long	L0_1_set_300
	.long	L0_1_set_301
	.long	L0_1_set_302
	.long	L0_1_set_304
	.long	L0_1_set_306
.set L0_2_set_164, LBB0_164-LJTI0_2
.set L0_2_set_182, LBB0_182-LJTI0_2
.set L0_2_set_183, LBB0_183-LJTI0_2
.set L0_2_set_184, LBB0_184-LJTI0_2
.set L0_2_set_185, LBB0_185-LJTI0_2
.set L0_2_set_187, LBB0_187-LJTI0_2
.set L0_2_set_188, LBB0_188-LJTI0_2
.set L0_2_set_189, LBB0_189-LJTI0_2
.set L0_2_set_190, LBB0_190-LJTI0_2
.set L0_2_set_191, LBB0_191-LJTI0_2
.set L0_2_set_192, LBB0_192-LJTI0_2
.set L0_2_set_193, LBB0_193-LJTI0_2
.set L0_2_set_194, LBB0_194-LJTI0_2
.set L0_2_set_195, LBB0_195-LJTI0_2
.set L0_2_set_196, LBB0_196-LJTI0_2
.set L0_2_set_198, LBB0_198-LJTI0_2
.set L0_2_set_199, LBB0_199-LJTI0_2
.set L0_2_set_200, LBB0_200-LJTI0_2
.set L0_2_set_201, LBB0_201-LJTI0_2
.set L0_2_set_202, LBB0_202-LJTI0_2
LJTI0_2:
	.long	L0_2_set_164
	.long	L0_2_set_182
	.long	L0_2_set_183
	.long	L0_2_set_184
	.long	L0_2_set_185
	.long	L0_2_set_187
	.long	L0_2_set_188
	.long	L0_2_set_189
	.long	L0_2_set_190
	.long	L0_2_set_191
	.long	L0_2_set_192
	.long	L0_2_set_193
	.long	L0_2_set_194
	.long	L0_2_set_195
	.long	L0_2_set_196
	.long	L0_2_set_198
	.long	L0_2_set_199
	.long	L0_2_set_200
	.long	L0_2_set_201
	.long	L0_2_set_202
.set L0_3_set_166, LBB0_166-LJTI0_3
.set L0_3_set_203, LBB0_203-LJTI0_3
.set L0_3_set_204, LBB0_204-LJTI0_3
.set L0_3_set_205, LBB0_205-LJTI0_3
.set L0_3_set_206, LBB0_206-LJTI0_3
.set L0_3_set_207, LBB0_207-LJTI0_3
.set L0_3_set_208, LBB0_208-LJTI0_3
.set L0_3_set_209, LBB0_209-LJTI0_3
.set L0_3_set_210, LBB0_210-LJTI0_3
.set L0_3_set_211, LBB0_211-LJTI0_3
.set L0_3_set_212, LBB0_212-LJTI0_3
.set L0_3_set_213, LBB0_213-LJTI0_3
.set L0_3_set_214, LBB0_214-LJTI0_3
.set L0_3_set_215, LBB0_215-LJTI0_3
.set L0_3_set_216, LBB0_216-LJTI0_3
.set L0_3_set_217, LBB0_217-LJTI0_3
.set L0_3_set_218, LBB0_218-LJTI0_3
.set L0_3_set_219, LBB0_219-LJTI0_3
.set L0_3_set_220, LBB0_220-LJTI0_3
.set L0_3_set_221, LBB0_221-LJTI0_3
.set L0_3_set_222, LBB0_222-LJTI0_3
.set L0_3_set_223, LBB0_223-LJTI0_3
.set L0_3_set_224, LBB0_224-LJTI0_3
.set L0_3_set_225, LBB0_225-LJTI0_3
.set L0_3_set_226, LBB0_226-LJTI0_3
.set L0_3_set_228, LBB0_228-LJTI0_3
.set L0_3_set_229, LBB0_229-LJTI0_3
.set L0_3_set_230, LBB0_230-LJTI0_3
.set L0_3_set_231, LBB0_231-LJTI0_3
.set L0_3_set_232, LBB0_232-LJTI0_3
.set L0_3_set_233, LBB0_233-LJTI0_3
.set L0_3_set_234, LBB0_234-LJTI0_3
.set L0_3_set_235, LBB0_235-LJTI0_3
.set L0_3_set_236, LBB0_236-LJTI0_3
.set L0_3_set_237, LBB0_237-LJTI0_3
LJTI0_3:
	.long	L0_3_set_166
	.long	L0_3_set_203
	.long	L0_3_set_204
	.long	L0_3_set_205
	.long	L0_3_set_206
	.long	L0_3_set_207
	.long	L0_3_set_208
	.long	L0_3_set_209
	.long	L0_3_set_210
	.long	L0_3_set_211
	.long	L0_3_set_212
	.long	L0_3_set_213
	.long	L0_3_set_214
	.long	L0_3_set_215
	.long	L0_3_set_216
	.long	L0_3_set_217
	.long	L0_3_set_218
	.long	L0_3_set_219
	.long	L0_3_set_220
	.long	L0_3_set_221
	.long	L0_3_set_222
	.long	L0_3_set_223
	.long	L0_3_set_224
	.long	L0_3_set_225
	.long	L0_3_set_226
	.long	L0_3_set_228
	.long	L0_3_set_229
	.long	L0_3_set_230
	.long	L0_3_set_231
	.long	L0_3_set_232
	.long	L0_3_set_233
	.long	L0_3_set_234
	.long	L0_3_set_235
	.long	L0_3_set_236
	.long	L0_3_set_237
	.end_data_region
                                        ## -- End function
	.section	__TEXT,__literal4,4byte_literals
	.p2align	2                               ## -- Begin function cost_model.par_for.prediction_output.s0.n.v7
LCPI1_0:
	.long	0x3f800000                      ## float 1
LCPI1_1:
	.long	2155872255                      ## 0x807fffff
LCPI1_2:
	.long	0xbf800000                      ## float -1
LCPI1_3:
	.long	4294967169                      ## 0xffffff81
LCPI1_4:
	.long	0x3f317218                      ## float 0.693147182
LCPI1_5:
	.long	0x3d9c7946                      ## float 0.0764031857
LCPI1_6:
	.long	0x3e5333c6                      ## float 0.206252187
LCPI1_7:
	.long	0x3eaa99cd                      ## float 0.333204657
LCPI1_8:
	.long	0xbe266e2a                      ## float -0.162529618
LCPI1_9:
	.long	0xbe809085                      ## float -0.251102597
LCPI1_10:
	.long	0xbefffcbe                      ## float -0.499975145
LCPI1_16:
	.long	0x45800000                      ## float 4096
LCPI1_17:
	.long	0x40000000                      ## float 2
LCPI1_18:
	.long	0x3089705f                      ## float 9.99999971E-10
	.section	__TEXT,__const
	.p2align	5
LCPI1_11:
	.quad	4                               ## 0x4
	.quad	5                               ## 0x5
	.quad	6                               ## 0x6
	.quad	7                               ## 0x7
LCPI1_12:
	.quad	0                               ## 0x0
	.quad	1                               ## 0x1
	.quad	2                               ## 0x2
	.quad	3                               ## 0x3
	.section	__TEXT,__literal8,8byte_literals
	.p2align	3
LCPI1_13:
	.quad	8                               ## 0x8
LCPI1_14:
	.quad	16                              ## 0x10
LCPI1_15:
	.quad	24                              ## 0x18
LCPI1_19:
	.quad	32                              ## 0x20
	.section	__TEXT,__text,regular,pure_instructions
	.p2align	4, 0x90
_cost_model.par_for.prediction_output.s0.n.v7: ## @cost_model.par_for.prediction_output.s0.n.v7
	.cfi_startproc
## %bb.0:                               ## %entry
	pushq	%rbp
	.cfi_def_cfa_offset 16
	pushq	%r15
	.cfi_def_cfa_offset 24
	pushq	%r14
	.cfi_def_cfa_offset 32
	pushq	%r13
	.cfi_def_cfa_offset 40
	pushq	%r12
	.cfi_def_cfa_offset 48
	pushq	%rbx
	.cfi_def_cfa_offset 56
	subq	$1960, %rsp                     ## imm = 0x7A8
	.cfi_def_cfa_offset 2016
	.cfi_offset %rbx, -56
	.cfi_offset %r12, -48
	.cfi_offset %r13, -40
	.cfi_offset %r14, -32
	.cfi_offset %r15, -24
	.cfi_offset %rbp, -16
	movl	(%rdx), %ecx
	movl	24(%rdx), %eax
	shll	$3, %esi
	addl	$-8, %ecx
	cmpl	%esi, %ecx
	cmovgl	%esi, %ecx
	movq	%rcx, (%rsp)                    ## 8-byte Spill
	cmpl	$-4, %eax
	movl	$-4, %r15d
	cmovgl	%eax, %r15d
	movq	%rdi, %rbp
	addl	$4, %r15d
	imulq	$1248, %r15, %rax               ## imm = 0x4E0
	movq	%rax, %rcx
	andq	$-2147483648, %rcx              ## imm = 0x80000000
	jne	LBB1_52
## %bb.1:                               ## %"assert succeeded"
	movslq	4(%rdx), %r12
	movslq	8(%rdx), %rcx
	movq	%rcx, 456(%rsp)                 ## 8-byte Spill
	movl	12(%rdx), %ebx
	movslq	16(%rdx), %r14
	movslq	20(%rdx), %rcx
	movq	%rcx, 416(%rsp)                 ## 8-byte Spill
	movl	28(%rdx), %ecx
	movl	%ecx, 304(%rsp)                 ## 4-byte Spill
	movl	32(%rdx), %ecx
	movq	%rcx, 504(%rsp)                 ## 8-byte Spill
	vmovss	36(%rdx), %xmm0                 ## xmm0 = mem[0],zero,zero,zero
	vmovaps	%xmm0, 912(%rsp)                ## 16-byte Spill
	movq	40(%rdx), %rcx
	movq	%rcx, 760(%rsp)                 ## 8-byte Spill
	movq	56(%rdx), %rcx
	movq	%rcx, 632(%rsp)                 ## 8-byte Spill
	movq	72(%rdx), %rcx
	movq	%rcx, 688(%rsp)                 ## 8-byte Spill
	movq	88(%rdx), %rcx
	movq	%rcx, 624(%rsp)                 ## 8-byte Spill
	movq	104(%rdx), %rcx
	movq	%rcx, 680(%rsp)                 ## 8-byte Spill
	movq	120(%rdx), %rcx
	movq	%rcx, 464(%rsp)                 ## 8-byte Spill
	orq	$12, %rax
	movq	%rbp, 176(%rsp)                 ## 8-byte Spill
	movq	%rbp, %rdi
	movq	%rax, %rsi
	callq	_halide_malloc
	testq	%rax, %rax
	je	LBB1_53
## %bb.2:                               ## %"assert succeeded2"
	movq	%rax, %r13
	movl	$2147483648, %r8d               ## imm = 0x80000000
	testl	%ebx, %ebx
	movq	416(%rsp), %r9                  ## 8-byte Reload
	jle	LBB1_7
## %bb.3:                               ## %"for normalized_schedule_features.s0.s.preheader"
	movq	(%rsp), %rax                    ## 8-byte Reload
	movl	%eax, %ecx
	subl	304(%rsp), %ecx                 ## 4-byte Folded Reload
	leaq	(,%r14,4), %rdx
	xorl	%esi, %esi
	vbroadcastss	LCPI1_0(%rip), %ymm0    ## ymm0 = [1.0E+0,1.0E+0,1.0E+0,1.0E+0,1.0E+0,1.0E+0,1.0E+0,1.0E+0]
	vbroadcastss	LCPI1_1(%rip), %ymm1    ## ymm1 = [2155872255,2155872255,2155872255,2155872255,2155872255,2155872255,2155872255,2155872255]
	vpbroadcastd	LCPI1_0(%rip), %ymm2    ## ymm2 = [1.0E+0,1.0E+0,1.0E+0,1.0E+0,1.0E+0,1.0E+0,1.0E+0,1.0E+0]
	vbroadcastss	LCPI1_2(%rip), %ymm3    ## ymm3 = [-1.0E+0,-1.0E+0,-1.0E+0,-1.0E+0,-1.0E+0,-1.0E+0,-1.0E+0,-1.0E+0]
	vpbroadcastd	LCPI1_3(%rip), %ymm4    ## ymm4 = [4294967169,4294967169,4294967169,4294967169,4294967169,4294967169,4294967169,4294967169]
	vbroadcastss	LCPI1_4(%rip), %ymm5    ## ymm5 = [6.93147182E-1,6.93147182E-1,6.93147182E-1,6.93147182E-1,6.93147182E-1,6.93147182E-1,6.93147182E-1,6.93147182E-1]
	vbroadcastss	LCPI1_5(%rip), %ymm6    ## ymm6 = [7.64031857E-2,7.64031857E-2,7.64031857E-2,7.64031857E-2,7.64031857E-2,7.64031857E-2,7.64031857E-2,7.64031857E-2]
	vbroadcastss	LCPI1_6(%rip), %ymm7    ## ymm7 = [2.06252187E-1,2.06252187E-1,2.06252187E-1,2.06252187E-1,2.06252187E-1,2.06252187E-1,2.06252187E-1,2.06252187E-1]
	vbroadcastss	LCPI1_7(%rip), %ymm8    ## ymm8 = [3.33204657E-1,3.33204657E-1,3.33204657E-1,3.33204657E-1,3.33204657E-1,3.33204657E-1,3.33204657E-1,3.33204657E-1]
	vbroadcastss	LCPI1_8(%rip), %ymm9    ## ymm9 = [-1.62529618E-1,-1.62529618E-1,-1.62529618E-1,-1.62529618E-1,-1.62529618E-1,-1.62529618E-1,-1.62529618E-1,-1.62529618E-1]
	vbroadcastss	LCPI1_9(%rip), %ymm10   ## ymm10 = [-2.51102597E-1,-2.51102597E-1,-2.51102597E-1,-2.51102597E-1,-2.51102597E-1,-2.51102597E-1,-2.51102597E-1,-2.51102597E-1]
	vbroadcastss	LCPI1_10(%rip), %ymm11  ## ymm11 = [-4.99975145E-1,-4.99975145E-1,-4.99975145E-1,-4.99975145E-1,-4.99975145E-1,-4.99975145E-1,-4.99975145E-1,-4.99975145E-1]
	movq	%r13, %rdi
	.p2align	4, 0x90
LBB1_4:                                 ## %"for normalized_schedule_features.s0.s"
                                        ## =>This Loop Header: Depth=1
                                        ##     Child Loop BB1_5 Depth 2
	movslq	%ecx, %rax
	movq	464(%rsp), %rbp                 ## 8-byte Reload
	leaq	(,%rax,4), %rax
	addq	%rbp, %rax
	xorl	%ebp, %ebp
	.p2align	4, 0x90
LBB1_5:                                 ## %"for normalized_schedule_features.s0.c"
                                        ##   Parent Loop BB1_4 Depth=1
                                        ## =>  This Inner Loop Header: Depth=2
	vaddps	(%rax), %ymm0, %ymm12
	vandps	%ymm1, %ymm12, %ymm13
	vpsrad	$22, %ymm13, %ymm14
	vpslld	$23, %ymm14, %ymm15
	vpsubd	%ymm15, %ymm13, %ymm13
	vpaddd	%ymm2, %ymm13, %ymm13
	vaddps	%ymm3, %ymm13, %ymm13
	vmulps	%ymm13, %ymm13, %ymm15
	vpsrad	$23, %ymm12, %ymm12
	vpaddd	%ymm4, %ymm12, %ymm12
	vpaddd	%ymm12, %ymm14, %ymm12
	vmovaps	%ymm6, %ymm14
	vfmadd213ps	%ymm7, %ymm15, %ymm14   ## ymm14 = (ymm15 * ymm14) + ymm7
	vfmadd213ps	%ymm8, %ymm15, %ymm14   ## ymm14 = (ymm15 * ymm14) + ymm8
	vfmadd213ps	%ymm0, %ymm15, %ymm14   ## ymm14 = (ymm15 * ymm14) + ymm0
	vmulps	%ymm14, %ymm13, %ymm13
	vmovaps	%ymm9, %ymm14
	vfmadd213ps	%ymm10, %ymm15, %ymm14  ## ymm14 = (ymm15 * ymm14) + ymm10
	vfmadd213ps	%ymm11, %ymm15, %ymm14  ## ymm14 = (ymm15 * ymm14) + ymm11
	vcvtdq2ps	%ymm12, %ymm12
	vfmadd213ps	%ymm13, %ymm15, %ymm14  ## ymm14 = (ymm15 * ymm14) + ymm13
	vfmadd231ps	%ymm12, %ymm5, %ymm14   ## ymm14 = (ymm5 * ymm12) + ymm14
	vmovaps	%ymm14, (%rdi,%rbp)
	addq	$32, %rbp
	addq	%rdx, %rax
	cmpq	$1248, %rbp                     ## imm = 0x4E0
	jne	LBB1_5
## %bb.6:                               ## %"end for normalized_schedule_features.s0.c"
                                        ##   in Loop: Header=BB1_4 Depth=1
	incq	%rsi
	addq	$1248, %rdi                     ## imm = 0x4E0
	addl	%r9d, %ecx
	cmpq	%rbx, %rsi
	jne	LBB1_4
LBB1_7:                                 ## %"end for normalized_schedule_features.s0.s"
	movq	%rbx, 312(%rsp)                 ## 8-byte Spill
	movq	%r15, %rax
	shlq	$5, %rax
	leaq	(%rax,%rax,2), %rdx
	cmpq	%r8, %rdx
	jae	LBB1_55
## %bb.8:                               ## %"assert succeeded4"
	orq	$12, %rdx
	movq	176(%rsp), %rbp                 ## 8-byte Reload
	movq	%rbp, %rdi
	movq	%rdx, %rsi
	vzeroupper
	callq	_halide_malloc
	movq	%rax, 424(%rsp)                 ## 8-byte Spill
	testq	%rax, %rax
	je	LBB1_56
## %bb.9:                               ## %"assert succeeded6"
	movq	%r13, 408(%rsp)                 ## 8-byte Spill
	movl	%r15d, %eax
	shlq	$7, %r15
	cmpl	$16777216, %eax                 ## imm = 0x1000000
	jae	LBB1_58
## %bb.10:                              ## %"assert succeeded8"
	orq	$12, %r15
	movq	%rbp, %rdi
	movq	%r15, %rsi
	callq	_halide_malloc
	testq	%rax, %rax
	je	LBB1_59
## %bb.11:                              ## %"assert succeeded10"
	movq	%rax, %r13
	cmpl	$31, 312(%rsp)                  ## 4-byte Folded Reload
	seta	96(%rsp)                        ## 1-byte Folded Spill
	cmpl	$1, 416(%rsp)                   ## 4-byte Folded Reload
	sete	%bl
	leal	(%r14,%r14,8), %r9d
	movq	(%rsp), %rdi                    ## 8-byte Reload
	leal	(%rdi,%r9,2), %ecx
	movl	304(%rsp), %r11d                ## 4-byte Reload
	subl	%r11d, %ecx
	movl	%ecx, 40(%rsp)                  ## 4-byte Spill
	leal	(%r14,%r9,2), %ecx
	addl	%edi, %ecx
	subl	%r11d, %ecx
	movl	%ecx, 32(%rsp)                  ## 4-byte Spill
	leal	(%r14,%r14,4), %r8d
	leal	(%r8,%r8,4), %r10d
	leal	(%r10,%r14), %edx
	addl	%edi, %edx
	subl	%r11d, %edx
	movl	%edx, 24(%rsp)                  ## 4-byte Spill
	leal	(%r9,%r9,2), %esi
	leal	(%rsi,%r14), %edx
	leal	(%rdi,%rdx), %eax
	subl	%r11d, %eax
	movl	%eax, 64(%rsp)                  ## 4-byte Spill
	addl	%edi, %esi
	subl	%r11d, %esi
	movq	%rsi, 16(%rsp)                  ## 8-byte Spill
	movl	%r14d, %esi
	shll	$5, %esi
	leal	(%r14,%rsi), %eax
	addl	%edi, %eax
	subl	%r11d, %eax
	movl	%eax, 56(%rsp)                  ## 4-byte Spill
	leal	(%rsi,%r14,2), %eax
	addl	%edi, %eax
	subl	%r11d, %eax
	movl	%eax, 88(%rsp)                  ## 4-byte Spill
	leal	(%r14,%r8,2), %eax
	addl	%edi, %eax
	subl	%r11d, %eax
	movl	%eax, 80(%rsp)                  ## 4-byte Spill
	addl	%r14d, %edx
	addl	%edi, %edx
	subl	%r11d, %edx
	movq	%rdx, 144(%rsp)                 ## 8-byte Spill
	leal	(%r14,%r14,2), %r15d
	leal	(%rdi,%r15,4), %eax
	subl	%r11d, %eax
	movl	%eax, 72(%rsp)                  ## 4-byte Spill
	movl	%r14d, %eax
	shll	$4, %eax
	addl	%r14d, %eax
	addl	%edi, %eax
	subl	%r11d, %eax
	movl	%eax, 512(%rsp)                 ## 4-byte Spill
	leal	(%r14,%r8,4), %eax
	leal	(%rdi,%rax), %edx
	subl	%r11d, %edx
	movl	%edx, 120(%rsp)                 ## 4-byte Spill
	leal	(%rdi,%rsi), %edx
	subl	%r14d, %esi
	leal	(%rdi,%rsi), %ecx
	subl	%r14d, %esi
	addl	%r14d, %eax
	addl	%edi, %r14d
	subl	%r11d, %r14d
	subl	%r11d, %ecx
	movl	%ecx, 104(%rsp)                 ## 4-byte Spill
	subl	%r11d, %edx
	movl	%edx, 112(%rsp)                 ## 4-byte Spill
	addl	%edi, %esi
	subl	%r11d, %esi
	movq	%rsi, 8(%rsp)                   ## 8-byte Spill
	leal	(%rdi,%r8,2), %r8d
	subl	%r11d, %r8d
	addl	%edi, %r9d
	subl	%r11d, %r9d
	addl	%edi, %r10d
	subl	%r11d, %r10d
	addl	%edi, %eax
	subl	%r11d, %eax
	movq	%rax, 544(%rsp)                 ## 8-byte Spill
	leal	(%rdi,%r15,8), %ebp
	subl	%r11d, %ebp
	movslq	%edi, %rcx
	movq	%rcx, 600(%rsp)                 ## 8-byte Spill
	subl	%r11d, %edi
	movq	%rdi, (%rsp)                    ## 8-byte Spill
	andb	96(%rsp), %bl                   ## 1-byte Folded Reload
	movb	%bl, 55(%rsp)                   ## 1-byte Spill
	movq	312(%rsp), %rsi                 ## 8-byte Reload
                                        ## kill: def $esi killed $esi killed $rsi def $rsi
	andl	$-32, %esi
	movq	%rsi, 592(%rsp)                 ## 8-byte Spill
	vmovaps	912(%rsp), %xmm13               ## 16-byte Reload
	vbroadcastss	%xmm13, %ymm14
	movq	408(%rsp), %rax                 ## 8-byte Reload
	addq	$3776, %rax                     ## imm = 0xEC0
	movq	%rax, 616(%rsp)                 ## 8-byte Spill
	leaq	(,%r12,4), %rax
	leaq	(%rax,%rax,8), %r11
	movq	424(%rsp), %r15                 ## 8-byte Reload
	movq	%r15, %rax
	addq	$292, %rax                      ## imm = 0x124
	movq	%rax, 672(%rsp)                 ## 8-byte Spill
	leaq	(,%r12,8), %rbx
	shlq	$5, %r12
	movq	416(%rsp), %rdx                 ## 8-byte Reload
	movq	%rdx, %rax
	shlq	$7, %rax
	movq	%rax, 696(%rsp)                 ## 8-byte Spill
	movq	%r13, %rax
	addq	$108, %rax
	movq	%rax, 664(%rsp)                 ## 8-byte Spill
	vxorps	%xmm12, %xmm12, %xmm12
	vmovss	LCPI1_0(%rip), %xmm15           ## xmm15 = mem[0],zero,zero,zero
                                        ## implicit-def: $ymm9
	movq	456(%rsp), %rdi                 ## 8-byte Reload
	leaq	(,%rdi,8), %rax
	movq	%rax, 304(%rsp)                 ## 8-byte Spill
	movq	624(%rsp), %rax                 ## 8-byte Reload
	leaq	32(%rax), %rcx
	movq	%rcx, 752(%rsp)                 ## 8-byte Spill
	leaq	64(%rax), %rax
	movq	%rax, 744(%rsp)                 ## 8-byte Spill
	movq	632(%rsp), %rax                 ## 8-byte Reload
	leaq	32(%rax), %rcx
	movq	%rcx, 736(%rsp)                 ## 8-byte Spill
	leaq	64(%rax), %rcx
	movq	%rcx, 728(%rsp)                 ## 8-byte Spill
	addq	$96, %rax
	movq	%rax, 720(%rsp)                 ## 8-byte Spill
	leaq	(,%rdx,4), %rax
	movq	%rax, 712(%rsp)                 ## 8-byte Spill
	xorl	%edx, %edx
	vmovups	%ymm14, 1472(%rsp)              ## 32-byte Spill
	movq	%r12, 656(%rsp)                 ## 8-byte Spill
	movq	%r11, 648(%rsp)                 ## 8-byte Spill
	movq	%rbx, 640(%rsp)                 ## 8-byte Spill
	.p2align	4, 0x90
LBB1_12:                                ## %"for prediction_output.s0.n.n"
                                        ## =>This Loop Header: Depth=1
                                        ##     Child Loop BB1_15 Depth 2
                                        ##       Child Loop BB1_16 Depth 3
                                        ##       Child Loop BB1_19 Depth 3
                                        ##       Child Loop BB1_22 Depth 3
                                        ##     Child Loop BB1_25 Depth 2
                                        ##       Child Loop BB1_26 Depth 3
                                        ##       Child Loop BB1_28 Depth 3
                                        ##       Child Loop BB1_30 Depth 3
                                        ##       Child Loop BB1_32 Depth 3
                                        ##     Child Loop BB1_37 Depth 2
                                        ##     Child Loop BB1_44 Depth 2
	movl	%r14d, 160(%rsp)                ## 4-byte Spill
	movl	%ebp, 168(%rsp)                 ## 4-byte Spill
	movl	%r8d, 128(%rsp)                 ## 4-byte Spill
	movq	%r10, 136(%rsp)                 ## 8-byte Spill
	movq	%r9, 96(%rsp)                   ## 8-byte Spill
	movq	%rdx, 608(%rsp)                 ## 8-byte Spill
	cmpl	$0, 504(%rsp)                   ## 4-byte Folded Reload
	jle	LBB1_34
## %bb.13:                              ## %"for head2_relu.s0.w.w.preheader"
                                        ##   in Loop: Header=BB1_12 Depth=1
	movq	688(%rsp), %rax                 ## 8-byte Reload
	vmovups	(%rax), %ymm0
	vmovups	32(%rax), %ymm1
	vmovups	64(%rax), %ymm2
	movq	616(%rsp), %rdx                 ## 8-byte Reload
	xorl	%r8d, %r8d
	jmp	LBB1_15
	.p2align	4, 0x90
LBB1_14:                                ## %"for head2_relu.s0.w.v9.preheader.2"
                                        ##   in Loop: Header=BB1_15 Depth=2
	vmaxps	%ymm12, %ymm10, %ymm3
	orq	$64, %rax
	vmovaps	%ymm3, (%r15,%rax)
	vmaxps	%ymm12, %ymm9, %ymm3
	vmovaps	%ymm3, 64(%r9,%r15)
	vmaxps	%ymm12, %ymm8, %ymm3
	vmovaps	%ymm3, 64(%r14,%r15)
	vmaxps	%ymm12, %ymm7, %ymm3
	shlq	$5, %rcx
	leaq	(%rcx,%rcx,2), %rax
	orq	$64, %rax
	vmovaps	%ymm3, (%r15,%rax)
	incq	%r8
	addq	$4992, %rdx                     ## imm = 0x1380
	cmpq	504(%rsp), %r8                  ## 8-byte Folded Reload
	je	LBB1_24
LBB1_15:                                ## %"for head2_relu.s0.w.w"
                                        ##   Parent Loop BB1_12 Depth=1
                                        ## =>  This Loop Header: Depth=2
                                        ##       Child Loop BB1_16 Depth 3
                                        ##       Child Loop BB1_19 Depth 3
                                        ##       Child Loop BB1_22 Depth 3
	leal	(,%r8,4), %ebp
	movq	$-1216, %rcx                    ## imm = 0xFB40
	movq	624(%rsp), %rsi                 ## 8-byte Reload
	vmovaps	%ymm0, %ymm4
	vmovaps	%ymm0, %ymm5
	vmovaps	%ymm0, %ymm6
	vmovaps	%ymm0, %ymm7
	.p2align	4, 0x90
LBB1_16:                                ## %"for head2_conv.s1.r40$x"
                                        ##   Parent Loop BB1_12 Depth=1
                                        ##     Parent Loop BB1_15 Depth=2
                                        ## =>    This Inner Loop Header: Depth=3
	vmovups	(%rsi), %ymm11
	vbroadcastss	-2560(%rdx,%rcx), %ymm10
	vfmadd213ps	%ymm7, %ymm11, %ymm10   ## ymm10 = (ymm11 * ymm10) + ymm7
	vbroadcastss	-1312(%rdx,%rcx), %ymm9
	vfmadd213ps	%ymm6, %ymm11, %ymm9    ## ymm9 = (ymm11 * ymm9) + ymm6
	vbroadcastss	-64(%rdx,%rcx), %ymm8
	vfmadd213ps	%ymm5, %ymm11, %ymm8    ## ymm8 = (ymm11 * ymm8) + ymm5
	vbroadcastss	1184(%rdx,%rcx), %ymm3
	vfmadd213ps	%ymm4, %ymm11, %ymm3    ## ymm3 = (ymm11 * ymm3) + ymm4
	testq	%rcx, %rcx
	je	LBB1_18
## %bb.17:                              ## %"for head2_conv.s1.r40$x.1363"
                                        ##   in Loop: Header=BB1_16 Depth=3
	vmovups	(%rsi,%rdi,4), %ymm11
	vbroadcastss	-2528(%rdx,%rcx), %ymm7
	vfmadd213ps	%ymm10, %ymm11, %ymm7   ## ymm7 = (ymm11 * ymm7) + ymm10
	vbroadcastss	-1280(%rdx,%rcx), %ymm6
	vfmadd213ps	%ymm9, %ymm11, %ymm6    ## ymm6 = (ymm11 * ymm6) + ymm9
	vbroadcastss	-32(%rdx,%rcx), %ymm5
	vfmadd213ps	%ymm8, %ymm11, %ymm5    ## ymm5 = (ymm11 * ymm5) + ymm8
	vbroadcastss	1216(%rdx,%rcx), %ymm4
	vfmadd213ps	%ymm3, %ymm11, %ymm4    ## ymm4 = (ymm11 * ymm4) + ymm3
	addq	$64, %rcx
	addq	304(%rsp), %rsi                 ## 8-byte Folded Reload
	jmp	LBB1_16
	.p2align	4, 0x90
LBB1_18:                                ## %"for head2_relu.s0.w.v9.preheader"
                                        ##   in Loop: Header=BB1_15 Depth=2
	vmaxps	%ymm12, %ymm10, %ymm4
	leaq	(,%rbp,2), %rcx
	addq	%rbp, %rcx
	shlq	$5, %rcx
	vmovaps	%ymm4, (%r15,%rcx)
	vmaxps	%ymm12, %ymm9, %ymm4
	movq	%rbp, %rcx
	orq	$1, %rcx
	leaq	(%rcx,%rcx,2), %r9
	shlq	$5, %r9
	vmovaps	%ymm4, (%r15,%r9)
	vmaxps	%ymm12, %ymm8, %ymm4
	movq	%rbp, %rsi
	orq	$2, %rsi
	leaq	(%rsi,%rsi,2), %r14
	shlq	$5, %r14
	vmovaps	%ymm4, (%r15,%r14)
	vmaxps	%ymm12, %ymm3, %ymm3
	movq	%rbp, %rcx
	orq	$3, %rcx
	leaq	(%rcx,%rcx,2), %r10
	shlq	$5, %r10
	vmovaps	%ymm3, (%r15,%r10)
	movq	$-1216, %rdi                    ## imm = 0xFB40
	movq	752(%rsp), %r15                 ## 8-byte Reload
	vmovaps	%ymm1, %ymm6
	vmovaps	%ymm1, %ymm5
	vmovaps	%ymm1, %ymm4
	vmovaps	%ymm1, %ymm3
	.p2align	4, 0x90
LBB1_19:                                ## %"for head2_conv.s1.r40$x.1"
                                        ##   Parent Loop BB1_12 Depth=1
                                        ##     Parent Loop BB1_15 Depth=2
                                        ## =>    This Inner Loop Header: Depth=3
	vmovups	(%r15), %ymm11
	vbroadcastss	-2560(%rdx,%rdi), %ymm10
	vfmadd213ps	%ymm3, %ymm11, %ymm10   ## ymm10 = (ymm11 * ymm10) + ymm3
	vbroadcastss	-1312(%rdx,%rdi), %ymm9
	vfmadd213ps	%ymm4, %ymm11, %ymm9    ## ymm9 = (ymm11 * ymm9) + ymm4
	vbroadcastss	-64(%rdx,%rdi), %ymm8
	vfmadd213ps	%ymm5, %ymm11, %ymm8    ## ymm8 = (ymm11 * ymm8) + ymm5
	vbroadcastss	1184(%rdx,%rdi), %ymm7
	vfmadd213ps	%ymm6, %ymm11, %ymm7    ## ymm7 = (ymm11 * ymm7) + ymm6
	testq	%rdi, %rdi
	je	LBB1_21
## %bb.20:                              ## %"for head2_conv.s1.r40$x.1.1"
                                        ##   in Loop: Header=BB1_19 Depth=3
	movq	456(%rsp), %rax                 ## 8-byte Reload
	vmovups	(%r15,%rax,4), %ymm11
	vbroadcastss	-2528(%rdx,%rdi), %ymm3
	vfmadd213ps	%ymm10, %ymm11, %ymm3   ## ymm3 = (ymm11 * ymm3) + ymm10
	vbroadcastss	-1280(%rdx,%rdi), %ymm4
	vfmadd213ps	%ymm9, %ymm11, %ymm4    ## ymm4 = (ymm11 * ymm4) + ymm9
	vbroadcastss	-32(%rdx,%rdi), %ymm5
	vfmadd213ps	%ymm8, %ymm11, %ymm5    ## ymm5 = (ymm11 * ymm5) + ymm8
	vbroadcastss	1216(%rdx,%rdi), %ymm6
	vfmadd213ps	%ymm7, %ymm11, %ymm6    ## ymm6 = (ymm11 * ymm6) + ymm7
	addq	$64, %rdi
	addq	304(%rsp), %r15                 ## 8-byte Folded Reload
	jmp	LBB1_19
	.p2align	4, 0x90
LBB1_21:                                ## %"for head2_relu.s0.w.v9.preheader.1"
                                        ##   in Loop: Header=BB1_15 Depth=2
	vmaxps	%ymm12, %ymm10, %ymm3
	shlq	$5, %rbp
	leaq	(,%rbp,2), %rax
	addq	%rbp, %rax
	movq	%rax, %rdi
	orq	$32, %rdi
	movq	424(%rsp), %r15                 ## 8-byte Reload
	vmovaps	%ymm3, (%r15,%rdi)
	vmaxps	%ymm12, %ymm9, %ymm3
	vmovaps	%ymm3, 32(%r9,%r15)
	vmaxps	%ymm12, %ymm8, %ymm3
	shlq	$5, %rsi
	leaq	(%rsi,%rsi,2), %rsi
	orq	$32, %rsi
	vmovaps	%ymm3, (%r15,%rsi)
	vmaxps	%ymm12, %ymm7, %ymm3
	vmovaps	%ymm3, 32(%r10,%r15)
	movq	$-1216, %rsi                    ## imm = 0xFB40
	movq	744(%rsp), %rbp                 ## 8-byte Reload
	vmovaps	%ymm2, %ymm6
	vmovaps	%ymm2, %ymm5
	vmovaps	%ymm2, %ymm4
	vmovaps	%ymm2, %ymm3
	movq	456(%rsp), %rdi                 ## 8-byte Reload
	movq	760(%rsp), %r10                 ## 8-byte Reload
	.p2align	4, 0x90
LBB1_22:                                ## %"for head2_conv.s1.r40$x.2"
                                        ##   Parent Loop BB1_12 Depth=1
                                        ##     Parent Loop BB1_15 Depth=2
                                        ## =>    This Inner Loop Header: Depth=3
	vmovups	(%rbp), %ymm11
	vbroadcastss	-2560(%rdx,%rsi), %ymm10
	vfmadd213ps	%ymm3, %ymm11, %ymm10   ## ymm10 = (ymm11 * ymm10) + ymm3
	vbroadcastss	-1312(%rdx,%rsi), %ymm9
	vfmadd213ps	%ymm4, %ymm11, %ymm9    ## ymm9 = (ymm11 * ymm9) + ymm4
	vbroadcastss	-64(%rdx,%rsi), %ymm8
	vfmadd213ps	%ymm5, %ymm11, %ymm8    ## ymm8 = (ymm11 * ymm8) + ymm5
	vbroadcastss	1184(%rdx,%rsi), %ymm7
	vfmadd213ps	%ymm6, %ymm11, %ymm7    ## ymm7 = (ymm11 * ymm7) + ymm6
	testq	%rsi, %rsi
	je	LBB1_14
## %bb.23:                              ## %"for head2_conv.s1.r40$x.2.1"
                                        ##   in Loop: Header=BB1_22 Depth=3
	vmovups	(%rbp,%rdi,4), %ymm11
	vbroadcastss	-2528(%rdx,%rsi), %ymm3
	vfmadd213ps	%ymm10, %ymm11, %ymm3   ## ymm3 = (ymm11 * ymm3) + ymm10
	vbroadcastss	-1280(%rdx,%rsi), %ymm4
	vfmadd213ps	%ymm9, %ymm11, %ymm4    ## ymm4 = (ymm11 * ymm4) + ymm9
	vbroadcastss	-32(%rdx,%rsi), %ymm5
	vfmadd213ps	%ymm8, %ymm11, %ymm5    ## ymm5 = (ymm11 * ymm5) + ymm8
	vbroadcastss	1216(%rdx,%rsi), %ymm6
	vfmadd213ps	%ymm7, %ymm11, %ymm6    ## ymm6 = (ymm11 * ymm6) + ymm7
	addq	$64, %rsi
	addq	304(%rsp), %rbp                 ## 8-byte Folded Reload
	jmp	LBB1_22
	.p2align	4, 0x90
LBB1_24:                                ## %"for relu1.s0.w.w.preheader"
                                        ##   in Loop: Header=BB1_12 Depth=1
	movq	672(%rsp), %rsi                 ## 8-byte Reload
	xorl	%r14d, %r14d
	.p2align	4, 0x90
LBB1_25:                                ## %"for relu1.s0.w.w"
                                        ##   Parent Loop BB1_12 Depth=1
                                        ## =>  This Loop Header: Depth=2
                                        ##       Child Loop BB1_26 Depth 3
                                        ##       Child Loop BB1_28 Depth 3
                                        ##       Child Loop BB1_30 Depth 3
                                        ##       Child Loop BB1_32 Depth 3
	movq	%r14, %rdx
	shlq	$7, %rdx
	leal	(,%r14,4), %r8d
	movq	%r14, %rcx
	shlq	$9, %rcx
	vmovaps	(%r10,%rcx), %ymm3
	vmovaps	128(%r10,%rcx), %ymm2
	vmovaps	256(%r10,%rcx), %ymm1
	vmovaps	384(%r10,%rcx), %ymm0
	movq	$-96, %rbp
	movq	632(%rsp), %rdi                 ## 8-byte Reload
	.p2align	4, 0x90
LBB1_26:                                ## %"for conv1_stage2.s1.r63$x"
                                        ##   Parent Loop BB1_12 Depth=1
                                        ##     Parent Loop BB1_25 Depth=2
                                        ## =>    This Inner Loop Header: Depth=3
	vmovups	(%rdi,%r12), %ymm4
	vbroadcastss	-196(%rsi,%rbp), %ymm5
	vfmadd213ps	%ymm3, %ymm4, %ymm5     ## ymm5 = (ymm4 * ymm5) + ymm3
	vbroadcastss	-100(%rsi,%rbp), %ymm6
	vfmadd213ps	%ymm2, %ymm4, %ymm6     ## ymm6 = (ymm4 * ymm6) + ymm2
	vbroadcastss	-4(%rsi,%rbp), %ymm7
	vfmadd213ps	%ymm1, %ymm4, %ymm7     ## ymm7 = (ymm4 * ymm7) + ymm1
	vbroadcastss	92(%rsi,%rbp), %ymm8
	vfmadd213ps	%ymm0, %ymm4, %ymm8     ## ymm8 = (ymm4 * ymm8) + ymm0
	vmovups	(%rdi,%r11), %ymm4
	vbroadcastss	-192(%rsi,%rbp), %ymm3
	vfmadd213ps	%ymm5, %ymm4, %ymm3     ## ymm3 = (ymm4 * ymm3) + ymm5
	vbroadcastss	-96(%rsi,%rbp), %ymm2
	vfmadd213ps	%ymm6, %ymm4, %ymm2     ## ymm2 = (ymm4 * ymm2) + ymm6
	vbroadcastss	(%rsi,%rbp), %ymm1
	vfmadd213ps	%ymm7, %ymm4, %ymm1     ## ymm1 = (ymm4 * ymm1) + ymm7
	vbroadcastss	96(%rsi,%rbp), %ymm0
	vfmadd213ps	%ymm8, %ymm4, %ymm0     ## ymm0 = (ymm4 * ymm0) + ymm8
	addq	%rbx, %rdi
	addq	$8, %rbp
	jne	LBB1_26
## %bb.27:                              ## %"for relu1.s0.w.v12.preheader"
                                        ##   in Loop: Header=BB1_25 Depth=2
	vmaxps	%ymm12, %ymm3, %ymm3
	movq	%r8, %r9
	shlq	$5, %r9
	shlq	$7, %r8
	vmovaps	%ymm3, (%r13,%r8)
	vmaxps	%ymm12, %ymm2, %ymm2
	movq	%r8, %rdi
	orq	$128, %rdi
	vmovaps	%ymm2, (%r13,%rdi)
	vmaxps	%ymm12, %ymm1, %ymm1
	movq	%r8, %rdi
	orq	$256, %rdi                      ## imm = 0x100
	vmovaps	%ymm1, (%r13,%rdi)
	vmaxps	%ymm12, %ymm0, %ymm0
	movq	%r8, %rdi
	orq	$384, %rdi                      ## imm = 0x180
	vmovaps	%ymm0, (%r13,%rdi)
	vmovaps	32(%r10,%rcx), %ymm3
	vmovaps	160(%r10,%rcx), %ymm2
	vmovaps	288(%r10,%rcx), %ymm1
	vmovaps	416(%r10,%rcx), %ymm0
	movq	$-96, %rdi
	movq	736(%rsp), %rbp                 ## 8-byte Reload
	.p2align	4, 0x90
LBB1_28:                                ## %"for conv1_stage2.s1.r63$x.1"
                                        ##   Parent Loop BB1_12 Depth=1
                                        ##     Parent Loop BB1_25 Depth=2
                                        ## =>    This Inner Loop Header: Depth=3
	vmovups	(%rbp,%r12), %ymm4
	vbroadcastss	-196(%rsi,%rdi), %ymm5
	vfmadd213ps	%ymm3, %ymm4, %ymm5     ## ymm5 = (ymm4 * ymm5) + ymm3
	vbroadcastss	-100(%rsi,%rdi), %ymm6
	vfmadd213ps	%ymm2, %ymm4, %ymm6     ## ymm6 = (ymm4 * ymm6) + ymm2
	vbroadcastss	-4(%rsi,%rdi), %ymm7
	vfmadd213ps	%ymm1, %ymm4, %ymm7     ## ymm7 = (ymm4 * ymm7) + ymm1
	vbroadcastss	92(%rsi,%rdi), %ymm8
	vfmadd213ps	%ymm0, %ymm4, %ymm8     ## ymm8 = (ymm4 * ymm8) + ymm0
	vmovups	(%rbp,%r11), %ymm4
	vbroadcastss	-192(%rsi,%rdi), %ymm3
	vfmadd213ps	%ymm5, %ymm4, %ymm3     ## ymm3 = (ymm4 * ymm3) + ymm5
	vbroadcastss	-96(%rsi,%rdi), %ymm2
	vfmadd213ps	%ymm6, %ymm4, %ymm2     ## ymm2 = (ymm4 * ymm2) + ymm6
	vbroadcastss	(%rsi,%rdi), %ymm1
	vfmadd213ps	%ymm7, %ymm4, %ymm1     ## ymm1 = (ymm4 * ymm1) + ymm7
	vbroadcastss	96(%rsi,%rdi), %ymm0
	vfmadd213ps	%ymm8, %ymm4, %ymm0     ## ymm0 = (ymm4 * ymm0) + ymm8
	addq	%rbx, %rbp
	addq	$8, %rdi
	jne	LBB1_28
## %bb.29:                              ## %"for relu1.s0.w.v12.preheader.1"
                                        ##   in Loop: Header=BB1_25 Depth=2
	vmaxps	%ymm12, %ymm3, %ymm3
	vmovaps	%ymm3, 32(%r13,%r8)
	vmaxps	%ymm12, %ymm2, %ymm2
	movq	%r8, %rax
	orq	$160, %rax
	vmovaps	%ymm2, (%r13,%rax)
	vmaxps	%ymm12, %ymm1, %ymm1
	movq	%r8, %rax
	orq	$288, %rax                      ## imm = 0x120
	vmovaps	%ymm1, (%r13,%rax)
	vmaxps	%ymm12, %ymm0, %ymm0
	movq	%r8, %rax
	orq	$416, %rax                      ## imm = 0x1A0
	vmovaps	%ymm0, (%r13,%rax)
	vmovaps	64(%r10,%rcx), %ymm3
	vmovaps	192(%r10,%rcx), %ymm2
	vmovaps	320(%r10,%rcx), %ymm1
	vmovaps	448(%r10,%rcx), %ymm0
	movq	$-96, %rcx
	movq	728(%rsp), %rdi                 ## 8-byte Reload
	.p2align	4, 0x90
LBB1_30:                                ## %"for conv1_stage2.s1.r63$x.2"
                                        ##   Parent Loop BB1_12 Depth=1
                                        ##     Parent Loop BB1_25 Depth=2
                                        ## =>    This Inner Loop Header: Depth=3
	vmovups	(%rdi,%r12), %ymm4
	vbroadcastss	-196(%rsi,%rcx), %ymm5
	vfmadd213ps	%ymm3, %ymm4, %ymm5     ## ymm5 = (ymm4 * ymm5) + ymm3
	vbroadcastss	-100(%rsi,%rcx), %ymm6
	vfmadd213ps	%ymm2, %ymm4, %ymm6     ## ymm6 = (ymm4 * ymm6) + ymm2
	vbroadcastss	-4(%rsi,%rcx), %ymm7
	vfmadd213ps	%ymm1, %ymm4, %ymm7     ## ymm7 = (ymm4 * ymm7) + ymm1
	vbroadcastss	92(%rsi,%rcx), %ymm8
	vfmadd213ps	%ymm0, %ymm4, %ymm8     ## ymm8 = (ymm4 * ymm8) + ymm0
	vmovups	(%rdi,%r11), %ymm4
	vbroadcastss	-192(%rsi,%rcx), %ymm3
	vfmadd213ps	%ymm5, %ymm4, %ymm3     ## ymm3 = (ymm4 * ymm3) + ymm5
	vbroadcastss	-96(%rsi,%rcx), %ymm2
	vfmadd213ps	%ymm6, %ymm4, %ymm2     ## ymm2 = (ymm4 * ymm2) + ymm6
	vbroadcastss	(%rsi,%rcx), %ymm1
	vfmadd213ps	%ymm7, %ymm4, %ymm1     ## ymm1 = (ymm4 * ymm1) + ymm7
	vbroadcastss	96(%rsi,%rcx), %ymm0
	vfmadd213ps	%ymm8, %ymm4, %ymm0     ## ymm0 = (ymm4 * ymm0) + ymm8
	addq	%rbx, %rdi
	addq	$8, %rcx
	jne	LBB1_30
## %bb.31:                              ## %"for relu1.s0.w.v12.preheader.2"
                                        ##   in Loop: Header=BB1_25 Depth=2
	vmaxps	%ymm12, %ymm3, %ymm3
	vmovaps	%ymm3, 64(%r13,%r8)
	vmaxps	%ymm12, %ymm2, %ymm2
	movq	%r8, %rax
	orq	$192, %rax
	vmovaps	%ymm2, (%r13,%rax)
	vmaxps	%ymm12, %ymm1, %ymm1
	movq	%r8, %rax
	orq	$320, %rax                      ## imm = 0x140
	vmovaps	%ymm1, (%r13,%rax)
	vmaxps	%ymm12, %ymm0, %ymm0
	movq	%r8, %rax
	orq	$448, %rax                      ## imm = 0x1C0
	vmovaps	%ymm0, (%r13,%rax)
	shlq	$2, %rdx
	orq	$96, %rdx
	vmovaps	(%r10,%rdx), %ymm9
	vmovaps	128(%r10,%rdx), %ymm2
	vmovaps	256(%r10,%rdx), %ymm1
	vmovaps	384(%r10,%rdx), %ymm0
	movq	$-96, %rcx
	movq	720(%rsp), %rdx                 ## 8-byte Reload
	.p2align	4, 0x90
LBB1_32:                                ## %"for conv1_stage2.s1.r63$x.3"
                                        ##   Parent Loop BB1_12 Depth=1
                                        ##     Parent Loop BB1_25 Depth=2
                                        ## =>    This Inner Loop Header: Depth=3
	vmovups	(%rdx,%r12), %ymm3
	vbroadcastss	-196(%rsi,%rcx), %ymm4
	vfmadd213ps	%ymm9, %ymm3, %ymm4     ## ymm4 = (ymm3 * ymm4) + ymm9
	vbroadcastss	-100(%rsi,%rcx), %ymm5
	vfmadd213ps	%ymm2, %ymm3, %ymm5     ## ymm5 = (ymm3 * ymm5) + ymm2
	vbroadcastss	-4(%rsi,%rcx), %ymm6
	vfmadd213ps	%ymm1, %ymm3, %ymm6     ## ymm6 = (ymm3 * ymm6) + ymm1
	vbroadcastss	92(%rsi,%rcx), %ymm7
	vfmadd213ps	%ymm0, %ymm3, %ymm7     ## ymm7 = (ymm3 * ymm7) + ymm0
	vmovups	(%rdx,%r11), %ymm3
	vbroadcastss	-192(%rsi,%rcx), %ymm9
	vfmadd213ps	%ymm4, %ymm3, %ymm9     ## ymm9 = (ymm3 * ymm9) + ymm4
	vbroadcastss	-96(%rsi,%rcx), %ymm2
	vfmadd213ps	%ymm5, %ymm3, %ymm2     ## ymm2 = (ymm3 * ymm2) + ymm5
	vbroadcastss	(%rsi,%rcx), %ymm1
	vfmadd213ps	%ymm6, %ymm3, %ymm1     ## ymm1 = (ymm3 * ymm1) + ymm6
	vbroadcastss	96(%rsi,%rcx), %ymm0
	vfmadd213ps	%ymm7, %ymm3, %ymm0     ## ymm0 = (ymm3 * ymm0) + ymm7
	addq	%rbx, %rdx
	addq	$8, %rcx
	jne	LBB1_32
## %bb.33:                              ## %"for relu1.s0.w.v12.preheader.3"
                                        ##   in Loop: Header=BB1_25 Depth=2
	vmaxps	%ymm12, %ymm9, %ymm3
	shlq	$2, %r9
	orq	$96, %r9
	vmovaps	%ymm3, (%r13,%r9)
	vmaxps	%ymm12, %ymm2, %ymm2
	movq	%r8, %rax
	orq	$224, %rax
	vmovaps	%ymm2, (%r13,%rax)
	vmaxps	%ymm12, %ymm1, %ymm1
	movq	%r8, %rax
	orq	$352, %rax                      ## imm = 0x160
	vmovaps	%ymm1, (%r13,%rax)
	vmaxps	%ymm12, %ymm0, %ymm0
	orq	$480, %r8                       ## imm = 0x1E0
	vmovaps	%ymm0, (%r13,%r8)
	incq	%r14
	addq	$384, %rsi                      ## imm = 0x180
	cmpq	504(%rsp), %r14                 ## 8-byte Folded Reload
	jne	LBB1_25
LBB1_34:                                ## %"produce f1"
                                        ##   in Loop: Header=BB1_12 Depth=1
	movslq	40(%rsp), %rdi                  ## 4-byte Folded Reload
	movslq	32(%rsp), %rcx                  ## 4-byte Folded Reload
	movq	%rcx, 256(%rsp)                 ## 8-byte Spill
	movslq	24(%rsp), %rax                  ## 4-byte Folded Reload
	movq	%rax, 248(%rsp)                 ## 8-byte Spill
	movslq	64(%rsp), %rsi                  ## 4-byte Folded Reload
	movslq	16(%rsp), %rcx                  ## 4-byte Folded Reload
	movq	%rcx, 240(%rsp)                 ## 8-byte Spill
	movslq	56(%rsp), %rcx                  ## 4-byte Folded Reload
	movq	%rcx, 352(%rsp)                 ## 8-byte Spill
	movslq	88(%rsp), %rcx                  ## 4-byte Folded Reload
	movq	%rcx, 344(%rsp)                 ## 8-byte Spill
	movslq	80(%rsp), %rcx                  ## 4-byte Folded Reload
	movq	%rcx, 336(%rsp)                 ## 8-byte Spill
	movslq	144(%rsp), %rcx                 ## 4-byte Folded Reload
	movq	%rcx, 328(%rsp)                 ## 8-byte Spill
	movslq	72(%rsp), %rcx                  ## 4-byte Folded Reload
	movq	%rcx, 232(%rsp)                 ## 8-byte Spill
	movslq	512(%rsp), %rcx                 ## 4-byte Folded Reload
	movq	%rcx, 440(%rsp)                 ## 8-byte Spill
	movslq	120(%rsp), %rcx                 ## 4-byte Folded Reload
	movq	%rcx, 496(%rsp)                 ## 8-byte Spill
	movslq	160(%rsp), %rcx                 ## 4-byte Folded Reload
	movq	%rcx, 488(%rsp)                 ## 8-byte Spill
	movslq	104(%rsp), %rcx                 ## 4-byte Folded Reload
	movq	%rcx, 224(%rsp)                 ## 8-byte Spill
	movslq	112(%rsp), %rcx                 ## 4-byte Folded Reload
	movq	%rcx, 216(%rsp)                 ## 8-byte Spill
	movslq	8(%rsp), %rcx                   ## 4-byte Folded Reload
	movq	%rcx, 320(%rsp)                 ## 8-byte Spill
	movslq	128(%rsp), %rcx                 ## 4-byte Folded Reload
	movq	%rcx, 208(%rsp)                 ## 8-byte Spill
	movslq	96(%rsp), %rcx                  ## 4-byte Folded Reload
	movq	%rcx, 200(%rsp)                 ## 8-byte Spill
	movslq	136(%rsp), %rcx                 ## 4-byte Folded Reload
	movq	%rcx, 480(%rsp)                 ## 8-byte Spill
	movslq	544(%rsp), %rcx                 ## 4-byte Folded Reload
	movq	%rcx, 472(%rsp)                 ## 8-byte Spill
	movslq	168(%rsp), %rcx                 ## 4-byte Folded Reload
	movq	%rcx, 432(%rsp)                 ## 8-byte Spill
	movslq	(%rsp), %rcx                    ## 4-byte Folded Reload
	movq	%rcx, 192(%rsp)                 ## 8-byte Spill
	vblendps	$1, %ymm12, %ymm9, %ymm9        ## ymm9 = ymm12[0],ymm9[1,2,3,4,5,6,7]
	cmpl	$0, 312(%rsp)                   ## 4-byte Folded Reload
	movq	%rdi, 264(%rsp)                 ## 8-byte Spill
	jle	LBB1_46
## %bb.35:                              ## %"for f1.s1.r79$x.preheader"
                                        ##   in Loop: Header=BB1_12 Depth=1
	cmpb	$0, 55(%rsp)                    ## 1-byte Folded Reload
	movq	%rsi, 184(%rsp)                 ## 8-byte Spill
	je	LBB1_41
## %bb.36:                              ## %vector.body.preheader
                                        ##   in Loop: Header=BB1_12 Depth=1
	vxorps	%xmm0, %xmm0, %xmm0
	vmovups	%ymm0, 1280(%rsp)               ## 32-byte Spill
	movq	464(%rsp), %rcx                 ## 8-byte Reload
	movq	%rcx, 448(%rsp)                 ## 8-byte Spill
	movq	592(%rsp), %rdx                 ## 8-byte Reload
	movq	%rdx, 704(%rsp)                 ## 8-byte Spill
	vmovdqa	LCPI1_12(%rip), %ymm1           ## ymm1 = [0,1,2,3]
	vmovdqa	LCPI1_11(%rip), %ymm4           ## ymm4 = [4,5,6,7]
	vxorps	%xmm0, %xmm0, %xmm0
	vmovups	%ymm0, 1248(%rsp)               ## 32-byte Spill
	vxorps	%xmm0, %xmm0, %xmm0
	vmovups	%ymm0, 1216(%rsp)               ## 32-byte Spill
	vxorps	%xmm0, %xmm0, %xmm0
	vmovups	%ymm0, 1184(%rsp)               ## 32-byte Spill
	.p2align	4, 0x90
LBB1_37:                                ## %vector.body
                                        ##   Parent Loop BB1_12 Depth=1
                                        ## =>  This Inner Loop Header: Depth=2
	vpbroadcastq	LCPI1_13(%rip), %ymm2   ## ymm2 = [8,8,8,8]
	vpbroadcastq	LCPI1_14(%rip), %ymm0   ## ymm0 = [16,16,16,16]
	vpbroadcastq	LCPI1_15(%rip), %ymm14  ## ymm14 = [24,24,24,24]
	vmovdqu	%ymm14, 512(%rsp)               ## 32-byte Spill
	vxorps	%xmm11, %xmm11, %xmm11
	vpcmpeqq	%ymm11, %ymm1, %ymm3
	vmovdqa	%ymm1, %ymm7
	vmovdqa	%ymm4, %ymm10
	vpsllq	$5, %ymm4, %ymm4
	vpsllq	$5, %ymm1, %ymm1
	vmovq	%xmm1, %rdi
	vpextrq	$1, %xmm1, %rcx
	vextracti128	$1, %ymm1, %xmm5
	vmovq	%xmm4, %r10
	vpextrq	$1, %xmm4, %r8
	vmovss	(%r13,%r10,4), %xmm1            ## xmm1 = mem[0],zero,zero,zero
	vinsertps	$16, (%r13,%r8,4), %xmm1, %xmm1 ## xmm1 = xmm1[0],mem[0],xmm1[2,3]
	vmovq	%xmm5, %rax
	vextracti128	$1, %ymm4, %xmm4
	vmovss	(%r13,%rdi,4), %xmm6            ## xmm6 = mem[0],zero,zero,zero
	vpextrq	$1, %xmm5, %rbp
	vinsertps	$16, (%r13,%rcx,4), %xmm6, %xmm6 ## xmm6 = xmm6[0],mem[0],xmm6[2,3]
	vpextrq	$1, %xmm4, %rdx
	vmovq	%xmm4, %rsi
	vmovss	68(%r13,%r10,4), %xmm4          ## xmm4 = mem[0],zero,zero,zero
	vinsertps	$32, (%r13,%rsi,4), %xmm1, %xmm1 ## xmm1 = xmm1[0,1],mem[0],xmm1[3]
	vmovaps	%xmm1, 864(%rsp)                ## 16-byte Spill
	vinsertps	$16, 68(%r13,%r8,4), %xmm4, %xmm1 ## xmm1 = xmm4[0],mem[0],xmm4[2,3]
	vinsertps	$32, (%r13,%rax,4), %xmm6, %xmm4 ## xmm4 = xmm6[0,1],mem[0],xmm6[3]
	vmovaps	%xmm4, 800(%rsp)                ## 16-byte Spill
	vinsertps	$32, 68(%r13,%rsi,4), %xmm1, %xmm1 ## xmm1 = xmm1[0,1],mem[0],xmm1[3]
	vinsertps	$48, 68(%r13,%rdx,4), %xmm1, %xmm8 ## xmm8 = xmm1[0,1,2],mem[0]
	vmovss	68(%r13,%rdi,4), %xmm4          ## xmm4 = mem[0],zero,zero,zero
	vinsertps	$16, 68(%r13,%rcx,4), %xmm4, %xmm6 ## xmm6 = xmm4[0],mem[0],xmm4[2,3]
	vmovdqa	%ymm10, %ymm13
	vpaddq	%ymm2, %ymm10, %ymm4
	vpcmpeqq	%ymm11, %ymm10, %ymm5
	vinsertps	$32, 68(%r13,%rax,4), %xmm6, %xmm6 ## xmm6 = xmm6[0,1],mem[0],xmm6[3]
	vmovdqu	%ymm7, 1440(%rsp)               ## 32-byte Spill
	vpaddq	%ymm2, %ymm7, %ymm11
	vperm2i128	$49, %ymm5, %ymm3, %ymm12 ## ymm12 = ymm3[2,3],ymm5[2,3]
	vinsertps	$48, 68(%r13,%rbp,4), %xmm6, %xmm10 ## xmm10 = xmm6[0,1,2],mem[0]
	vpaddq	%ymm0, %ymm13, %ymm2
	vmovdqu	%ymm13, 1920(%rsp)              ## 32-byte Spill
	vmovss	72(%r13,%r10,4), %xmm6          ## xmm6 = mem[0],zero,zero,zero
	vinsertps	$16, 72(%r13,%r8,4), %xmm6, %xmm6 ## xmm6 = xmm6[0],mem[0],xmm6[2,3]
	vpaddq	%ymm0, %ymm7, %ymm7
	vinserti128	$1, %xmm5, %ymm3, %ymm0
	vinsertps	$32, 72(%r13,%rsi,4), %xmm6, %xmm5 ## xmm5 = xmm6[0,1],mem[0],xmm6[3]
	vpaddq	%ymm14, %ymm13, %ymm1
	vmovdqu	%ymm1, 544(%rsp)                ## 32-byte Spill
	vpackssdw	%ymm12, %ymm0, %ymm3
	vinsertps	$48, 72(%r13,%rdx,4), %xmm5, %xmm5 ## xmm5 = xmm5[0,1,2],mem[0]
	vxorps	%xmm6, %xmm6, %xmm6
	vpcmpeqq	%ymm6, %ymm11, %ymm0
	vmovss	72(%r13,%rdi,4), %xmm1          ## xmm1 = mem[0],zero,zero,zero
	vinsertps	$16, 72(%r13,%rcx,4), %xmm1, %xmm1 ## xmm1 = xmm1[0],mem[0],xmm1[2,3]
	vpcmpeqq	%ymm6, %ymm4, %ymm14
	vpxor	%xmm6, %xmm6, %xmm6
	vperm2i128	$49, %ymm14, %ymm0, %ymm12 ## ymm12 = ymm0[2,3],ymm14[2,3]
	vinsertps	$32, 72(%r13,%rax,4), %xmm1, %xmm1 ## xmm1 = xmm1[0,1],mem[0],xmm1[3]
	vinserti128	$1, %xmm14, %ymm0, %ymm15
	vinsertps	$48, 72(%r13,%rbp,4), %xmm1, %xmm14 ## xmm14 = xmm1[0,1,2],mem[0]
	movq	%rbp, 72(%rsp)                  ## 8-byte Spill
	vmovss	80(%r13,%r10,4), %xmm0          ## xmm0 = mem[0],zero,zero,zero
	vpsllq	$5, %ymm11, %ymm1
	movq	%r8, 280(%rsp)                  ## 8-byte Spill
	vinsertps	$16, 80(%r13,%r8,4), %xmm0, %xmm0 ## xmm0 = xmm0[0],mem[0],xmm0[2,3]
	vpextrq	$1, %xmm1, (%rsp)               ## 8-byte Folded Spill
	vmovq	%xmm1, %r15
	vinsertps	$32, 80(%r13,%rsi,4), %xmm0, %xmm0 ## xmm0 = xmm0[0,1],mem[0],xmm0[3]
	vpackssdw	%ymm12, %ymm15, %ymm13
	movq	%rdx, 288(%rsp)                 ## 8-byte Spill
	vinsertps	$48, 80(%r13,%rdx,4), %xmm0, %xmm15 ## xmm15 = xmm0[0,1,2],mem[0]
	movq	%rdi, 56(%rsp)                  ## 8-byte Spill
	vmovss	80(%r13,%rdi,4), %xmm0          ## xmm0 = mem[0],zero,zero,zero
	vextracti128	$1, %ymm1, %xmm1
	vinsertps	$16, 80(%r13,%rcx,4), %xmm0, %xmm0 ## xmm0 = xmm0[0],mem[0],xmm0[2,3]
	vpextrq	$1, %xmm1, 40(%rsp)             ## 8-byte Folded Spill
	vmovq	%xmm1, %r9
	movq	%rax, 144(%rsp)                 ## 8-byte Spill
	vinsertps	$32, 80(%r13,%rax,4), %xmm0, %xmm1 ## xmm1 = xmm0[0,1],mem[0],xmm0[3]
	vpcmpeqq	%ymm6, %ymm7, %ymm11
	vinsertf128	$1, %xmm8, %ymm10, %ymm0
	vinsertps	$48, 80(%r13,%rbp,4), %xmm1, %xmm8 ## xmm8 = xmm1[0,1,2],mem[0]
	vinsertf128	$1, %xmm5, %ymm14, %ymm14
	movq	%r10, 272(%rsp)                 ## 8-byte Spill
	vmovss	84(%r13,%r10,4), %xmm5          ## xmm5 = mem[0],zero,zero,zero
	vinsertps	$16, 84(%r13,%r8,4), %xmm5, %xmm1 ## xmm1 = xmm5[0],mem[0],xmm5[2,3]
	vpcmpeqq	%ymm6, %ymm2, %ymm5
	vpsllq	$5, %ymm4, %ymm4
	vinsertps	$32, 84(%r13,%rsi,4), %xmm1, %xmm1 ## xmm1 = xmm1[0,1],mem[0],xmm1[3]
	movq	%rsi, 88(%rsp)                  ## 8-byte Spill
	vpsllq	$5, %ymm2, %ymm10
	vinsertps	$48, 84(%r13,%rdx,4), %xmm1, %xmm1 ## xmm1 = xmm1[0,1,2],mem[0]
	vblendvps	%ymm3, %ymm0, %ymm14, %ymm0
	vmovups	%ymm0, 1120(%rsp)               ## 32-byte Spill
	vmovss	84(%r13,%rdi,4), %xmm0          ## xmm0 = mem[0],zero,zero,zero
	movq	%rcx, 80(%rsp)                  ## 8-byte Spill
	vinsertps	$16, 84(%r13,%rcx,4), %xmm0, %xmm2 ## xmm2 = xmm0[0],mem[0],xmm0[2,3]
	vpsllq	$5, %ymm7, %ymm7
	vinsertf128	$1, %xmm15, %ymm8, %ymm0
	vinsertps	$32, 84(%r13,%rax,4), %xmm2, %xmm2 ## xmm2 = xmm2[0,1],mem[0],xmm2[3]
	vperm2i128	$49, %ymm5, %ymm11, %ymm8 ## ymm8 = ymm11[2,3],ymm5[2,3]
	vinsertps	$48, 84(%r13,%rbp,4), %xmm2, %xmm2 ## xmm2 = xmm2[0,1,2],mem[0]
	vpextrq	$1, %xmm4, %rdx
	vmovq	%xmm4, %r12
	vinsertf128	$1, %xmm1, %ymm2, %ymm1
	vmovss	4(%r13,%r10,4), %xmm2           ## xmm2 = mem[0],zero,zero,zero
	vextracti128	$1, %ymm4, %xmm4
	vinsertps	$16, 4(%r13,%r8,4), %xmm2, %xmm2 ## xmm2 = xmm2[0],mem[0],xmm2[2,3]
	vblendvps	%ymm3, %ymm0, %ymm1, %ymm0
	vmovups	%ymm0, 960(%rsp)                ## 32-byte Spill
	vextracti128	$1, %ymm7, %xmm0
	vinsertps	$32, 4(%r13,%rsi,4), %xmm2, %xmm12 ## xmm12 = xmm2[0,1],mem[0],xmm2[3]
	vmovq	%xmm4, %rbx
	vextracti128	$1, %ymm10, %xmm2
	vpextrq	$1, %xmm4, %r8
	movq	%r12, 24(%rsp)                  ## 8-byte Spill
	vmovss	68(%r13,%r12,4), %xmm4          ## xmm4 = mem[0],zero,zero,zero
	movq	%rdx, 64(%rsp)                  ## 8-byte Spill
	vinsertps	$16, 68(%r13,%rdx,4), %xmm4, %xmm4 ## xmm4 = xmm4[0],mem[0],xmm4[2,3]
	vpextrq	$1, %xmm7, %r11
	movq	%r11, 112(%rsp)                 ## 8-byte Spill
	vmovq	%xmm7, %rsi
	movq	%r15, %rbp
	movq	%r15, 32(%rsp)                  ## 8-byte Spill
	vmovss	68(%r13,%r15,4), %xmm7          ## xmm7 = mem[0],zero,zero,zero
	vinsertps	$32, 68(%r13,%rbx,4), %xmm4, %xmm4 ## xmm4 = xmm4[0,1],mem[0],xmm4[3]
	movq	(%rsp), %rdi                    ## 8-byte Reload
	vinsertps	$16, 68(%r13,%rdi,4), %xmm7, %xmm7 ## xmm7 = xmm7[0],mem[0],xmm7[2,3]
	vpextrq	$1, %xmm10, %r15
	vmovq	%xmm10, %r14
	vmovss	68(%r13,%r14,4), %xmm3          ## xmm3 = mem[0],zero,zero,zero
	movq	%r9, 16(%rsp)                   ## 8-byte Spill
	vinsertps	$32, 68(%r13,%r9,4), %xmm7, %xmm10 ## xmm10 = xmm7[0,1],mem[0],xmm7[3]
	vinsertps	$16, 68(%r13,%r15,4), %xmm3, %xmm3 ## xmm3 = xmm3[0],mem[0],xmm3[2,3]
	vpextrq	$1, %xmm2, %rcx
	vmovq	%xmm2, %r10
	vmovss	68(%r13,%rsi,4), %xmm2          ## xmm2 = mem[0],zero,zero,zero
	movq	%rsi, 168(%rsp)                 ## 8-byte Spill
	vinsertps	$32, 68(%r13,%r10,4), %xmm3, %xmm3 ## xmm3 = xmm3[0,1],mem[0],xmm3[3]
	movq	%r10, 296(%rsp)                 ## 8-byte Spill
	vinsertps	$16, 68(%r13,%r11,4), %xmm2, %xmm2 ## xmm2 = xmm2[0],mem[0],xmm2[2,3]
	vpextrq	$1, %xmm0, %r11
	vmovq	%xmm0, %rax
	vmovss	72(%r13,%r12,4), %xmm0          ## xmm0 = mem[0],zero,zero,zero
	vinsertps	$32, 68(%r13,%rax,4), %xmm2, %xmm2 ## xmm2 = xmm2[0,1],mem[0],xmm2[3]
	vinsertps	$16, 72(%r13,%rdx,4), %xmm0, %xmm0 ## xmm0 = xmm0[0],mem[0],xmm0[2,3]
	vinsertps	$32, 72(%r13,%rbx,4), %xmm0, %xmm0 ## xmm0 = xmm0[0,1],mem[0],xmm0[3]
	vinserti128	$1, %xmm5, %ymm11, %ymm5
	vmovss	72(%r13,%rbp,4), %xmm7          ## xmm7 = mem[0],zero,zero,zero
	vinsertps	$48, 68(%r13,%r8,4), %xmm4, %xmm4 ## xmm4 = xmm4[0,1,2],mem[0]
	vinsertps	$16, 72(%r13,%rdi,4), %xmm7, %xmm7 ## xmm7 = xmm7[0],mem[0],xmm7[2,3]
	movq	40(%rsp), %rdi                  ## 8-byte Reload
	vinsertps	$48, 68(%r13,%rdi,4), %xmm10, %xmm10 ## xmm10 = xmm10[0,1,2],mem[0]
	vinsertps	$32, 72(%r13,%r9,4), %xmm7, %xmm7 ## xmm7 = xmm7[0,1],mem[0],xmm7[3]
	movq	%rcx, 160(%rsp)                 ## 8-byte Spill
	vinsertps	$48, 68(%r13,%rcx,4), %xmm3, %xmm3 ## xmm3 = xmm3[0,1,2],mem[0]
	vpackssdw	%ymm8, %ymm5, %ymm8
	movq	%r14, %rbp
	vmovss	72(%r13,%r14,4), %xmm5          ## xmm5 = mem[0],zero,zero,zero
	movq	%r11, 120(%rsp)                 ## 8-byte Spill
	vinsertps	$48, 68(%r13,%r11,4), %xmm2, %xmm2 ## xmm2 = xmm2[0,1,2],mem[0]
	vinsertps	$16, 72(%r13,%r15,4), %xmm5, %xmm5 ## xmm5 = xmm5[0],mem[0],xmm5[2,3]
	vinsertps	$48, 72(%r13,%r8,4), %xmm0, %xmm0 ## xmm0 = xmm0[0,1,2],mem[0]
	movq	%r8, %r9
	movq	%r8, 8(%rsp)                    ## 8-byte Spill
	vinsertps	$32, 72(%r13,%r10,4), %xmm5, %xmm5 ## xmm5 = xmm5[0,1],mem[0],xmm5[3]
	vinsertps	$48, 72(%r13,%rdi,4), %xmm7, %xmm7 ## xmm7 = xmm7[0,1,2],mem[0]
	vmovdqu	512(%rsp), %ymm1                ## 32-byte Reload
	vpaddq	1440(%rsp), %ymm1, %ymm11       ## 32-byte Folded Reload
	vinsertps	$48, 72(%r13,%rcx,4), %xmm5, %xmm5 ## xmm5 = xmm5[0,1,2],mem[0]
	vinsertf128	$1, %xmm4, %ymm10, %ymm4
	vmovss	72(%r13,%rsi,4), %xmm6          ## xmm6 = mem[0],zero,zero,zero
	movq	112(%rsp), %r14                 ## 8-byte Reload
	vinsertps	$16, 72(%r13,%r14,4), %xmm6, %xmm6 ## xmm6 = xmm6[0],mem[0],xmm6[2,3]
	vinsertf128	$1, %xmm3, %ymm2, %ymm2
	vpxor	%xmm1, %xmm1, %xmm1
	vpcmpeqq	%ymm1, %ymm11, %ymm3
	vinsertps	$32, 72(%r13,%rax,4), %xmm6, %xmm6 ## xmm6 = xmm6[0,1],mem[0],xmm6[3]
	movq	%rax, %r12
	vinsertf128	$1, %xmm0, %ymm7, %ymm0
	vmovdqu	544(%rsp), %ymm14               ## 32-byte Reload
	vpcmpeqq	%ymm1, %ymm14, %ymm10
	vxorps	%xmm15, %xmm15, %xmm15
	vinsertps	$48, 72(%r13,%r11,4), %xmm6, %xmm6 ## xmm6 = xmm6[0,1,2],mem[0]
	vinsertf128	$1, %xmm5, %ymm6, %ymm5
	movq	56(%rsp), %rax                  ## 8-byte Reload
	vmovss	4(%r13,%rax,4), %xmm6           ## xmm6 = mem[0],zero,zero,zero
	movq	80(%rsp), %rax                  ## 8-byte Reload
	vinsertps	$16, 4(%r13,%rax,4), %xmm6, %xmm7 ## xmm7 = xmm6[0],mem[0],xmm6[2,3]
	vpsllq	$5, %ymm14, %ymm6
	vpsllq	$5, %ymm11, %ymm11
	vblendvps	%ymm13, %ymm4, %ymm0, %ymm14
	vextracti128	$1, %ymm11, %xmm4
	vmovq	%xmm11, %r10
	movq	%r10, 512(%rsp)                 ## 8-byte Spill
	vpextrq	$1, %xmm11, %r8
	movq	%r8, 128(%rsp)                  ## 8-byte Spill
	vextracti128	$1, %ymm6, %xmm0
	vmovq	%xmm6, %rdi
	movq	%rdi, 392(%rsp)                 ## 8-byte Spill
	vpextrq	$1, %xmm6, %rdx
	movq	%rdx, 384(%rsp)                 ## 8-byte Spill
	vmovss	68(%r13,%rdi,4), %xmm6          ## xmm6 = mem[0],zero,zero,zero
	vinsertps	$16, 68(%r13,%rdx,4), %xmm6, %xmm6 ## xmm6 = xmm6[0],mem[0],xmm6[2,3]
	vpextrq	$1, %xmm0, %rcx
	vmovq	%xmm0, %rsi
	movq	%rsi, 544(%rsp)                 ## 8-byte Spill
	vmovss	68(%r13,%r10,4), %xmm0          ## xmm0 = mem[0],zero,zero,zero
	vinsertps	$32, 68(%r13,%rsi,4), %xmm6, %xmm6 ## xmm6 = xmm6[0,1],mem[0],xmm6[3]
	vinsertps	$16, 68(%r13,%r8,4), %xmm0, %xmm0 ## xmm0 = xmm0[0],mem[0],xmm0[2,3]
	vpextrq	$1, %xmm4, %rax
	vmovq	%xmm4, %r11
	movq	%r11, 136(%rsp)                 ## 8-byte Spill
	vmovss	72(%r13,%rdi,4), %xmm4          ## xmm4 = mem[0],zero,zero,zero
	vinsertps	$32, 68(%r13,%r11,4), %xmm0, %xmm0 ## xmm0 = xmm0[0,1],mem[0],xmm0[3]
	vinsertps	$16, 72(%r13,%rdx,4), %xmm4, %xmm4 ## xmm4 = xmm4[0],mem[0],xmm4[2,3]
	movq	%rcx, 96(%rsp)                  ## 8-byte Spill
	vinsertps	$48, 68(%r13,%rcx,4), %xmm6, %xmm6 ## xmm6 = xmm6[0,1,2],mem[0]
	vinsertps	$32, 72(%r13,%rsi,4), %xmm4, %xmm4 ## xmm4 = xmm4[0,1],mem[0],xmm4[3]
	movq	%rax, 400(%rsp)                 ## 8-byte Spill
	vinsertps	$48, 68(%r13,%rax,4), %xmm0, %xmm11 ## xmm11 = xmm0[0,1,2],mem[0]
	vinsertps	$48, 72(%r13,%rcx,4), %xmm4, %xmm4 ## xmm4 = xmm4[0,1,2],mem[0]
	vblendvps	%ymm8, %ymm2, %ymm5, %ymm0
	vmovups	%ymm0, 832(%rsp)                ## 32-byte Spill
	vmovss	72(%r13,%r10,4), %xmm0          ## xmm0 = mem[0],zero,zero,zero
	vinsertps	$16, 72(%r13,%r8,4), %xmm0, %xmm0 ## xmm0 = xmm0[0],mem[0],xmm0[2,3]
	vperm2i128	$49, %ymm10, %ymm3, %ymm2 ## ymm2 = ymm3[2,3],ymm10[2,3]
	vinserti128	$1, %xmm10, %ymm3, %ymm3
	vinsertps	$32, 72(%r13,%r11,4), %xmm0, %xmm0 ## xmm0 = xmm0[0,1],mem[0],xmm0[3]
	movq	24(%rsp), %rdi                  ## 8-byte Reload
	vmovss	80(%r13,%rdi,4), %xmm5          ## xmm5 = mem[0],zero,zero,zero
	movq	64(%rsp), %r8                   ## 8-byte Reload
	vinsertps	$16, 80(%r13,%r8,4), %xmm5, %xmm5 ## xmm5 = xmm5[0],mem[0],xmm5[2,3]
	vinsertps	$48, 72(%r13,%rax,4), %xmm0, %xmm10 ## xmm10 = xmm0[0,1,2],mem[0]
	vpackssdw	%ymm2, %ymm3, %ymm0
	movq	%rbx, %r10
	movq	%rbx, 104(%rsp)                 ## 8-byte Spill
	vinsertps	$32, 80(%r13,%rbx,4), %xmm5, %xmm2 ## xmm2 = xmm5[0,1],mem[0],xmm5[3]
	movq	32(%rsp), %rbx                  ## 8-byte Reload
	vmovss	80(%r13,%rbx,4), %xmm3          ## xmm3 = mem[0],zero,zero,zero
	movq	(%rsp), %rax                    ## 8-byte Reload
	vinsertps	$16, 80(%r13,%rax,4), %xmm3, %xmm3 ## xmm3 = xmm3[0],mem[0],xmm3[2,3]
	vinsertps	$48, 80(%r13,%r9,4), %xmm2, %xmm2 ## xmm2 = xmm2[0,1,2],mem[0]
	vinsertf128	$1, %xmm6, %ymm11, %ymm5
	movq	16(%rsp), %r11                  ## 8-byte Reload
	vinsertps	$32, 80(%r13,%r11,4), %xmm3, %xmm3 ## xmm3 = xmm3[0,1],mem[0],xmm3[3]
	movq	%rbp, %rdx
	movq	%rbp, 376(%rsp)                 ## 8-byte Spill
	vmovss	80(%r13,%rbp,4), %xmm6          ## xmm6 = mem[0],zero,zero,zero
	movq	%r15, %rcx
	movq	%r15, 368(%rsp)                 ## 8-byte Spill
	vinsertps	$16, 80(%r13,%r15,4), %xmm6, %xmm6 ## xmm6 = xmm6[0],mem[0],xmm6[2,3]
	movq	40(%rsp), %rsi                  ## 8-byte Reload
	vinsertps	$48, 80(%r13,%rsi,4), %xmm3, %xmm3 ## xmm3 = xmm3[0,1,2],mem[0]
	vinsertf128	$1, %xmm4, %ymm10, %ymm4
	movq	296(%rsp), %r15                 ## 8-byte Reload
	vinsertps	$32, 80(%r13,%r15,4), %xmm6, %xmm6 ## xmm6 = xmm6[0,1],mem[0],xmm6[3]
	movq	168(%rsp), %rax                 ## 8-byte Reload
	vmovss	80(%r13,%rax,4), %xmm1          ## xmm1 = mem[0],zero,zero,zero
	movq	%r14, %rbp
	vinsertps	$16, 80(%r13,%r14,4), %xmm1, %xmm1 ## xmm1 = xmm1[0],mem[0],xmm1[2,3]
	movq	160(%rsp), %r14                 ## 8-byte Reload
	vinsertps	$48, 80(%r13,%r14,4), %xmm6, %xmm6 ## xmm6 = xmm6[0,1,2],mem[0]
	movq	%r12, %r9
	movq	%r12, 360(%rsp)                 ## 8-byte Spill
	vinsertps	$32, 80(%r13,%r12,4), %xmm1, %xmm1 ## xmm1 = xmm1[0,1],mem[0],xmm1[3]
	movq	120(%rsp), %r12                 ## 8-byte Reload
	vinsertps	$48, 80(%r13,%r12,4), %xmm1, %xmm1 ## xmm1 = xmm1[0,1,2],mem[0]
	vinsertf128	$1, %xmm2, %ymm3, %ymm2
	vinsertf128	$1, %xmm6, %ymm1, %ymm1
	vblendvps	%ymm0, %ymm5, %ymm4, %ymm3
	vmovups	%ymm3, 1152(%rsp)               ## 32-byte Spill
	vmovss	84(%r13,%rdi,4), %xmm3          ## xmm3 = mem[0],zero,zero,zero
	vmovss	84(%r13,%rbx,4), %xmm4          ## xmm4 = mem[0],zero,zero,zero
	vinsertps	$16, 84(%r13,%r8,4), %xmm3, %xmm3 ## xmm3 = xmm3[0],mem[0],xmm3[2,3]
	movq	(%rsp), %r8                     ## 8-byte Reload
	vinsertps	$16, 84(%r13,%r8,4), %xmm4, %xmm4 ## xmm4 = xmm4[0],mem[0],xmm4[2,3]
	vinsertps	$32, 84(%r13,%r10,4), %xmm3, %xmm3 ## xmm3 = xmm3[0,1],mem[0],xmm3[3]
	vinsertps	$32, 84(%r13,%r11,4), %xmm4, %xmm4 ## xmm4 = xmm4[0,1],mem[0],xmm4[3]
	movq	8(%rsp), %rdi                   ## 8-byte Reload
	vinsertps	$48, 84(%r13,%rdi,4), %xmm3, %xmm3 ## xmm3 = xmm3[0,1,2],mem[0]
	vinsertps	$48, 84(%r13,%rsi,4), %xmm4, %xmm4 ## xmm4 = xmm4[0,1,2],mem[0]
	vinsertf128	$1, %xmm3, %ymm4, %ymm3
	vmovss	84(%r13,%rdx,4), %xmm4          ## xmm4 = mem[0],zero,zero,zero
	vinsertps	$16, 84(%r13,%rcx,4), %xmm4, %xmm5 ## xmm5 = xmm4[0],mem[0],xmm4[2,3]
	vblendvps	%ymm13, %ymm2, %ymm3, %ymm2
	vmovups	%ymm2, 1088(%rsp)               ## 32-byte Spill
	movq	392(%rsp), %rsi                 ## 8-byte Reload
	vmovss	80(%r13,%rsi,4), %xmm2          ## xmm2 = mem[0],zero,zero,zero
	vmovss	84(%r13,%rax,4), %xmm3          ## xmm3 = mem[0],zero,zero,zero
	vinsertps	$32, 84(%r13,%r15,4), %xmm5, %xmm5 ## xmm5 = xmm5[0,1],mem[0],xmm5[3]
	vinsertps	$16, 84(%r13,%rbp,4), %xmm3, %xmm3 ## xmm3 = xmm3[0],mem[0],xmm3[2,3]
	vinsertps	$48, 84(%r13,%r14,4), %xmm5, %xmm5 ## xmm5 = xmm5[0,1,2],mem[0]
	vinsertps	$32, 84(%r13,%r9,4), %xmm3, %xmm3 ## xmm3 = xmm3[0,1],mem[0],xmm3[3]
	vinsertps	$48, 84(%r13,%r12,4), %xmm3, %xmm3 ## xmm3 = xmm3[0,1,2],mem[0]
	vinsertf128	$1, %xmm5, %ymm3, %ymm3
	movq	512(%rsp), %r12                 ## 8-byte Reload
	vmovss	80(%r13,%r12,4), %xmm5          ## xmm5 = mem[0],zero,zero,zero
	movq	384(%rsp), %rax                 ## 8-byte Reload
	vinsertps	$16, 80(%r13,%rax,4), %xmm2, %xmm2 ## xmm2 = xmm2[0],mem[0],xmm2[2,3]
	movq	128(%rsp), %rcx                 ## 8-byte Reload
	vinsertps	$16, 80(%r13,%rcx,4), %xmm5, %xmm5 ## xmm5 = xmm5[0],mem[0],xmm5[2,3]
	movq	544(%rsp), %rdx                 ## 8-byte Reload
	vinsertps	$32, 80(%r13,%rdx,4), %xmm2, %xmm2 ## xmm2 = xmm2[0,1],mem[0],xmm2[3]
	movq	136(%rsp), %rdi                 ## 8-byte Reload
	vinsertps	$32, 80(%r13,%rdi,4), %xmm5, %xmm5 ## xmm5 = xmm5[0,1],mem[0],xmm5[3]
	vblendvps	%ymm8, %ymm1, %ymm3, %ymm1
	vmovups	%ymm1, 928(%rsp)                ## 32-byte Spill
	movq	56(%rsp), %r15                  ## 8-byte Reload
	vmovss	8(%r13,%r15,4), %xmm1           ## xmm1 = mem[0],zero,zero,zero
	vmovss	84(%r13,%rsi,4), %xmm3          ## xmm3 = mem[0],zero,zero,zero
	movq	96(%rsp), %rsi                  ## 8-byte Reload
	vinsertps	$48, 80(%r13,%rsi,4), %xmm2, %xmm2 ## xmm2 = xmm2[0,1,2],mem[0]
	vinsertps	$16, 84(%r13,%rax,4), %xmm3, %xmm3 ## xmm3 = xmm3[0],mem[0],xmm3[2,3]
	movq	400(%rsp), %rax                 ## 8-byte Reload
	vinsertps	$48, 80(%r13,%rax,4), %xmm5, %xmm5 ## xmm5 = xmm5[0,1,2],mem[0]
	vinsertf128	$1, %xmm2, %ymm5, %ymm2
	vmovss	84(%r13,%r12,4), %xmm5          ## xmm5 = mem[0],zero,zero,zero
	vinsertps	$32, 84(%r13,%rdx,4), %xmm3, %xmm3 ## xmm3 = xmm3[0,1],mem[0],xmm3[3]
	vinsertps	$16, 84(%r13,%rcx,4), %xmm5, %xmm5 ## xmm5 = xmm5[0],mem[0],xmm5[2,3]
	vinsertps	$48, 84(%r13,%rsi,4), %xmm3, %xmm3 ## xmm3 = xmm3[0,1,2],mem[0]
	vinsertps	$32, 84(%r13,%rdi,4), %xmm5, %xmm5 ## xmm5 = xmm5[0,1],mem[0],xmm5[3]
	vinsertps	$48, 84(%r13,%rax,4), %xmm5, %xmm5 ## xmm5 = xmm5[0,1,2],mem[0]
	vinsertf128	$1, %xmm3, %ymm5, %ymm3
	movq	272(%rsp), %rbx                 ## 8-byte Reload
	vmovss	8(%r13,%rbx,4), %xmm5           ## xmm5 = mem[0],zero,zero,zero
	movq	144(%rsp), %rax                 ## 8-byte Reload
	vinsertps	$32, 4(%r13,%rax,4), %xmm7, %xmm6 ## xmm6 = xmm7[0,1],mem[0],xmm7[3]
	movq	280(%rsp), %rsi                 ## 8-byte Reload
	vinsertps	$16, 8(%r13,%rsi,4), %xmm5, %xmm5 ## xmm5 = xmm5[0],mem[0],xmm5[2,3]
	movq	288(%rsp), %rdx                 ## 8-byte Reload
	vmovaps	864(%rsp), %xmm4                ## 16-byte Reload
	vinsertps	$48, (%r13,%rdx,4), %xmm4, %xmm7 ## xmm7 = xmm4[0,1,2],mem[0]
	movq	88(%rsp), %rcx                  ## 8-byte Reload
	vinsertps	$32, 8(%r13,%rcx,4), %xmm5, %xmm5 ## xmm5 = xmm5[0,1],mem[0],xmm5[3]
	movq	72(%rsp), %rdi                  ## 8-byte Reload
	vmovaps	800(%rsp), %xmm4                ## 16-byte Reload
	vinsertps	$48, (%r13,%rdi,4), %xmm4, %xmm8 ## xmm8 = xmm4[0,1,2],mem[0]
	movq	80(%rsp), %r9                   ## 8-byte Reload
	vinsertps	$16, 8(%r13,%r9,4), %xmm1, %xmm1 ## xmm1 = xmm1[0],mem[0],xmm1[2,3]
	vinsertps	$48, 4(%r13,%rdx,4), %xmm12, %xmm4 ## xmm4 = xmm12[0,1,2],mem[0]
	vinsertps	$32, 8(%r13,%rax,4), %xmm1, %xmm1 ## xmm1 = xmm1[0,1],mem[0],xmm1[3]
	vblendvps	%ymm0, %ymm2, %ymm3, %ymm0
	vmovups	%ymm0, 1792(%rsp)               ## 32-byte Spill
	vmovss	12(%r13,%rbx,4), %xmm0          ## xmm0 = mem[0],zero,zero,zero
	vinsertps	$48, 4(%r13,%rdi,4), %xmm6, %xmm2 ## xmm2 = xmm6[0,1,2],mem[0]
	vinsertps	$16, 12(%r13,%rsi,4), %xmm0, %xmm0 ## xmm0 = xmm0[0],mem[0],xmm0[2,3]
	vinsertps	$48, 8(%r13,%rdx,4), %xmm5, %xmm3 ## xmm3 = xmm5[0,1,2],mem[0]
	vinsertps	$32, 12(%r13,%rcx,4), %xmm0, %xmm0 ## xmm0 = xmm0[0,1],mem[0],xmm0[3]
	vinsertps	$48, 8(%r13,%rdi,4), %xmm1, %xmm6 ## xmm6 = xmm1[0,1,2],mem[0]
	movq	432(%rsp), %rbp                 ## 8-byte Reload
	movq	448(%rsp), %rbx                 ## 8-byte Reload
	vmovups	(%rbx,%rbp,4), %ymm10
	vmovups	%ymm10, 1024(%rsp)              ## 32-byte Spill
	vinsertps	$48, 12(%r13,%rdx,4), %xmm0, %xmm0 ## xmm0 = xmm0[0,1,2],mem[0]
	vinsertf128	$1, %xmm7, %ymm8, %ymm7
	vmovss	12(%r13,%r15,4), %xmm5          ## xmm5 = mem[0],zero,zero,zero
	vinsertps	$16, 12(%r13,%r9,4), %xmm5, %xmm1 ## xmm1 = xmm5[0],mem[0],xmm5[2,3]
	vinsertf128	$1, %xmm4, %ymm2, %ymm2
	movq	480(%rsp), %r11                 ## 8-byte Reload
	vmovups	(%rbx,%r11,4), %ymm5
	vmovups	%ymm5, 768(%rsp)                ## 32-byte Spill
	vinsertps	$32, 12(%r13,%rax,4), %xmm1, %xmm1 ## xmm1 = xmm1[0,1],mem[0],xmm1[3]
	vmulps	%ymm2, %ymm5, %ymm2
	vinsertps	$48, 12(%r13,%rdi,4), %xmm1, %xmm1 ## xmm1 = xmm1[0,1,2],mem[0]
	movq	472(%rsp), %r15                 ## 8-byte Reload
	vmulps	(%rbx,%r15,4), %ymm10, %ymm4
	vfmadd231ps	%ymm7, %ymm4, %ymm2     ## ymm2 = (ymm4 * ymm7) + ymm2
	movq	24(%rsp), %r14                  ## 8-byte Reload
	vmovss	(%r13,%r14,4), %xmm7            ## xmm7 = mem[0],zero,zero,zero
	movq	64(%rsp), %rsi                  ## 8-byte Reload
	vinsertps	$16, (%r13,%rsi,4), %xmm7, %xmm7 ## xmm7 = xmm7[0],mem[0],xmm7[2,3]
	vinsertf128	$1, %xmm3, %ymm6, %ymm3
	movq	32(%rsp), %r9                   ## 8-byte Reload
	vmovss	(%r13,%r9,4), %xmm6             ## xmm6 = mem[0],zero,zero,zero
	movq	%r8, %rdx
	vinsertps	$16, (%r13,%r8,4), %xmm6, %xmm6 ## xmm6 = xmm6[0],mem[0],xmm6[2,3]
	vinsertf128	$1, %xmm0, %ymm1, %ymm0
	vmulps	%ymm0, %ymm5, %ymm0
	movq	440(%rsp), %rcx                 ## 8-byte Reload
	vcmpeqps	(%rbx,%rcx,4), %ymm15, %ymm1
	vfmadd231ps	%ymm3, %ymm4, %ymm0     ## ymm0 = (ymm4 * ymm3) + ymm0
	vmovss	4(%r13,%r14,4), %xmm3           ## xmm3 = mem[0],zero,zero,zero
	movq	104(%rsp), %rax                 ## 8-byte Reload
	vinsertps	$32, (%r13,%rax,4), %xmm7, %xmm4 ## xmm4 = xmm7[0,1],mem[0],xmm7[3]
	vinsertps	$16, 4(%r13,%rsi,4), %xmm3, %xmm3 ## xmm3 = xmm3[0],mem[0],xmm3[2,3]
	movq	%rsi, %r12
	vblendvps	%ymm1, %ymm2, %ymm0, %ymm0
	vmovups	%ymm0, 1824(%rsp)               ## 32-byte Spill
	vmovss	4(%r13,%r9,4), %xmm0            ## xmm0 = mem[0],zero,zero,zero
	movq	16(%rsp), %rsi                  ## 8-byte Reload
	vinsertps	$32, (%r13,%rsi,4), %xmm6, %xmm1 ## xmm1 = xmm6[0,1],mem[0],xmm6[3]
	vinsertps	$16, 4(%r13,%r8,4), %xmm0, %xmm0 ## xmm0 = xmm0[0],mem[0],xmm0[2,3]
	vinsertps	$32, 4(%r13,%rax,4), %xmm3, %xmm2 ## xmm2 = xmm3[0,1],mem[0],xmm3[3]
	vinsertps	$32, 4(%r13,%rsi,4), %xmm0, %xmm0 ## xmm0 = xmm0[0,1],mem[0],xmm0[3]
	movq	8(%rsp), %r8                    ## 8-byte Reload
	vinsertps	$48, (%r13,%r8,4), %xmm4, %xmm3 ## xmm3 = xmm4[0,1,2],mem[0]
	vmovss	8(%r13,%r14,4), %xmm4           ## xmm4 = mem[0],zero,zero,zero
	movq	40(%rsp), %rdi                  ## 8-byte Reload
	vinsertps	$48, (%r13,%rdi,4), %xmm1, %xmm1 ## xmm1 = xmm1[0,1,2],mem[0]
	vinsertps	$16, 8(%r13,%r12,4), %xmm4, %xmm4 ## xmm4 = xmm4[0],mem[0],xmm4[2,3]
	vinsertps	$48, 4(%r13,%r8,4), %xmm2, %xmm2 ## xmm2 = xmm2[0,1,2],mem[0]
	vinsertps	$32, 8(%r13,%rax,4), %xmm4, %xmm4 ## xmm4 = xmm4[0,1],mem[0],xmm4[3]
	vmovss	8(%r13,%r9,4), %xmm6            ## xmm6 = mem[0],zero,zero,zero
	vinsertps	$16, 8(%r13,%rdx,4), %xmm6, %xmm6 ## xmm6 = xmm6[0],mem[0],xmm6[2,3]
	vinsertps	$48, 4(%r13,%rdi,4), %xmm0, %xmm0 ## xmm0 = xmm0[0,1,2],mem[0]
	vinsertps	$32, 8(%r13,%rsi,4), %xmm6, %xmm6 ## xmm6 = xmm6[0,1],mem[0],xmm6[3]
	movq	%r8, %rcx
	vinsertps	$48, 8(%r13,%r8,4), %xmm4, %xmm4 ## xmm4 = xmm4[0,1,2],mem[0]
	vinsertf128	$1, %xmm3, %ymm1, %ymm1
	vinsertps	$48, 8(%r13,%rdi,4), %xmm6, %xmm3 ## xmm3 = xmm6[0,1,2],mem[0]
	vinsertf128	$1, %xmm2, %ymm0, %ymm0
	vmovss	12(%r13,%r14,4), %xmm2          ## xmm2 = mem[0],zero,zero,zero
	vinsertps	$16, 12(%r13,%r12,4), %xmm2, %xmm2 ## xmm2 = xmm2[0],mem[0],xmm2[2,3]
	vmovups	32(%rbx,%r11,4), %ymm13
	vmulps	%ymm0, %ymm13, %ymm12
	vmovups	32(%rbx,%rbp,4), %ymm0
	vmovups	%ymm0, 864(%rsp)                ## 32-byte Spill
	vmulps	32(%rbx,%r15,4), %ymm0, %ymm0
	vfmadd231ps	%ymm1, %ymm0, %ymm12    ## ymm12 = (ymm0 * ymm1) + ymm12
	vinsertps	$32, 12(%r13,%rax,4), %xmm2, %xmm1 ## xmm1 = xmm2[0,1],mem[0],xmm2[3]
	vmovss	12(%r13,%r9,4), %xmm2           ## xmm2 = mem[0],zero,zero,zero
	vinsertps	$16, 12(%r13,%rdx,4), %xmm2, %xmm2 ## xmm2 = xmm2[0],mem[0],xmm2[2,3]
	vinsertps	$48, 12(%r13,%r8,4), %xmm1, %xmm1 ## xmm1 = xmm1[0,1,2],mem[0]
	vinsertps	$32, 12(%r13,%rsi,4), %xmm2, %xmm2 ## xmm2 = xmm2[0,1],mem[0],xmm2[3]
	vinsertps	$48, 12(%r13,%rdi,4), %xmm2, %xmm2 ## xmm2 = xmm2[0,1,2],mem[0]
	vinsertf128	$1, %xmm4, %ymm3, %ymm3
	vinsertf128	$1, %xmm1, %ymm2, %ymm1
	vmulps	%ymm1, %ymm13, %ymm11
	vmovups	%ymm13, 992(%rsp)               ## 32-byte Spill
	vfmadd231ps	%ymm3, %ymm0, %ymm11    ## ymm11 = (ymm0 * ymm3) + ymm11
	movq	376(%rsp), %r8                  ## 8-byte Reload
	vmovss	(%r13,%r8,4), %xmm0             ## xmm0 = mem[0],zero,zero,zero
	movq	368(%rsp), %rsi                 ## 8-byte Reload
	vinsertps	$16, (%r13,%rsi,4), %xmm0, %xmm0 ## xmm0 = xmm0[0],mem[0],xmm0[2,3]
	movq	168(%rsp), %r9                  ## 8-byte Reload
	vmovss	(%r13,%r9,4), %xmm1             ## xmm1 = mem[0],zero,zero,zero
	movq	112(%rsp), %rax                 ## 8-byte Reload
	vinsertps	$16, (%r13,%rax,4), %xmm1, %xmm1 ## xmm1 = xmm1[0],mem[0],xmm1[2,3]
	movq	296(%rsp), %rdx                 ## 8-byte Reload
	vinsertps	$32, (%r13,%rdx,4), %xmm0, %xmm0 ## xmm0 = xmm0[0,1],mem[0],xmm0[3]
	movq	360(%rsp), %r14                 ## 8-byte Reload
	vinsertps	$32, (%r13,%r14,4), %xmm1, %xmm1 ## xmm1 = xmm1[0,1],mem[0],xmm1[3]
	movq	160(%rsp), %r10                 ## 8-byte Reload
	vinsertps	$48, (%r13,%r10,4), %xmm0, %xmm0 ## xmm0 = xmm0[0,1,2],mem[0]
	movq	392(%rsp), %rcx                 ## 8-byte Reload
	vmovss	(%r13,%rcx,4), %xmm2            ## xmm2 = mem[0],zero,zero,zero
	movq	384(%rsp), %r12                 ## 8-byte Reload
	vinsertps	$16, (%r13,%r12,4), %xmm2, %xmm2 ## xmm2 = xmm2[0],mem[0],xmm2[2,3]
	vmovss	4(%r13,%r8,4), %xmm3            ## xmm3 = mem[0],zero,zero,zero
	vinsertps	$16, 4(%r13,%rsi,4), %xmm3, %xmm3 ## xmm3 = xmm3[0],mem[0],xmm3[2,3]
	movq	120(%rsp), %rdi                 ## 8-byte Reload
	vinsertps	$48, (%r13,%rdi,4), %xmm1, %xmm1 ## xmm1 = xmm1[0,1,2],mem[0]
	vinsertf128	$1, %xmm0, %ymm1, %ymm0
	vinsertps	$32, 4(%r13,%rdx,4), %xmm3, %xmm1 ## xmm1 = xmm3[0,1],mem[0],xmm3[3]
	vmovss	4(%r13,%r9,4), %xmm3            ## xmm3 = mem[0],zero,zero,zero
	vinsertps	$16, 4(%r13,%rax,4), %xmm3, %xmm3 ## xmm3 = xmm3[0],mem[0],xmm3[2,3]
	vinsertps	$48, 4(%r13,%r10,4), %xmm1, %xmm1 ## xmm1 = xmm1[0,1,2],mem[0]
	vinsertps	$32, 4(%r13,%r14,4), %xmm3, %xmm3 ## xmm3 = xmm3[0,1],mem[0],xmm3[3]
	vinsertps	$48, 4(%r13,%rdi,4), %xmm3, %xmm3 ## xmm3 = xmm3[0,1,2],mem[0]
	vinsertf128	$1, %xmm1, %ymm3, %ymm1
	vmovss	8(%r13,%r8,4), %xmm3            ## xmm3 = mem[0],zero,zero,zero
	vinsertps	$16, 8(%r13,%rsi,4), %xmm3, %xmm3 ## xmm3 = xmm3[0],mem[0],xmm3[2,3]
	vinsertps	$32, 8(%r13,%rdx,4), %xmm3, %xmm3 ## xmm3 = xmm3[0,1],mem[0],xmm3[3]
	vinsertps	$48, 8(%r13,%r10,4), %xmm3, %xmm3 ## xmm3 = xmm3[0,1,2],mem[0]
	vmovss	8(%r13,%r9,4), %xmm4            ## xmm4 = mem[0],zero,zero,zero
	vinsertps	$16, 8(%r13,%rax,4), %xmm4, %xmm4 ## xmm4 = xmm4[0],mem[0],xmm4[2,3]
	vmovups	64(%rbx,%r11,4), %ymm5
	vmulps	%ymm1, %ymm5, %ymm7
	vmovaps	%ymm5, %ymm6
	vmovups	%ymm5, 1312(%rsp)               ## 32-byte Spill
	vmovups	64(%rbx,%rbp,4), %ymm1
	vmovups	%ymm1, 1344(%rsp)               ## 32-byte Spill
	vmulps	64(%rbx,%r15,4), %ymm1, %ymm8
	vfmadd231ps	%ymm0, %ymm8, %ymm7     ## ymm7 = (ymm8 * ymm0) + ymm7
	vinsertps	$32, 8(%r13,%r14,4), %xmm4, %xmm0 ## xmm0 = xmm4[0,1],mem[0],xmm4[3]
	vmovss	12(%r13,%r8,4), %xmm4           ## xmm4 = mem[0],zero,zero,zero
	vinsertps	$16, 12(%r13,%rsi,4), %xmm4, %xmm4 ## xmm4 = xmm4[0],mem[0],xmm4[2,3]
	vinsertps	$48, 8(%r13,%rdi,4), %xmm0, %xmm0 ## xmm0 = xmm0[0,1,2],mem[0]
	vinsertf128	$1, %xmm3, %ymm0, %ymm0
	vinsertps	$32, 12(%r13,%rdx,4), %xmm4, %xmm3 ## xmm3 = xmm4[0,1],mem[0],xmm4[3]
	vmovss	12(%r13,%r9,4), %xmm4           ## xmm4 = mem[0],zero,zero,zero
	vinsertps	$16, 12(%r13,%rax,4), %xmm4, %xmm4 ## xmm4 = xmm4[0],mem[0],xmm4[2,3]
	vinsertps	$48, 12(%r13,%r10,4), %xmm3, %xmm3 ## xmm3 = xmm3[0,1,2],mem[0]
	vinsertps	$32, 12(%r13,%r14,4), %xmm4, %xmm4 ## xmm4 = xmm4[0,1],mem[0],xmm4[3]
	vinsertps	$48, 12(%r13,%rdi,4), %xmm4, %xmm4 ## xmm4 = xmm4[0,1,2],mem[0]
	vinsertf128	$1, %xmm3, %ymm4, %ymm3
	movq	544(%rsp), %r9                  ## 8-byte Reload
	vinsertps	$32, (%r13,%r9,4), %xmm2, %xmm2 ## xmm2 = xmm2[0,1],mem[0],xmm2[3]
	movq	512(%rsp), %r10                 ## 8-byte Reload
	vmovss	(%r13,%r10,4), %xmm4            ## xmm4 = mem[0],zero,zero,zero
	movq	128(%rsp), %rbp                 ## 8-byte Reload
	vinsertps	$16, (%r13,%rbp,4), %xmm4, %xmm4 ## xmm4 = xmm4[0],mem[0],xmm4[2,3]
	movq	96(%rsp), %r8                   ## 8-byte Reload
	vinsertps	$48, (%r13,%r8,4), %xmm2, %xmm2 ## xmm2 = xmm2[0,1,2],mem[0]
	movq	136(%rsp), %rdi                 ## 8-byte Reload
	vinsertps	$32, (%r13,%rdi,4), %xmm4, %xmm4 ## xmm4 = xmm4[0,1],mem[0],xmm4[3]
	movq	400(%rsp), %rax                 ## 8-byte Reload
	vinsertps	$48, (%r13,%rax,4), %xmm4, %xmm4 ## xmm4 = xmm4[0,1,2],mem[0]
	movq	%rcx, %rdx
	vmovss	4(%r13,%rcx,4), %xmm5           ## xmm5 = mem[0],zero,zero,zero
	vinsertps	$16, 4(%r13,%r12,4), %xmm5, %xmm5 ## xmm5 = xmm5[0],mem[0],xmm5[2,3]
	vmulps	%ymm3, %ymm6, %ymm3
	vfmadd231ps	%ymm0, %ymm8, %ymm3     ## ymm3 = (ymm8 * ymm0) + ymm3
	vinsertps	$32, 4(%r13,%r9,4), %xmm5, %xmm0 ## xmm0 = xmm5[0,1],mem[0],xmm5[3]
	vmovss	4(%r13,%r10,4), %xmm5           ## xmm5 = mem[0],zero,zero,zero
	vinsertps	$16, 4(%r13,%rbp,4), %xmm5, %xmm5 ## xmm5 = xmm5[0],mem[0],xmm5[2,3]
	vinsertps	$48, 4(%r13,%r8,4), %xmm0, %xmm0 ## xmm0 = xmm0[0,1,2],mem[0]
	vinsertf128	$1, %xmm2, %ymm4, %ymm2
	vinsertps	$32, 4(%r13,%rdi,4), %xmm5, %xmm4 ## xmm4 = xmm5[0,1],mem[0],xmm5[3]
	vmovss	8(%r13,%rcx,4), %xmm5           ## xmm5 = mem[0],zero,zero,zero
	vinsertps	$16, 8(%r13,%r12,4), %xmm5, %xmm5 ## xmm5 = xmm5[0],mem[0],xmm5[2,3]
	vinsertps	$48, 4(%r13,%rax,4), %xmm4, %xmm4 ## xmm4 = xmm4[0,1,2],mem[0]
	vinsertf128	$1, %xmm0, %ymm4, %ymm0
	vinsertps	$32, 8(%r13,%r9,4), %xmm5, %xmm4 ## xmm4 = xmm5[0,1],mem[0],xmm5[3]
	vmovss	8(%r13,%r10,4), %xmm5           ## xmm5 = mem[0],zero,zero,zero
	vinsertps	$16, 8(%r13,%rbp,4), %xmm5, %xmm5 ## xmm5 = xmm5[0],mem[0],xmm5[2,3]
	vinsertps	$48, 8(%r13,%r8,4), %xmm4, %xmm10 ## xmm10 = xmm4[0,1,2],mem[0]
	vinsertps	$32, 8(%r13,%rdi,4), %xmm5, %xmm5 ## xmm5 = xmm5[0,1],mem[0],xmm5[3]
	vinsertps	$48, 8(%r13,%rax,4), %xmm5, %xmm5 ## xmm5 = xmm5[0,1,2],mem[0]
	vmovss	12(%r13,%rcx,4), %xmm6          ## xmm6 = mem[0],zero,zero,zero
	vinsertps	$16, 12(%r13,%r12,4), %xmm6, %xmm6 ## xmm6 = xmm6[0],mem[0],xmm6[2,3]
	vmovups	96(%rbx,%r11,4), %ymm4
	vmulps	%ymm0, %ymm4, %ymm0
	vmovups	%ymm4, 1408(%rsp)               ## 32-byte Spill
	movq	432(%rsp), %rcx                 ## 8-byte Reload
	vmovups	96(%rbx,%rcx,4), %ymm1
	vmovups	%ymm1, 1376(%rsp)               ## 32-byte Spill
	vmulps	96(%rbx,%r15,4), %ymm1, %ymm8
	vfmadd231ps	%ymm2, %ymm8, %ymm0     ## ymm0 = (ymm8 * ymm2) + ymm0
	vinsertps	$32, 12(%r13,%r9,4), %xmm6, %xmm2 ## xmm2 = xmm6[0,1],mem[0],xmm6[3]
	vmovss	12(%r13,%r10,4), %xmm6          ## xmm6 = mem[0],zero,zero,zero
	vinsertps	$16, 12(%r13,%rbp,4), %xmm6, %xmm6 ## xmm6 = xmm6[0],mem[0],xmm6[2,3]
	vinsertps	$48, 12(%r13,%r8,4), %xmm2, %xmm2 ## xmm2 = xmm2[0,1,2],mem[0]
	vinsertps	$32, 12(%r13,%rdi,4), %xmm6, %xmm6 ## xmm6 = xmm6[0,1],mem[0],xmm6[3]
	vinsertps	$48, 12(%r13,%rax,4), %xmm6, %xmm6 ## xmm6 = xmm6[0,1,2],mem[0]
	vinsertf128	$1, %xmm10, %ymm5, %ymm1
	vinsertf128	$1, %xmm2, %ymm6, %ymm2
	vmulps	%ymm2, %ymm4, %ymm2
	vfmadd231ps	%ymm1, %ymm8, %ymm2     ## ymm2 = (ymm8 * ymm1) + ymm2
	vxorps	%xmm5, %xmm5, %xmm5
	movq	440(%rsp), %rax                 ## 8-byte Reload
	vcmpeqps	32(%rbx,%rax,4), %ymm5, %ymm4
	vblendvps	%ymm4, %ymm12, %ymm11, %ymm1
	vmovups	%ymm1, 1696(%rsp)               ## 32-byte Spill
	vcmpeqps	64(%rbx,%rax,4), %ymm5, %ymm4
	vxorps	%xmm5, %xmm5, %xmm5
	vblendvps	%ymm4, %ymm7, %ymm3, %ymm1
	vmovups	%ymm1, 1760(%rsp)               ## 32-byte Spill
	vcmpeqps	96(%rbx,%rax,4), %ymm5, %ymm1
	movq	%rbx, 448(%rsp)                 ## 8-byte Spill
	vblendvps	%ymm1, %ymm0, %ymm2, %ymm0
	vmovups	%ymm0, 1728(%rsp)               ## 32-byte Spill
	movq	272(%rsp), %rdx                 ## 8-byte Reload
	vmovss	64(%r13,%rdx,4), %xmm0          ## xmm0 = mem[0],zero,zero,zero
	movq	280(%rsp), %rax                 ## 8-byte Reload
	vinsertps	$16, 64(%r13,%rax,4), %xmm0, %xmm0 ## xmm0 = xmm0[0],mem[0],xmm0[2,3]
	movq	56(%rsp), %r11                  ## 8-byte Reload
	vmovss	64(%r13,%r11,4), %xmm1          ## xmm1 = mem[0],zero,zero,zero
	movq	80(%rsp), %r10                  ## 8-byte Reload
	vinsertps	$16, 64(%r13,%r10,4), %xmm1, %xmm1 ## xmm1 = xmm1[0],mem[0],xmm1[2,3]
	movq	88(%rsp), %rsi                  ## 8-byte Reload
	vinsertps	$32, 64(%r13,%rsi,4), %xmm0, %xmm0 ## xmm0 = xmm0[0,1],mem[0],xmm0[3]
	movq	144(%rsp), %rcx                 ## 8-byte Reload
	vinsertps	$32, 64(%r13,%rcx,4), %xmm1, %xmm1 ## xmm1 = xmm1[0,1],mem[0],xmm1[3]
	movq	288(%rsp), %r9                  ## 8-byte Reload
	vinsertps	$48, 64(%r13,%r9,4), %xmm0, %xmm0 ## xmm0 = xmm0[0,1,2],mem[0]
	movq	72(%rsp), %rcx                  ## 8-byte Reload
	vinsertps	$48, 64(%r13,%rcx,4), %xmm1, %xmm1 ## xmm1 = xmm1[0,1,2],mem[0]
	vinsertf128	$1, %xmm0, %ymm1, %ymm0
	movq	200(%rsp), %rcx                 ## 8-byte Reload
	vmovups	(%rbx,%rcx,4), %ymm1
	vbroadcastss	LCPI1_0(%rip), %ymm3    ## ymm3 = [1.0E+0,1.0E+0,1.0E+0,1.0E+0,1.0E+0,1.0E+0,1.0E+0,1.0E+0]
	vcmpltps	%ymm1, %ymm3, %ymm2
	vmovaps	%ymm3, %ymm12
	vmovaps	%ymm1, %ymm8
	vmovups	1120(%rsp), %ymm1               ## 32-byte Reload
	vblendvps	%ymm2, %ymm0, %ymm1, %ymm0
	vmovaps	%ymm2, %ymm7
	movq	24(%rsp), %r14                  ## 8-byte Reload
	vmovss	64(%r13,%r14,4), %xmm1          ## xmm1 = mem[0],zero,zero,zero
	movq	64(%rsp), %r8                   ## 8-byte Reload
	vinsertps	$16, 64(%r13,%r8,4), %xmm1, %xmm1 ## xmm1 = xmm1[0],mem[0],xmm1[2,3]
	movq	104(%rsp), %r15                 ## 8-byte Reload
	vinsertps	$32, 64(%r13,%r15,4), %xmm1, %xmm1 ## xmm1 = xmm1[0,1],mem[0],xmm1[3]
	movq	32(%rsp), %rbp                  ## 8-byte Reload
	vmovss	64(%r13,%rbp,4), %xmm2          ## xmm2 = mem[0],zero,zero,zero
	movq	(%rsp), %rdi                    ## 8-byte Reload
	vinsertps	$16, 64(%r13,%rdi,4), %xmm2, %xmm2 ## xmm2 = xmm2[0],mem[0],xmm2[2,3]
	movq	8(%rsp), %r12                   ## 8-byte Reload
	vinsertps	$48, 64(%r13,%r12,4), %xmm1, %xmm1 ## xmm1 = xmm1[0,1,2],mem[0]
	movq	16(%rsp), %rcx                  ## 8-byte Reload
	vinsertps	$32, 64(%r13,%rcx,4), %xmm2, %xmm2 ## xmm2 = xmm2[0,1],mem[0],xmm2[3]
	vmovss	76(%r13,%rdx,4), %xmm3          ## xmm3 = mem[0],zero,zero,zero
	vinsertps	$16, 76(%r13,%rax,4), %xmm3, %xmm3 ## xmm3 = xmm3[0],mem[0],xmm3[2,3]
	movq	40(%rsp), %rbx                  ## 8-byte Reload
	vinsertps	$48, 64(%r13,%rbx,4), %xmm2, %xmm2 ## xmm2 = xmm2[0,1,2],mem[0]
	vinsertf128	$1, %xmm1, %ymm2, %ymm1
	vinsertps	$32, 76(%r13,%rsi,4), %xmm3, %xmm2 ## xmm2 = xmm3[0,1],mem[0],xmm3[3]
	vmovss	76(%r13,%r11,4), %xmm3          ## xmm3 = mem[0],zero,zero,zero
	vinsertps	$16, 76(%r13,%r10,4), %xmm3, %xmm3 ## xmm3 = xmm3[0],mem[0],xmm3[2,3]
	vinsertps	$48, 76(%r13,%r9,4), %xmm2, %xmm2 ## xmm2 = xmm2[0,1,2],mem[0]
	movq	144(%rsp), %rbx                 ## 8-byte Reload
	vinsertps	$32, 76(%r13,%rbx,4), %xmm3, %xmm3 ## xmm3 = xmm3[0,1],mem[0],xmm3[3]
	movq	72(%rsp), %rbx                  ## 8-byte Reload
	vinsertps	$48, 76(%r13,%rbx,4), %xmm3, %xmm3 ## xmm3 = xmm3[0,1,2],mem[0]
	vmovss	76(%r13,%r14,4), %xmm4          ## xmm4 = mem[0],zero,zero,zero
	vinsertps	$16, 76(%r13,%r8,4), %xmm4, %xmm4 ## xmm4 = xmm4[0],mem[0],xmm4[2,3]
	vmovss	76(%r13,%rbp,4), %xmm5          ## xmm5 = mem[0],zero,zero,zero
	vinsertps	$16, 76(%r13,%rdi,4), %xmm5, %xmm5 ## xmm5 = xmm5[0],mem[0],xmm5[2,3]
	vinsertps	$32, 76(%r13,%r15,4), %xmm4, %xmm4 ## xmm4 = xmm4[0,1],mem[0],xmm4[3]
	vinsertps	$32, 76(%r13,%rcx,4), %xmm5, %xmm5 ## xmm5 = xmm5[0,1],mem[0],xmm5[3]
	vinsertps	$48, 76(%r13,%r12,4), %xmm4, %xmm4 ## xmm4 = xmm4[0,1,2],mem[0]
	vinsertf128	$1, %xmm2, %ymm3, %ymm2
	movq	40(%rsp), %r14                  ## 8-byte Reload
	vinsertps	$48, 76(%r13,%r14,4), %xmm5, %xmm3 ## xmm3 = xmm5[0,1,2],mem[0]
	vinsertf128	$1, %xmm4, %ymm3, %ymm3
	movq	448(%rsp), %rbx                 ## 8-byte Reload
	movq	200(%rsp), %rcx                 ## 8-byte Reload
	vmovups	32(%rbx,%rcx,4), %ymm4
	vcmpltps	%ymm4, %ymm12, %ymm5
	vmovaps	%ymm4, %ymm6
	vblendvps	%ymm5, %ymm1, %ymm14, %ymm1
	vmovups	960(%rsp), %ymm4                ## 32-byte Reload
	vblendvps	%ymm7, %ymm2, %ymm4, %ymm15
	vmovaps	%ymm7, %ymm14
	vmovups	%ymm7, 1888(%rsp)               ## 32-byte Spill
	vmovups	1088(%rsp), %ymm2               ## 32-byte Reload
	vblendvps	%ymm5, %ymm3, %ymm2, %ymm10
	vmovaps	%ymm5, %ymm2
	vmovups	%ymm5, 1120(%rsp)               ## 32-byte Spill
	vmovss	92(%r13,%rdx,4), %xmm3          ## xmm3 = mem[0],zero,zero,zero
	vinsertps	$16, 92(%r13,%rax,4), %xmm3, %xmm3 ## xmm3 = xmm3[0],mem[0],xmm3[2,3]
	vinsertps	$32, 92(%r13,%rsi,4), %xmm3, %xmm3 ## xmm3 = xmm3[0,1],mem[0],xmm3[3]
	vmovss	92(%r13,%r11,4), %xmm4          ## xmm4 = mem[0],zero,zero,zero
	vinsertps	$16, 92(%r13,%r10,4), %xmm4, %xmm4 ## xmm4 = xmm4[0],mem[0],xmm4[2,3]
	vinsertps	$48, 92(%r13,%r9,4), %xmm3, %xmm3 ## xmm3 = xmm3[0,1,2],mem[0]
	movq	144(%rsp), %rcx                 ## 8-byte Reload
	vinsertps	$32, 92(%r13,%rcx,4), %xmm4, %xmm4 ## xmm4 = xmm4[0,1],mem[0],xmm4[3]
	vmovss	88(%r13,%rdx,4), %xmm5          ## xmm5 = mem[0],zero,zero,zero
	vinsertps	$16, 88(%r13,%rax,4), %xmm5, %xmm5 ## xmm5 = xmm5[0],mem[0],xmm5[2,3]
	movq	72(%rsp), %rax                  ## 8-byte Reload
	vinsertps	$48, 92(%r13,%rax,4), %xmm4, %xmm4 ## xmm4 = xmm4[0,1,2],mem[0]
	vinsertf128	$1, %xmm3, %ymm4, %ymm3
	vinsertps	$32, 88(%r13,%rsi,4), %xmm5, %xmm4 ## xmm4 = xmm5[0,1],mem[0],xmm5[3]
	vmovss	88(%r13,%r11,4), %xmm5          ## xmm5 = mem[0],zero,zero,zero
	vinsertps	$16, 88(%r13,%r10,4), %xmm5, %xmm5 ## xmm5 = xmm5[0],mem[0],xmm5[2,3]
	vinsertps	$48, 88(%r13,%r9,4), %xmm4, %xmm4 ## xmm4 = xmm4[0,1,2],mem[0]
	movq	%r9, %r10
	vinsertps	$32, 88(%r13,%rcx,4), %xmm5, %xmm5 ## xmm5 = xmm5[0,1],mem[0],xmm5[3]
	movq	328(%rsp), %r11                 ## 8-byte Reload
	vmulps	(%rbx,%r11,4), %ymm8, %ymm7
	movq	%r11, %r12
	vmulps	%ymm0, %ymm7, %ymm0
	vinsertps	$48, 88(%r13,%rax,4), %xmm5, %xmm5 ## xmm5 = xmm5[0,1,2],mem[0]
	movq	320(%rsp), %r8                  ## 8-byte Reload
	vmovups	(%rbx,%r8,4), %ymm7
	vmaxps	%ymm12, %ymm7, %ymm7
	vdivps	%ymm7, %ymm0, %ymm0
	vinsertf128	$1, %xmm4, %ymm5, %ymm4
	movq	336(%rsp), %rsi                 ## 8-byte Reload
	vfmadd132ps	(%rbx,%rsi,4), %ymm0, %ymm15 ## ymm15 = (ymm15 * mem) + ymm0
	vmovups	1024(%rsp), %ymm0               ## 32-byte Reload
	vaddps	768(%rsp), %ymm0, %ymm0         ## 32-byte Folded Reload
	vmulps	%ymm4, %ymm0, %ymm0
	vdivps	%ymm7, %ymm0, %ymm0
	vbroadcastss	LCPI1_16(%rip), %ymm5   ## ymm5 = [4.096E+3,4.096E+3,4.096E+3,4.096E+3,4.096E+3,4.096E+3,4.096E+3,4.096E+3]
	vdivps	%ymm7, %ymm5, %ymm4
	vmovaps	%ymm5, %ymm11
	vmovups	%ymm8, 1856(%rsp)               ## 32-byte Spill
	vminps	%ymm8, %ymm4, %ymm4
	movq	232(%rsp), %rax                 ## 8-byte Reload
	vmulps	(%rbx,%rax,4), %ymm4, %ymm4
	movq	24(%rsp), %rax                  ## 8-byte Reload
	vmovss	88(%r13,%rax,4), %xmm5          ## xmm5 = mem[0],zero,zero,zero
	movq	64(%rsp), %rbp                  ## 8-byte Reload
	vinsertps	$16, 88(%r13,%rbp,4), %xmm5, %xmm5 ## xmm5 = xmm5[0],mem[0],xmm5[2,3]
	vandps	%ymm0, %ymm14, %ymm0
	movq	192(%rsp), %rcx                 ## 8-byte Reload
	vmovups	(%rbx,%rcx,4), %ymm14
	vfmadd213ps	%ymm0, %ymm14, %ymm15   ## ymm15 = (ymm14 * ymm15) + ymm0
	movq	208(%rsp), %rcx                 ## 8-byte Reload
	vmulps	(%rbx,%rcx,4), %ymm8, %ymm0
	vmovups	%ymm0, 1056(%rsp)               ## 32-byte Spill
	vmulps	%ymm0, %ymm4, %ymm0
	vfmadd231ps	%ymm0, %ymm3, %ymm15    ## ymm15 = (ymm3 * ymm0) + ymm15
	vmovups	%ymm15, 1088(%rsp)              ## 32-byte Spill
	vinsertps	$32, 88(%r13,%r15,4), %xmm5, %xmm0 ## xmm0 = xmm5[0,1],mem[0],xmm5[3]
	movq	32(%rsp), %rdx                  ## 8-byte Reload
	vmovss	88(%r13,%rdx,4), %xmm3          ## xmm3 = mem[0],zero,zero,zero
	vinsertps	$16, 88(%r13,%rdi,4), %xmm3, %xmm3 ## xmm3 = xmm3[0],mem[0],xmm3[2,3]
	movq	8(%rsp), %r9                    ## 8-byte Reload
	vinsertps	$48, 88(%r13,%r9,4), %xmm0, %xmm0 ## xmm0 = xmm0[0,1,2],mem[0]
	movq	16(%rsp), %r11                  ## 8-byte Reload
	vinsertps	$32, 88(%r13,%r11,4), %xmm3, %xmm3 ## xmm3 = xmm3[0,1],mem[0],xmm3[3]
	vmulps	32(%rbx,%r12,4), %ymm6, %ymm4
	vmulps	%ymm1, %ymm4, %ymm1
	vinsertps	$48, 88(%r13,%r14,4), %xmm3, %xmm3 ## xmm3 = xmm3[0,1,2],mem[0]
	vmovups	32(%rbx,%r8,4), %ymm4
	vmaxps	%ymm12, %ymm4, %ymm5
	vdivps	%ymm5, %ymm1, %ymm1
	vinsertf128	$1, %xmm0, %ymm3, %ymm0
	vfmadd132ps	32(%rbx,%rsi,4), %ymm1, %ymm10 ## ymm10 = (ymm10 * mem) + ymm1
	vmovss	92(%r13,%rax,4), %xmm1          ## xmm1 = mem[0],zero,zero,zero
	vinsertps	$16, 92(%r13,%rbp,4), %xmm1, %xmm1 ## xmm1 = xmm1[0],mem[0],xmm1[2,3]
	vinsertps	$32, 92(%r13,%r15,4), %xmm1, %xmm4 ## xmm4 = xmm1[0,1],mem[0],xmm1[3]
	vmovss	92(%r13,%rdx,4), %xmm1          ## xmm1 = mem[0],zero,zero,zero
	vinsertps	$16, 92(%r13,%rdi,4), %xmm1, %xmm7 ## xmm7 = xmm1[0],mem[0],xmm1[2,3]
	vaddps	864(%rsp), %ymm13, %ymm1        ## 32-byte Folded Reload
	vmulps	%ymm0, %ymm1, %ymm0
	vdivps	%ymm5, %ymm0, %ymm3
	vdivps	%ymm5, %ymm11, %ymm0
	movq	272(%rsp), %rax                 ## 8-byte Reload
	vmovss	20(%r13,%rax,4), %xmm1          ## xmm1 = mem[0],zero,zero,zero
	movq	280(%rsp), %r15                 ## 8-byte Reload
	vinsertps	$16, 20(%r13,%r15,4), %xmm1, %xmm1 ## xmm1 = xmm1[0],mem[0],xmm1[2,3]
	vinsertps	$32, 92(%r13,%r11,4), %xmm7, %xmm5 ## xmm5 = xmm7[0,1],mem[0],xmm7[3]
	movq	88(%rsp), %rcx                  ## 8-byte Reload
	vinsertps	$32, 20(%r13,%rcx,4), %xmm1, %xmm1 ## xmm1 = xmm1[0,1],mem[0],xmm1[3]
	vinsertps	$48, 92(%r13,%r9,4), %xmm4, %xmm7 ## xmm7 = xmm4[0,1,2],mem[0]
	vinsertps	$48, 20(%r13,%r10,4), %xmm1, %xmm4 ## xmm4 = xmm1[0,1,2],mem[0]
	vinsertps	$48, 92(%r13,%r14,4), %xmm5, %xmm1 ## xmm1 = xmm5[0,1,2],mem[0]
	movq	56(%rsp), %r8                   ## 8-byte Reload
	vmovss	20(%r13,%r8,4), %xmm5           ## xmm5 = mem[0],zero,zero,zero
	movq	80(%rsp), %rdx                  ## 8-byte Reload
	vinsertps	$16, 20(%r13,%rdx,4), %xmm5, %xmm5 ## xmm5 = xmm5[0],mem[0],xmm5[2,3]
	vmovups	%ymm6, 1664(%rsp)               ## 32-byte Spill
	vminps	%ymm6, %ymm0, %ymm0
	movq	144(%rsp), %r11                 ## 8-byte Reload
	vinsertps	$32, 20(%r13,%r11,4), %xmm5, %xmm5 ## xmm5 = xmm5[0,1],mem[0],xmm5[3]
	movq	232(%rsp), %rsi                 ## 8-byte Reload
	vmulps	32(%rbx,%rsi,4), %ymm0, %ymm8
	vinsertf128	$1, %xmm7, %ymm1, %ymm7
	movq	72(%rsp), %r12                  ## 8-byte Reload
	vinsertps	$48, 20(%r13,%r12,4), %xmm5, %xmm0 ## xmm0 = xmm5[0,1,2],mem[0]
	vandps	%ymm3, %ymm2, %ymm3
	vmovss	24(%r13,%rax,4), %xmm1          ## xmm1 = mem[0],zero,zero,zero
	vinsertps	$16, 24(%r13,%r15,4), %xmm1, %xmm1 ## xmm1 = xmm1[0],mem[0],xmm1[2,3]
	movq	192(%rsp), %rsi                 ## 8-byte Reload
	vmovups	32(%rbx,%rsi,4), %ymm2
	vmovups	%ymm2, 960(%rsp)                ## 32-byte Spill
	vfmadd213ps	%ymm3, %ymm2, %ymm10    ## ymm10 = (ymm2 * ymm10) + ymm3
	movq	208(%rsp), %rsi                 ## 8-byte Reload
	vmulps	32(%rbx,%rsi,4), %ymm6, %ymm2
	vmovups	%ymm2, 1536(%rsp)               ## 32-byte Spill
	vmulps	%ymm2, %ymm8, %ymm5
	vfmadd231ps	%ymm5, %ymm7, %ymm10    ## ymm10 = (ymm7 * ymm5) + ymm10
	vmovups	%ymm10, 800(%rsp)               ## 32-byte Spill
	vinsertps	$32, 24(%r13,%rcx,4), %xmm1, %xmm1 ## xmm1 = xmm1[0,1],mem[0],xmm1[3]
	vmovss	24(%r13,%r8,4), %xmm5           ## xmm5 = mem[0],zero,zero,zero
	vinsertps	$16, 24(%r13,%rdx,4), %xmm5, %xmm5 ## xmm5 = xmm5[0],mem[0],xmm5[2,3]
	vinsertps	$48, 24(%r13,%r10,4), %xmm1, %xmm1 ## xmm1 = xmm1[0,1,2],mem[0]
	vinsertps	$32, 24(%r13,%r11,4), %xmm5, %xmm5 ## xmm5 = xmm5[0,1],mem[0],xmm5[3]
	vinsertps	$48, 24(%r13,%r12,4), %xmm5, %xmm5 ## xmm5 = xmm5[0,1,2],mem[0]
	vinsertf128	$1, %xmm4, %ymm0, %ymm0
	vinsertf128	$1, %xmm1, %ymm5, %ymm1
	vmovups	%ymm14, 1568(%rsp)              ## 32-byte Spill
	movq	264(%rsp), %rdi                 ## 8-byte Reload
	vmulps	(%rbx,%rdi,4), %ymm14, %ymm4
	vmulps	%ymm4, %ymm1, %ymm10
	vmovss	32(%r13,%rax,4), %xmm1          ## xmm1 = mem[0],zero,zero,zero
	movq	%r15, %rsi
	vinsertps	$16, 32(%r13,%r15,4), %xmm1, %xmm1 ## xmm1 = xmm1[0],mem[0],xmm1[2,3]
	vmovss	28(%r13,%rax,4), %xmm4          ## xmm4 = mem[0],zero,zero,zero
	vinsertps	$16, 28(%r13,%r15,4), %xmm4, %xmm4 ## xmm4 = xmm4[0],mem[0],xmm4[2,3]
	movq	%r8, %rsi
	vmovss	28(%r13,%r8,4), %xmm5           ## xmm5 = mem[0],zero,zero,zero
	movq	%rdx, %rdi
	vinsertps	$16, 28(%r13,%rdx,4), %xmm5, %xmm5 ## xmm5 = xmm5[0],mem[0],xmm5[2,3]
	vinsertps	$32, 28(%r13,%rcx,4), %xmm4, %xmm4 ## xmm4 = xmm4[0,1],mem[0],xmm4[3]
	movq	%r11, %rdx
	vinsertps	$32, 28(%r13,%r11,4), %xmm5, %xmm5 ## xmm5 = xmm5[0,1],mem[0],xmm5[3]
	vinsertps	$48, 28(%r13,%r10,4), %xmm4, %xmm4 ## xmm4 = xmm4[0,1,2],mem[0]
	vinsertps	$48, 28(%r13,%r12,4), %xmm5, %xmm5 ## xmm5 = xmm5[0,1,2],mem[0]
	movq	256(%rsp), %rax                 ## 8-byte Reload
	vmulps	(%rbx,%rax,4), %ymm14, %ymm7
	vfmadd231ps	%ymm7, %ymm0, %ymm10    ## ymm10 = (ymm0 * ymm7) + ymm10
	movq	376(%rsp), %r9                  ## 8-byte Reload
	vmovss	64(%r13,%r9,4), %xmm0           ## xmm0 = mem[0],zero,zero,zero
	movq	368(%rsp), %rax                 ## 8-byte Reload
	vinsertps	$16, 64(%r13,%rax,4), %xmm0, %xmm0 ## xmm0 = xmm0[0],mem[0],xmm0[2,3]
	vinsertf128	$1, %xmm4, %ymm5, %ymm4
	movq	168(%rsp), %r11                 ## 8-byte Reload
	vmovss	64(%r13,%r11,4), %xmm5          ## xmm5 = mem[0],zero,zero,zero
	movq	112(%rsp), %rbp                 ## 8-byte Reload
	vinsertps	$16, 64(%r13,%rbp,4), %xmm5, %xmm5 ## xmm5 = xmm5[0],mem[0],xmm5[2,3]
	vinsertps	$32, 32(%r13,%rcx,4), %xmm1, %xmm1 ## xmm1 = xmm1[0,1],mem[0],xmm1[3]
	vmovups	1024(%rsp), %ymm3               ## 32-byte Reload
	movq	248(%rsp), %rcx                 ## 8-byte Reload
	vmulps	(%rbx,%rcx,4), %ymm3, %ymm7
	vfmadd231ps	%ymm7, %ymm4, %ymm10    ## ymm10 = (ymm4 * ymm7) + ymm10
	vinsertps	$48, 32(%r13,%r10,4), %xmm1, %xmm1 ## xmm1 = xmm1[0,1,2],mem[0]
	vmovss	32(%r13,%r8,4), %xmm4           ## xmm4 = mem[0],zero,zero,zero
	vinsertps	$16, 32(%r13,%rdi,4), %xmm4, %xmm4 ## xmm4 = xmm4[0],mem[0],xmm4[2,3]
	movq	%rdi, %rsi
	movq	296(%rsp), %r10                 ## 8-byte Reload
	vinsertps	$32, 64(%r13,%r10,4), %xmm0, %xmm0 ## xmm0 = xmm0[0,1],mem[0],xmm0[3]
	vinsertps	$32, 32(%r13,%rdx,4), %xmm4, %xmm4 ## xmm4 = xmm4[0,1],mem[0],xmm4[3]
	movq	360(%rsp), %rcx                 ## 8-byte Reload
	vinsertps	$32, 64(%r13,%rcx,4), %xmm5, %xmm5 ## xmm5 = xmm5[0,1],mem[0],xmm5[3]
	vinsertps	$48, 32(%r13,%r12,4), %xmm4, %xmm4 ## xmm4 = xmm4[0,1,2],mem[0]
	movq	160(%rsp), %rdi                 ## 8-byte Reload
	vinsertps	$48, 64(%r13,%rdi,4), %xmm0, %xmm0 ## xmm0 = xmm0[0,1,2],mem[0]
	vinsertf128	$1, %xmm1, %ymm4, %ymm1
	movq	120(%rsp), %r14                 ## 8-byte Reload
	vinsertps	$48, 64(%r13,%r14,4), %xmm5, %xmm4 ## xmm4 = xmm5[0,1,2],mem[0]
	vinsertf128	$1, %xmm0, %ymm4, %ymm0
	movq	200(%rsp), %r8                  ## 8-byte Reload
	vmovups	64(%rbx,%r8,4), %ymm8
	vcmpltps	%ymm8, %ymm12, %ymm4
	vmovups	832(%rsp), %ymm2                ## 32-byte Reload
	vblendvps	%ymm4, %ymm0, %ymm2, %ymm0
	vmovaps	%ymm4, %ymm13
	vmovss	76(%r13,%r9,4), %xmm4           ## xmm4 = mem[0],zero,zero,zero
	vinsertps	$16, 76(%r13,%rax,4), %xmm4, %xmm4 ## xmm4 = xmm4[0],mem[0],xmm4[2,3]
	vmovss	76(%r13,%r11,4), %xmm5          ## xmm5 = mem[0],zero,zero,zero
	movq	%r11, %r8
	vinsertps	$16, 76(%r13,%rbp,4), %xmm5, %xmm5 ## xmm5 = xmm5[0],mem[0],xmm5[2,3]
	movq	%rbp, %rdx
	vinsertps	$32, 76(%r13,%r10,4), %xmm4, %xmm4 ## xmm4 = xmm4[0,1],mem[0],xmm4[3]
	vinsertps	$32, 76(%r13,%rcx,4), %xmm5, %xmm5 ## xmm5 = xmm5[0,1],mem[0],xmm5[3]
	movq	%rcx, %r11
	vinsertps	$48, 76(%r13,%rdi,4), %xmm4, %xmm4 ## xmm4 = xmm4[0,1,2],mem[0]
	vinsertps	$48, 76(%r13,%r14,4), %xmm5, %xmm5 ## xmm5 = xmm5[0,1,2],mem[0]
	movq	%r14, %r15
	vmovups	768(%rsp), %ymm6                ## 32-byte Reload
	movq	184(%rsp), %rcx                 ## 8-byte Reload
	vmulps	(%rbx,%rcx,4), %ymm6, %ymm7
	vfmadd231ps	%ymm7, %ymm1, %ymm10    ## ymm10 = (ymm1 * ymm7) + ymm10
	vinsertf128	$1, %xmm4, %ymm5, %ymm1
	vmovups	928(%rsp), %ymm2                ## 32-byte Reload
	vblendvps	%ymm13, %ymm1, %ymm2, %ymm15
	vmovups	%ymm13, 1600(%rsp)              ## 32-byte Spill
	movq	272(%rsp), %rcx                 ## 8-byte Reload
	vmovss	36(%r13,%rcx,4), %xmm1          ## xmm1 = mem[0],zero,zero,zero
	movq	280(%rsp), %r14                 ## 8-byte Reload
	vinsertps	$16, 36(%r13,%r14,4), %xmm1, %xmm1 ## xmm1 = xmm1[0],mem[0],xmm1[2,3]
	movq	56(%rsp), %rbp                  ## 8-byte Reload
	vmovss	36(%r13,%rbp,4), %xmm4          ## xmm4 = mem[0],zero,zero,zero
	vinsertps	$16, 36(%r13,%rsi,4), %xmm4, %xmm4 ## xmm4 = xmm4[0],mem[0],xmm4[2,3]
	movq	88(%rsp), %rsi                  ## 8-byte Reload
	vinsertps	$32, 36(%r13,%rsi,4), %xmm1, %xmm1 ## xmm1 = xmm1[0,1],mem[0],xmm1[3]
	movq	144(%rsp), %rbp                 ## 8-byte Reload
	vinsertps	$32, 36(%r13,%rbp,4), %xmm4, %xmm4 ## xmm4 = xmm4[0,1],mem[0],xmm4[3]
	movq	288(%rsp), %rsi                 ## 8-byte Reload
	vinsertps	$48, 36(%r13,%rsi,4), %xmm1, %xmm1 ## xmm1 = xmm1[0,1,2],mem[0]
	vinsertps	$48, 36(%r13,%r12,4), %xmm4, %xmm4 ## xmm4 = xmm4[0,1,2],mem[0]
	vinsertf128	$1, %xmm1, %ymm4, %ymm1
	vmovss	88(%r13,%r9,4), %xmm4           ## xmm4 = mem[0],zero,zero,zero
	movq	%rax, %rbp
	vinsertps	$16, 88(%r13,%rax,4), %xmm4, %xmm4 ## xmm4 = xmm4[0],mem[0],xmm4[2,3]
	movq	240(%rsp), %rax                 ## 8-byte Reload
	vmulps	(%rbx,%rax,4), %ymm3, %ymm5
	vfmadd231ps	%ymm5, %ymm1, %ymm10    ## ymm10 = (ymm1 * ymm5) + ymm10
	vinsertps	$32, 88(%r13,%r10,4), %xmm4, %xmm1 ## xmm1 = xmm4[0,1],mem[0],xmm4[3]
	movq	%r8, %rax
	vmovss	88(%r13,%r8,4), %xmm4           ## xmm4 = mem[0],zero,zero,zero
	vinsertps	$16, 88(%r13,%rdx,4), %xmm4, %xmm4 ## xmm4 = xmm4[0],mem[0],xmm4[2,3]
	vinsertps	$48, 88(%r13,%rdi,4), %xmm1, %xmm1 ## xmm1 = xmm1[0,1,2],mem[0]
	vinsertps	$32, 88(%r13,%r11,4), %xmm4, %xmm4 ## xmm4 = xmm4[0,1],mem[0],xmm4[3]
	vinsertps	$48, 88(%r13,%r15,4), %xmm4, %xmm4 ## xmm4 = xmm4[0,1,2],mem[0]
	vinsertf128	$1, %xmm1, %ymm4, %ymm1
	vmovss	92(%r13,%r9,4), %xmm4           ## xmm4 = mem[0],zero,zero,zero
	vinsertps	$16, 92(%r13,%rbp,4), %xmm4, %xmm4 ## xmm4 = xmm4[0],mem[0],xmm4[2,3]
	vinsertps	$32, 92(%r13,%r10,4), %xmm4, %xmm4 ## xmm4 = xmm4[0,1],mem[0],xmm4[3]
	vinsertps	$48, 92(%r13,%rdi,4), %xmm4, %xmm4 ## xmm4 = xmm4[0,1,2],mem[0]
	vmovss	92(%r13,%r8,4), %xmm5           ## xmm5 = mem[0],zero,zero,zero
	vinsertps	$16, 92(%r13,%rdx,4), %xmm5, %xmm5 ## xmm5 = xmm5[0],mem[0],xmm5[2,3]
	vmovups	1312(%rsp), %ymm2               ## 32-byte Reload
	vaddps	1344(%rsp), %ymm2, %ymm7        ## 32-byte Folded Reload
	vmulps	%ymm1, %ymm7, %ymm1
	vinsertps	$32, 92(%r13,%r11,4), %xmm5, %xmm5 ## xmm5 = xmm5[0,1],mem[0],xmm5[3]
	movq	320(%rsp), %rax                 ## 8-byte Reload
	vmovups	64(%rbx,%rax,4), %ymm7
	vmaxps	%ymm12, %ymm7, %ymm7
	vdivps	%ymm7, %ymm1, %ymm1
	vinsertps	$48, 92(%r13,%r15,4), %xmm5, %xmm5 ## xmm5 = xmm5[0,1,2],mem[0]
	movq	328(%rsp), %rax                 ## 8-byte Reload
	vmovups	%ymm8, 1632(%rsp)               ## 32-byte Spill
	vmulps	64(%rbx,%rax,4), %ymm8, %ymm14
	vmulps	%ymm0, %ymm14, %ymm0
	vdivps	%ymm7, %ymm0, %ymm0
	vinsertf128	$1, %xmm4, %ymm5, %ymm4
	vdivps	%ymm7, %ymm11, %ymm5
	vmovaps	%ymm11, %ymm7
	movq	336(%rsp), %rax                 ## 8-byte Reload
	vmovaps	%ymm15, %ymm2
	vfmadd132ps	64(%rbx,%rax,4), %ymm0, %ymm2 ## ymm2 = (ymm2 * mem) + ymm0
	vandps	%ymm1, %ymm13, %ymm0
	movq	192(%rsp), %rax                 ## 8-byte Reload
	vmovups	64(%rbx,%rax,4), %ymm1
	vmovups	%ymm1, 928(%rsp)                ## 32-byte Spill
	vfmadd213ps	%ymm0, %ymm1, %ymm2     ## ymm2 = (ymm1 * ymm2) + ymm0
	vminps	%ymm8, %ymm5, %ymm0
	movq	232(%rsp), %rax                 ## 8-byte Reload
	vmulps	64(%rbx,%rax,4), %ymm0, %ymm0
	movq	208(%rsp), %rax                 ## 8-byte Reload
	vmulps	64(%rbx,%rax,4), %ymm8, %ymm1
	vmovups	%ymm1, 1504(%rsp)               ## 32-byte Spill
	movq	%rbx, %r11
	vmulps	%ymm1, %ymm0, %ymm0
	vfmadd231ps	%ymm0, %ymm4, %ymm2     ## ymm2 = (ymm4 * ymm0) + ymm2
	vmovups	%ymm2, 832(%rsp)                ## 32-byte Spill
	movq	%rcx, %rbp
	vmovss	40(%r13,%rcx,4), %xmm0          ## xmm0 = mem[0],zero,zero,zero
	vinsertps	$16, 40(%r13,%r14,4), %xmm0, %xmm0 ## xmm0 = xmm0[0],mem[0],xmm0[2,3]
	movq	56(%rsp), %rax                  ## 8-byte Reload
	vmovss	40(%r13,%rax,4), %xmm1          ## xmm1 = mem[0],zero,zero,zero
	movq	80(%rsp), %r8                   ## 8-byte Reload
	vinsertps	$16, 40(%r13,%r8,4), %xmm1, %xmm1 ## xmm1 = xmm1[0],mem[0],xmm1[2,3]
	movq	88(%rsp), %rsi                  ## 8-byte Reload
	vinsertps	$32, 40(%r13,%rsi,4), %xmm0, %xmm0 ## xmm0 = xmm0[0,1],mem[0],xmm0[3]
	movq	144(%rsp), %r9                  ## 8-byte Reload
	vinsertps	$32, 40(%r13,%r9,4), %xmm1, %xmm1 ## xmm1 = xmm1[0,1],mem[0],xmm1[3]
	movq	288(%rsp), %rcx                 ## 8-byte Reload
	vinsertps	$48, 40(%r13,%rcx,4), %xmm0, %xmm0 ## xmm0 = xmm0[0,1,2],mem[0]
	vinsertps	$48, 40(%r13,%r12,4), %xmm1, %xmm1 ## xmm1 = xmm1[0,1,2],mem[0]
	vinsertf128	$1, %xmm0, %ymm1, %ymm0
	vmovss	44(%r13,%rbp,4), %xmm1          ## xmm1 = mem[0],zero,zero,zero
	vinsertps	$16, 44(%r13,%r14,4), %xmm1, %xmm1 ## xmm1 = xmm1[0],mem[0],xmm1[2,3]
	movq	224(%rsp), %rbx                 ## 8-byte Reload
	vmovups	(%r11,%rbx,4), %ymm4
	vmulps	%ymm4, %ymm6, %ymm5
	vfmadd231ps	%ymm5, %ymm0, %ymm10    ## ymm10 = (ymm0 * ymm5) + ymm10
	vinsertps	$32, 44(%r13,%rsi,4), %xmm1, %xmm0 ## xmm0 = xmm1[0,1],mem[0],xmm1[3]
	vmovss	44(%r13,%rax,4), %xmm1          ## xmm1 = mem[0],zero,zero,zero
	vinsertps	$16, 44(%r13,%r8,4), %xmm1, %xmm1 ## xmm1 = xmm1[0],mem[0],xmm1[2,3]
	vinsertps	$48, 44(%r13,%rcx,4), %xmm0, %xmm0 ## xmm0 = xmm0[0,1,2],mem[0]
	vinsertps	$32, 44(%r13,%r9,4), %xmm1, %xmm1 ## xmm1 = xmm1[0,1],mem[0],xmm1[3]
	vinsertps	$48, 44(%r13,%r12,4), %xmm1, %xmm1 ## xmm1 = xmm1[0,1,2],mem[0]
	vinsertf128	$1, %xmm0, %ymm1, %ymm0
	vmovss	52(%r13,%rbp,4), %xmm1          ## xmm1 = mem[0],zero,zero,zero
	vinsertps	$16, 52(%r13,%r14,4), %xmm1, %xmm1 ## xmm1 = xmm1[0],mem[0],xmm1[2,3]
	vmovss	48(%r13,%rbp,4), %xmm5          ## xmm5 = mem[0],zero,zero,zero
	vinsertps	$16, 48(%r13,%r14,4), %xmm5, %xmm5 ## xmm5 = xmm5[0],mem[0],xmm5[2,3]
	vmovaps	%ymm3, %ymm2
	vmulps	%ymm4, %ymm3, %ymm4
	vfmadd231ps	%ymm4, %ymm0, %ymm10    ## ymm10 = (ymm0 * ymm4) + ymm10
	vinsertps	$32, 48(%r13,%rsi,4), %xmm5, %xmm0 ## xmm0 = xmm5[0,1],mem[0],xmm5[3]
	vmovss	48(%r13,%rax,4), %xmm4          ## xmm4 = mem[0],zero,zero,zero
	vinsertps	$16, 48(%r13,%r8,4), %xmm4, %xmm4 ## xmm4 = xmm4[0],mem[0],xmm4[2,3]
	vinsertps	$48, 48(%r13,%rcx,4), %xmm0, %xmm0 ## xmm0 = xmm0[0,1,2],mem[0]
	vinsertps	$32, 48(%r13,%r9,4), %xmm4, %xmm4 ## xmm4 = xmm4[0,1],mem[0],xmm4[3]
	vinsertps	$48, 48(%r13,%r12,4), %xmm4, %xmm4 ## xmm4 = xmm4[0,1,2],mem[0]
	vinsertf128	$1, %xmm0, %ymm4, %ymm0
	vinsertps	$32, 52(%r13,%rsi,4), %xmm1, %xmm1 ## xmm1 = xmm1[0,1],mem[0],xmm1[3]
	vmovss	52(%r13,%rax,4), %xmm4          ## xmm4 = mem[0],zero,zero,zero
	vinsertps	$16, 52(%r13,%r8,4), %xmm4, %xmm4 ## xmm4 = xmm4[0],mem[0],xmm4[2,3]
	vinsertps	$48, 52(%r13,%rcx,4), %xmm1, %xmm1 ## xmm1 = xmm1[0,1,2],mem[0]
	vinsertps	$32, 52(%r13,%r9,4), %xmm4, %xmm4 ## xmm4 = xmm4[0,1],mem[0],xmm4[3]
	movq	216(%rsp), %rbx                 ## 8-byte Reload
	vmovups	(%r11,%rbx,4), %ymm5
	vmulps	%ymm5, %ymm6, %ymm13
	vfmadd231ps	%ymm13, %ymm0, %ymm10   ## ymm10 = (ymm0 * ymm13) + ymm10
	vinsertps	$48, 52(%r13,%r12,4), %xmm4, %xmm0 ## xmm0 = xmm4[0,1,2],mem[0]
	vinsertf128	$1, %xmm1, %ymm0, %ymm0
	vmulps	%ymm5, %ymm3, %ymm1
	vmovss	56(%r13,%rbp,4), %xmm4          ## xmm4 = mem[0],zero,zero,zero
	vinsertps	$16, 56(%r13,%r14,4), %xmm4, %xmm4 ## xmm4 = xmm4[0],mem[0],xmm4[2,3]
	vfmadd231ps	%ymm1, %ymm0, %ymm10    ## ymm10 = (ymm0 * ymm1) + ymm10
	vinsertps	$32, 56(%r13,%rsi,4), %xmm4, %xmm0 ## xmm0 = xmm4[0,1],mem[0],xmm4[3]
	vmovss	56(%r13,%rax,4), %xmm1          ## xmm1 = mem[0],zero,zero,zero
	vinsertps	$16, 56(%r13,%r8,4), %xmm1, %xmm1 ## xmm1 = xmm1[0],mem[0],xmm1[2,3]
	vinsertps	$48, 56(%r13,%rcx,4), %xmm0, %xmm0 ## xmm0 = xmm0[0,1,2],mem[0]
	vinsertps	$32, 56(%r13,%r9,4), %xmm1, %xmm1 ## xmm1 = xmm1[0,1],mem[0],xmm1[3]
	vinsertps	$48, 56(%r13,%r12,4), %xmm1, %xmm1 ## xmm1 = xmm1[0,1,2],mem[0]
	vinsertf128	$1, %xmm0, %ymm1, %ymm1
	movq	24(%rsp), %rdx                  ## 8-byte Reload
	vmovss	20(%r13,%rdx,4), %xmm0          ## xmm0 = mem[0],zero,zero,zero
	movq	64(%rsp), %rbp                  ## 8-byte Reload
	vinsertps	$16, 20(%r13,%rbp,4), %xmm0, %xmm4 ## xmm4 = xmm0[0],mem[0],xmm0[2,3]
	vmovups	1056(%rsp), %ymm0               ## 32-byte Reload
	vmaxps	%ymm12, %ymm0, %ymm15
	vmovaps	%ymm12, %ymm2
	movq	104(%rsp), %r12                 ## 8-byte Reload
	vinsertps	$32, 20(%r13,%r12,4), %xmm4, %xmm4 ## xmm4 = xmm4[0,1],mem[0],xmm4[3]
	movq	32(%rsp), %rdi                  ## 8-byte Reload
	vmovss	20(%r13,%rdi,4), %xmm5          ## xmm5 = mem[0],zero,zero,zero
	movq	(%rsp), %rsi                    ## 8-byte Reload
	vinsertps	$16, 20(%r13,%rsi,4), %xmm5, %xmm5 ## xmm5 = xmm5[0],mem[0],xmm5[2,3]
	movq	8(%rsp), %rcx                   ## 8-byte Reload
	vinsertps	$48, 20(%r13,%rcx,4), %xmm4, %xmm4 ## xmm4 = xmm4[0,1,2],mem[0]
	movq	16(%rsp), %r8                   ## 8-byte Reload
	vinsertps	$32, 20(%r13,%r8,4), %xmm5, %xmm5 ## xmm5 = xmm5[0,1],mem[0],xmm5[3]
	movq	352(%rsp), %rax                 ## 8-byte Reload
	vmulps	(%r11,%rax,4), %ymm15, %ymm11
	vfmadd231ps	%ymm11, %ymm1, %ymm10   ## ymm10 = (ymm1 * ymm11) + ymm10
	movq	40(%rsp), %r10                  ## 8-byte Reload
	vinsertps	$48, 20(%r13,%r10,4), %xmm5, %xmm1 ## xmm1 = xmm5[0,1,2],mem[0]
	vinsertf128	$1, %xmm4, %ymm1, %ymm1
	vmovss	24(%r13,%rdx,4), %xmm4          ## xmm4 = mem[0],zero,zero,zero
	vinsertps	$16, 24(%r13,%rbp,4), %xmm4, %xmm4 ## xmm4 = xmm4[0],mem[0],xmm4[2,3]
	vmovss	24(%r13,%rdi,4), %xmm5          ## xmm5 = mem[0],zero,zero,zero
	vinsertps	$16, 24(%r13,%rsi,4), %xmm5, %xmm5 ## xmm5 = xmm5[0],mem[0],xmm5[2,3]
	vinsertps	$32, 24(%r13,%r12,4), %xmm4, %xmm4 ## xmm4 = xmm4[0,1],mem[0],xmm4[3]
	vinsertps	$32, 24(%r13,%r8,4), %xmm5, %xmm5 ## xmm5 = xmm5[0,1],mem[0],xmm5[3]
	vinsertps	$48, 24(%r13,%rcx,4), %xmm4, %xmm4 ## xmm4 = xmm4[0,1,2],mem[0]
	vinsertps	$48, 24(%r13,%r10,4), %xmm5, %xmm5 ## xmm5 = xmm5[0,1,2],mem[0]
	vinsertf128	$1, %xmm4, %ymm5, %ymm4
	vmovss	28(%r13,%rdx,4), %xmm5          ## xmm5 = mem[0],zero,zero,zero
	vinsertps	$16, 28(%r13,%rbp,4), %xmm5, %xmm5 ## xmm5 = xmm5[0],mem[0],xmm5[2,3]
	vmovups	960(%rsp), %ymm0                ## 32-byte Reload
	movq	264(%rsp), %rax                 ## 8-byte Reload
	vmulps	32(%r11,%rax,4), %ymm0, %ymm11
	vmulps	%ymm4, %ymm11, %ymm13
	vinsertps	$32, 28(%r13,%r12,4), %xmm5, %xmm4 ## xmm4 = xmm5[0,1],mem[0],xmm5[3]
	vmovss	28(%r13,%rdi,4), %xmm5          ## xmm5 = mem[0],zero,zero,zero
	vinsertps	$16, 28(%r13,%rsi,4), %xmm5, %xmm5 ## xmm5 = xmm5[0],mem[0],xmm5[2,3]
	vinsertps	$48, 28(%r13,%rcx,4), %xmm4, %xmm4 ## xmm4 = xmm4[0,1,2],mem[0]
	vinsertps	$32, 28(%r13,%r8,4), %xmm5, %xmm5 ## xmm5 = xmm5[0,1],mem[0],xmm5[3]
	movq	256(%rsp), %rax                 ## 8-byte Reload
	vmulps	32(%r11,%rax,4), %ymm0, %ymm11
	vfmadd231ps	%ymm11, %ymm1, %ymm13   ## ymm13 = (ymm1 * ymm11) + ymm13
	vinsertps	$48, 28(%r13,%r10,4), %xmm5, %xmm1 ## xmm1 = xmm5[0,1,2],mem[0]
	vinsertf128	$1, %xmm4, %ymm1, %ymm1
	vmovss	36(%r13,%rdx,4), %xmm4          ## xmm4 = mem[0],zero,zero,zero
	vinsertps	$16, 36(%r13,%rbp,4), %xmm4, %xmm4 ## xmm4 = xmm4[0],mem[0],xmm4[2,3]
	vmovss	32(%r13,%rdx,4), %xmm5          ## xmm5 = mem[0],zero,zero,zero
	movq	%rdx, %rbx
	vinsertps	$16, 32(%r13,%rbp,4), %xmm5, %xmm5 ## xmm5 = xmm5[0],mem[0],xmm5[2,3]
	vmovups	864(%rsp), %ymm0                ## 32-byte Reload
	movq	248(%rsp), %rax                 ## 8-byte Reload
	vmulps	32(%r11,%rax,4), %ymm0, %ymm11
	vfmadd231ps	%ymm11, %ymm1, %ymm13   ## ymm13 = (ymm1 * ymm11) + ymm13
	vinsertps	$32, 32(%r13,%r12,4), %xmm5, %xmm1 ## xmm1 = xmm5[0,1],mem[0],xmm5[3]
	vmovss	32(%r13,%rdi,4), %xmm5          ## xmm5 = mem[0],zero,zero,zero
	vinsertps	$16, 32(%r13,%rsi,4), %xmm5, %xmm5 ## xmm5 = xmm5[0],mem[0],xmm5[2,3]
	vinsertps	$48, 32(%r13,%rcx,4), %xmm1, %xmm1 ## xmm1 = xmm1[0,1,2],mem[0]
	vinsertps	$32, 32(%r13,%r8,4), %xmm5, %xmm5 ## xmm5 = xmm5[0,1],mem[0],xmm5[3]
	movq	%r10, %rdx
	vinsertps	$48, 32(%r13,%r10,4), %xmm5, %xmm5 ## xmm5 = xmm5[0,1,2],mem[0]
	vinsertf128	$1, %xmm1, %ymm5, %ymm1
	vinsertps	$32, 36(%r13,%r12,4), %xmm4, %xmm4 ## xmm4 = xmm4[0,1],mem[0],xmm4[3]
	movq	%r12, %r10
	vmovss	36(%r13,%rdi,4), %xmm5          ## xmm5 = mem[0],zero,zero,zero
	vinsertps	$16, 36(%r13,%rsi,4), %xmm5, %xmm5 ## xmm5 = xmm5[0],mem[0],xmm5[2,3]
	vinsertps	$48, 36(%r13,%rcx,4), %xmm4, %xmm4 ## xmm4 = xmm4[0,1,2],mem[0]
	movq	%rcx, %rax
	vinsertps	$32, 36(%r13,%r8,4), %xmm5, %xmm5 ## xmm5 = xmm5[0,1],mem[0],xmm5[3]
	vmovups	992(%rsp), %ymm8                ## 32-byte Reload
	movq	184(%rsp), %rcx                 ## 8-byte Reload
	vmulps	32(%r11,%rcx,4), %ymm8, %ymm11
	vfmadd231ps	%ymm11, %ymm1, %ymm13   ## ymm13 = (ymm1 * ymm11) + ymm13
	vinsertps	$48, 36(%r13,%rdx,4), %xmm5, %xmm1 ## xmm1 = xmm5[0,1,2],mem[0]
	vinsertf128	$1, %xmm4, %ymm1, %ymm1
	movq	392(%rsp), %r12                 ## 8-byte Reload
	vmovss	92(%r13,%r12,4), %xmm4          ## xmm4 = mem[0],zero,zero,zero
	movq	384(%rsp), %rcx                 ## 8-byte Reload
	vinsertps	$16, 92(%r13,%rcx,4), %xmm4, %xmm4 ## xmm4 = xmm4[0],mem[0],xmm4[2,3]
	vmovss	40(%r13,%rbx,4), %xmm5          ## xmm5 = mem[0],zero,zero,zero
	vinsertps	$16, 40(%r13,%rbp,4), %xmm5, %xmm5 ## xmm5 = xmm5[0],mem[0],xmm5[2,3]
	movq	%rbp, %rbx
	movq	240(%rsp), %rbp                 ## 8-byte Reload
	vmulps	32(%r11,%rbp,4), %ymm0, %ymm11
	vfmadd231ps	%ymm11, %ymm1, %ymm13   ## ymm13 = (ymm1 * ymm11) + ymm13
	movq	%r10, %rbp
	vinsertps	$32, 40(%r13,%r10,4), %xmm5, %xmm1 ## xmm1 = xmm5[0,1],mem[0],xmm5[3]
	vmovss	40(%r13,%rdi,4), %xmm5          ## xmm5 = mem[0],zero,zero,zero
	vinsertps	$16, 40(%r13,%rsi,4), %xmm5, %xmm5 ## xmm5 = xmm5[0],mem[0],xmm5[2,3]
	vinsertps	$48, 40(%r13,%rax,4), %xmm1, %xmm1 ## xmm1 = xmm1[0,1,2],mem[0]
	vinsertps	$32, 40(%r13,%r8,4), %xmm5, %xmm5 ## xmm5 = xmm5[0,1],mem[0],xmm5[3]
	vinsertps	$48, 40(%r13,%rdx,4), %xmm5, %xmm5 ## xmm5 = xmm5[0,1,2],mem[0]
	movq	%rdx, %r8
	vinsertf128	$1, %xmm1, %ymm5, %ymm1
	movq	544(%rsp), %rdx                 ## 8-byte Reload
	vinsertps	$32, 92(%r13,%rdx,4), %xmm4, %xmm4 ## xmm4 = xmm4[0,1],mem[0],xmm4[3]
	movq	512(%rsp), %r14                 ## 8-byte Reload
	vmovss	92(%r13,%r14,4), %xmm5          ## xmm5 = mem[0],zero,zero,zero
	movq	128(%rsp), %r10                 ## 8-byte Reload
	vinsertps	$16, 92(%r13,%r10,4), %xmm5, %xmm5 ## xmm5 = xmm5[0],mem[0],xmm5[2,3]
	movq	136(%rsp), %r15                 ## 8-byte Reload
	vinsertps	$32, 92(%r13,%r15,4), %xmm5, %xmm5 ## xmm5 = xmm5[0,1],mem[0],xmm5[3]
	vmovss	88(%r13,%r12,4), %xmm3          ## xmm3 = mem[0],zero,zero,zero
	vinsertps	$16, 88(%r13,%rcx,4), %xmm3, %xmm3 ## xmm3 = xmm3[0],mem[0],xmm3[2,3]
	movq	%rcx, %r9
	movq	24(%rsp), %rcx                  ## 8-byte Reload
	vmovss	44(%r13,%rcx,4), %xmm6          ## xmm6 = mem[0],zero,zero,zero
	vinsertps	$16, 44(%r13,%rbx,4), %xmm6, %xmm6 ## xmm6 = xmm6[0],mem[0],xmm6[2,3]
	movq	224(%rsp), %rcx                 ## 8-byte Reload
	vmovups	32(%r11,%rcx,4), %ymm11
	vmulps	%ymm11, %ymm8, %ymm14
	vfmadd231ps	%ymm14, %ymm1, %ymm13   ## ymm13 = (ymm1 * ymm14) + ymm13
	vinsertps	$32, 44(%r13,%rbp,4), %xmm6, %xmm1 ## xmm1 = xmm6[0,1],mem[0],xmm6[3]
	vmovss	44(%r13,%rdi,4), %xmm6          ## xmm6 = mem[0],zero,zero,zero
	vinsertps	$16, 44(%r13,%rsi,4), %xmm6, %xmm6 ## xmm6 = xmm6[0],mem[0],xmm6[2,3]
	vinsertps	$48, 44(%r13,%rax,4), %xmm1, %xmm1 ## xmm1 = xmm1[0,1,2],mem[0]
	movq	16(%rsp), %rsi                  ## 8-byte Reload
	vinsertps	$32, 44(%r13,%rsi,4), %xmm6, %xmm6 ## xmm6 = xmm6[0,1],mem[0],xmm6[3]
	vinsertps	$48, 44(%r13,%r8,4), %xmm6, %xmm6 ## xmm6 = xmm6[0,1,2],mem[0]
	vinsertf128	$1, %xmm1, %ymm6, %ymm1
	vmulps	%ymm0, %ymm11, %ymm6
	movq	96(%rsp), %rcx                  ## 8-byte Reload
	vinsertps	$48, 92(%r13,%rcx,4), %xmm4, %xmm4 ## xmm4 = xmm4[0,1,2],mem[0]
	vfmadd231ps	%ymm6, %ymm1, %ymm13    ## ymm13 = (ymm1 * ymm6) + ymm13
	vinsertps	$32, 88(%r13,%rdx,4), %xmm3, %xmm1 ## xmm1 = xmm3[0,1],mem[0],xmm3[3]
	vmovss	88(%r13,%r14,4), %xmm3          ## xmm3 = mem[0],zero,zero,zero
	vinsertps	$16, 88(%r13,%r10,4), %xmm3, %xmm3 ## xmm3 = xmm3[0],mem[0],xmm3[2,3]
	movq	400(%rsp), %rdi                 ## 8-byte Reload
	vinsertps	$48, 92(%r13,%rdi,4), %xmm5, %xmm5 ## xmm5 = xmm5[0,1,2],mem[0]
	vinsertps	$32, 88(%r13,%r15,4), %xmm3, %xmm3 ## xmm3 = xmm3[0,1],mem[0],xmm3[3]
	vinsertps	$48, 88(%r13,%rcx,4), %xmm1, %xmm1 ## xmm1 = xmm1[0,1,2],mem[0]
	vinsertf128	$1, %xmm4, %ymm5, %ymm12
	vinsertps	$48, 88(%r13,%rdi,4), %xmm3, %xmm3 ## xmm3 = xmm3[0,1,2],mem[0]
	vinsertf128	$1, %xmm1, %ymm3, %ymm6
	vmovss	64(%r13,%r12,4), %xmm1          ## xmm1 = mem[0],zero,zero,zero
	vinsertps	$16, 64(%r13,%r9,4), %xmm1, %xmm1 ## xmm1 = xmm1[0],mem[0],xmm1[2,3]
	vmovss	64(%r13,%r14,4), %xmm4          ## xmm4 = mem[0],zero,zero,zero
	vinsertps	$16, 64(%r13,%r10,4), %xmm4, %xmm4 ## xmm4 = xmm4[0],mem[0],xmm4[2,3]
	vinsertps	$32, 64(%r13,%rdx,4), %xmm1, %xmm1 ## xmm1 = xmm1[0,1],mem[0],xmm1[3]
	vinsertps	$32, 64(%r13,%r15,4), %xmm4, %xmm4 ## xmm4 = xmm4[0,1],mem[0],xmm4[3]
	movq	%r15, %rbp
	vinsertps	$48, 64(%r13,%rcx,4), %xmm1, %xmm1 ## xmm1 = xmm1[0,1,2],mem[0]
	vinsertps	$48, 64(%r13,%rdi,4), %xmm4, %xmm4 ## xmm4 = xmm4[0,1,2],mem[0]
	vinsertf128	$1, %xmm1, %ymm4, %ymm4
	movq	200(%rsp), %rax                 ## 8-byte Reload
	vmovups	96(%r11,%rax,4), %ymm3
	vmovaps	%ymm2, %ymm8
	vcmpltps	%ymm3, %ymm2, %ymm2
	vmovups	1152(%rsp), %ymm0               ## 32-byte Reload
	vblendvps	%ymm2, %ymm4, %ymm0, %ymm4
	movq	320(%rsp), %rax                 ## 8-byte Reload
	vmovups	96(%r11,%rax,4), %ymm11
	vmaxps	%ymm8, %ymm11, %ymm11
	vdivps	%ymm11, %ymm7, %ymm0
	vmovss	76(%r13,%r12,4), %xmm1          ## xmm1 = mem[0],zero,zero,zero
	vinsertps	$16, 76(%r13,%r9,4), %xmm1, %xmm1 ## xmm1 = xmm1[0],mem[0],xmm1[2,3]
	vmovups	1376(%rsp), %ymm5               ## 32-byte Reload
	vaddps	1408(%rsp), %ymm5, %ymm14       ## 32-byte Folded Reload
	vmulps	%ymm6, %ymm14, %ymm5
	vinsertps	$32, 76(%r13,%rdx,4), %xmm1, %xmm1 ## xmm1 = xmm1[0,1],mem[0],xmm1[3]
	vmovss	76(%r13,%r14,4), %xmm7          ## xmm7 = mem[0],zero,zero,zero
	vinsertps	$16, 76(%r13,%r10,4), %xmm7, %xmm7 ## xmm7 = xmm7[0],mem[0],xmm7[2,3]
	vinsertps	$48, 76(%r13,%rcx,4), %xmm1, %xmm1 ## xmm1 = xmm1[0,1,2],mem[0]
	vinsertps	$32, 76(%r13,%r15,4), %xmm7, %xmm7 ## xmm7 = xmm7[0,1],mem[0],xmm7[3]
	vinsertps	$48, 76(%r13,%rdi,4), %xmm7, %xmm7 ## xmm7 = xmm7[0,1,2],mem[0]
	vinsertf128	$1, %xmm1, %ymm7, %ymm1
	vdivps	%ymm11, %ymm5, %ymm6
	vmovups	1792(%rsp), %ymm5               ## 32-byte Reload
	vmovups	%ymm2, 1024(%rsp)               ## 32-byte Spill
	vblendvps	%ymm2, %ymm1, %ymm5, %ymm14
	movq	328(%rsp), %rax                 ## 8-byte Reload
	vmovups	%ymm3, 1056(%rsp)               ## 32-byte Spill
	vmulps	96(%r11,%rax,4), %ymm3, %ymm1
	vmulps	%ymm4, %ymm1, %ymm1
	vdivps	%ymm11, %ymm1, %ymm1
	movq	336(%rsp), %rax                 ## 8-byte Reload
	vfmadd132ps	96(%r11,%rax,4), %ymm1, %ymm14 ## ymm14 = (ymm14 * mem) + ymm1
	vandps	%ymm6, %ymm2, %ymm1
	movq	192(%rsp), %rax                 ## 8-byte Reload
	vmovups	96(%r11,%rax,4), %ymm2
	vmovups	%ymm2, 768(%rsp)                ## 32-byte Spill
	vfmadd213ps	%ymm1, %ymm2, %ymm14    ## ymm14 = (ymm2 * ymm14) + ymm1
	vminps	%ymm3, %ymm0, %ymm0
	movq	232(%rsp), %rax                 ## 8-byte Reload
	vmulps	96(%r11,%rax,4), %ymm0, %ymm0
	movq	208(%rsp), %rax                 ## 8-byte Reload
	vmulps	96(%r11,%rax,4), %ymm3, %ymm1
	movq	%r11, %rbx
	vmulps	%ymm1, %ymm0, %ymm0
	vfmadd231ps	%ymm0, %ymm12, %ymm14   ## ymm14 = (ymm12 * ymm0) + ymm14
	movq	24(%rsp), %r9                   ## 8-byte Reload
	vmovss	48(%r13,%r9,4), %xmm0           ## xmm0 = mem[0],zero,zero,zero
	movq	64(%rsp), %r10                  ## 8-byte Reload
	vinsertps	$16, 48(%r13,%r10,4), %xmm0, %xmm0 ## xmm0 = xmm0[0],mem[0],xmm0[2,3]
	movq	32(%rsp), %rdx                  ## 8-byte Reload
	vmovss	48(%r13,%rdx,4), %xmm3          ## xmm3 = mem[0],zero,zero,zero
	movq	(%rsp), %rdi                    ## 8-byte Reload
	vinsertps	$16, 48(%r13,%rdi,4), %xmm3, %xmm3 ## xmm3 = xmm3[0],mem[0],xmm3[2,3]
	movq	104(%rsp), %r15                 ## 8-byte Reload
	vinsertps	$32, 48(%r13,%r15,4), %xmm0, %xmm0 ## xmm0 = xmm0[0,1],mem[0],xmm0[3]
	movq	%rsi, %rax
	vinsertps	$32, 48(%r13,%rsi,4), %xmm3, %xmm3 ## xmm3 = xmm3[0,1],mem[0],xmm3[3]
	movq	8(%rsp), %rsi                   ## 8-byte Reload
	vinsertps	$48, 48(%r13,%rsi,4), %xmm0, %xmm0 ## xmm0 = xmm0[0,1,2],mem[0]
	vinsertps	$48, 48(%r13,%r8,4), %xmm3, %xmm3 ## xmm3 = xmm3[0,1,2],mem[0]
	vinsertf128	$1, %xmm0, %ymm3, %ymm0
	movq	272(%rsp), %rbp                 ## 8-byte Reload
	vmovss	60(%r13,%rbp,4), %xmm3          ## xmm3 = mem[0],zero,zero,zero
	movq	280(%rsp), %rbp                 ## 8-byte Reload
	vinsertps	$16, 60(%r13,%rbp,4), %xmm3, %xmm3 ## xmm3 = xmm3[0],mem[0],xmm3[2,3]
	movq	216(%rsp), %rbp                 ## 8-byte Reload
	vmovups	32(%r11,%rbp,4), %ymm6
	vmulps	992(%rsp), %ymm6, %ymm7         ## 32-byte Folded Reload
	vfmadd231ps	%ymm7, %ymm0, %ymm13    ## ymm13 = (ymm0 * ymm7) + ymm13
	movq	88(%rsp), %rbp                  ## 8-byte Reload
	vinsertps	$32, 60(%r13,%rbp,4), %xmm3, %xmm0 ## xmm0 = xmm3[0,1],mem[0],xmm3[3]
	movq	56(%rsp), %rbp                  ## 8-byte Reload
	vmovss	60(%r13,%rbp,4), %xmm3          ## xmm3 = mem[0],zero,zero,zero
	movq	80(%rsp), %rbp                  ## 8-byte Reload
	vinsertps	$16, 60(%r13,%rbp,4), %xmm3, %xmm3 ## xmm3 = xmm3[0],mem[0],xmm3[2,3]
	movq	288(%rsp), %rbp                 ## 8-byte Reload
	vinsertps	$48, 60(%r13,%rbp,4), %xmm0, %xmm0 ## xmm0 = xmm0[0,1,2],mem[0]
	movq	144(%rsp), %rbp                 ## 8-byte Reload
	vinsertps	$32, 60(%r13,%rbp,4), %xmm3, %xmm3 ## xmm3 = xmm3[0,1],mem[0],xmm3[3]
	movq	72(%rsp), %rbp                  ## 8-byte Reload
	vinsertps	$48, 60(%r13,%rbp,4), %xmm3, %xmm3 ## xmm3 = xmm3[0,1,2],mem[0]
	vinsertf128	$1, %xmm0, %ymm3, %ymm0
	movq	344(%rsp), %rbp                 ## 8-byte Reload
	vmulps	(%r11,%rbp,4), %ymm15, %ymm3
	vfmadd231ps	%ymm3, %ymm0, %ymm10    ## ymm10 = (ymm0 * ymm3) + ymm10
	vmovups	1536(%rsp), %ymm0               ## 32-byte Reload
	vmaxps	%ymm8, %ymm0, %ymm11
	vmovups	1472(%rsp), %ymm5               ## 32-byte Reload
	vdivps	%ymm5, %ymm15, %ymm2
	vdivps	%ymm5, %ymm11, %ymm4
	vroundps	$10, %ymm2, %ymm3
	vmulps	1824(%rsp), %ymm3, %ymm7        ## 32-byte Folded Reload
	vroundps	$10, %ymm4, %ymm3
	vmulps	1696(%rsp), %ymm3, %ymm3        ## 32-byte Folded Reload
	vmovups	1504(%rsp), %ymm0               ## 32-byte Reload
	vmaxps	%ymm8, %ymm0, %ymm0
	vmovups	%ymm0, 1152(%rsp)               ## 32-byte Spill
	vmaxps	%ymm8, %ymm1, %ymm1
	vmovups	%ymm1, 992(%rsp)                ## 32-byte Spill
	vdivps	%ymm5, %ymm0, %ymm0
	vdivps	%ymm5, %ymm1, %ymm1
	vroundps	$10, %ymm0, %ymm5
	vmulps	1760(%rsp), %ymm5, %ymm5        ## 32-byte Folded Reload
	vroundps	$10, %ymm1, %ymm15
	vmulps	1728(%rsp), %ymm15, %ymm15      ## 32-byte Folded Reload
	vmaxps	%ymm8, %ymm2, %ymm2
	vdivps	%ymm2, %ymm7, %ymm2
	vmaxps	%ymm8, %ymm4, %ymm4
	vdivps	%ymm4, %ymm3, %ymm3
	vmaxps	%ymm8, %ymm0, %ymm0
	vmaxps	%ymm8, %ymm1, %ymm1
	vdivps	%ymm0, %ymm5, %ymm0
	vdivps	%ymm1, %ymm15, %ymm1
	vbroadcastss	LCPI1_17(%rip), %ymm4   ## ymm4 = [2.0E+0,2.0E+0,2.0E+0,2.0E+0,2.0E+0,2.0E+0,2.0E+0,2.0E+0]
	vmovups	1088(%rsp), %ymm15              ## 32-byte Reload
	vfmadd213ps	%ymm2, %ymm4, %ymm15    ## ymm15 = (ymm4 * ymm15) + ymm2
	vmovups	800(%rsp), %ymm2                ## 32-byte Reload
	vfmadd213ps	%ymm3, %ymm4, %ymm2     ## ymm2 = (ymm4 * ymm2) + ymm3
	vmovups	%ymm2, 800(%rsp)                ## 32-byte Spill
	vmovups	832(%rsp), %ymm2                ## 32-byte Reload
	vfmadd213ps	%ymm0, %ymm4, %ymm2     ## ymm2 = (ymm4 * ymm2) + ymm0
	vmovups	%ymm2, 832(%rsp)                ## 32-byte Spill
	vfmadd213ps	%ymm1, %ymm4, %ymm14    ## ymm14 = (ymm4 * ymm14) + ymm1
	vmovss	52(%r13,%r9,4), %xmm0           ## xmm0 = mem[0],zero,zero,zero
	movq	%r9, %r12
	vinsertps	$16, 52(%r13,%r10,4), %xmm0, %xmm0 ## xmm0 = xmm0[0],mem[0],xmm0[2,3]
	vmovss	52(%r13,%rdx,4), %xmm1          ## xmm1 = mem[0],zero,zero,zero
	vinsertps	$16, 52(%r13,%rdi,4), %xmm1, %xmm1 ## xmm1 = xmm1[0],mem[0],xmm1[2,3]
	vinsertps	$32, 52(%r13,%r15,4), %xmm0, %xmm0 ## xmm0 = xmm0[0,1],mem[0],xmm0[3]
	vinsertps	$32, 52(%r13,%rax,4), %xmm1, %xmm1 ## xmm1 = xmm1[0,1],mem[0],xmm1[3]
	vinsertps	$48, 52(%r13,%rsi,4), %xmm0, %xmm0 ## xmm0 = xmm0[0,1,2],mem[0]
	vmulps	864(%rsp), %ymm6, %ymm2         ## 32-byte Folded Reload
	vinsertps	$48, 52(%r13,%r8,4), %xmm1, %xmm1 ## xmm1 = xmm1[0,1,2],mem[0]
	movq	%r8, %rax
	vinsertf128	$1, %xmm0, %ymm1, %ymm0
	movq	376(%rsp), %rsi                 ## 8-byte Reload
	vmovss	20(%r13,%rsi,4), %xmm1          ## xmm1 = mem[0],zero,zero,zero
	movq	368(%rsp), %r14                 ## 8-byte Reload
	vinsertps	$16, 20(%r13,%r14,4), %xmm1, %xmm1 ## xmm1 = xmm1[0],mem[0],xmm1[2,3]
	movq	168(%rsp), %rcx                 ## 8-byte Reload
	vmovss	20(%r13,%rcx,4), %xmm3          ## xmm3 = mem[0],zero,zero,zero
	movq	112(%rsp), %r8                  ## 8-byte Reload
	vinsertps	$16, 20(%r13,%r8,4), %xmm3, %xmm3 ## xmm3 = xmm3[0],mem[0],xmm3[2,3]
	movq	296(%rsp), %r10                 ## 8-byte Reload
	vinsertps	$32, 20(%r13,%r10,4), %xmm1, %xmm1 ## xmm1 = xmm1[0,1],mem[0],xmm1[3]
	movq	360(%rsp), %r11                 ## 8-byte Reload
	vinsertps	$32, 20(%r13,%r11,4), %xmm3, %xmm3 ## xmm3 = xmm3[0,1],mem[0],xmm3[3]
	movq	160(%rsp), %rbp                 ## 8-byte Reload
	vinsertps	$48, 20(%r13,%rbp,4), %xmm1, %xmm1 ## xmm1 = xmm1[0,1,2],mem[0]
	vfmadd231ps	%ymm2, %ymm0, %ymm13    ## ymm13 = (ymm0 * ymm2) + ymm13
	movq	120(%rsp), %r9                  ## 8-byte Reload
	vinsertps	$48, 20(%r13,%r9,4), %xmm3, %xmm0 ## xmm0 = xmm3[0,1,2],mem[0]
	vinsertf128	$1, %xmm1, %ymm0, %ymm0
	vmovss	24(%r13,%rsi,4), %xmm1          ## xmm1 = mem[0],zero,zero,zero
	vinsertps	$16, 24(%r13,%r14,4), %xmm1, %xmm1 ## xmm1 = xmm1[0],mem[0],xmm1[2,3]
	vmovss	24(%r13,%rcx,4), %xmm2          ## xmm2 = mem[0],zero,zero,zero
	vinsertps	$16, 24(%r13,%r8,4), %xmm2, %xmm2 ## xmm2 = xmm2[0],mem[0],xmm2[2,3]
	vinsertps	$32, 24(%r13,%r10,4), %xmm1, %xmm1 ## xmm1 = xmm1[0,1],mem[0],xmm1[3]
	vinsertps	$32, 24(%r13,%r11,4), %xmm2, %xmm2 ## xmm2 = xmm2[0,1],mem[0],xmm2[3]
	vinsertps	$48, 24(%r13,%rbp,4), %xmm1, %xmm1 ## xmm1 = xmm1[0,1,2],mem[0]
	vinsertps	$48, 24(%r13,%r9,4), %xmm2, %xmm2 ## xmm2 = xmm2[0,1,2],mem[0]
	vinsertf128	$1, %xmm1, %ymm2, %ymm1
	vmovss	28(%r13,%rsi,4), %xmm2          ## xmm2 = mem[0],zero,zero,zero
	vinsertps	$16, 28(%r13,%r14,4), %xmm2, %xmm2 ## xmm2 = xmm2[0],mem[0],xmm2[2,3]
	vmovups	928(%rsp), %ymm4                ## 32-byte Reload
	movq	264(%rsp), %rdi                 ## 8-byte Reload
	vmulps	64(%rbx,%rdi,4), %ymm4, %ymm3
	vmulps	%ymm3, %ymm1, %ymm12
	vinsertps	$32, 28(%r13,%r10,4), %xmm2, %xmm1 ## xmm1 = xmm2[0,1],mem[0],xmm2[3]
	vmovss	28(%r13,%rcx,4), %xmm2          ## xmm2 = mem[0],zero,zero,zero
	vinsertps	$16, 28(%r13,%r8,4), %xmm2, %xmm2 ## xmm2 = xmm2[0],mem[0],xmm2[2,3]
	vinsertps	$48, 28(%r13,%rbp,4), %xmm1, %xmm1 ## xmm1 = xmm1[0,1,2],mem[0]
	vinsertps	$32, 28(%r13,%r11,4), %xmm2, %xmm2 ## xmm2 = xmm2[0,1],mem[0],xmm2[3]
	movq	256(%rsp), %rdi                 ## 8-byte Reload
	vmulps	64(%rbx,%rdi,4), %ymm4, %ymm3
	vfmadd231ps	%ymm3, %ymm0, %ymm12    ## ymm12 = (ymm0 * ymm3) + ymm12
	vinsertps	$48, 28(%r13,%r9,4), %xmm2, %xmm0 ## xmm0 = xmm2[0,1,2],mem[0]
	vinsertf128	$1, %xmm1, %ymm0, %ymm0
	vmovss	36(%r13,%rsi,4), %xmm1          ## xmm1 = mem[0],zero,zero,zero
	vinsertps	$16, 36(%r13,%r14,4), %xmm1, %xmm1 ## xmm1 = xmm1[0],mem[0],xmm1[2,3]
	vmovss	32(%r13,%rsi,4), %xmm2          ## xmm2 = mem[0],zero,zero,zero
	vinsertps	$16, 32(%r13,%r14,4), %xmm2, %xmm2 ## xmm2 = xmm2[0],mem[0],xmm2[2,3]
	vmovups	1344(%rsp), %ymm5               ## 32-byte Reload
	movq	248(%rsp), %rdi                 ## 8-byte Reload
	vmulps	64(%rbx,%rdi,4), %ymm5, %ymm3
	vfmadd231ps	%ymm3, %ymm0, %ymm12    ## ymm12 = (ymm0 * ymm3) + ymm12
	vinsertps	$32, 32(%r13,%r10,4), %xmm2, %xmm0 ## xmm0 = xmm2[0,1],mem[0],xmm2[3]
	vmovss	32(%r13,%rcx,4), %xmm2          ## xmm2 = mem[0],zero,zero,zero
	vinsertps	$16, 32(%r13,%r8,4), %xmm2, %xmm2 ## xmm2 = xmm2[0],mem[0],xmm2[2,3]
	vinsertps	$48, 32(%r13,%rbp,4), %xmm0, %xmm0 ## xmm0 = xmm0[0,1,2],mem[0]
	vinsertps	$32, 32(%r13,%r11,4), %xmm2, %xmm2 ## xmm2 = xmm2[0,1],mem[0],xmm2[3]
	vinsertps	$48, 32(%r13,%r9,4), %xmm2, %xmm2 ## xmm2 = xmm2[0,1,2],mem[0]
	vinsertf128	$1, %xmm0, %ymm2, %ymm0
	vinsertps	$32, 36(%r13,%r10,4), %xmm1, %xmm1 ## xmm1 = xmm1[0,1],mem[0],xmm1[3]
	vmovss	36(%r13,%rcx,4), %xmm2          ## xmm2 = mem[0],zero,zero,zero
	vinsertps	$16, 36(%r13,%r8,4), %xmm2, %xmm2 ## xmm2 = xmm2[0],mem[0],xmm2[2,3]
	vinsertps	$48, 36(%r13,%rbp,4), %xmm1, %xmm1 ## xmm1 = xmm1[0,1,2],mem[0]
	vinsertps	$32, 36(%r13,%r11,4), %xmm2, %xmm2 ## xmm2 = xmm2[0,1],mem[0],xmm2[3]
	vmovups	1312(%rsp), %ymm6               ## 32-byte Reload
	movq	184(%rsp), %rdi                 ## 8-byte Reload
	vmulps	64(%rbx,%rdi,4), %ymm6, %ymm3
	vfmadd231ps	%ymm3, %ymm0, %ymm12    ## ymm12 = (ymm0 * ymm3) + ymm12
	vinsertps	$48, 36(%r13,%r9,4), %xmm2, %xmm0 ## xmm0 = xmm2[0,1,2],mem[0]
	vinsertf128	$1, %xmm1, %ymm0, %ymm0
	vmovss	44(%r13,%rsi,4), %xmm1          ## xmm1 = mem[0],zero,zero,zero
	vinsertps	$16, 44(%r13,%r14,4), %xmm1, %xmm1 ## xmm1 = xmm1[0],mem[0],xmm1[2,3]
	vmovss	40(%r13,%rsi,4), %xmm2          ## xmm2 = mem[0],zero,zero,zero
	vinsertps	$16, 40(%r13,%r14,4), %xmm2, %xmm2 ## xmm2 = xmm2[0],mem[0],xmm2[2,3]
	movq	240(%rsp), %rdx                 ## 8-byte Reload
	vmulps	64(%rbx,%rdx,4), %ymm5, %ymm3
	vfmadd231ps	%ymm3, %ymm0, %ymm12    ## ymm12 = (ymm0 * ymm3) + ymm12
	movq	%r10, %rdx
	vinsertps	$32, 40(%r13,%r10,4), %xmm2, %xmm0 ## xmm0 = xmm2[0,1],mem[0],xmm2[3]
	vmovss	40(%r13,%rcx,4), %xmm2          ## xmm2 = mem[0],zero,zero,zero
	vinsertps	$16, 40(%r13,%r8,4), %xmm2, %xmm2 ## xmm2 = xmm2[0],mem[0],xmm2[2,3]
	vinsertps	$48, 40(%r13,%rbp,4), %xmm0, %xmm0 ## xmm0 = xmm0[0,1,2],mem[0]
	vinsertps	$32, 40(%r13,%r11,4), %xmm2, %xmm2 ## xmm2 = xmm2[0,1],mem[0],xmm2[3]
	vinsertps	$48, 40(%r13,%r9,4), %xmm2, %xmm2 ## xmm2 = xmm2[0,1,2],mem[0]
	vinsertf128	$1, %xmm0, %ymm2, %ymm0
	vinsertps	$32, 44(%r13,%r10,4), %xmm1, %xmm1 ## xmm1 = xmm1[0,1],mem[0],xmm1[3]
	movq	%r10, %rdi
	vmovss	44(%r13,%rcx,4), %xmm2          ## xmm2 = mem[0],zero,zero,zero
	movq	%rcx, %r10
	vinsertps	$16, 44(%r13,%r8,4), %xmm2, %xmm2 ## xmm2 = xmm2[0],mem[0],xmm2[2,3]
	movq	%rbp, %rcx
	vinsertps	$48, 44(%r13,%rbp,4), %xmm1, %xmm1 ## xmm1 = xmm1[0,1,2],mem[0]
	vinsertps	$32, 44(%r13,%r11,4), %xmm2, %xmm2 ## xmm2 = xmm2[0,1],mem[0],xmm2[3]
	movq	224(%rsp), %rdx                 ## 8-byte Reload
	vmovups	64(%rbx,%rdx,4), %ymm3
	vmulps	%ymm3, %ymm6, %ymm4
	vfmadd231ps	%ymm4, %ymm0, %ymm12    ## ymm12 = (ymm0 * ymm4) + ymm12
	vinsertps	$48, 44(%r13,%r9,4), %xmm2, %xmm0 ## xmm0 = xmm2[0,1,2],mem[0]
	vinsertf128	$1, %xmm1, %ymm0, %ymm0
	vmulps	%ymm3, %ymm5, %ymm1
	vmovss	56(%r13,%r12,4), %xmm2          ## xmm2 = mem[0],zero,zero,zero
	movq	64(%rsp), %rdx                  ## 8-byte Reload
	vinsertps	$16, 56(%r13,%rdx,4), %xmm2, %xmm2 ## xmm2 = xmm2[0],mem[0],xmm2[2,3]
	vfmadd231ps	%ymm1, %ymm0, %ymm12    ## ymm12 = (ymm0 * ymm1) + ymm12
	vinsertps	$32, 56(%r13,%r15,4), %xmm2, %xmm0 ## xmm0 = xmm2[0,1],mem[0],xmm2[3]
	movq	32(%rsp), %rdx                  ## 8-byte Reload
	vmovss	56(%r13,%rdx,4), %xmm1          ## xmm1 = mem[0],zero,zero,zero
	movq	(%rsp), %rdx                    ## 8-byte Reload
	vinsertps	$16, 56(%r13,%rdx,4), %xmm1, %xmm1 ## xmm1 = xmm1[0],mem[0],xmm1[2,3]
	movq	8(%rsp), %r12                   ## 8-byte Reload
	vinsertps	$48, 56(%r13,%r12,4), %xmm0, %xmm0 ## xmm0 = xmm0[0,1,2],mem[0]
	movq	16(%rsp), %rdx                  ## 8-byte Reload
	vinsertps	$32, 56(%r13,%rdx,4), %xmm1, %xmm1 ## xmm1 = xmm1[0,1],mem[0],xmm1[3]
	vinsertps	$48, 56(%r13,%rax,4), %xmm1, %xmm1 ## xmm1 = xmm1[0,1,2],mem[0]
	vinsertf128	$1, %xmm0, %ymm1, %ymm0
	movq	352(%rsp), %rax                 ## 8-byte Reload
	vmulps	32(%rbx,%rax,4), %ymm11, %ymm1
	vfmadd231ps	%ymm1, %ymm0, %ymm13    ## ymm13 = (ymm0 * ymm1) + ymm13
	vmovss	48(%r13,%rsi,4), %xmm0          ## xmm0 = mem[0],zero,zero,zero
	vinsertps	$16, 48(%r13,%r14,4), %xmm0, %xmm0 ## xmm0 = xmm0[0],mem[0],xmm0[2,3]
	vmovss	48(%r13,%r10,4), %xmm1          ## xmm1 = mem[0],zero,zero,zero
	vinsertps	$16, 48(%r13,%r8,4), %xmm1, %xmm1 ## xmm1 = xmm1[0],mem[0],xmm1[2,3]
	vinsertps	$32, 48(%r13,%rdi,4), %xmm0, %xmm0 ## xmm0 = xmm0[0,1],mem[0],xmm0[3]
	vinsertps	$32, 48(%r13,%r11,4), %xmm1, %xmm1 ## xmm1 = xmm1[0,1],mem[0],xmm1[3]
	vinsertps	$48, 48(%r13,%rbp,4), %xmm0, %xmm0 ## xmm0 = xmm0[0,1,2],mem[0]
	vinsertps	$48, 48(%r13,%r9,4), %xmm1, %xmm1 ## xmm1 = xmm1[0,1,2],mem[0]
	vinsertf128	$1, %xmm0, %ymm1, %ymm0
	movq	216(%rsp), %rdx                 ## 8-byte Reload
	vmovups	64(%rbx,%rdx,4), %ymm1
	vmulps	%ymm1, %ymm6, %ymm2
	vmovss	52(%r13,%rsi,4), %xmm3          ## xmm3 = mem[0],zero,zero,zero
	vinsertps	$16, 52(%r13,%r14,4), %xmm3, %xmm3 ## xmm3 = xmm3[0],mem[0],xmm3[2,3]
	vfmadd231ps	%ymm2, %ymm0, %ymm12    ## ymm12 = (ymm0 * ymm2) + ymm12
	vinsertps	$32, 52(%r13,%rdi,4), %xmm3, %xmm0 ## xmm0 = xmm3[0,1],mem[0],xmm3[3]
	vmovss	52(%r13,%r10,4), %xmm2          ## xmm2 = mem[0],zero,zero,zero
	vinsertps	$16, 52(%r13,%r8,4), %xmm2, %xmm2 ## xmm2 = xmm2[0],mem[0],xmm2[2,3]
	vinsertps	$48, 52(%r13,%rbp,4), %xmm0, %xmm0 ## xmm0 = xmm0[0,1,2],mem[0]
	vinsertps	$32, 52(%r13,%r11,4), %xmm2, %xmm2 ## xmm2 = xmm2[0,1],mem[0],xmm2[3]
	vinsertps	$48, 52(%r13,%r9,4), %xmm2, %xmm2 ## xmm2 = xmm2[0,1,2],mem[0]
	vinsertf128	$1, %xmm0, %ymm2, %ymm0
	vmovss	56(%r13,%rsi,4), %xmm2          ## xmm2 = mem[0],zero,zero,zero
	vinsertps	$16, 56(%r13,%r14,4), %xmm2, %xmm2 ## xmm2 = xmm2[0],mem[0],xmm2[2,3]
	vmulps	%ymm1, %ymm5, %ymm1
	vinsertps	$32, 56(%r13,%rdi,4), %xmm2, %xmm2 ## xmm2 = xmm2[0,1],mem[0],xmm2[3]
	vmovss	56(%r13,%r10,4), %xmm3          ## xmm3 = mem[0],zero,zero,zero
	vinsertps	$16, 56(%r13,%r8,4), %xmm3, %xmm3 ## xmm3 = xmm3[0],mem[0],xmm3[2,3]
	vinsertps	$48, 56(%r13,%rbp,4), %xmm2, %xmm2 ## xmm2 = xmm2[0,1,2],mem[0]
	vfmadd231ps	%ymm1, %ymm0, %ymm12    ## ymm12 = (ymm0 * ymm1) + ymm12
	vinsertps	$32, 56(%r13,%r11,4), %xmm3, %xmm0 ## xmm0 = xmm3[0,1],mem[0],xmm3[3]
	movq	392(%rsp), %rdx                 ## 8-byte Reload
	vmovss	20(%r13,%rdx,4), %xmm1          ## xmm1 = mem[0],zero,zero,zero
	movq	384(%rsp), %r8                  ## 8-byte Reload
	vinsertps	$16, 20(%r13,%r8,4), %xmm1, %xmm1 ## xmm1 = xmm1[0],mem[0],xmm1[2,3]
	vinsertps	$48, 56(%r13,%r9,4), %xmm0, %xmm0 ## xmm0 = xmm0[0,1,2],mem[0]
	vinsertf128	$1, %xmm2, %ymm0, %ymm0
	movq	544(%rsp), %rdi                 ## 8-byte Reload
	vinsertps	$32, 20(%r13,%rdi,4), %xmm1, %xmm1 ## xmm1 = xmm1[0,1],mem[0],xmm1[3]
	movq	512(%rsp), %rbp                 ## 8-byte Reload
	vmovss	20(%r13,%rbp,4), %xmm2          ## xmm2 = mem[0],zero,zero,zero
	movq	128(%rsp), %r11                 ## 8-byte Reload
	vinsertps	$16, 20(%r13,%r11,4), %xmm2, %xmm2 ## xmm2 = xmm2[0],mem[0],xmm2[2,3]
	movq	96(%rsp), %r14                  ## 8-byte Reload
	vinsertps	$48, 20(%r13,%r14,4), %xmm1, %xmm1 ## xmm1 = xmm1[0,1,2],mem[0]
	movq	136(%rsp), %r9                  ## 8-byte Reload
	vinsertps	$32, 20(%r13,%r9,4), %xmm2, %xmm2 ## xmm2 = xmm2[0,1],mem[0],xmm2[3]
	vmovups	1152(%rsp), %ymm8               ## 32-byte Reload
	vmulps	64(%rbx,%rax,4), %ymm8, %ymm3
	vfmadd231ps	%ymm3, %ymm0, %ymm12    ## ymm12 = (ymm0 * ymm3) + ymm12
	movq	400(%rsp), %r15                 ## 8-byte Reload
	vinsertps	$48, 20(%r13,%r15,4), %xmm2, %xmm0 ## xmm0 = xmm2[0,1,2],mem[0]
	vinsertf128	$1, %xmm1, %ymm0, %ymm1
	vmovss	24(%r13,%rdx,4), %xmm0          ## xmm0 = mem[0],zero,zero,zero
	vinsertps	$16, 24(%r13,%r8,4), %xmm0, %xmm0 ## xmm0 = xmm0[0],mem[0],xmm0[2,3]
	vmovss	24(%r13,%rbp,4), %xmm2          ## xmm2 = mem[0],zero,zero,zero
	vinsertps	$16, 24(%r13,%r11,4), %xmm2, %xmm2 ## xmm2 = xmm2[0],mem[0],xmm2[2,3]
	vinsertps	$32, 24(%r13,%rdi,4), %xmm0, %xmm0 ## xmm0 = xmm0[0,1],mem[0],xmm0[3]
	vinsertps	$32, 24(%r13,%r9,4), %xmm2, %xmm2 ## xmm2 = xmm2[0,1],mem[0],xmm2[3]
	vinsertps	$48, 24(%r13,%r14,4), %xmm0, %xmm0 ## xmm0 = xmm0[0,1,2],mem[0]
	vinsertps	$48, 24(%r13,%r15,4), %xmm2, %xmm2 ## xmm2 = xmm2[0,1,2],mem[0]
	vinsertf128	$1, %xmm0, %ymm2, %ymm0
	vmovss	28(%r13,%rdx,4), %xmm2          ## xmm2 = mem[0],zero,zero,zero
	vinsertps	$16, 28(%r13,%r8,4), %xmm2, %xmm2 ## xmm2 = xmm2[0],mem[0],xmm2[2,3]
	vmovups	768(%rsp), %ymm4                ## 32-byte Reload
	movq	264(%rsp), %rax                 ## 8-byte Reload
	vmulps	96(%rbx,%rax,4), %ymm4, %ymm3
	vmulps	%ymm3, %ymm0, %ymm0
	vinsertps	$32, 28(%r13,%rdi,4), %xmm2, %xmm2 ## xmm2 = xmm2[0,1],mem[0],xmm2[3]
	vmovss	28(%r13,%rbp,4), %xmm3          ## xmm3 = mem[0],zero,zero,zero
	vinsertps	$16, 28(%r13,%r11,4), %xmm3, %xmm3 ## xmm3 = xmm3[0],mem[0],xmm3[2,3]
	vinsertps	$48, 28(%r13,%r14,4), %xmm2, %xmm2 ## xmm2 = xmm2[0,1,2],mem[0]
	vinsertps	$32, 28(%r13,%r9,4), %xmm3, %xmm3 ## xmm3 = xmm3[0,1],mem[0],xmm3[3]
	movq	256(%rsp), %rax                 ## 8-byte Reload
	vmulps	96(%rbx,%rax,4), %ymm4, %ymm4
	vfmadd231ps	%ymm4, %ymm1, %ymm0     ## ymm0 = (ymm1 * ymm4) + ymm0
	vinsertps	$48, 28(%r13,%r15,4), %xmm3, %xmm1 ## xmm1 = xmm3[0,1,2],mem[0]
	vinsertf128	$1, %xmm2, %ymm1, %ymm1
	vmovss	36(%r13,%rdx,4), %xmm2          ## xmm2 = mem[0],zero,zero,zero
	vinsertps	$16, 36(%r13,%r8,4), %xmm2, %xmm2 ## xmm2 = xmm2[0],mem[0],xmm2[2,3]
	vmovss	32(%r13,%rdx,4), %xmm3          ## xmm3 = mem[0],zero,zero,zero
	vinsertps	$16, 32(%r13,%r8,4), %xmm3, %xmm3 ## xmm3 = xmm3[0],mem[0],xmm3[2,3]
	vmovups	1376(%rsp), %ymm6               ## 32-byte Reload
	movq	248(%rsp), %rax                 ## 8-byte Reload
	vmulps	96(%rbx,%rax,4), %ymm6, %ymm4
	vfmadd231ps	%ymm4, %ymm1, %ymm0     ## ymm0 = (ymm1 * ymm4) + ymm0
	vinsertps	$32, 32(%r13,%rdi,4), %xmm3, %xmm1 ## xmm1 = xmm3[0,1],mem[0],xmm3[3]
	vmovss	32(%r13,%rbp,4), %xmm3          ## xmm3 = mem[0],zero,zero,zero
	vinsertps	$16, 32(%r13,%r11,4), %xmm3, %xmm3 ## xmm3 = xmm3[0],mem[0],xmm3[2,3]
	vinsertps	$48, 32(%r13,%r14,4), %xmm1, %xmm1 ## xmm1 = xmm1[0,1,2],mem[0]
	vinsertps	$32, 32(%r13,%r9,4), %xmm3, %xmm3 ## xmm3 = xmm3[0,1],mem[0],xmm3[3]
	vinsertps	$48, 32(%r13,%r15,4), %xmm3, %xmm3 ## xmm3 = xmm3[0,1,2],mem[0]
	vinsertf128	$1, %xmm1, %ymm3, %ymm1
	vinsertps	$32, 36(%r13,%rdi,4), %xmm2, %xmm2 ## xmm2 = xmm2[0,1],mem[0],xmm2[3]
	vmovss	36(%r13,%rbp,4), %xmm3          ## xmm3 = mem[0],zero,zero,zero
	vinsertps	$16, 36(%r13,%r11,4), %xmm3, %xmm3 ## xmm3 = xmm3[0],mem[0],xmm3[2,3]
	vinsertps	$48, 36(%r13,%r14,4), %xmm2, %xmm2 ## xmm2 = xmm2[0,1,2],mem[0]
	vinsertps	$32, 36(%r13,%r9,4), %xmm3, %xmm3 ## xmm3 = xmm3[0,1],mem[0],xmm3[3]
	vmovups	1408(%rsp), %ymm7               ## 32-byte Reload
	movq	184(%rsp), %rax                 ## 8-byte Reload
	vmulps	96(%rbx,%rax,4), %ymm7, %ymm4
	vfmadd231ps	%ymm4, %ymm1, %ymm0     ## ymm0 = (ymm1 * ymm4) + ymm0
	vinsertps	$48, 36(%r13,%r15,4), %xmm3, %xmm1 ## xmm1 = xmm3[0,1,2],mem[0]
	vinsertf128	$1, %xmm2, %ymm1, %ymm1
	vmovss	44(%r13,%rdx,4), %xmm2          ## xmm2 = mem[0],zero,zero,zero
	vinsertps	$16, 44(%r13,%r8,4), %xmm2, %xmm2 ## xmm2 = xmm2[0],mem[0],xmm2[2,3]
	vmovss	40(%r13,%rdx,4), %xmm3          ## xmm3 = mem[0],zero,zero,zero
	vinsertps	$16, 40(%r13,%r8,4), %xmm3, %xmm3 ## xmm3 = xmm3[0],mem[0],xmm3[2,3]
	movq	240(%rsp), %rax                 ## 8-byte Reload
	vmulps	96(%rbx,%rax,4), %ymm6, %ymm4
	vfmadd231ps	%ymm4, %ymm1, %ymm0     ## ymm0 = (ymm1 * ymm4) + ymm0
	vinsertps	$32, 40(%r13,%rdi,4), %xmm3, %xmm1 ## xmm1 = xmm3[0,1],mem[0],xmm3[3]
	vmovss	40(%r13,%rbp,4), %xmm3          ## xmm3 = mem[0],zero,zero,zero
	vinsertps	$16, 40(%r13,%r11,4), %xmm3, %xmm3 ## xmm3 = xmm3[0],mem[0],xmm3[2,3]
	vinsertps	$48, 40(%r13,%r14,4), %xmm1, %xmm1 ## xmm1 = xmm1[0,1,2],mem[0]
	vinsertps	$32, 40(%r13,%r9,4), %xmm3, %xmm3 ## xmm3 = xmm3[0,1],mem[0],xmm3[3]
	vinsertps	$48, 40(%r13,%r15,4), %xmm3, %xmm3 ## xmm3 = xmm3[0,1,2],mem[0]
	vinsertf128	$1, %xmm1, %ymm3, %ymm1
	vinsertps	$32, 44(%r13,%rdi,4), %xmm2, %xmm2 ## xmm2 = xmm2[0,1],mem[0],xmm2[3]
	vmovss	44(%r13,%rbp,4), %xmm3          ## xmm3 = mem[0],zero,zero,zero
	vinsertps	$16, 44(%r13,%r11,4), %xmm3, %xmm3 ## xmm3 = xmm3[0],mem[0],xmm3[2,3]
	movq	%r11, %rdi
	vinsertps	$48, 44(%r13,%r14,4), %xmm2, %xmm2 ## xmm2 = xmm2[0,1,2],mem[0]
	vinsertps	$32, 44(%r13,%r9,4), %xmm3, %xmm3 ## xmm3 = xmm3[0,1],mem[0],xmm3[3]
	movq	224(%rsp), %rax                 ## 8-byte Reload
	vmovups	96(%rbx,%rax,4), %ymm4
	vmulps	%ymm4, %ymm7, %ymm5
	vfmadd231ps	%ymm5, %ymm1, %ymm0     ## ymm0 = (ymm1 * ymm5) + ymm0
	vinsertps	$48, 44(%r13,%r15,4), %xmm3, %xmm1 ## xmm1 = xmm3[0,1,2],mem[0]
	vinsertf128	$1, %xmm2, %ymm1, %ymm1
	vmulps	%ymm4, %ymm6, %ymm2
	movq	24(%rsp), %rax                  ## 8-byte Reload
	vmovss	60(%r13,%rax,4), %xmm3          ## xmm3 = mem[0],zero,zero,zero
	movq	64(%rsp), %rax                  ## 8-byte Reload
	vinsertps	$16, 60(%r13,%rax,4), %xmm3, %xmm3 ## xmm3 = xmm3[0],mem[0],xmm3[2,3]
	vfmadd231ps	%ymm2, %ymm1, %ymm0     ## ymm0 = (ymm1 * ymm2) + ymm0
	movq	104(%rsp), %rax                 ## 8-byte Reload
	vinsertps	$32, 60(%r13,%rax,4), %xmm3, %xmm1 ## xmm1 = xmm3[0,1],mem[0],xmm3[3]
	movq	32(%rsp), %rax                  ## 8-byte Reload
	vmovss	60(%r13,%rax,4), %xmm2          ## xmm2 = mem[0],zero,zero,zero
	movq	(%rsp), %rax                    ## 8-byte Reload
	vinsertps	$16, 60(%r13,%rax,4), %xmm2, %xmm2 ## xmm2 = xmm2[0],mem[0],xmm2[2,3]
	vinsertps	$48, 60(%r13,%r12,4), %xmm1, %xmm1 ## xmm1 = xmm1[0,1,2],mem[0]
	movq	16(%rsp), %rax                  ## 8-byte Reload
	vinsertps	$32, 60(%r13,%rax,4), %xmm2, %xmm2 ## xmm2 = xmm2[0,1],mem[0],xmm2[3]
	movq	40(%rsp), %rax                  ## 8-byte Reload
	vinsertps	$48, 60(%r13,%rax,4), %xmm2, %xmm2 ## xmm2 = xmm2[0,1,2],mem[0]
	vinsertf128	$1, %xmm1, %ymm2, %ymm1
	movq	344(%rsp), %rcx                 ## 8-byte Reload
	vmulps	32(%rbx,%rcx,4), %ymm11, %ymm2
	vfmadd231ps	%ymm2, %ymm1, %ymm13    ## ymm13 = (ymm1 * ymm2) + ymm13
	vmovss	48(%r13,%rdx,4), %xmm1          ## xmm1 = mem[0],zero,zero,zero
	vinsertps	$16, 48(%r13,%r8,4), %xmm1, %xmm1 ## xmm1 = xmm1[0],mem[0],xmm1[2,3]
	vmovss	48(%r13,%rbp,4), %xmm2          ## xmm2 = mem[0],zero,zero,zero
	vinsertps	$16, 48(%r13,%r11,4), %xmm2, %xmm2 ## xmm2 = xmm2[0],mem[0],xmm2[2,3]
	movq	544(%rsp), %r10                 ## 8-byte Reload
	vinsertps	$32, 48(%r13,%r10,4), %xmm1, %xmm1 ## xmm1 = xmm1[0,1],mem[0],xmm1[3]
	vinsertps	$32, 48(%r13,%r9,4), %xmm2, %xmm2 ## xmm2 = xmm2[0,1],mem[0],xmm2[3]
	movq	%r14, %rsi
	vinsertps	$48, 48(%r13,%r14,4), %xmm1, %xmm1 ## xmm1 = xmm1[0,1,2],mem[0]
	vinsertps	$48, 48(%r13,%r15,4), %xmm2, %xmm2 ## xmm2 = xmm2[0,1,2],mem[0]
	vinsertf128	$1, %xmm1, %ymm2, %ymm1
	movq	216(%rsp), %rax                 ## 8-byte Reload
	vmovups	96(%rbx,%rax,4), %ymm2
	vmulps	%ymm2, %ymm7, %ymm3
	movq	376(%rsp), %r11                 ## 8-byte Reload
	vmovss	60(%r13,%r11,4), %xmm4          ## xmm4 = mem[0],zero,zero,zero
	movq	368(%rsp), %r12                 ## 8-byte Reload
	vinsertps	$16, 60(%r13,%r12,4), %xmm4, %xmm4 ## xmm4 = xmm4[0],mem[0],xmm4[2,3]
	vfmadd231ps	%ymm3, %ymm1, %ymm0     ## ymm0 = (ymm1 * ymm3) + ymm0
	movq	296(%rsp), %rax                 ## 8-byte Reload
	vinsertps	$32, 60(%r13,%rax,4), %xmm4, %xmm1 ## xmm1 = xmm4[0,1],mem[0],xmm4[3]
	movq	168(%rsp), %rax                 ## 8-byte Reload
	vmovss	60(%r13,%rax,4), %xmm3          ## xmm3 = mem[0],zero,zero,zero
	movq	112(%rsp), %rax                 ## 8-byte Reload
	vinsertps	$16, 60(%r13,%rax,4), %xmm3, %xmm3 ## xmm3 = xmm3[0],mem[0],xmm3[2,3]
	movq	160(%rsp), %r14                 ## 8-byte Reload
	vinsertps	$48, 60(%r13,%r14,4), %xmm1, %xmm1 ## xmm1 = xmm1[0,1,2],mem[0]
	movq	360(%rsp), %rax                 ## 8-byte Reload
	vinsertps	$32, 60(%r13,%rax,4), %xmm3, %xmm3 ## xmm3 = xmm3[0,1],mem[0],xmm3[3]
	movq	120(%rsp), %rbp                 ## 8-byte Reload
	vinsertps	$48, 60(%r13,%rbp,4), %xmm3, %xmm3 ## xmm3 = xmm3[0,1,2],mem[0]
	vinsertf128	$1, %xmm1, %ymm3, %ymm1
	vmulps	64(%rbx,%rcx,4), %ymm8, %ymm3
	vfmadd231ps	%ymm3, %ymm1, %ymm12    ## ymm12 = (ymm1 * ymm3) + ymm12
	movq	%rdx, %rax
	vmovss	52(%r13,%rdx,4), %xmm1          ## xmm1 = mem[0],zero,zero,zero
	vinsertps	$16, 52(%r13,%r8,4), %xmm1, %xmm1 ## xmm1 = xmm1[0],mem[0],xmm1[2,3]
	movq	512(%rsp), %rcx                 ## 8-byte Reload
	vmovss	52(%r13,%rcx,4), %xmm3          ## xmm3 = mem[0],zero,zero,zero
	vinsertps	$16, 52(%r13,%rdi,4), %xmm3, %xmm3 ## xmm3 = xmm3[0],mem[0],xmm3[2,3]
	vinsertps	$32, 52(%r13,%r10,4), %xmm1, %xmm1 ## xmm1 = xmm1[0,1],mem[0],xmm1[3]
	movq	%r9, %rdx
	vinsertps	$32, 52(%r13,%r9,4), %xmm3, %xmm3 ## xmm3 = xmm3[0,1],mem[0],xmm3[3]
	movq	%rsi, %r9
	vinsertps	$48, 52(%r13,%rsi,4), %xmm1, %xmm1 ## xmm1 = xmm1[0,1,2],mem[0]
	vmulps	%ymm2, %ymm6, %ymm2
	vinsertps	$48, 52(%r13,%r15,4), %xmm3, %xmm3 ## xmm3 = xmm3[0,1,2],mem[0]
	vinsertf128	$1, %xmm1, %ymm3, %ymm1
	vmovss	56(%r13,%rax,4), %xmm3          ## xmm3 = mem[0],zero,zero,zero
	movq	%rax, %rsi
	vinsertps	$16, 56(%r13,%r8,4), %xmm3, %xmm3 ## xmm3 = xmm3[0],mem[0],xmm3[2,3]
	vmovss	56(%r13,%rcx,4), %xmm4          ## xmm4 = mem[0],zero,zero,zero
	vinsertps	$16, 56(%r13,%rdi,4), %xmm4, %xmm4 ## xmm4 = xmm4[0],mem[0],xmm4[2,3]
	movq	%r10, %rcx
	vinsertps	$32, 56(%r13,%r10,4), %xmm3, %xmm3 ## xmm3 = xmm3[0,1],mem[0],xmm3[3]
	vinsertps	$32, 56(%r13,%rdx,4), %xmm4, %xmm4 ## xmm4 = xmm4[0,1],mem[0],xmm4[3]
	movq	%rdx, %rdi
	vinsertps	$48, 56(%r13,%r9,4), %xmm3, %xmm3 ## xmm3 = xmm3[0,1,2],mem[0]
	vfmadd231ps	%ymm2, %ymm1, %ymm0     ## ymm0 = (ymm1 * ymm2) + ymm0
	vinsertps	$48, 56(%r13,%r15,4), %xmm4, %xmm1 ## xmm1 = xmm4[0,1,2],mem[0]
	vinsertf128	$1, %xmm3, %ymm1, %ymm1
	movq	272(%rsp), %rdx                 ## 8-byte Reload
	vmovss	96(%r13,%rdx,4), %xmm2          ## xmm2 = mem[0],zero,zero,zero
	movq	280(%rsp), %rax                 ## 8-byte Reload
	vinsertps	$16, 96(%r13,%rax,4), %xmm2, %xmm2 ## xmm2 = xmm2[0],mem[0],xmm2[2,3]
	vmovss	60(%r13,%rsi,4), %xmm3          ## xmm3 = mem[0],zero,zero,zero
	vinsertps	$16, 60(%r13,%r8,4), %xmm3, %xmm3 ## xmm3 = xmm3[0],mem[0],xmm3[2,3]
	vmovups	992(%rsp), %ymm5                ## 32-byte Reload
	movq	352(%rsp), %rsi                 ## 8-byte Reload
	vmulps	96(%rbx,%rsi,4), %ymm5, %ymm4
	vfmadd231ps	%ymm4, %ymm1, %ymm0     ## ymm0 = (ymm1 * ymm4) + ymm0
	vinsertps	$32, 60(%r13,%r10,4), %xmm3, %xmm1 ## xmm1 = xmm3[0,1],mem[0],xmm3[3]
	movq	512(%rsp), %rsi                 ## 8-byte Reload
	vmovss	60(%r13,%rsi,4), %xmm3          ## xmm3 = mem[0],zero,zero,zero
	movq	128(%rsp), %rcx                 ## 8-byte Reload
	vinsertps	$16, 60(%r13,%rcx,4), %xmm3, %xmm3 ## xmm3 = xmm3[0],mem[0],xmm3[2,3]
	vinsertps	$48, 60(%r13,%r9,4), %xmm1, %xmm1 ## xmm1 = xmm1[0,1,2],mem[0]
	vinsertps	$32, 60(%r13,%rdi,4), %xmm3, %xmm3 ## xmm3 = xmm3[0,1],mem[0],xmm3[3]
	vinsertps	$48, 60(%r13,%r15,4), %xmm3, %xmm3 ## xmm3 = xmm3[0,1,2],mem[0]
	vinsertf128	$1, %xmm1, %ymm3, %ymm1
	movq	88(%rsp), %rdi                  ## 8-byte Reload
	vinsertps	$32, 96(%r13,%rdi,4), %xmm2, %xmm2 ## xmm2 = xmm2[0,1],mem[0],xmm2[3]
	movq	56(%rsp), %rdi                  ## 8-byte Reload
	vmovss	96(%r13,%rdi,4), %xmm3          ## xmm3 = mem[0],zero,zero,zero
	movq	80(%rsp), %rdi                  ## 8-byte Reload
	vinsertps	$16, 96(%r13,%rdi,4), %xmm3, %xmm3 ## xmm3 = xmm3[0],mem[0],xmm3[2,3]
	movq	288(%rsp), %r10                 ## 8-byte Reload
	vinsertps	$48, 96(%r13,%r10,4), %xmm2, %xmm2 ## xmm2 = xmm2[0,1,2],mem[0]
	movq	144(%rsp), %rdi                 ## 8-byte Reload
	vinsertps	$32, 96(%r13,%rdi,4), %xmm3, %xmm3 ## xmm3 = xmm3[0,1],mem[0],xmm3[3]
	movq	344(%rsp), %rdi                 ## 8-byte Reload
	vmulps	96(%rbx,%rdi,4), %ymm5, %ymm4
	vfmadd231ps	%ymm4, %ymm1, %ymm0     ## ymm0 = (ymm1 * ymm4) + ymm0
	movq	72(%rsp), %r9                   ## 8-byte Reload
	vinsertps	$48, 96(%r13,%r9,4), %xmm3, %xmm1 ## xmm1 = xmm3[0,1,2],mem[0]
	vinsertf128	$1, %xmm2, %ymm1, %ymm1
	vaddps	%ymm10, %ymm15, %ymm10
	movq	24(%rsp), %rdi                  ## 8-byte Reload
	vmovss	96(%r13,%rdi,4), %xmm2          ## xmm2 = mem[0],zero,zero,zero
	movq	64(%rsp), %r9                   ## 8-byte Reload
	vinsertps	$16, 96(%r13,%r9,4), %xmm2, %xmm2 ## xmm2 = xmm2[0],mem[0],xmm2[2,3]
	vfmadd231ps	1568(%rsp), %ymm1, %ymm10 ## 32-byte Folded Reload
                                        ## ymm10 = (ymm1 * mem) + ymm10
	movq	104(%rsp), %rdi                 ## 8-byte Reload
	vinsertps	$32, 96(%r13,%rdi,4), %xmm2, %xmm1 ## xmm1 = xmm2[0,1],mem[0],xmm2[3]
	movq	32(%rsp), %rdi                  ## 8-byte Reload
	vmovss	96(%r13,%rdi,4), %xmm2          ## xmm2 = mem[0],zero,zero,zero
	movq	(%rsp), %rdi                    ## 8-byte Reload
	vinsertps	$16, 96(%r13,%rdi,4), %xmm2, %xmm2 ## xmm2 = xmm2[0],mem[0],xmm2[2,3]
	movq	8(%rsp), %rdi                   ## 8-byte Reload
	vinsertps	$48, 96(%r13,%rdi,4), %xmm1, %xmm1 ## xmm1 = xmm1[0,1,2],mem[0]
	movq	16(%rsp), %rdi                  ## 8-byte Reload
	vinsertps	$32, 96(%r13,%rdi,4), %xmm2, %xmm2 ## xmm2 = xmm2[0,1],mem[0],xmm2[3]
	movq	40(%rsp), %rdi                  ## 8-byte Reload
	vinsertps	$48, 96(%r13,%rdi,4), %xmm2, %xmm2 ## xmm2 = xmm2[0,1,2],mem[0]
	vinsertf128	$1, %xmm1, %ymm2, %ymm1
	vmovss	108(%r13,%rdx,4), %xmm2         ## xmm2 = mem[0],zero,zero,zero
	vinsertps	$16, 108(%r13,%rax,4), %xmm2, %xmm11 ## xmm11 = xmm2[0],mem[0],xmm2[2,3]
	vaddps	800(%rsp), %ymm13, %ymm3        ## 32-byte Folded Reload
	vmovss	96(%r13,%r11,4), %xmm2          ## xmm2 = mem[0],zero,zero,zero
	vinsertps	$16, 96(%r13,%r12,4), %xmm2, %xmm2 ## xmm2 = xmm2[0],mem[0],xmm2[2,3]
	vfmadd231ps	960(%rsp), %ymm1, %ymm3 ## 32-byte Folded Reload
                                        ## ymm3 = (ymm1 * mem) + ymm3
	movq	296(%rsp), %r12                 ## 8-byte Reload
	vinsertps	$32, 96(%r13,%r12,4), %xmm2, %xmm1 ## xmm1 = xmm2[0,1],mem[0],xmm2[3]
	movq	168(%rsp), %r11                 ## 8-byte Reload
	vmovss	96(%r13,%r11,4), %xmm2          ## xmm2 = mem[0],zero,zero,zero
	movq	112(%rsp), %rdi                 ## 8-byte Reload
	vinsertps	$16, 96(%r13,%rdi,4), %xmm2, %xmm2 ## xmm2 = xmm2[0],mem[0],xmm2[2,3]
	vinsertps	$48, 96(%r13,%r14,4), %xmm1, %xmm1 ## xmm1 = xmm1[0,1,2],mem[0]
	movq	360(%rsp), %r14                 ## 8-byte Reload
	vinsertps	$32, 96(%r13,%r14,4), %xmm2, %xmm2 ## xmm2 = xmm2[0,1],mem[0],xmm2[3]
	vinsertps	$48, 96(%r13,%rbp,4), %xmm2, %xmm2 ## xmm2 = xmm2[0,1,2],mem[0]
	vinsertf128	$1, %xmm1, %ymm2, %ymm1
	movq	392(%rsp), %rbp                 ## 8-byte Reload
	vmovss	96(%r13,%rbp,4), %xmm2          ## xmm2 = mem[0],zero,zero,zero
	vinsertps	$16, 96(%r13,%r8,4), %xmm2, %xmm4 ## xmm4 = xmm2[0],mem[0],xmm2[2,3]
	vaddps	832(%rsp), %ymm12, %ymm13       ## 32-byte Folded Reload
	movq	544(%rsp), %rdi                 ## 8-byte Reload
	vinsertps	$32, 96(%r13,%rdi,4), %xmm4, %xmm4 ## xmm4 = xmm4[0,1],mem[0],xmm4[3]
	vmovss	96(%r13,%rsi,4), %xmm5          ## xmm5 = mem[0],zero,zero,zero
	vinsertps	$16, 96(%r13,%rcx,4), %xmm5, %xmm5 ## xmm5 = xmm5[0],mem[0],xmm5[2,3]
	movq	96(%rsp), %rcx                  ## 8-byte Reload
	vinsertps	$48, 96(%r13,%rcx,4), %xmm4, %xmm4 ## xmm4 = xmm4[0,1,2],mem[0]
	vfmadd231ps	928(%rsp), %ymm1, %ymm13 ## 32-byte Folded Reload
                                        ## ymm13 = (ymm1 * mem) + ymm13
	movq	136(%rsp), %rcx                 ## 8-byte Reload
	vinsertps	$32, 96(%r13,%rcx,4), %xmm5, %xmm1 ## xmm1 = xmm5[0,1],mem[0],xmm5[3]
	vmovss	104(%r13,%rdx,4), %xmm5         ## xmm5 = mem[0],zero,zero,zero
	vinsertps	$16, 104(%r13,%rax,4), %xmm5, %xmm5 ## xmm5 = xmm5[0],mem[0],xmm5[2,3]
	vinsertps	$48, 96(%r13,%r15,4), %xmm1, %xmm1 ## xmm1 = xmm1[0,1,2],mem[0]
	vinsertf128	$1, %xmm4, %ymm1, %ymm1
	vmovss	100(%r13,%rdx,4), %xmm4         ## xmm4 = mem[0],zero,zero,zero
	vinsertps	$16, 100(%r13,%rax,4), %xmm4, %xmm4 ## xmm4 = xmm4[0],mem[0],xmm4[2,3]
	vaddps	%ymm0, %ymm14, %ymm0
	movq	56(%rsp), %rsi                  ## 8-byte Reload
	vmovss	108(%r13,%rsi,4), %xmm6         ## xmm6 = mem[0],zero,zero,zero
	movq	80(%rsp), %rax                  ## 8-byte Reload
	vinsertps	$16, 108(%r13,%rax,4), %xmm6, %xmm6 ## xmm6 = xmm6[0],mem[0],xmm6[2,3]
	vfmadd231ps	768(%rsp), %ymm1, %ymm0 ## 32-byte Folded Reload
                                        ## ymm0 = (ymm1 * mem) + ymm0
	movq	88(%rsp), %rcx                  ## 8-byte Reload
	vinsertps	$32, 104(%r13,%rcx,4), %xmm5, %xmm1 ## xmm1 = xmm5[0,1],mem[0],xmm5[3]
	vmovss	104(%r13,%rsi,4), %xmm5         ## xmm5 = mem[0],zero,zero,zero
	vinsertps	$16, 104(%r13,%rax,4), %xmm5, %xmm5 ## xmm5 = xmm5[0],mem[0],xmm5[2,3]
	vinsertps	$48, 104(%r13,%r10,4), %xmm1, %xmm1 ## xmm1 = xmm1[0,1,2],mem[0]
	movq	144(%rsp), %r8                  ## 8-byte Reload
	vinsertps	$32, 104(%r13,%r8,4), %xmm5, %xmm5 ## xmm5 = xmm5[0,1],mem[0],xmm5[3]
	movq	72(%rsp), %rdx                  ## 8-byte Reload
	vinsertps	$48, 104(%r13,%rdx,4), %xmm5, %xmm5 ## xmm5 = xmm5[0,1,2],mem[0]
	vinsertf128	$1, %xmm1, %ymm5, %ymm1
	vmovss	100(%r13,%rsi,4), %xmm5         ## xmm5 = mem[0],zero,zero,zero
	vinsertps	$16, 100(%r13,%rax,4), %xmm5, %xmm5 ## xmm5 = xmm5[0],mem[0],xmm5[2,3]
	vinsertps	$32, 108(%r13,%rcx,4), %xmm11, %xmm7 ## xmm7 = xmm11[0,1],mem[0],xmm11[3]
	vinsertps	$32, 100(%r13,%rcx,4), %xmm4, %xmm4 ## xmm4 = xmm4[0,1],mem[0],xmm4[3]
	movq	%r8, %rax
	vinsertps	$32, 108(%r13,%r8,4), %xmm6, %xmm6 ## xmm6 = xmm6[0,1],mem[0],xmm6[3]
	vinsertps	$32, 100(%r13,%r8,4), %xmm5, %xmm5 ## xmm5 = xmm5[0,1],mem[0],xmm5[3]
	vinsertps	$48, 108(%r13,%r10,4), %xmm7, %xmm2 ## xmm2 = xmm7[0,1,2],mem[0]
	vmovaps	%xmm2, 144(%rsp)                ## 16-byte Spill
	vinsertps	$48, 100(%r13,%r10,4), %xmm4, %xmm4 ## xmm4 = xmm4[0,1,2],mem[0]
	vinsertps	$48, 108(%r13,%rdx,4), %xmm6, %xmm12 ## xmm12 = xmm6[0,1,2],mem[0]
	vinsertps	$48, 100(%r13,%rdx,4), %xmm5, %xmm5 ## xmm5 = xmm5[0,1,2],mem[0]
	vinsertf128	$1, %xmm4, %ymm5, %ymm4
	vandps	1888(%rsp), %ymm4, %ymm7        ## 32-byte Folded Reload
	movq	488(%rsp), %r8                  ## 8-byte Reload
	vmovups	(%rbx,%r8,4), %ymm4
	vfmadd213ps	%ymm10, %ymm4, %ymm7    ## ymm7 = (ymm4 * ymm7) + ymm10
	movq	24(%rsp), %rax                  ## 8-byte Reload
	vmovss	108(%r13,%rax,4), %xmm5         ## xmm5 = mem[0],zero,zero,zero
	vinsertps	$16, 108(%r13,%r9,4), %xmm5, %xmm5 ## xmm5 = xmm5[0],mem[0],xmm5[2,3]
	vbroadcastss	LCPI1_2(%rip), %ymm10   ## ymm10 = [-1.0E+0,-1.0E+0,-1.0E+0,-1.0E+0,-1.0E+0,-1.0E+0,-1.0E+0,-1.0E+0]
	vaddps	1856(%rsp), %ymm10, %ymm6       ## 32-byte Folded Reload
	vmovss	104(%r13,%rax,4), %xmm2         ## xmm2 = mem[0],zero,zero,zero
	vinsertps	$16, 104(%r13,%r9,4), %xmm2, %xmm2 ## xmm2 = xmm2[0],mem[0],xmm2[2,3]
	vmulps	%ymm4, %ymm6, %ymm4
	vmovss	100(%r13,%rax,4), %xmm6         ## xmm6 = mem[0],zero,zero,zero
	vinsertps	$16, 100(%r13,%r9,4), %xmm6, %xmm6 ## xmm6 = xmm6[0],mem[0],xmm6[2,3]
	vfmadd231ps	%ymm4, %ymm1, %ymm7     ## ymm7 = (ymm1 * ymm4) + ymm7
	movq	104(%rsp), %rdx                 ## 8-byte Reload
	vinsertps	$32, 104(%r13,%rdx,4), %xmm2, %xmm1 ## xmm1 = xmm2[0,1],mem[0],xmm2[3]
	movq	32(%rsp), %rsi                  ## 8-byte Reload
	vmovss	104(%r13,%rsi,4), %xmm2         ## xmm2 = mem[0],zero,zero,zero
	movq	(%rsp), %rdi                    ## 8-byte Reload
	vinsertps	$16, 104(%r13,%rdi,4), %xmm2, %xmm2 ## xmm2 = xmm2[0],mem[0],xmm2[2,3]
	movq	8(%rsp), %rax                   ## 8-byte Reload
	vinsertps	$48, 104(%r13,%rax,4), %xmm1, %xmm1 ## xmm1 = xmm1[0,1,2],mem[0]
	movq	16(%rsp), %r9                   ## 8-byte Reload
	vinsertps	$32, 104(%r13,%r9,4), %xmm2, %xmm2 ## xmm2 = xmm2[0,1],mem[0],xmm2[3]
	movq	40(%rsp), %rcx                  ## 8-byte Reload
	vinsertps	$48, 104(%r13,%rcx,4), %xmm2, %xmm2 ## xmm2 = xmm2[0,1,2],mem[0]
	vinsertf128	$1, %xmm1, %ymm2, %ymm1
	vinsertps	$32, 100(%r13,%rdx,4), %xmm6, %xmm2 ## xmm2 = xmm6[0,1],mem[0],xmm6[3]
	vmovss	100(%r13,%rsi,4), %xmm4         ## xmm4 = mem[0],zero,zero,zero
	vinsertps	$16, 100(%r13,%rdi,4), %xmm4, %xmm4 ## xmm4 = xmm4[0],mem[0],xmm4[2,3]
	vinsertps	$48, 100(%r13,%rax,4), %xmm2, %xmm2 ## xmm2 = xmm2[0,1,2],mem[0]
	vinsertps	$32, 100(%r13,%r9,4), %xmm4, %xmm4 ## xmm4 = xmm4[0,1],mem[0],xmm4[3]
	vinsertps	$48, 100(%r13,%rcx,4), %xmm4, %xmm4 ## xmm4 = xmm4[0,1,2],mem[0]
	vinsertf128	$1, %xmm2, %ymm4, %ymm2
	vinsertps	$32, 108(%r13,%rdx,4), %xmm5, %xmm15 ## xmm15 = xmm5[0,1],mem[0],xmm5[3]
	vmovss	108(%r13,%rsi,4), %xmm5         ## xmm5 = mem[0],zero,zero,zero
	vinsertps	$16, 108(%r13,%rdi,4), %xmm5, %xmm5 ## xmm5 = xmm5[0],mem[0],xmm5[2,3]
	vinsertps	$32, 108(%r13,%r9,4), %xmm5, %xmm11 ## xmm11 = xmm5[0,1],mem[0],xmm5[3]
	vandps	1120(%rsp), %ymm2, %ymm8        ## 32-byte Folded Reload
	movq	376(%rsp), %r10                 ## 8-byte Reload
	vmovss	104(%r13,%r10,4), %xmm2         ## xmm2 = mem[0],zero,zero,zero
	movq	368(%rsp), %r9                  ## 8-byte Reload
	vinsertps	$16, 104(%r13,%r9,4), %xmm2, %xmm2 ## xmm2 = xmm2[0],mem[0],xmm2[2,3]
	vmovss	104(%r13,%r11,4), %xmm5         ## xmm5 = mem[0],zero,zero,zero
	movq	112(%rsp), %rdi                 ## 8-byte Reload
	vinsertps	$16, 104(%r13,%rdi,4), %xmm5, %xmm5 ## xmm5 = xmm5[0],mem[0],xmm5[2,3]
	vmovups	32(%rbx,%r8,4), %ymm14
	vfmadd213ps	%ymm3, %ymm14, %ymm8    ## ymm8 = (ymm14 * ymm8) + ymm3
	vaddps	1664(%rsp), %ymm10, %ymm3       ## 32-byte Folded Reload
	vinsertps	$32, 104(%r13,%r12,4), %xmm2, %xmm2 ## xmm2 = xmm2[0,1],mem[0],xmm2[3]
	vmulps	%ymm3, %ymm14, %ymm3
	movq	%r14, %rdx
	vinsertps	$32, 104(%r13,%r14,4), %xmm5, %xmm5 ## xmm5 = xmm5[0,1],mem[0],xmm5[3]
	vmovss	100(%r13,%r10,4), %xmm4         ## xmm4 = mem[0],zero,zero,zero
	vinsertps	$16, 100(%r13,%r9,4), %xmm4, %xmm4 ## xmm4 = xmm4[0],mem[0],xmm4[2,3]
	movq	160(%rsp), %r15                 ## 8-byte Reload
	vinsertps	$48, 104(%r13,%r15,4), %xmm2, %xmm2 ## xmm2 = xmm2[0,1,2],mem[0]
	vfmadd231ps	%ymm3, %ymm1, %ymm8     ## ymm8 = (ymm1 * ymm3) + ymm8
	vinsertps	$32, 100(%r13,%r12,4), %xmm4, %xmm1 ## xmm1 = xmm4[0,1],mem[0],xmm4[3]
	movq	%r12, %r14
	vmovss	100(%r13,%r11,4), %xmm3         ## xmm3 = mem[0],zero,zero,zero
	vinsertps	$16, 100(%r13,%rdi,4), %xmm3, %xmm3 ## xmm3 = xmm3[0],mem[0],xmm3[2,3]
	movq	120(%rsp), %rcx                 ## 8-byte Reload
	vinsertps	$48, 104(%r13,%rcx,4), %xmm5, %xmm4 ## xmm4 = xmm5[0,1,2],mem[0]
	vinsertps	$32, 100(%r13,%rdx,4), %xmm3, %xmm3 ## xmm3 = xmm3[0,1],mem[0],xmm3[3]
	vinsertps	$48, 100(%r13,%r15,4), %xmm1, %xmm1 ## xmm1 = xmm1[0,1,2],mem[0]
	vinsertf128	$1, %xmm2, %ymm4, %ymm2
	vinsertps	$48, 100(%r13,%rcx,4), %xmm3, %xmm3 ## xmm3 = xmm3[0,1,2],mem[0]
	vinsertf128	$1, %xmm1, %ymm3, %ymm1
	vmovss	108(%r13,%r10,4), %xmm3         ## xmm3 = mem[0],zero,zero,zero
	vinsertps	$16, 108(%r13,%r9,4), %xmm3, %xmm14 ## xmm14 = xmm3[0],mem[0],xmm3[2,3]
	vandps	1600(%rsp), %ymm1, %ymm3        ## 32-byte Folded Reload
	vmovups	64(%rbx,%r8,4), %ymm1
	vfmadd213ps	%ymm13, %ymm1, %ymm3    ## ymm3 = (ymm1 * ymm3) + ymm13
	vmovss	108(%r13,%r11,4), %xmm5         ## xmm5 = mem[0],zero,zero,zero
	vaddps	1632(%rsp), %ymm10, %ymm13      ## 32-byte Folded Reload
	vmulps	%ymm1, %ymm13, %ymm1
	movq	%rbp, %rsi
	vmovss	104(%r13,%rbp,4), %xmm6         ## xmm6 = mem[0],zero,zero,zero
	movq	384(%rsp), %rbp                 ## 8-byte Reload
	vinsertps	$16, 104(%r13,%rbp,4), %xmm6, %xmm6 ## xmm6 = xmm6[0],mem[0],xmm6[2,3]
	vfmadd231ps	%ymm1, %ymm2, %ymm3     ## ymm3 = (ymm2 * ymm1) + ymm3
	movq	512(%rsp), %r9                  ## 8-byte Reload
	vmovss	104(%r13,%r9,4), %xmm1          ## xmm1 = mem[0],zero,zero,zero
	movq	544(%rsp), %rcx                 ## 8-byte Reload
	vinsertps	$32, 104(%r13,%rcx,4), %xmm6, %xmm2 ## xmm2 = xmm6[0,1],mem[0],xmm6[3]
	movq	128(%rsp), %r11                 ## 8-byte Reload
	vinsertps	$16, 104(%r13,%r11,4), %xmm1, %xmm1 ## xmm1 = xmm1[0],mem[0],xmm1[2,3]
	movq	96(%rsp), %rax                  ## 8-byte Reload
	vinsertps	$48, 104(%r13,%rax,4), %xmm2, %xmm2 ## xmm2 = xmm2[0,1,2],mem[0]
	movq	136(%rsp), %r12                 ## 8-byte Reload
	vinsertps	$32, 104(%r13,%r12,4), %xmm1, %xmm1 ## xmm1 = xmm1[0,1],mem[0],xmm1[3]
	movq	400(%rsp), %r10                 ## 8-byte Reload
	vinsertps	$48, 104(%r13,%r10,4), %xmm1, %xmm1 ## xmm1 = xmm1[0,1,2],mem[0]
	vinsertf128	$1, %xmm2, %ymm1, %ymm1
	vmovss	100(%r13,%rsi,4), %xmm2         ## xmm2 = mem[0],zero,zero,zero
	vinsertps	$16, 100(%r13,%rbp,4), %xmm2, %xmm2 ## xmm2 = xmm2[0],mem[0],xmm2[2,3]
	vaddps	1056(%rsp), %ymm10, %ymm6       ## 32-byte Folded Reload
	vmovss	100(%r13,%r9,4), %xmm4          ## xmm4 = mem[0],zero,zero,zero
	vinsertps	$32, 100(%r13,%rcx,4), %xmm2, %xmm2 ## xmm2 = xmm2[0,1],mem[0],xmm2[3]
	vinsertps	$16, 100(%r13,%r11,4), %xmm4, %xmm4 ## xmm4 = xmm4[0],mem[0],xmm4[2,3]
	vinsertps	$48, 100(%r13,%rax,4), %xmm2, %xmm2 ## xmm2 = xmm2[0,1,2],mem[0]
	vinsertps	$32, 100(%r13,%r12,4), %xmm4, %xmm4 ## xmm4 = xmm4[0,1],mem[0],xmm4[3]
	vinsertps	$48, 100(%r13,%r10,4), %xmm4, %xmm4 ## xmm4 = xmm4[0,1,2],mem[0]
	vinsertf128	$1, %xmm2, %ymm4, %ymm2
	vandps	1024(%rsp), %ymm2, %ymm2        ## 32-byte Folded Reload
	vmovups	96(%rbx,%r8,4), %ymm4
	vfmadd213ps	%ymm0, %ymm4, %ymm2     ## ymm2 = (ymm4 * ymm2) + ymm0
	vmulps	%ymm4, %ymm6, %ymm0
	vfmadd231ps	%ymm0, %ymm1, %ymm2     ## ymm2 = (ymm1 * ymm0) + ymm2
	vinsertps	$32, 108(%r13,%r14,4), %xmm14, %xmm0 ## xmm0 = xmm14[0,1],mem[0],xmm14[3]
	vinsertps	$16, 108(%r13,%rdi,4), %xmm5, %xmm1 ## xmm1 = xmm5[0],mem[0],xmm5[2,3]
	vinsertps	$32, 108(%r13,%rdx,4), %xmm1, %xmm1 ## xmm1 = xmm1[0,1],mem[0],xmm1[3]
	vinsertf128	$1, 144(%rsp), %ymm12, %ymm12 ## 16-byte Folded Reload
	vmovss	108(%r13,%rsi,4), %xmm5         ## xmm5 = mem[0],zero,zero,zero
	movq	8(%rsp), %rdx                   ## 8-byte Reload
	vinsertps	$48, 108(%r13,%rdx,4), %xmm15, %xmm6 ## xmm6 = xmm15[0,1,2],mem[0]
	vinsertps	$16, 108(%r13,%rbp,4), %xmm5, %xmm5 ## xmm5 = xmm5[0],mem[0],xmm5[2,3]
	movq	40(%rsp), %rdx                  ## 8-byte Reload
	vinsertps	$48, 108(%r13,%rdx,4), %xmm11, %xmm10 ## xmm10 = xmm11[0,1,2],mem[0]
	vinsertps	$32, 108(%r13,%rcx,4), %xmm5, %xmm5 ## xmm5 = xmm5[0,1],mem[0],xmm5[3]
	vinsertps	$48, 108(%r13,%r15,4), %xmm0, %xmm0 ## xmm0 = xmm0[0,1,2],mem[0]
	vinsertf128	$1, %xmm6, %ymm10, %ymm6
	vmovss	108(%r13,%r9,4), %xmm4          ## xmm4 = mem[0],zero,zero,zero
	movq	120(%rsp), %rcx                 ## 8-byte Reload
	vinsertps	$48, 108(%r13,%rcx,4), %xmm1, %xmm1 ## xmm1 = xmm1[0,1,2],mem[0]
	vinsertps	$16, 108(%r13,%r11,4), %xmm4, %xmm4 ## xmm4 = xmm4[0],mem[0],xmm4[2,3]
	vinsertps	$48, 108(%r13,%rax,4), %xmm5, %xmm5 ## xmm5 = xmm5[0,1,2],mem[0]
	vinsertps	$32, 108(%r13,%r12,4), %xmm4, %xmm4 ## xmm4 = xmm4[0,1],mem[0],xmm4[3]
	vinsertps	$48, 108(%r13,%r10,4), %xmm4, %xmm4 ## xmm4 = xmm4[0,1,2],mem[0]
	vinsertf128	$1, %xmm0, %ymm1, %ymm0
	vinsertf128	$1, %xmm5, %ymm4, %ymm1
	movq	496(%rsp), %rax                 ## 8-byte Reload
	vfmadd231ps	(%rbx,%rax,4), %ymm12, %ymm7 ## ymm7 = (ymm12 * mem) + ymm7
	vfmadd231ps	32(%rbx,%rax,4), %ymm6, %ymm8 ## ymm8 = (ymm6 * mem) + ymm8
	vfmadd231ps	64(%rbx,%rax,4), %ymm0, %ymm3 ## ymm3 = (ymm0 * mem) + ymm3
	vfmadd231ps	96(%rbx,%rax,4), %ymm1, %ymm2 ## ymm2 = (ymm1 * mem) + ymm2
	vbroadcastss	LCPI1_18(%rip), %ymm0   ## ymm0 = [9.99999971E-10,9.99999971E-10,9.99999971E-10,9.99999971E-10,9.99999971E-10,9.99999971E-10,9.99999971E-10,9.99999971E-10]
	vmovups	1280(%rsp), %ymm1               ## 32-byte Reload
	vfmadd231ps	%ymm7, %ymm0, %ymm1     ## ymm1 = (ymm0 * ymm7) + ymm1
	vmovups	%ymm1, 1280(%rsp)               ## 32-byte Spill
	vmovups	1248(%rsp), %ymm1               ## 32-byte Reload
	vfmadd231ps	%ymm8, %ymm0, %ymm1     ## ymm1 = (ymm0 * ymm8) + ymm1
	vmovups	%ymm1, 1248(%rsp)               ## 32-byte Spill
	vmovups	1216(%rsp), %ymm1               ## 32-byte Reload
	vfmadd231ps	%ymm3, %ymm0, %ymm1     ## ymm1 = (ymm0 * ymm3) + ymm1
	vmovups	%ymm1, 1216(%rsp)               ## 32-byte Spill
	vmovups	1184(%rsp), %ymm1               ## 32-byte Reload
	vfmadd231ps	%ymm2, %ymm0, %ymm1     ## ymm1 = (ymm0 * ymm2) + ymm1
	vmovups	%ymm1, 1184(%rsp)               ## 32-byte Spill
	vpbroadcastq	LCPI1_19(%rip), %ymm0   ## ymm0 = [32,32,32,32]
	vmovdqu	1440(%rsp), %ymm1               ## 32-byte Reload
	vpaddq	%ymm0, %ymm1, %ymm1
	vmovdqu	1920(%rsp), %ymm4               ## 32-byte Reload
	vpaddq	%ymm0, %ymm4, %ymm4
	addq	696(%rsp), %rbx                 ## 8-byte Folded Reload
	movq	%rbx, 448(%rsp)                 ## 8-byte Spill
	addq	$-32, 704(%rsp)                 ## 8-byte Folded Spill
	jne	LBB1_37
## %bb.38:                              ## %middle.block
                                        ##   in Loop: Header=BB1_12 Depth=1
	vmovups	1248(%rsp), %ymm0               ## 32-byte Reload
	vaddps	1280(%rsp), %ymm0, %ymm0        ## 32-byte Folded Reload
	vaddps	1216(%rsp), %ymm0, %ymm0        ## 32-byte Folded Reload
	vaddps	1184(%rsp), %ymm0, %ymm0        ## 32-byte Folded Reload
	vextractf128	$1, %ymm0, %xmm1
	vaddps	%xmm1, %xmm0, %xmm0
	vpermilpd	$1, %xmm0, %xmm1        ## xmm1 = xmm0[1,0]
	vaddps	%xmm1, %xmm0, %xmm0
	vmovshdup	%xmm0, %xmm1            ## xmm1 = xmm0[1,1,3,3]
	vaddss	%xmm1, %xmm0, %xmm8
	movq	592(%rsp), %rax                 ## 8-byte Reload
	movq	%rax, %rcx
	cmpq	312(%rsp), %rax                 ## 8-byte Folded Reload
	vmovaps	912(%rsp), %xmm13               ## 16-byte Reload
	vmovss	LCPI1_0(%rip), %xmm15           ## xmm15 = mem[0],zero,zero,zero
	movq	656(%rsp), %r12                 ## 8-byte Reload
	movq	648(%rsp), %r11                 ## 8-byte Reload
	movq	640(%rsp), %rbx                 ## 8-byte Reload
	jne	LBB1_42
LBB1_39:                                ## %"consume f1.loopexit"
                                        ##   in Loop: Header=BB1_12 Depth=1
	movq	184(%rsp), %rsi                 ## 8-byte Reload
	movq	600(%rsp), %rax                 ## 8-byte Reload
	movq	608(%rsp), %rdx                 ## 8-byte Reload
	leaq	(%rdx,%rax), %rcx
	vblendps	$1, %ymm8, %ymm9, %ymm9         ## ymm9 = ymm8[0],ymm9[1,2,3,4,5,6,7]
LBB1_40:                                ## %"consume f1"
                                        ##   in Loop: Header=BB1_12 Depth=1
	movq	680(%rsp), %rax                 ## 8-byte Reload
	vmovss	%xmm9, (%rax,%rcx,4)
	incq	%rdx
	addq	$4, 616(%rsp)                   ## 8-byte Folded Spill
	movq	264(%rsp), %rax                 ## 8-byte Reload
	incl	%eax
	movl	%eax, 40(%rsp)                  ## 4-byte Spill
	movq	256(%rsp), %rax                 ## 8-byte Reload
	leal	1(%rax), %eax
	movl	%eax, 32(%rsp)                  ## 4-byte Spill
	movq	248(%rsp), %rax                 ## 8-byte Reload
	leal	1(%rax), %eax
	movl	%eax, 24(%rsp)                  ## 4-byte Spill
	leal	1(%rsi), %eax
	movl	%eax, 64(%rsp)                  ## 4-byte Spill
	movq	240(%rsp), %rax                 ## 8-byte Reload
	leal	1(%rax), %eax
	movq	%rax, 16(%rsp)                  ## 8-byte Spill
	movq	352(%rsp), %rax                 ## 8-byte Reload
	leal	1(%rax), %eax
	movl	%eax, 56(%rsp)                  ## 4-byte Spill
	movq	344(%rsp), %rax                 ## 8-byte Reload
	leal	1(%rax), %eax
	movl	%eax, 88(%rsp)                  ## 4-byte Spill
	movq	336(%rsp), %rax                 ## 8-byte Reload
	leal	1(%rax), %eax
	movl	%eax, 80(%rsp)                  ## 4-byte Spill
	movq	328(%rsp), %rax                 ## 8-byte Reload
	leal	1(%rax), %eax
	movq	%rax, 144(%rsp)                 ## 8-byte Spill
	movq	232(%rsp), %rax                 ## 8-byte Reload
	leal	1(%rax), %eax
	movl	%eax, 72(%rsp)                  ## 4-byte Spill
	movq	440(%rsp), %rax                 ## 8-byte Reload
	leal	1(%rax), %eax
	movl	%eax, 512(%rsp)                 ## 4-byte Spill
	movq	496(%rsp), %rax                 ## 8-byte Reload
	leal	1(%rax), %eax
	movl	%eax, 120(%rsp)                 ## 4-byte Spill
	movq	488(%rsp), %rax                 ## 8-byte Reload
	leal	1(%rax), %r14d
	movq	224(%rsp), %rax                 ## 8-byte Reload
	leal	1(%rax), %eax
	movl	%eax, 104(%rsp)                 ## 4-byte Spill
	movq	216(%rsp), %rax                 ## 8-byte Reload
	leal	1(%rax), %eax
	movl	%eax, 112(%rsp)                 ## 4-byte Spill
	movq	320(%rsp), %rax                 ## 8-byte Reload
	leal	1(%rax), %eax
	movq	%rax, 8(%rsp)                   ## 8-byte Spill
	movq	208(%rsp), %rax                 ## 8-byte Reload
	leal	1(%rax), %r8d
	movq	200(%rsp), %rax                 ## 8-byte Reload
	leal	1(%rax), %r9d
	movq	480(%rsp), %rax                 ## 8-byte Reload
	leal	1(%rax), %r10d
	movq	472(%rsp), %rax                 ## 8-byte Reload
	incl	%eax
	movq	%rax, 544(%rsp)                 ## 8-byte Spill
	movq	432(%rsp), %rax                 ## 8-byte Reload
	leal	1(%rax), %ebp
	movq	192(%rsp), %rax                 ## 8-byte Reload
	incl	%eax
	movq	%rax, (%rsp)                    ## 8-byte Spill
	cmpq	$8, %rdx
	movq	456(%rsp), %rdi                 ## 8-byte Reload
	movq	424(%rsp), %r15                 ## 8-byte Reload
	vxorps	%xmm12, %xmm12, %xmm12
	jne	LBB1_12
	jmp	LBB1_47
	.p2align	4, 0x90
LBB1_41:                                ##   in Loop: Header=BB1_12 Depth=1
	vxorps	%xmm8, %xmm8, %xmm8
	xorl	%ecx, %ecx
LBB1_42:                                ## %"for f1.s1.r79$x.preheader326"
                                        ##   in Loop: Header=BB1_12 Depth=1
	movq	%rcx, %rdx
	shlq	$7, %rdx
	addq	664(%rsp), %rdx                 ## 8-byte Folded Reload
	movq	416(%rsp), %r10                 ## 8-byte Reload
	imulq	%rcx, %r10
	movq	264(%rsp), %rax                 ## 8-byte Reload
	addq	%r10, %rax
	movq	%rax, (%rsp)                    ## 8-byte Spill
	movq	256(%rsp), %rax                 ## 8-byte Reload
	addq	%r10, %rax
	movq	%rax, 40(%rsp)                  ## 8-byte Spill
	movq	248(%rsp), %rax                 ## 8-byte Reload
	addq	%r10, %rax
	movq	%rax, 24(%rsp)                  ## 8-byte Spill
	movq	184(%rsp), %rax                 ## 8-byte Reload
	addq	%r10, %rax
	movq	%rax, 32(%rsp)                  ## 8-byte Spill
	movq	240(%rsp), %rax                 ## 8-byte Reload
	addq	%r10, %rax
	movq	%rax, 144(%rsp)                 ## 8-byte Spill
	movq	352(%rsp), %rax                 ## 8-byte Reload
	addq	%r10, %rax
	movq	%rax, 8(%rsp)                   ## 8-byte Spill
	movq	344(%rsp), %rax                 ## 8-byte Reload
	addq	%r10, %rax
	movq	%rax, 16(%rsp)                  ## 8-byte Spill
	movq	336(%rsp), %rax                 ## 8-byte Reload
	addq	%r10, %rax
	movq	%rax, 64(%rsp)                  ## 8-byte Spill
	movq	328(%rsp), %rax                 ## 8-byte Reload
	addq	%r10, %rax
	movq	%rax, 88(%rsp)                  ## 8-byte Spill
	movq	232(%rsp), %rax                 ## 8-byte Reload
	addq	%r10, %rax
	movq	%rax, 56(%rsp)                  ## 8-byte Spill
	movq	440(%rsp), %rax                 ## 8-byte Reload
	addq	%r10, %rax
	movq	%rax, 80(%rsp)                  ## 8-byte Spill
	movq	496(%rsp), %rax                 ## 8-byte Reload
	addq	%r10, %rax
	movq	%rax, 544(%rsp)                 ## 8-byte Spill
	movq	488(%rsp), %rax                 ## 8-byte Reload
	addq	%r10, %rax
	movq	%rax, 72(%rsp)                  ## 8-byte Spill
	movq	224(%rsp), %rax                 ## 8-byte Reload
	addq	%r10, %rax
	movq	%rax, 512(%rsp)                 ## 8-byte Spill
	movq	216(%rsp), %rax                 ## 8-byte Reload
	addq	%r10, %rax
	movq	%rax, 112(%rsp)                 ## 8-byte Spill
	movq	320(%rsp), %rax                 ## 8-byte Reload
	addq	%r10, %rax
	movq	%rax, 120(%rsp)                 ## 8-byte Spill
	movq	208(%rsp), %rax                 ## 8-byte Reload
	addq	%r10, %rax
	movq	%rax, 104(%rsp)                 ## 8-byte Spill
	movq	200(%rsp), %rax                 ## 8-byte Reload
	addq	%r10, %rax
	movq	%rax, 136(%rsp)                 ## 8-byte Spill
	movq	480(%rsp), %rax                 ## 8-byte Reload
	addq	%r10, %rax
	movq	%rax, 96(%rsp)                  ## 8-byte Spill
	movq	472(%rsp), %rax                 ## 8-byte Reload
	addq	%r10, %rax
	movq	%rax, 128(%rsp)                 ## 8-byte Spill
	movq	432(%rsp), %r9                  ## 8-byte Reload
	addq	%r10, %r9
	addq	192(%rsp), %r10                 ## 8-byte Folded Reload
	movq	464(%rsp), %r14                 ## 8-byte Reload
	jmp	LBB1_44
	.p2align	4, 0x90
LBB1_43:                                ## %"for f1.s1.r79$x"
                                        ##   in Loop: Header=BB1_44 Depth=2
	cmovbq	%r8, %rax
	movq	88(%rsp), %rsi                  ## 8-byte Reload
	vmulss	(%r14,%rsi,4), %xmm1, %xmm2
	vmulss	(%rax), %xmm2, %xmm2
	cmovbq	%r15, %rbp
	vdivss	%xmm0, %xmm2, %xmm0
	movq	64(%rsp), %rax                  ## 8-byte Reload
	vmovss	(%r14,%rax,4), %xmm2            ## xmm2 = mem[0],zero,zero,zero
	vfmadd132ss	(%rbp), %xmm0, %xmm2    ## xmm2 = (xmm2 * mem) + xmm0
	vmovss	(%r14,%r10,4), %xmm0            ## xmm0 = mem[0],zero,zero,zero
	vfmadd213ss	%xmm6, %xmm0, %xmm2     ## xmm2 = (xmm0 * xmm2) + xmm6
	vfmadd132ss	-16(%rdx), %xmm2, %xmm7 ## xmm7 = (xmm7 * mem) + xmm2
	vfmadd231ss	LCPI1_17(%rip), %xmm7, %xmm4 ## xmm4 = (xmm7 * mem) + xmm4
	vinsertps	$16, %xmm11, %xmm3, %xmm2 ## xmm2 = xmm3[0],xmm11[0],xmm3[2,3]
	vshufps	$4, %xmm10, %xmm2, %xmm3        ## xmm3 = xmm2[0,1],xmm10[0,0]
	vmovddup	%xmm2, %xmm2                    ## xmm2 = xmm2[0,0]
	vinsertf128	$1, %xmm3, %ymm2, %ymm2
	movq	32(%rsp), %rax                  ## 8-byte Reload
	vmovss	(%r14,%rax,4), %xmm3            ## xmm3 = mem[0],zero,zero,zero
	movq	144(%rsp), %rax                 ## 8-byte Reload
	vinsertps	$16, (%r14,%rax,4), %xmm3, %xmm3 ## xmm3 = xmm3[0],mem[0],xmm3[2,3]
	movq	512(%rsp), %rax                 ## 8-byte Reload
	vmovss	(%r14,%rax,4), %xmm5            ## xmm5 = mem[0],zero,zero,zero
	vshufps	$4, %xmm5, %xmm3, %xmm3         ## xmm3 = xmm3[0,1],xmm5[0,0]
	movq	112(%rsp), %rax                 ## 8-byte Reload
	vbroadcastss	(%r14,%rax,4), %xmm5
	movq	8(%rsp), %rax                   ## 8-byte Reload
	vinsertps	$32, (%r14,%rax,4), %xmm5, %xmm5 ## xmm5 = xmm5[0,1],mem[0],xmm5[3]
	movq	16(%rsp), %rax                  ## 8-byte Reload
	vinsertps	$48, (%r14,%rax,4), %xmm5, %xmm5 ## xmm5 = xmm5[0,1,2],mem[0]
	vinsertf128	$1, %xmm5, %ymm3, %ymm3
	vmulps	%ymm3, %ymm2, %ymm2
	vmulps	-76(%rdx), %ymm2, %ymm2
	vextractf128	$1, %ymm2, %xmm3
	vaddps	%xmm3, %xmm2, %xmm2
	vpermilpd	$1, %xmm2, %xmm3        ## xmm3 = xmm2[1,0]
	vaddps	%xmm3, %xmm2, %xmm2
	vmovshdup	%xmm2, %xmm3            ## xmm3 = xmm2[1,1,3,3]
	vaddss	%xmm3, %xmm2, %xmm2
	movq	24(%rsp), %rax                  ## 8-byte Reload
	vmulss	(%r14,%rax,4), %xmm11, %xmm3
	vfmadd231ss	-80(%rdx), %xmm3, %xmm2 ## xmm2 = (xmm3 * mem) + xmm2
	movq	40(%rsp), %rax                  ## 8-byte Reload
	vmulss	(%r14,%rax,4), %xmm0, %xmm3
	vfmadd231ss	-88(%rdx), %xmm3, %xmm2 ## xmm2 = (xmm3 * mem) + xmm2
	movq	(%rsp), %rax                    ## 8-byte Reload
	vmulss	(%r14,%rax,4), %xmm0, %xmm3
	vfmadd231ss	-84(%rdx), %xmm3, %xmm2 ## xmm2 = (xmm3 * mem) + xmm2
	vaddss	%xmm2, %xmm4, %xmm2
	vfmadd231ss	-12(%rdx), %xmm0, %xmm2 ## xmm2 = (xmm0 * mem) + xmm2
	vcmpltss	%xmm1, %xmm15, %xmm0
	vmovss	-8(%rdx), %xmm3                 ## xmm3 = mem[0],zero,zero,zero
	vandps	%xmm3, %xmm0, %xmm0
	movq	72(%rsp), %rax                  ## 8-byte Reload
	vmovss	(%r14,%rax,4), %xmm3            ## xmm3 = mem[0],zero,zero,zero
	vfmadd213ss	%xmm2, %xmm3, %xmm0     ## xmm0 = (xmm3 * xmm0) + xmm2
	vaddss	LCPI1_2(%rip), %xmm1, %xmm1
	vmulss	%xmm3, %xmm1, %xmm1
	vfmadd231ss	-4(%rdx), %xmm1, %xmm0  ## xmm0 = (xmm1 * mem) + xmm0
	movq	544(%rsp), %rax                 ## 8-byte Reload
	vmovss	(%r14,%rax,4), %xmm1            ## xmm1 = mem[0],zero,zero,zero
	vfmadd231ss	(%rdx), %xmm1, %xmm0    ## xmm0 = (xmm1 * mem) + xmm0
	vfmadd231ss	LCPI1_18(%rip), %xmm0, %xmm8 ## xmm8 = (xmm0 * mem) + xmm8
	incq	%rcx
	subq	$-128, %rdx
	addq	712(%rsp), %r14                 ## 8-byte Folded Reload
	cmpq	%rcx, 312(%rsp)                 ## 8-byte Folded Reload
	je	LBB1_39
LBB1_44:                                ## %"for f1.s1.r79$x"
                                        ##   Parent Loop BB1_12 Depth=1
                                        ## =>  This Inner Loop Header: Depth=2
	vmovss	(%r14,%r9,4), %xmm11            ## xmm11 = mem[0],zero,zero,zero
	movq	96(%rsp), %rax                  ## 8-byte Reload
	vmovss	(%r14,%rax,4), %xmm3            ## xmm3 = mem[0],zero,zero,zero
	movq	136(%rsp), %rax                 ## 8-byte Reload
	vmovss	(%r14,%rax,4), %xmm1            ## xmm1 = mem[0],zero,zero,zero
	movq	104(%rsp), %rax                 ## 8-byte Reload
	vmulss	(%r14,%rax,4), %xmm1, %xmm12
	vmaxss	%xmm15, %xmm12, %xmm10
	vdivss	%xmm13, %xmm10, %xmm4
	movq	120(%rsp), %rax                 ## 8-byte Reload
	vmovss	(%r14,%rax,4), %xmm0            ## xmm0 = mem[0],zero,zero,zero
	leaq	-40(%rdx), %rdi
	leaq	-36(%rdx), %rax
	leaq	-28(%rdx), %rsi
	leaq	-24(%rdx), %rbp
	testq	%rcx, %rcx
	cmoveq	%rdi, %rax
	cmoveq	%rsi, %rbp
	leaq	-44(%rdx), %r8
	leaq	-32(%rdx), %r15
	movq	128(%rsp), %rsi                 ## 8-byte Reload
	vmulss	(%r14,%rsi,4), %xmm11, %xmm6
	vmulss	-104(%rdx), %xmm3, %xmm5
	vfmadd231ss	-108(%rdx), %xmm6, %xmm5 ## xmm5 = (xmm6 * mem) + xmm5
	vmulss	-96(%rdx), %xmm3, %xmm2
	vfmadd231ss	-100(%rdx), %xmm6, %xmm2 ## xmm2 = (xmm6 * mem) + xmm2
	vucomiss	%xmm1, %xmm15
	vxorps	%xmm6, %xmm6, %xmm6
	movq	80(%rsp), %rsi                  ## 8-byte Reload
	vcmpeqss	(%r14,%rsi,4), %xmm6, %xmm7
	vmaxss	%xmm15, %xmm0, %xmm0
	vblendvps	%xmm7, %xmm5, %xmm2, %xmm2
	vroundss	$10, %xmm4, %xmm4, %xmm5
	vmaxss	%xmm15, %xmm4, %xmm4
	vmulss	%xmm2, %xmm5, %xmm2
	vdivss	%xmm4, %xmm2, %xmm4
	vmovss	LCPI1_16(%rip), %xmm2           ## xmm2 = mem[0],zero,zero,zero
	vdivss	%xmm0, %xmm2, %xmm2
	vminss	%xmm1, %xmm2, %xmm2
	movq	56(%rsp), %rsi                  ## 8-byte Reload
	vmulss	(%r14,%rsi,4), %xmm2, %xmm2
	vmulss	%xmm2, %xmm12, %xmm7
	jae	LBB1_43
## %bb.45:                              ##   in Loop: Header=BB1_44 Depth=2
	vaddss	%xmm3, %xmm11, %xmm2
	vmulss	-20(%rdx), %xmm2, %xmm2
	vdivss	%xmm0, %xmm2, %xmm6
	jmp	LBB1_43
LBB1_46:                                ## %"produce f1.consume f1_crit_edge"
                                        ##   in Loop: Header=BB1_12 Depth=1
	movq	600(%rsp), %rax                 ## 8-byte Reload
	movq	608(%rsp), %rdx                 ## 8-byte Reload
	leaq	(%rdx,%rax), %rcx
	jmp	LBB1_40
LBB1_47:                                ## %call_destructor.exit40
	movq	176(%rsp), %rbp                 ## 8-byte Reload
	movq	%rbp, %rdi
	movq	408(%rsp), %rsi                 ## 8-byte Reload
	vzeroupper
	callq	_halide_free
	movq	%rbp, %rdi
	movq	%r15, %rsi
	callq	_halide_free
	movq	%rbp, %rdi
	movq	%r13, %rsi
	callq	_halide_free
	xorl	%r13d, %r13d
	xorl	%ebx, %ebx
LBB1_48:                                ## %call_destructor.exit.thread
	testl	%ebx, %ebx
	setne	%al
	testb	%al, %al
	je	LBB1_51
LBB1_49:                                ## %call_destructor.exit36
	testq	%r13, %r13
	je	LBB1_51
## %bb.50:
	movq	%rbp, %rdi
	movq	%r13, %rsi
	callq	_halide_free
LBB1_51:                                ## %call_destructor.exit37
	movl	%ebx, %eax
	addq	$1960, %rsp                     ## imm = 0x7A8
	popq	%rbx
	popq	%r12
	popq	%r13
	popq	%r14
	popq	%r15
	popq	%rbp
	retq
LBB1_52:                                ## %"assert failed"
	leaq	l_str.61(%rip), %rsi
	movl	$2147483647, %ecx               ## imm = 0x7FFFFFFF
	movq	%rbp, %rdi
	movq	%rax, %rdx
	callq	_halide_error_buffer_allocation_too_large
	jmp	LBB1_54
LBB1_53:                                ## %"assert failed1"
	movq	176(%rsp), %rbp                 ## 8-byte Reload
	movq	%rbp, %rdi
	callq	_halide_error_out_of_memory
LBB1_54:                                ## %call_destructor.exit.thread
	movl	%eax, %ebx
	xorl	%r13d, %r13d
	jmp	LBB1_48
LBB1_55:                                ## %"assert failed3"
	leaq	l_str.62(%rip), %rsi
	movl	$2147483647, %ecx               ## imm = 0x7FFFFFFF
	movq	176(%rsp), %rbp                 ## 8-byte Reload
	movq	%rbp, %rdi
	vzeroupper
	callq	_halide_error_buffer_allocation_too_large
	jmp	LBB1_57
LBB1_56:                                ## %"assert failed5"
	movq	%rbp, %rdi
	callq	_halide_error_out_of_memory
LBB1_57:                                ## %call_destructor.exit.thread
	movl	%eax, %ebx
	jmp	LBB1_48
LBB1_58:                                ## %"assert failed7"
	movq	408(%rsp), %r13                 ## 8-byte Reload
	leaq	l_str.63(%rip), %rsi
	movl	$2147483647, %ecx               ## imm = 0x7FFFFFFF
	movq	%rbp, %rdi
	movq	%r15, %rdx
	callq	_halide_error_buffer_allocation_too_large
	jmp	LBB1_60
LBB1_59:                                ## %"assert failed9"
	movq	408(%rsp), %r13                 ## 8-byte Reload
	movq	176(%rsp), %rdi                 ## 8-byte Reload
	callq	_halide_error_out_of_memory
LBB1_60:                                ## %call_destructor.exit
	movl	%eax, %ebx
	testl	%ebx, %ebx
	je	LBB1_63
## %bb.62:
	movq	176(%rsp), %rbp                 ## 8-byte Reload
	movq	%rbp, %rdi
	movq	424(%rsp), %rsi                 ## 8-byte Reload
	callq	_halide_free
	movb	$1, %al
	testb	%al, %al
	jne	LBB1_49
	jmp	LBB1_51
LBB1_63:
	xorl	%ebx, %ebx
	jmp	LBB1_51
	.cfi_endproc
                                        ## -- End function
	.globl	_cost_model_argv                ## -- Begin function cost_model_argv
	.p2align	4, 0x90
_cost_model_argv:                       ## @cost_model_argv
	.cfi_startproc
## %bb.0:                               ## %entry
	subq	$88, %rsp
	.cfi_def_cfa_offset 96
	movq	(%rdi), %rax
	movq	8(%rdi), %rcx
	movl	(%rax), %r10d
	movl	(%rcx), %esi
	movq	16(%rdi), %rcx
	movl	(%rcx), %edx
	movq	24(%rdi), %rcx
	movq	32(%rdi), %r8
	movq	40(%rdi), %r9
	vmovups	48(%rdi), %ymm0
	movq	80(%rdi), %rax
	vmovups	120(%rdi), %xmm1
	vmovups	%xmm1, 64(%rsp)
	movq	%rax, 32(%rsp)
	vmovups	%ymm0, (%rsp)
	movl	%r10d, %edi
	vzeroupper
	callq	_cost_model
	addq	$88, %rsp
	retq
	.cfi_endproc
                                        ## -- End function
	.globl	_cost_model_metadata            ## -- Begin function cost_model_metadata
	.p2align	4, 0x90
_cost_model_metadata:                   ## @cost_model_metadata
## %bb.0:                               ## %entry
	leaq	l_cost_model_metadata_storage(%rip), %rax
	retq
                                        ## -- End function
	.section	__TEXT,__const
	.p2align	5                               ## @str
l_str:
	.asciz	"schedule_features"

	.p2align	5                               ## @str.3
l_str.3:
	.asciz	"prediction_output"

	.p2align	5                               ## @str.4
l_str.4:
	.asciz	"pipeline_features"

	.p2align	5                               ## @str.5
l_str.5:
	.asciz	"loss_output"

	.p2align	5                               ## @str.6
l_str.6:
	.asciz	"head2_filter"

	.p2align	5                               ## @str.7
l_str.7:
	.asciz	"head2_bias"

	.p2align	5                               ## @str.8
l_str.8:
	.asciz	"head1_filter"

	.p2align	5                               ## @str.9
l_str.9:
	.asciz	"head1_bias"

	.p2align	5                               ## @str.10
l_str.10:
	.asciz	"filter1"

	.p2align	5                               ## @str.11
l_str.11:
	.asciz	"bias1"

	.p2align	5                               ## @str.12
l_str.12:
	.asciz	"Input buffer filter1"

	.p2align	5                               ## @str.13
l_str.13:
	.asciz	"Input buffer bias1"

	.p2align	5                               ## @str.14
l_str.14:
	.asciz	"Input buffer head1_bias"

	.p2align	5                               ## @str.15
l_str.15:
	.asciz	"Input buffer head1_filter"

	.p2align	5                               ## @str.16
l_str.16:
	.asciz	"Input buffer head2_bias"

	.p2align	5                               ## @str.17
l_str.17:
	.asciz	"Input buffer head2_filter"

	.p2align	5                               ## @str.18
l_str.18:
	.asciz	"Output buffer loss_output"

	.p2align	5                               ## @str.19
l_str.19:
	.asciz	"Input buffer pipeline_features"

	.p2align	5                               ## @str.20
l_str.20:
	.asciz	"Output buffer prediction_output"

	.p2align	5                               ## @str.21
l_str.21:
	.asciz	"Input buffer schedule_features"

	.p2align	5                               ## @str.22
l_str.22:
	.asciz	"bias1.stride.0"

	.p2align	5                               ## @str.23
l_str.23:
	.asciz	"1"

	.p2align	5                               ## @str.24
l_str.24:
	.asciz	"bias1.min.0"

	.p2align	5                               ## @str.25
l_str.25:
	.asciz	"0"

	.p2align	5                               ## @str.26
l_str.26:
	.asciz	"bias1.extent.0"

	.p2align	5                               ## @str.27
l_str.27:
	.asciz	"32"

	.p2align	5                               ## @str.28
l_str.28:
	.asciz	"filter1.stride.0"

	.p2align	5                               ## @str.29
l_str.29:
	.asciz	"filter1.min.0"

	.p2align	5                               ## @str.30
l_str.30:
	.asciz	"filter1.extent.0"

	.p2align	5                               ## @str.31
l_str.31:
	.asciz	"filter1.min.1"

	.p2align	5                               ## @str.32
l_str.32:
	.asciz	"filter1.extent.1"

	.p2align	5                               ## @str.33
l_str.33:
	.asciz	"head1_bias.stride.0"

	.p2align	5                               ## @str.34
l_str.34:
	.asciz	"head1_bias.min.0"

	.p2align	5                               ## @str.35
l_str.35:
	.asciz	"head1_bias.extent.0"

	.p2align	5                               ## @str.36
l_str.36:
	.asciz	"8"

	.p2align	5                               ## @str.37
l_str.37:
	.asciz	"head1_filter.stride.0"

	.p2align	5                               ## @str.38
l_str.38:
	.asciz	"head1_filter.min.0"

	.p2align	5                               ## @str.39
l_str.39:
	.asciz	"head1_filter.extent.0"

	.p2align	5                               ## @str.40
l_str.40:
	.asciz	"head1_filter.min.1"

	.p2align	5                               ## @str.41
l_str.41:
	.asciz	"head1_filter.extent.1"

	.p2align	5                               ## @str.42
l_str.42:
	.asciz	"40"

	.p2align	5                               ## @str.43
l_str.43:
	.asciz	"head1_filter.min.2"

	.p2align	5                               ## @str.44
l_str.44:
	.asciz	"head1_filter.extent.2"

	.p2align	5                               ## @str.45
l_str.45:
	.asciz	"7"

	.p2align	5                               ## @str.46
l_str.46:
	.asciz	"head2_bias.stride.0"

	.p2align	5                               ## @str.47
l_str.47:
	.asciz	"head2_bias.min.0"

	.p2align	5                               ## @str.48
l_str.48:
	.asciz	"head2_bias.extent.0"

	.p2align	5                               ## @str.49
l_str.49:
	.asciz	"24"

	.p2align	5                               ## @str.50
l_str.50:
	.asciz	"head2_filter.stride.0"

	.p2align	5                               ## @str.51
l_str.51:
	.asciz	"head2_filter.min.0"

	.p2align	5                               ## @str.52
l_str.52:
	.asciz	"head2_filter.extent.0"

	.p2align	5                               ## @str.53
l_str.53:
	.asciz	"head2_filter.min.1"

	.p2align	5                               ## @str.54
l_str.54:
	.asciz	"head2_filter.extent.1"

	.p2align	5                               ## @str.55
l_str.55:
	.asciz	"39"

	.p2align	5                               ## @str.56
l_str.56:
	.asciz	"pipeline_features.stride.0"

	.p2align	5                               ## @str.57
l_str.57:
	.asciz	"prediction_output.stride.0"

	.p2align	5                               ## @str.58
l_str.58:
	.asciz	"schedule_features.stride.0"

	.p2align	5                               ## @str.59
l_str.59:
	.asciz	"conv1_stage1"

	.p2align	5                               ## @str.60
l_str.60:
	.asciz	"n"

	.p2align	5                               ## @str.61
l_str.61:
	.asciz	"normalized_schedule_features"

	.p2align	5                               ## @str.62
l_str.62:
	.asciz	"head2_relu"

	.p2align	5                               ## @str.63
l_str.63:
	.asciz	"relu1"

	.p2align	5                               ## @str.64
l_str.64:
	.asciz	"num_stages"

	.p2align	3                               ## @0
l___unnamed_1:
	.long	1                               ## 0x1
	.long	0                               ## 0x0

	.p2align	3                               ## @1
l___unnamed_2:
	.long	13                              ## 0xd
	.long	0                               ## 0x0

	.p2align	5                               ## @str.65
l_str.65:
	.asciz	"batch_size"

	.p2align	3                               ## @2
l___unnamed_3:
	.long	1                               ## 0x1
	.long	0                               ## 0x0

	.p2align	3                               ## @3
l___unnamed_4:
	.long	80                              ## 0x50
	.long	0                               ## 0x0

	.p2align	5                               ## @str.66
l_str.66:
	.asciz	"num_cores"

	.p2align	3                               ## @4
l___unnamed_5:
	.long	1                               ## 0x1
	.long	0                               ## 0x0

	.p2align	3                               ## @5
l___unnamed_6:
	.long	32                              ## 0x20
	.long	0                               ## 0x0

	.p2align	3                               ## @6
l___unnamed_7:
	.quad	0                               ## 0x0

	.p2align	3                               ## @7
l___unnamed_8:
	.quad	40                              ## 0x28

	.p2align	3                               ## @8
l___unnamed_9:
	.quad	0                               ## 0x0

	.p2align	3                               ## @9
l___unnamed_10:
	.quad	7                               ## 0x7

	.p2align	3                               ## @10
l___unnamed_11:
	.quad	0                               ## 0x0

	.p2align	3                               ## @11
l___unnamed_12:
	.quad	13                              ## 0xd

	.section	__DATA,__const
	.p2align	4                               ## @12
l___unnamed_13:
	.quad	l___unnamed_7
	.quad	l___unnamed_8
	.quad	l___unnamed_9
	.quad	l___unnamed_10
	.quad	l___unnamed_11
	.quad	l___unnamed_12

	.section	__TEXT,__const
	.p2align	3                               ## @13
l___unnamed_14:
	.quad	0                               ## 0x0

	.p2align	3                               ## @14
l___unnamed_15:
	.quad	80                              ## 0x50

	.p2align	3                               ## @15
l___unnamed_16:
	.quad	0                               ## 0x0

	.p2align	3                               ## @16
l___unnamed_17:
	.quad	39                              ## 0x27

	.p2align	3                               ## @17
l___unnamed_18:
	.quad	0                               ## 0x0

	.p2align	3                               ## @18
l___unnamed_19:
	.quad	13                              ## 0xd

	.section	__DATA,__const
	.p2align	4                               ## @19
l___unnamed_20:
	.quad	l___unnamed_14
	.quad	l___unnamed_15
	.quad	l___unnamed_16
	.quad	l___unnamed_17
	.quad	l___unnamed_18
	.quad	l___unnamed_19

	.section	__TEXT,__const
	.p2align	3                               ## @20
l___unnamed_21:
	.quad	0                               ## 0x0

	.p2align	3                               ## @21
l___unnamed_22:
	.quad	8                               ## 0x8

	.p2align	3                               ## @22
l___unnamed_23:
	.quad	0                               ## 0x0

	.p2align	3                               ## @23
l___unnamed_24:
	.quad	40                              ## 0x28

	.p2align	3                               ## @24
l___unnamed_25:
	.quad	0                               ## 0x0

	.p2align	3                               ## @25
l___unnamed_26:
	.quad	7                               ## 0x7

	.section	__DATA,__const
	.p2align	4                               ## @26
l___unnamed_27:
	.quad	l___unnamed_21
	.quad	l___unnamed_22
	.quad	l___unnamed_23
	.quad	l___unnamed_24
	.quad	l___unnamed_25
	.quad	l___unnamed_26

	.section	__TEXT,__const
	.p2align	3                               ## @27
l___unnamed_28:
	.quad	0                               ## 0x0

	.p2align	3                               ## @28
l___unnamed_29:
	.quad	8                               ## 0x8

	.section	__DATA,__const
	.p2align	3                               ## @29
l___unnamed_30:
	.quad	l___unnamed_28
	.quad	l___unnamed_29

	.section	__TEXT,__const
	.p2align	3                               ## @30
l___unnamed_31:
	.quad	0                               ## 0x0

	.p2align	3                               ## @31
l___unnamed_32:
	.quad	24                              ## 0x18

	.p2align	3                               ## @32
l___unnamed_33:
	.quad	0                               ## 0x0

	.p2align	3                               ## @33
l___unnamed_34:
	.quad	39                              ## 0x27

	.section	__DATA,__const
	.p2align	4                               ## @34
l___unnamed_35:
	.quad	l___unnamed_31
	.quad	l___unnamed_32
	.quad	l___unnamed_33
	.quad	l___unnamed_34

	.section	__TEXT,__const
	.p2align	3                               ## @35
l___unnamed_36:
	.quad	0                               ## 0x0

	.p2align	3                               ## @36
l___unnamed_37:
	.quad	24                              ## 0x18

	.section	__DATA,__const
	.p2align	3                               ## @37
l___unnamed_38:
	.quad	l___unnamed_36
	.quad	l___unnamed_37

	.section	__TEXT,__const
	.p2align	3                               ## @38
l___unnamed_39:
	.quad	0                               ## 0x0

	.p2align	3                               ## @39
l___unnamed_40:
	.quad	32                              ## 0x20

	.p2align	3                               ## @40
l___unnamed_41:
	.quad	0                               ## 0x0

	.p2align	3                               ## @41
l___unnamed_42:
	.quad	32                              ## 0x20

	.section	__DATA,__const
	.p2align	4                               ## @42
l___unnamed_43:
	.quad	l___unnamed_39
	.quad	l___unnamed_40
	.quad	l___unnamed_41
	.quad	l___unnamed_42

	.section	__TEXT,__const
	.p2align	3                               ## @43
l___unnamed_44:
	.quad	0                               ## 0x0

	.p2align	3                               ## @44
l___unnamed_45:
	.quad	32                              ## 0x20

	.section	__DATA,__const
	.p2align	3                               ## @45
l___unnamed_46:
	.quad	l___unnamed_44
	.quad	l___unnamed_45

	.section	__TEXT,__const
	.p2align	5                               ## @str.67
l_str.67:
	.asciz	"learning_rate"

	.p2align	3                               ## @46
l___unnamed_47:
	.long	0x3f800000                      ## float 1
	.long	0x00000000                      ## float 0

	.p2align	3                               ## @47
l___unnamed_48:
	.long	0x3a83126f                      ## float 0.00100000005
	.long	0x00000000                      ## float 0

	.p2align	5                               ## @str.68
l_str.68:
	.asciz	"timestep"

	.p2align	3                               ## @48
l___unnamed_49:
	.space	8

	.p2align	3                               ## @49
l___unnamed_50:
	.long	37                              ## 0x25
	.long	0                               ## 0x0

	.p2align	5                               ## @str.69
l_str.69:
	.asciz	"reference"

	.p2align	3                               ## @50
l___unnamed_51:
	.space	8

	.p2align	3                               ## @51
l___unnamed_52:
	.space	8

	.p2align	3                               ## @52
l___unnamed_53:
	.quad	0                               ## 0x0

	.p2align	3                               ## @53
l___unnamed_54:
	.quad	80                              ## 0x50

	.section	__DATA,__const
	.p2align	3                               ## @54
l___unnamed_55:
	.quad	l___unnamed_53
	.quad	l___unnamed_54

	.section	__TEXT,__const
	.p2align	5                               ## @str.70
l_str.70:
	.asciz	"true_runtime"

	.p2align	3                               ## @55
l___unnamed_56:
	.quad	0                               ## 0x0

	.p2align	3                               ## @56
l___unnamed_57:
	.quad	80                              ## 0x50

	.section	__DATA,__const
	.p2align	3                               ## @57
l___unnamed_58:
	.quad	l___unnamed_56
	.quad	l___unnamed_57

	.p2align	4                               ## @58
l___unnamed_59:
	.quad	l_str.64
	.long	0                               ## 0x0
	.long	0                               ## 0x0
	.byte	0                               ## 0x0
	.byte	32                              ## 0x20
	.short	1                               ## 0x1
	.space	4
	.quad	l___unnamed_1
	.quad	0
	.quad	0
	.quad	l___unnamed_2
	.quad	0
	.quad	l_str.65
	.long	0                               ## 0x0
	.long	0                               ## 0x0
	.byte	0                               ## 0x0
	.byte	32                              ## 0x20
	.short	1                               ## 0x1
	.space	4
	.quad	l___unnamed_3
	.quad	0
	.quad	0
	.quad	l___unnamed_4
	.quad	0
	.quad	l_str.66
	.long	0                               ## 0x0
	.long	0                               ## 0x0
	.byte	0                               ## 0x0
	.byte	32                              ## 0x20
	.short	1                               ## 0x1
	.space	4
	.quad	l___unnamed_5
	.quad	0
	.quad	0
	.quad	l___unnamed_6
	.quad	0
	.quad	l_str.4
	.long	1                               ## 0x1
	.long	3                               ## 0x3
	.byte	2                               ## 0x2
	.byte	32                              ## 0x20
	.short	1                               ## 0x1
	.space	4
	.quad	0
	.quad	0
	.quad	0
	.quad	0
	.quad	l___unnamed_13
	.quad	l_str
	.long	1                               ## 0x1
	.long	3                               ## 0x3
	.byte	2                               ## 0x2
	.byte	32                              ## 0x20
	.short	1                               ## 0x1
	.space	4
	.quad	0
	.quad	0
	.quad	0
	.quad	0
	.quad	l___unnamed_20
	.quad	l_str.8
	.long	1                               ## 0x1
	.long	3                               ## 0x3
	.byte	2                               ## 0x2
	.byte	32                              ## 0x20
	.short	1                               ## 0x1
	.space	4
	.quad	0
	.quad	0
	.quad	0
	.quad	0
	.quad	l___unnamed_27
	.quad	l_str.9
	.long	1                               ## 0x1
	.long	1                               ## 0x1
	.byte	2                               ## 0x2
	.byte	32                              ## 0x20
	.short	1                               ## 0x1
	.space	4
	.quad	0
	.quad	0
	.quad	0
	.quad	0
	.quad	l___unnamed_30
	.quad	l_str.6
	.long	1                               ## 0x1
	.long	2                               ## 0x2
	.byte	2                               ## 0x2
	.byte	32                              ## 0x20
	.short	1                               ## 0x1
	.space	4
	.quad	0
	.quad	0
	.quad	0
	.quad	0
	.quad	l___unnamed_35
	.quad	l_str.7
	.long	1                               ## 0x1
	.long	1                               ## 0x1
	.byte	2                               ## 0x2
	.byte	32                              ## 0x20
	.short	1                               ## 0x1
	.space	4
	.quad	0
	.quad	0
	.quad	0
	.quad	0
	.quad	l___unnamed_38
	.quad	l_str.10
	.long	1                               ## 0x1
	.long	2                               ## 0x2
	.byte	2                               ## 0x2
	.byte	32                              ## 0x20
	.short	1                               ## 0x1
	.space	4
	.quad	0
	.quad	0
	.quad	0
	.quad	0
	.quad	l___unnamed_43
	.quad	l_str.11
	.long	1                               ## 0x1
	.long	1                               ## 0x1
	.byte	2                               ## 0x2
	.byte	32                              ## 0x20
	.short	1                               ## 0x1
	.space	4
	.quad	0
	.quad	0
	.quad	0
	.quad	0
	.quad	l___unnamed_46
	.quad	l_str.67
	.long	0                               ## 0x0
	.long	0                               ## 0x0
	.byte	2                               ## 0x2
	.byte	32                              ## 0x20
	.short	1                               ## 0x1
	.space	4
	.quad	l___unnamed_47
	.quad	0
	.quad	0
	.quad	l___unnamed_48
	.quad	0
	.quad	l_str.68
	.long	0                               ## 0x0
	.long	0                               ## 0x0
	.byte	0                               ## 0x0
	.byte	32                              ## 0x20
	.short	1                               ## 0x1
	.space	4
	.quad	l___unnamed_49
	.quad	0
	.quad	0
	.quad	l___unnamed_50
	.quad	0
	.quad	l_str.69
	.long	0                               ## 0x0
	.long	0                               ## 0x0
	.byte	0                               ## 0x0
	.byte	32                              ## 0x20
	.short	1                               ## 0x1
	.space	4
	.quad	l___unnamed_51
	.quad	0
	.quad	0
	.quad	l___unnamed_52
	.quad	0
	.quad	l_str.70
	.long	1                               ## 0x1
	.long	1                               ## 0x1
	.byte	2                               ## 0x2
	.byte	32                              ## 0x20
	.short	1                               ## 0x1
	.space	4
	.quad	0
	.quad	0
	.quad	0
	.quad	0
	.quad	l___unnamed_55
	.quad	l_str.3
	.long	2                               ## 0x2
	.long	1                               ## 0x1
	.byte	2                               ## 0x2
	.byte	32                              ## 0x20
	.short	1                               ## 0x1
	.space	4
	.quad	0
	.quad	0
	.quad	0
	.quad	0
	.quad	l___unnamed_58
	.quad	l_str.5
	.long	2                               ## 0x2
	.long	0                               ## 0x0
	.byte	2                               ## 0x2
	.byte	32                              ## 0x20
	.short	1                               ## 0x1
	.space	4
	.quad	0
	.quad	0
	.quad	0
	.quad	0
	.quad	0

	.section	__TEXT,__const
	.p2align	5                               ## @str.71
l_str.71:
	.asciz	"x86-64-osx-avx-avx2-f16c-fma-no_runtime-sse41"

	.p2align	5                               ## @str.72
l_str.72:
	.asciz	"cost_model"

	.section	__DATA,__const
	.p2align	4                               ## @cost_model_metadata_storage
l_cost_model_metadata_storage:
	.long	1                               ## 0x1
	.long	17                              ## 0x11
	.quad	l___unnamed_59
	.quad	l_str.71
	.quad	l_str.72

.subsections_via_symbols
