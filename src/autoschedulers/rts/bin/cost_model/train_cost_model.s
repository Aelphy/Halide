	.section	__TEXT,__text,regular,pure_instructions
	.section	__TEXT,__literal16,16byte_literals
	.p2align	4                               ## -- Begin function train_cost_model
LCPI0_0:
	.quad	-9223372036854775808            ## 0x8000000000000000
	.quad	0                               ## 0x0
LCPI0_5:
	.long	0                               ## 0x0
	.long	32                              ## 0x20
	.long	1                               ## 0x1
	.long	0                               ## 0x0
LCPI0_7:
	.long	0                               ## 0x0
	.long	8                               ## 0x8
	.long	1                               ## 0x1
	.long	0                               ## 0x0
LCPI0_8:
	.long	0                               ## 0x0
	.long	40                              ## 0x28
	.long	8                               ## 0x8
	.long	0                               ## 0x0
LCPI0_9:
	.long	0                               ## 0x0
	.long	7                               ## 0x7
	.long	320                             ## 0x140
	.long	0                               ## 0x0
LCPI0_10:
	.long	0                               ## 0x0
	.long	24                              ## 0x18
	.long	1                               ## 0x1
	.long	0                               ## 0x0
LCPI0_12:
	.long	0                               ## 0x0
	.long	40                              ## 0x28
	.long	1                               ## 0x1
	.long	0                               ## 0x0
LCPI0_13:
	.long	0                               ## 0x0
	.long	7                               ## 0x7
	.long	40                              ## 0x28
	.long	0                               ## 0x0
LCPI0_15:
	.long	0                               ## 0x0
	.long	32                              ## 0x20
	.long	32                              ## 0x20
	.long	0                               ## 0x0
LCPI0_16:
	.long	0                               ## 0x0
	.long	4                               ## 0x4
	.long	1024                            ## 0x400
	.long	0                               ## 0x0
LCPI0_21:
	.long	0                               ## 0x0
	.long	39                              ## 0x27
	.long	24                              ## 0x18
	.long	0                               ## 0x0
LCPI0_22:
	.long	0                               ## 0x0
	.long	4                               ## 0x4
	.long	936                             ## 0x3a8
	.long	0                               ## 0x0
LCPI0_25:
	.long	32768                           ## 0x8000
	.long	0                               ## 0x0
	.long	0                               ## 0x0
	.long	0                               ## 0x0
LCPI0_35:
	.quad	4503599627370496                ## 0x10000000000000
	.quad	0                               ## 0x0
LCPI0_43:
	.long	1                               ## 0x1
	.long	0                               ## 0x0
	.long	24                              ## 0x18
	.long	1                               ## 0x1
LCPI0_67:
	.quad	2                               ## 0x2
	.quad	5                               ## 0x5
LCPI0_68:
	.quad	-9223372036854775808            ## 0x8000000000000000
	.quad	-9223372036854775808            ## 0x8000000000000000
LCPI0_69:
	.quad	-9223372034707292161            ## 0x800000007fffffff
	.quad	-9223372034707292161            ## 0x800000007fffffff
	.section	__TEXT,__const
	.p2align	5
LCPI0_1:
	.quad	-9223372036854775296            ## 0x8000000000000200
	.quad	128                             ## 0x80
	.quad	8192                            ## 0x2000
	.quad	16384                           ## 0x4000
LCPI0_2:
	.quad	64                              ## 0x40
	.quad	4                               ## 0x4
	.quad	8                               ## 0x8
	.quad	2                               ## 0x2
LCPI0_3:
	.quad	16                              ## 0x10
	.quad	32                              ## 0x20
	.quad	256                             ## 0x100
	.quad	1024                            ## 0x400
LCPI0_4:
	.quad	2048                            ## 0x800
	.quad	4096                            ## 0x1000
	.quad	32768                           ## 0x8000
	.quad	65536                           ## 0x10000
LCPI0_6:
	.long	0                               ## 0x0
	.long	32                              ## 0x20
	.long	1                               ## 0x1
	.long	0                               ## 0x0
	.long	0                               ## 0x0
	.long	32                              ## 0x20
	.long	32                              ## 0x20
	.long	0                               ## 0x0
LCPI0_11:
	.long	0                               ## 0x0
	.long	24                              ## 0x18
	.long	1                               ## 0x1
	.long	0                               ## 0x0
	.long	0                               ## 0x0
	.long	39                              ## 0x27
	.long	24                              ## 0x18
	.long	0                               ## 0x0
LCPI0_14:
	.long	0                               ## 0x0
	.long	32                              ## 0x20
	.long	1                               ## 0x1
	.long	0                               ## 0x0
	.long	0                               ## 0x0
	.long	4                               ## 0x4
	.long	32                              ## 0x20
	.long	0                               ## 0x0
LCPI0_17:
	.long	0                               ## 0x0
	.long	8                               ## 0x8
	.long	1                               ## 0x1
	.long	0                               ## 0x0
	.long	0                               ## 0x0
	.long	4                               ## 0x4
	.long	8                               ## 0x8
	.long	0                               ## 0x0
LCPI0_18:
	.long	0                               ## 0x0
	.long	8                               ## 0x8
	.long	1                               ## 0x1
	.long	0                               ## 0x0
	.long	0                               ## 0x0
	.long	40                              ## 0x28
	.long	8                               ## 0x8
	.long	0                               ## 0x0
LCPI0_19:
	.long	0                               ## 0x0
	.long	7                               ## 0x7
	.long	320                             ## 0x140
	.long	0                               ## 0x0
	.long	0                               ## 0x0
	.long	4                               ## 0x4
	.long	2240                            ## 0x8c0
	.long	0                               ## 0x0
LCPI0_20:
	.long	0                               ## 0x0
	.long	24                              ## 0x18
	.long	1                               ## 0x1
	.long	0                               ## 0x0
	.long	0                               ## 0x0
	.long	4                               ## 0x4
	.long	24                              ## 0x18
	.long	0                               ## 0x0
LCPI0_23:
	.quad	0                               ## 0x0
	.quad	0                               ## 0x0
	.quad	4398046511104                   ## 0x40000000000
	.quad	274877906944                    ## 0x4000000000
LCPI0_24:
	.quad	64                              ## 0x40
	.quad	128                             ## 0x80
	.quad	0                               ## 0x0
	.quad	0                               ## 0x0
LCPI0_26:
	.quad	0                               ## 0x0
	.quad	18014398509481984               ## 0x40000000000000
	.quad	72057594037927936               ## 0x100000000000000
	.quad	288230376151711744              ## 0x400000000000000
LCPI0_27:
	.quad	281474976710656                 ## 0x1000000000000
	.quad	0                               ## 0x0
	.quad	0                               ## 0x0
	.quad	1125899906842624                ## 0x4000000000000
LCPI0_28:
	.quad	0                               ## 0x0
	.quad	1024                            ## 0x400
	.quad	2048                            ## 0x800
	.quad	0                               ## 0x0
LCPI0_29:
	.quad	68719476736                     ## 0x1000000000
	.quad	0                               ## 0x0
	.quad	0                               ## 0x0
	.quad	1099511627776                   ## 0x10000000000
LCPI0_30:
	.quad	0                               ## 0x0
	.quad	16                              ## 0x10
	.quad	32                              ## 0x20
	.quad	0                               ## 0x0
LCPI0_31:
	.quad	17592186044416                  ## 0x100000000000
	.quad	70368744177664                  ## 0x400000000000
	.quad	0                               ## 0x0
	.quad	0                               ## 0x0
LCPI0_32:
	.quad	0                               ## 0x0
	.quad	0                               ## 0x0
	.quad	256                             ## 0x100
	.quad	512                             ## 0x200
LCPI0_33:
	.quad	0                               ## 0x0
	.quad	17179869184                     ## 0x400000000
	.quad	0                               ## 0x0
	.quad	0                               ## 0x0
LCPI0_34:
	.quad	2                               ## 0x2
	.quad	0                               ## 0x0
	.quad	4                               ## 0x4
	.quad	8                               ## 0x8
LCPI0_36:
	.quad	0                               ## 0x0
	.quad	4096                            ## 0x1000
	.quad	8192                            ## 0x2000
	.quad	16384                           ## 0x4000
LCPI0_37:
	.quad	65536                           ## 0x10000
	.quad	131072                          ## 0x20000
	.quad	262144                          ## 0x40000
	.quad	524288                          ## 0x80000
LCPI0_38:
	.long	8                               ## 0x8
	.long	4                               ## 0x4
	.long	8                               ## 0x8
	.long	40                              ## 0x28
	.long	7                               ## 0x7
	.long	4                               ## 0x4
	.long	24                              ## 0x18
	.long	4                               ## 0x4
LCPI0_39:
	.quad	33554432                        ## 0x2000000
	.quad	134217728                       ## 0x8000000
	.quad	536870912                       ## 0x20000000
	.quad	2147483648                      ## 0x80000000
LCPI0_40:
	.quad	131072                          ## 0x20000
	.quad	524288                          ## 0x80000
	.quad	2097152                         ## 0x200000
	.quad	8388608                         ## 0x800000
LCPI0_41:
	.long	32                              ## 0x20
	.long	1                               ## 0x1
	.long	0                               ## 0x0
	.long	32                              ## 0x20
	.long	0                               ## 0x0
	.long	8                               ## 0x8
	.long	1                               ## 0x1
	.long	0                               ## 0x0
LCPI0_42:
	.long	8                               ## 0x8
	.long	1                               ## 0x1
	.long	0                               ## 0x0
	.long	40                              ## 0x28
	.long	0                               ## 0x0
	.long	7                               ## 0x7
	.long	0                               ## 0x0
	.long	24                              ## 0x18
LCPI0_44:
	.quad	4503599627370496                ## 0x10000000000000
	.quad	1125899906842624                ## 0x4000000000000
	.quad	9007199254740992                ## 0x20000000000000
	.quad	18014398509481984               ## 0x40000000000000
LCPI0_45:
	.quad	17592186044416                  ## 0x100000000000
	.quad	4398046511104                   ## 0x40000000000
	.quad	35184372088832                  ## 0x200000000000
	.quad	70368744177664                  ## 0x400000000000
LCPI0_46:
	.quad	36028797018963968               ## 0x80000000000000
	.quad	72057594037927936               ## 0x100000000000000
	.quad	288230376151711744              ## 0x400000000000000
	.quad	576460752303423488              ## 0x800000000000000
LCPI0_47:
	.quad	281474976710656                 ## 0x1000000000000
	.quad	562949953421312                 ## 0x2000000000000
	.quad	140737488355328                 ## 0x800000000000
	.quad	2251799813685248                ## 0x8000000000000
LCPI0_48:
	.quad	512                             ## 0x200
	.quad	2048                            ## 0x800
	.quad	8192                            ## 0x2000
	.quad	32768                           ## 0x8000
LCPI0_49:
	.quad	144115188075855872              ## 0x200000000000000
	.quad	2305843009213693952             ## 0x2000000000000000
	.quad	4611686018427387904             ## 0x4000000000000000
	.quad	1152921504606846976             ## 0x1000000000000000
LCPI0_50:
	.quad	2                               ## 0x2
	.quad	8                               ## 0x8
	.quad	32                              ## 0x20
	.quad	128                             ## 0x80
LCPI0_51:
	.quad	-9223370937343148032            ## 0x8000010000000000
	.quad	2199023255552                   ## 0x20000000000
	.quad	549755813888                    ## 0x8000000000
	.quad	8796093022208                   ## 0x80000000000
LCPI0_52:
	.long	0                               ## 0x0
	.long	4                               ## 0x4
	.long	0                               ## 0x0
	.long	32                              ## 0x20
	.long	1                               ## 0x1
	.long	0                               ## 0x0
	.long	32                              ## 0x20
	.long	0                               ## 0x0
LCPI0_53:
	.long	1                               ## 0x1
	.long	0                               ## 0x0
	.long	40                              ## 0x28
	.long	0                               ## 0x0
	.long	7                               ## 0x7
	.long	0                               ## 0x0
	.long	4                               ## 0x4
	.long	0                               ## 0x0
LCPI0_54:
	.long	39                              ## 0x27
	.long	1                               ## 0x1
	.long	1                               ## 0x1
	.long	1                               ## 0x1
	.long	1                               ## 0x1
	.long	0                               ## 0x0
	.long	32                              ## 0x20
	.long	1                               ## 0x1
LCPI0_55:
	.long	4                               ## 0x4
	.long	0                               ## 0x0
	.long	8                               ## 0x8
	.long	1                               ## 0x1
	.long	0                               ## 0x0
	.long	4                               ## 0x4
	.long	0                               ## 0x0
	.long	8                               ## 0x8
LCPI0_56:
	.quad	2097152                         ## 0x200000
	.quad	4194304                         ## 0x400000
	.quad	16777216                        ## 0x1000000
	.quad	33554432                        ## 0x2000000
LCPI0_57:
	.quad	32                              ## 0x20
	.quad	128                             ## 0x80
	.quad	256                             ## 0x100
	.quad	64                              ## 0x40
LCPI0_58:
	.quad	536870912                       ## 0x20000000
	.quad	1073741824                      ## 0x40000000
	.quad	2147483648                      ## 0x80000000
	.quad	8589934592                      ## 0x200000000
LCPI0_59:
	.quad	2048                            ## 0x800
	.quad	16384                           ## 0x4000
	.quad	32768                           ## 0x8000
	.quad	65536                           ## 0x10000
LCPI0_60:
	.quad	131072                          ## 0x20000
	.quad	524288                          ## 0x80000
	.quad	1048576                         ## 0x100000
	.quad	262144                          ## 0x40000
LCPI0_61:
	.quad	2                               ## 0x2
	.quad	4                               ## 0x4
	.quad	8                               ## 0x8
	.quad	16                              ## 0x10
LCPI0_62:
	.quad	8388608                         ## 0x800000
	.quad	67108864                        ## 0x4000000
	.quad	134217728                       ## 0x8000000
	.quad	268435456                       ## 0x10000000
LCPI0_63:
	.quad	512                             ## 0x200
	.quad	1024                            ## 0x400
	.quad	4096                            ## 0x1000
	.quad	8192                            ## 0x2000
LCPI0_64:
	.long	24                              ## 0x18
	.long	1                               ## 0x1
	.long	0                               ## 0x0
	.long	4                               ## 0x4
	.long	0                               ## 0x0
	.long	24                              ## 0x18
	.long	1                               ## 0x1
	.long	0                               ## 0x0
LCPI0_65:
	.quad	274877906944                    ## 0x4000000000
	.quad	549755813888                    ## 0x8000000000
	.quad	137438953472                    ## 0x2000000000
	.quad	1099511627776                   ## 0x10000000000
LCPI0_66:
	.quad	17179869184                     ## 0x400000000
	.quad	4294967296                      ## 0x100000000
	.quad	34359738368                     ## 0x800000000
	.quad	68719476736                     ## 0x1000000000
LCPI0_70:
	.quad	32                              ## 0x20
	.quad	256                             ## 0x100
	.quad	2251799813685248                ## 0x8000000000000
	.quad	128                             ## 0x80
LCPI0_71:
	.quad	65536                           ## 0x10000
	.quad	131072                          ## 0x20000
	.quad	72057594037927936               ## 0x100000000000000
	.quad	262144                          ## 0x40000
LCPI0_72:
	.quad	4503599627370496                ## 0x10000000000000
	.quad	4096                            ## 0x1000
	.quad	2048                            ## 0x800
	.quad	16384                           ## 0x4000
LCPI0_73:
	.quad	144115188075855872              ## 0x200000000000000
	.quad	524288                          ## 0x80000
	.quad	1048576                         ## 0x100000
	.quad	2097152                         ## 0x200000
LCPI0_74:
	.quad	9007199254740992                ## 0x20000000000000
	.quad	8192                            ## 0x2000
	.quad	18014398509481984               ## 0x40000000000000
	.quad	36028797018963968               ## 0x80000000000000
LCPI0_75:
	.quad	2                               ## 0x2
	.quad	140737488355328                 ## 0x800000000000
	.quad	4                               ## 0x4
	.quad	281474976710656                 ## 0x1000000000000
LCPI0_76:
	.quad	562949953421312                 ## 0x2000000000000
	.quad	8                               ## 0x8
	.quad	1125899906842624                ## 0x4000000000000
	.quad	64                              ## 0x40
LCPI0_77:
	.quad	-9223363240761753600            ## 0x8000080000000000
	.quad	17592186044416                  ## 0x100000000000
	.quad	35184372088832                  ## 0x200000000000
	.quad	70368744177664                  ## 0x400000000000
	.section	__TEXT,__literal4,4byte_literals
	.p2align	2
LCPI0_78:
	.long	0xbabd3069                      ## float -0.0014433983
LCPI0_79:
	.long	0xbf317218                      ## float -0.693147182
LCPI0_80:
	.long	0xba8322ca                      ## float -0.00100048748
LCPI0_81:
	.long	0xb9a797f3                      ## float -3.19659332E-4
LCPI0_82:
	.long	0xbc0b192a                      ## float -0.00848988629
LCPI0_83:
	.long	0xbe2aae1f                      ## float -0.166679844
LCPI0_84:
	.long	0xbf800000                      ## float -1
LCPI0_85:
	.long	0xba9c2e66                      ## float -0.00119156833
LCPI0_86:
	.long	0xbd2a66bc                      ## float -0.0416018814
LCPI0_87:
	.long	0xbeffffde                      ## float -0.499998987
LCPI0_88:
	.long	0x3f800000                      ## float 1
LCPI0_89:
	.long	0x80000000                      ## float -0
LCPI0_90:
	.long	0xbe1ba6b6                      ## float -0.152003139
LCPI0_91:
	.long	0xbdd7c745                      ## float -0.105360545
LCPI0_92:
	.long	0x3f666666                      ## float 0.899999976
LCPI0_93:
	.long	0x3dcccccd                      ## float 0.100000001
LCPI0_94:
	.long	0x3f7fbe77                      ## float 0.999000012
LCPI0_95:
	.long	0x3a83126f                      ## float 0.00100000005
LCPI0_96:
	.long	0xb727c5ac                      ## float -9.99999974E-6
LCPI0_97:
	.long	0x2edbe6ff                      ## float 1.00000001E-10
LCPI0_98:
	.long	0x3727c5ac                      ## float 9.99999974E-6
	.section	__TEXT,__text,regular,pure_instructions
	.globl	_train_cost_model
	.p2align	4, 0x90
_train_cost_model:                      ## @train_cost_model
	.cfi_startproc
## %bb.0:                               ## %entry
	pushq	%rbp
	.cfi_def_cfa_offset 16
	.cfi_offset %rbp, -16
	movq	%rsp, %rbp
	.cfi_def_cfa_register %rbp
	pushq	%r15
	pushq	%r14
	pushq	%r13
	pushq	%r12
	pushq	%rbx
	andq	$-32, %rsp
	subq	$16032, %rsp                    ## imm = 0x3EA0
	.cfi_offset %rbx, -56
	.cfi_offset %r12, -48
	.cfi_offset %r13, -40
	.cfi_offset %r14, -32
	.cfi_offset %r15, -24
	vmovss	%xmm0, 244(%rsp)                ## 4-byte Spill
	movq	%rcx, %r14
	movl	%edx, 1336(%rsp)                ## 4-byte Spill
                                        ## kill: def $edi killed $edi def $rdi
	movq	%rdi, 120(%rsp)                 ## 8-byte Spill
	xorl	%eax, %eax
	cmpq	$0, 96(%rbp)
	vmovq	16(%rbp), %xmm0                 ## xmm0 = mem[0],zero
	vmovq	%r9, %xmm1
	vpunpcklqdq	%xmm0, %xmm1, %xmm0     ## xmm0 = xmm1[0],xmm0[0]
	movq	%r8, 24(%rsp)                   ## 8-byte Spill
	vmovq	%r8, %xmm1
	vmovq	%rcx, %xmm2
	vpunpcklqdq	%xmm1, %xmm2, %xmm1     ## xmm1 = xmm2[0],xmm1[0]
	vinserti128	$1, %xmm0, %ymm1, %ymm0
	vmovdqu	72(%rbp), %xmm1
	vmovq	104(%rbp), %xmm2                ## xmm2 = mem[0],zero
	vmovq	88(%rbp), %xmm3                 ## xmm3 = mem[0],zero
	vpunpcklqdq	%xmm2, %xmm3, %xmm2     ## xmm2 = xmm3[0],xmm2[0]
	vinserti128	$1, %xmm2, %ymm1, %ymm1
	vpxor	%xmm2, %xmm2, %xmm2
	vpcmpeqq	%ymm2, %ymm1, %ymm1
	vpcmpeqq	%ymm2, %ymm0, %ymm0
	vpcmpeqq	24(%rbp), %ymm2, %ymm3
	vmovapd	LCPI0_0(%rip), %xmm4            ## xmm4 = [9223372036854775808,0]
	vblendvpd	%ymm0, LCPI0_1(%rip), %ymm4, %ymm0
	vpcmpeqq	112(%rbp), %ymm2, %ymm2
	vpand	LCPI0_2(%rip), %ymm1, %ymm1
	vpand	LCPI0_3(%rip), %ymm2, %ymm2
	vpand	LCPI0_4(%rip), %ymm3, %ymm3
	sete	%al
	vpor	%ymm2, %ymm3, %ymm2
	vorpd	%ymm2, %ymm0, %ymm0
	vpor	%ymm0, %ymm1, %ymm0
	vextracti128	$1, %ymm0, %xmm1
	vpor	%xmm1, %xmm0, %xmm0
	vpshufd	$238, %xmm0, %xmm1              ## xmm1 = xmm0[2,3,2,3]
	vpor	%xmm1, %xmm0, %xmm0
	vmovq	%xmm0, %rcx
	orq	%rax, %rcx
	xorl	%eax, %eax
	tzcntq	%rcx, %rax
	cmpl	$16, %eax
	jbe	LBB0_271
## %bb.1:                               ## %no_errors_bb
	movq	%r9, %r13
	movl	%esi, %r12d
	movq	48(%rbp), %r15
	movq	40(%rbp), %rbx
	movq	16(%r15), %rax
	movq	%rax, 1192(%rsp)                ## 8-byte Spill
	leaq	32(%r15), %rsi
	leaq	76(%rsp), %rdi
	movl	$4, %edx
	vzeroupper
	callq	_memcpy
	movl	76(%rsp), %eax
	movl	%eax, 1040(%rsp)                ## 4-byte Spill
	movq	24(%r15), %rax
	movq	%rax, 1704(%rsp)                ## 8-byte Spill
	movl	36(%r15), %eax
	movl	%eax, 1032(%rsp)                ## 4-byte Spill
	movq	40(%r15), %rax
	movl	(%rax), %ecx
	movq	%rcx, 808(%rsp)                 ## 8-byte Spill
	movl	4(%rax), %ecx
	movq	%rcx, 800(%rsp)                 ## 8-byte Spill
	movl	8(%rax), %eax
	movl	%eax, 920(%rsp)                 ## 4-byte Spill
	movq	16(%rbx), %rax
	movq	%rax, 856(%rsp)                 ## 8-byte Spill
	leaq	32(%rbx), %rsi
	leaq	76(%rsp), %rdi
	movl	$4, %edx
	callq	_memcpy
	movl	76(%rsp), %eax
	movl	%eax, 420(%rsp)                 ## 4-byte Spill
	movq	24(%rbx), %rax
	movq	%rax, 1696(%rsp)                ## 8-byte Spill
	movl	36(%rbx), %eax
	movl	%eax, 1024(%rsp)                ## 4-byte Spill
	movq	40(%rbx), %rax
	movl	(%rax), %ecx
	movq	%rcx, 760(%rsp)                 ## 8-byte Spill
	movl	16(%rax), %ecx
	movq	%rcx, 776(%rsp)                 ## 8-byte Spill
	vmovsd	4(%rax), %xmm0                  ## xmm0 = mem[0],zero
	vmovaps	%xmm0, 1360(%rsp)               ## 16-byte Spill
	movl	20(%rax), %ecx
	movq	%rcx, 744(%rsp)                 ## 8-byte Spill
	movl	24(%rax), %eax
	movl	%eax, 240(%rsp)                 ## 4-byte Spill
	movq	16(%rbp), %rbx
	movq	16(%rbx), %rax
	movq	%rax, 1168(%rsp)                ## 8-byte Spill
	leaq	32(%rbx), %rsi
	leaq	76(%rsp), %rdi
	movl	$4, %edx
	callq	_memcpy
	movl	76(%rsp), %eax
	movl	%eax, 416(%rsp)                 ## 4-byte Spill
	movq	24(%rbx), %rax
	movq	%rax, 1688(%rsp)                ## 8-byte Spill
	movl	36(%rbx), %eax
	movl	%eax, 1016(%rsp)                ## 4-byte Spill
	movq	40(%rbx), %rax
	movl	(%rax), %ecx
	movq	%rcx, 720(%rsp)                 ## 8-byte Spill
	movl	4(%rax), %ecx
	movq	%rcx, 688(%rsp)                 ## 8-byte Spill
	movl	8(%rax), %eax
	movl	%eax, 908(%rsp)                 ## 4-byte Spill
	movq	16(%r13), %rax
	movq	%rax, 1136(%rsp)                ## 8-byte Spill
	leaq	32(%r13), %rsi
	leaq	76(%rsp), %rdi
	movl	$4, %edx
	callq	_memcpy
	movl	76(%rsp), %eax
	movl	%eax, 412(%rsp)                 ## 4-byte Spill
	movq	24(%r13), %rax
	movq	%rax, 1680(%rsp)                ## 8-byte Spill
	movl	36(%r13), %eax
	movl	%eax, 1008(%rsp)                ## 4-byte Spill
	movq	%r13, 816(%rsp)                 ## 8-byte Spill
	movq	40(%r13), %rax
	movl	(%rax), %ecx
	movq	%rcx, 672(%rsp)                 ## 8-byte Spill
	movl	4(%rax), %ecx
	movq	%rcx, 40(%rsp)                  ## 8-byte Spill
	movl	8(%rax), %ecx
	movl	%ecx, 900(%rsp)                 ## 4-byte Spill
	movl	16(%rax), %ecx
	movq	%rcx, 664(%rsp)                 ## 8-byte Spill
	movl	20(%rax), %ecx
	movq	%rcx, 640(%rsp)                 ## 8-byte Spill
	movl	24(%rax), %ecx
	movl	%ecx, 428(%rsp)                 ## 4-byte Spill
	movl	32(%rax), %ecx
	movq	%rcx, 656(%rsp)                 ## 8-byte Spill
	movl	36(%rax), %ecx
	movq	%rcx, 632(%rsp)                 ## 8-byte Spill
	movl	40(%rax), %eax
	movl	%eax, 424(%rsp)                 ## 4-byte Spill
	movq	32(%rbp), %rbx
	movq	16(%rbx), %rax
	movq	%rax, 1184(%rsp)                ## 8-byte Spill
	leaq	32(%rbx), %rsi
	leaq	76(%rsp), %rdi
	movl	$4, %edx
	callq	_memcpy
	movl	76(%rsp), %eax
	movl	%eax, 1000(%rsp)                ## 4-byte Spill
	movq	24(%rbx), %rax
	movq	%rax, 1656(%rsp)                ## 8-byte Spill
	movl	36(%rbx), %eax
	movl	%eax, 992(%rsp)                 ## 4-byte Spill
	movq	40(%rbx), %rax
	movl	(%rax), %ecx
	movq	%rcx, 608(%rsp)                 ## 8-byte Spill
	movl	4(%rax), %ecx
	movq	%rcx, 584(%rsp)                 ## 8-byte Spill
	movl	8(%rax), %eax
	movl	%eax, 892(%rsp)                 ## 4-byte Spill
	movq	24(%rbp), %rbx
	movq	16(%rbx), %rax
	movq	%rax, 1176(%rsp)                ## 8-byte Spill
	leaq	32(%rbx), %rsi
	leaq	76(%rsp), %rdi
	movl	$4, %edx
	callq	_memcpy
	movl	76(%rsp), %eax
	movl	%eax, 984(%rsp)                 ## 4-byte Spill
	movq	24(%rbx), %rax
	movq	%rax, 1648(%rsp)                ## 8-byte Spill
	movl	36(%rbx), %eax
	movl	%eax, 976(%rsp)                 ## 4-byte Spill
	movq	40(%rbx), %rax
	movl	(%rax), %ecx
	movq	%rcx, 576(%rsp)                 ## 8-byte Spill
	movl	4(%rax), %ecx
	movq	%rcx, 552(%rsp)                 ## 8-byte Spill
	movl	8(%rax), %ecx
	movl	%ecx, 888(%rsp)                 ## 4-byte Spill
	movl	16(%rax), %ecx
	movq	%rcx, 344(%rsp)                 ## 8-byte Spill
	movl	20(%rax), %ecx
	movq	%rcx, 336(%rsp)                 ## 8-byte Spill
	movl	24(%rax), %eax
	movl	%eax, 516(%rsp)                 ## 4-byte Spill
	movq	136(%rbp), %rcx
	movq	16(%rcx), %rax
	movq	%rax, 1480(%rsp)                ## 8-byte Spill
	leaq	32(%rcx), %rsi
	movq	%rcx, %rbx
	leaq	76(%rsp), %rdi
	movl	$4, %edx
	callq	_memcpy
	movl	76(%rsp), %eax
	movl	%eax, 968(%rsp)                 ## 4-byte Spill
	movq	24(%rbx), %rax
	movq	%rax, 1640(%rsp)                ## 8-byte Spill
	movl	36(%rbx), %eax
	movl	%eax, 408(%rsp)                 ## 4-byte Spill
	movq	16(%r14), %rax
	movq	%rax, 1160(%rsp)                ## 8-byte Spill
	leaq	32(%r14), %rsi
	leaq	76(%rsp), %rdi
	movl	$4, %edx
	callq	_memcpy
	movl	76(%rsp), %eax
	movl	%eax, 404(%rsp)                 ## 4-byte Spill
	movq	24(%r14), %rax
	movq	%rax, 56(%rsp)                  ## 8-byte Spill
	movl	36(%r14), %eax
	movl	%eax, 400(%rsp)                 ## 4-byte Spill
	movq	%r14, 1208(%rsp)                ## 8-byte Spill
	movq	40(%r14), %rax
	movl	(%rax), %ecx
	movq	%rcx, 1112(%rsp)                ## 8-byte Spill
	movl	4(%rax), %ecx
	movq	%rcx, 160(%rsp)                 ## 8-byte Spill
	movl	8(%rax), %ecx
	movl	%ecx, 936(%rsp)                 ## 4-byte Spill
	movl	16(%rax), %ecx
	movq	%rcx, 1104(%rsp)                ## 8-byte Spill
	movl	20(%rax), %ecx
	movq	%rcx, 464(%rsp)                 ## 8-byte Spill
	movl	24(%rax), %ecx
	movl	%ecx, 512(%rsp)                 ## 4-byte Spill
	movl	32(%rax), %ecx
	movq	%rcx, 840(%rsp)                 ## 8-byte Spill
	movl	36(%rax), %ecx
	movq	%rcx, 472(%rsp)                 ## 8-byte Spill
	movl	40(%rax), %eax
	movl	%eax, 508(%rsp)                 ## 4-byte Spill
	movq	128(%rbp), %rbx
	movq	16(%rbx), %rax
	movq	%rax, 1472(%rsp)                ## 8-byte Spill
	leaq	32(%rbx), %rsi
	leaq	76(%rsp), %rdi
	movl	$4, %edx
	callq	_memcpy
	movl	76(%rsp), %eax
	movl	%eax, 396(%rsp)                 ## 4-byte Spill
	movq	24(%rbx), %rax
	movq	%rax, 168(%rsp)                 ## 8-byte Spill
	movl	36(%rbx), %eax
	movl	%eax, 960(%rsp)                 ## 4-byte Spill
	movq	40(%rbx), %rax
	movq	120(%rbp), %r14
	movq	112(%rbp), %r13
	movl	(%rax), %ecx
	movq	%rcx, 848(%rsp)                 ## 8-byte Spill
	movl	4(%rax), %r15d
	movl	8(%rax), %eax
	movl	%eax, 932(%rsp)                 ## 4-byte Spill
	movq	24(%rsp), %rbx                  ## 8-byte Reload
	movq	16(%rbx), %rax
	movq	%rax, 184(%rsp)                 ## 8-byte Spill
	leaq	32(%rbx), %rsi
	leaq	76(%rsp), %rdi
	movl	$4, %edx
	callq	_memcpy
	movl	76(%rsp), %eax
	movl	%eax, 952(%rsp)                 ## 4-byte Spill
	movq	24(%rbx), %rax
	movq	%rax, 200(%rsp)                 ## 8-byte Spill
	movl	36(%rbx), %eax
	movl	%eax, 392(%rsp)                 ## 4-byte Spill
	movq	40(%rbx), %rax
	movl	(%rax), %ecx
	movq	%rcx, 320(%rsp)                 ## 8-byte Spill
	movl	4(%rax), %ecx
	movq	%rcx, 208(%rsp)                 ## 8-byte Spill
	movl	8(%rax), %ecx
	movl	%ecx, 928(%rsp)                 ## 4-byte Spill
	movl	16(%rax), %ecx
	movq	%rcx, 288(%rsp)                 ## 8-byte Spill
	movl	20(%rax), %ecx
	movq	%rcx, 448(%rsp)                 ## 8-byte Spill
	movl	24(%rax), %ecx
	movq	%rcx, 216(%rsp)                 ## 8-byte Spill
	movl	32(%rax), %ecx
	movq	%rcx, 440(%rsp)                 ## 8-byte Spill
	movl	36(%rax), %ecx
	movq	%rcx, 456(%rsp)                 ## 8-byte Spill
	movl	40(%rax), %eax
	movl	%eax, 32(%rsp)                  ## 4-byte Spill
	movq	72(%rbp), %rbx
	movq	16(%rbx), %rax
	movq	%rax, 1200(%rsp)                ## 8-byte Spill
	leaq	32(%rbx), %rsi
	leaq	76(%rsp), %rdi
	movl	$4, %edx
	callq	_memcpy
	movl	76(%rsp), %eax
	movl	%eax, 388(%rsp)                 ## 4-byte Spill
	movq	24(%rbx), %rax
	movq	%rax, 128(%rsp)                 ## 8-byte Spill
	movl	36(%rbx), %eax
	movl	%eax, 384(%rsp)                 ## 4-byte Spill
	movq	40(%rbx), %rax
	movl	(%rax), %ecx
	movq	%rcx, 1128(%rsp)                ## 8-byte Spill
	movl	4(%rax), %ecx
	movq	%rcx, 480(%rsp)                 ## 8-byte Spill
	movl	8(%rax), %eax
	movl	%eax, 924(%rsp)                 ## 4-byte Spill
	movq	16(%r14), %rax
	movq	%rax, 1464(%rsp)                ## 8-byte Spill
	leaq	32(%r14), %rsi
	leaq	76(%rsp), %rdi
	movl	$4, %edx
	callq	_memcpy
	movl	76(%rsp), %eax
	movl	%eax, 380(%rsp)                 ## 4-byte Spill
	movq	24(%r14), %rax
	movq	%rax, 312(%rsp)                 ## 8-byte Spill
	movl	36(%r14), %eax
	movl	%eax, 376(%rsp)                 ## 4-byte Spill
	movq	40(%r14), %rax
	movl	(%rax), %ecx
	movq	%rcx, 792(%rsp)                 ## 8-byte Spill
	movl	4(%rax), %ecx
	movq	%rcx, 768(%rsp)                 ## 8-byte Spill
	movl	8(%rax), %ecx
	movl	%ecx, 916(%rsp)                 ## 4-byte Spill
	movl	16(%rax), %ecx
	movq	%rcx, 784(%rsp)                 ## 8-byte Spill
	movl	20(%rax), %ecx
	movq	%rcx, 752(%rsp)                 ## 8-byte Spill
	movl	24(%rax), %eax
	movq	%rax, 1456(%rsp)                ## 8-byte Spill
	movq	16(%r13), %rax
	movq	%rax, 1448(%rsp)                ## 8-byte Spill
	leaq	32(%r13), %rsi
	leaq	76(%rsp), %rdi
	movl	$4, %edx
	callq	_memcpy
	movl	76(%rsp), %eax
	movl	%eax, 372(%rsp)                 ## 4-byte Spill
	movq	24(%r13), %rax
	movq	%rax, 304(%rsp)                 ## 8-byte Spill
	movl	36(%r13), %eax
	movl	%eax, 368(%rsp)                 ## 4-byte Spill
	movq	40(%r13), %rax
	movl	(%rax), %ecx
	movq	%rcx, 736(%rsp)                 ## 8-byte Spill
	movl	4(%rax), %ecx
	movq	%rcx, 712(%rsp)                 ## 8-byte Spill
	movl	8(%rax), %ecx
	movl	%ecx, 912(%rsp)                 ## 4-byte Spill
	movl	16(%rax), %ecx
	movq	%rcx, 728(%rsp)                 ## 8-byte Spill
	movl	20(%rax), %ecx
	movq	%rcx, 696(%rsp)                 ## 8-byte Spill
	movl	24(%rax), %ecx
	movl	%ecx, 948(%rsp)                 ## 4-byte Spill
	movl	32(%rax), %ecx
	movq	%rcx, 704(%rsp)                 ## 8-byte Spill
	movl	36(%rax), %ecx
	movq	%rcx, 680(%rsp)                 ## 8-byte Spill
	movl	40(%rax), %eax
	movq	%rax, 1440(%rsp)                ## 8-byte Spill
	movq	88(%rbp), %rbx
	movq	16(%rbx), %rax
	movq	%rax, 1408(%rsp)                ## 8-byte Spill
	leaq	32(%rbx), %rsi
	leaq	76(%rsp), %rdi
	movl	$4, %edx
	callq	_memcpy
	movl	76(%rsp), %eax
	movl	%eax, 364(%rsp)                 ## 4-byte Spill
	movq	24(%rbx), %rax
	movq	%rax, 280(%rsp)                 ## 8-byte Spill
	movl	36(%rbx), %eax
	movl	%eax, 360(%rsp)                 ## 4-byte Spill
	movq	40(%rbx), %rax
	movl	(%rax), %ecx
	movl	%ecx, 500(%rsp)                 ## 4-byte Spill
	movl	4(%rax), %ecx
	movq	%rcx, 264(%rsp)                 ## 8-byte Spill
	movl	8(%rax), %ecx
	movl	%ecx, 904(%rsp)                 ## 4-byte Spill
	movl	16(%rax), %ecx
	movl	%ecx, 496(%rsp)                 ## 4-byte Spill
	movl	20(%rax), %ecx
	movq	%rcx, 648(%rsp)                 ## 8-byte Spill
	movslq	24(%rax), %rax
	movq	%rax, 1392(%rsp)                ## 8-byte Spill
	movq	80(%rbp), %rcx
	movq	16(%rcx), %rax
	movq	%rax, 1384(%rsp)                ## 8-byte Spill
	leaq	32(%rcx), %rsi
	movq	%rcx, %rbx
	leaq	76(%rsp), %rdi
	movl	$4, %edx
	callq	_memcpy
	movl	76(%rsp), %eax
	movl	%eax, 356(%rsp)                 ## 4-byte Spill
	movq	40(%rbx), %rax
	movl	(%rax), %ecx
	movl	%ecx, 492(%rsp)                 ## 4-byte Spill
	movl	4(%rax), %ecx
	movq	%rcx, 600(%rsp)                 ## 8-byte Spill
	movl	8(%rax), %ecx
	movl	%ecx, 896(%rsp)                 ## 4-byte Spill
	movl	16(%rax), %ecx
	movl	%ecx, 624(%rsp)                 ## 4-byte Spill
	movl	20(%rax), %ecx
	movq	%rcx, 256(%rsp)                 ## 8-byte Spill
	movl	24(%rax), %ecx
	movl	%ecx, 940(%rsp)                 ## 4-byte Spill
	movl	32(%rax), %ecx
	movl	%ecx, 616(%rsp)                 ## 4-byte Spill
	movl	36(%rax), %ecx
	movq	%rcx, 248(%rsp)                 ## 8-byte Spill
	movl	40(%rax), %ecx
	movq	%rcx, 1376(%rsp)                ## 8-byte Spill
	movl	48(%rax), %ecx
	movl	%ecx, 592(%rsp)                 ## 4-byte Spill
	movl	52(%rax), %ecx
	movq	%rcx, 48(%rsp)                  ## 8-byte Spill
	movl	56(%rax), %eax
	movq	%rax, 1120(%rsp)                ## 8-byte Spill
	movq	24(%rbx), %rax
	movq	%rax, 272(%rsp)                 ## 8-byte Spill
	movl	36(%rbx), %eax
	movl	%eax, 352(%rsp)                 ## 4-byte Spill
	movq	104(%rbp), %rbx
	movq	16(%rbx), %rax
	movq	%rax, 1432(%rsp)                ## 8-byte Spill
	leaq	32(%rbx), %rsi
	leaq	76(%rsp), %rdi
	movl	$4, %edx
	callq	_memcpy
	movl	76(%rsp), %eax
	movl	%eax, 880(%rsp)                 ## 4-byte Spill
	movq	24(%rbx), %rax
	movq	%rax, 432(%rsp)                 ## 8-byte Spill
	movl	36(%rbx), %eax
	movl	%eax, 876(%rsp)                 ## 4-byte Spill
	movq	40(%rbx), %rax
	movl	(%rax), %ecx
	movl	%ecx, 568(%rsp)                 ## 4-byte Spill
	movl	16(%rax), %ecx
	movl	%ecx, 560(%rsp)                 ## 4-byte Spill
	vmovsd	4(%rax), %xmm0                  ## xmm0 = mem[0],zero
	vmovaps	%xmm0, 1344(%rsp)               ## 16-byte Spill
	movl	20(%rax), %ecx
	movq	%rcx, 296(%rsp)                 ## 8-byte Spill
	movl	24(%rax), %eax
	movq	%rax, 1424(%rsp)                ## 8-byte Spill
	movq	96(%rbp), %r13
	movq	16(%r13), %rax
	movq	%rax, 1416(%rsp)                ## 8-byte Spill
	leaq	32(%r13), %rsi
	leaq	76(%rsp), %rdi
	movl	$4, %edx
	callq	_memcpy
	movq	848(%rsp), %rsi                 ## 8-byte Reload
	leal	(%r15,%rsi), %eax
	cmpl	%r12d, %eax
	movl	%r12d, %edx
	movl	%eax, 1328(%rsp)                ## 4-byte Spill
	cmovgl	%eax, %edx
	movq	40(%rsp), %rcx                  ## 8-byte Reload
	cmpl	$8, %ecx
	movl	$8, %eax
	cmovgl	%ecx, %eax
	movq	%rax, 1144(%rsp)                ## 8-byte Spill
	movl	%ecx, %eax
	sarl	$31, %eax
	andl	%ecx, %eax
	movl	%eax, 488(%rsp)                 ## 4-byte Spill
	movq	120(%rsp), %rcx                 ## 8-byte Reload
	testl	%ecx, %ecx
	movl	$1, %eax
	cmovgl	%ecx, %eax
	movq	%rax, 80(%rsp)                  ## 8-byte Spill
	movl	$1, %eax
	cmovlel	%ecx, %eax
	movq	%rax, 104(%rsp)                 ## 8-byte Spill
	cmpl	$8, %r15d
	movl	$8, %eax
	cmovll	%r15d, %eax
	leal	-1(%r15), %ecx
	andl	$-8, %ecx
	addl	%eax, %ecx
	cmpl	%ecx, %r15d
	movq	%r15, 832(%rsp)                 ## 8-byte Spill
	cmovlel	%r15d, %ecx
	movq	%rcx, 1152(%rsp)                ## 8-byte Spill
	movl	$1, %eax
	testl	%edx, %edx
	cmovlel	%eax, %edx
	movl	%esi, %r11d
	sarl	$31, %r11d
	movl	%r11d, %ecx
	andl	%esi, %ecx
	movl	%edx, %esi
	subl	%ecx, %esi
	leal	-1(%rsi), %r15d
	movl	%r15d, %ebx
	andl	$-8, %ebx
	movq	%rcx, 232(%rsp)                 ## 8-byte Spill
	addl	%ecx, %ebx
	leal	-1(%rdx), %r8d
	cmpl	%ebx, %r8d
	cmovlel	%r8d, %ebx
	cmpl	$8, %esi
	movl	$8, %r10d
	movq	%rsi, 152(%rsp)                 ## 8-byte Spill
	cmovll	%esi, %r10d
	addl	%r10d, %ebx
	cmpl	%edx, %ebx
	movq	%rdx, 112(%rsp)                 ## 8-byte Spill
	cmovgl	%edx, %ebx
	cmpl	$8, %r12d
	movl	$8, %esi
	cmovll	%r12d, %esi
	movl	$-8, %ecx
	subl	%r12d, %ecx
	cmpl	%r12d, %ecx
	leal	-1(%r12), %edi
	cmovll	%edi, %ecx
	movl	%ecx, %edx
	sarl	$3, %edx
	sarl	$31, %ecx
	andnl	%edx, %ecx, %ecx
	movl	%edi, %edx
	sarl	$3, %edx
	cmpl	%ecx, %edx
	cmovgl	%ecx, %edx
	movq	%rsi, 176(%rsp)                 ## 8-byte Spill
	leal	(%rsi,%rdx,8), %edx
	cmpl	%r12d, %edx
	cmovgel	%r12d, %edx
	cmpl	%edx, %ebx
	movl	%edx, 196(%rsp)                 ## 4-byte Spill
	movl	%ebx, 144(%rsp)                 ## 4-byte Spill
	cmovgl	%ebx, %edx
	testl	%edx, %edx
	cmovlel	%eax, %edx
	movl	%edx, 824(%rsp)                 ## 4-byte Spill
	leal	8(,%rcx,8), %eax
	cmpl	%r12d, %eax
	cmovgel	%r12d, %eax
	leal	-1(%rax), %edx
	sarl	$3, %edx
	cmpl	%ecx, %edx
	cmovlel	%edx, %ecx
	leal	8(,%rcx,8), %ebx
	cmpl	%r12d, %ebx
	cmovgel	%r12d, %ebx
	cmpl	$8, %eax
	movl	$8, %esi
	cmovgel	%esi, %eax
	addl	%edx, %ecx
	leal	(%rax,%rcx,8), %eax
	cmpl	%ebx, %eax
	cmovlel	%eax, %ebx
	leal	-1(%rbx), %eax
	movl	64(%rbp), %ecx
	cmpl	%ecx, %eax
	cmovlel	%ecx, %eax
	cmpl	%eax, %edi
	movl	%edi, 224(%rsp)                 ## 4-byte Spill
	cmovlel	%edi, %eax
	movl	%eax, %ecx
	sarl	$31, %ecx
	andnl	%eax, %ecx, %eax
	movq	40(%r13), %rcx
	movl	(%rcx), %edx
	movq	%rdx, 1096(%rsp)                ## 8-byte Spill
	movl	4(%rcx), %edx
	movq	%rdx, 1072(%rsp)                ## 8-byte Spill
	movl	8(%rcx), %edx
	movl	%edx, 884(%rsp)                 ## 4-byte Spill
	movl	16(%rcx), %edx
	movq	%rdx, 1088(%rsp)                ## 8-byte Spill
	movl	20(%rcx), %edx
	movq	%rdx, 1064(%rsp)                ## 8-byte Spill
	movl	24(%rcx), %r14d
	movl	32(%rcx), %edx
	movq	%rdx, 1080(%rsp)                ## 8-byte Spill
	movl	36(%rcx), %edx
	movq	%rdx, 1056(%rsp)                ## 8-byte Spill
	movl	40(%rcx), %ecx
	movq	%rcx, 1400(%rsp)                ## 8-byte Spill
	cmpl	%r12d, %eax
	leal	1(%rax), %edi
	movq	%r12, %r9
	cmovll	%r12d, %edi
	movl	76(%rsp), %eax
	movl	%eax, 872(%rsp)                 ## 4-byte Spill
	movq	24(%r13), %rsi
	movl	36(%r13), %eax
	movl	%eax, 868(%rsp)                 ## 4-byte Spill
	movq	40(%rbp), %rax
	movq	16(%rax), %rax
	testq	%rax, %rax
	movq	%r13, %r12
	jne	LBB0_4
## %bb.2:                               ## %_halide_buffer_is_bounds_query.exit
	movq	40(%rsp), %rdx                  ## 8-byte Reload
	testl	%edx, %edx
	sets	%cl
	cmpl	$8, %edx
	setg	%dl
	cmpb	%cl, %dl
	je	LBB0_4
## %bb.3:                               ## %_halide_buffer_is_bounds_query.exit
	movq	40(%rbp), %rcx
	cmpq	$0, (%rcx)
	je	LBB0_358
LBB0_4:                                 ## %"assert succeeded"
	movl	%r14d, 944(%rsp)                ## 4-byte Spill
	movl	%edi, 504(%rsp)                 ## 4-byte Spill
	movq	%rsi, 64(%rsp)                  ## 8-byte Spill
	movq	%r15, 1664(%rsp)                ## 8-byte Spill
	movq	%r10, 1672(%rsp)                ## 8-byte Spill
	movl	%r8d, 1332(%rsp)                ## 4-byte Spill
	movq	%rbx, 1712(%rsp)                ## 8-byte Spill
	movq	48(%rbp), %r10
	cmpq	$0, 16(%r10)
	je	LBB0_6
## %bb.5:
	movq	128(%rbp), %r14
	movq	104(%rbp), %r15
	movq	32(%rbp), %r8
	movq	24(%rbp), %rsi
	movq	816(%rsp), %rdi                 ## 8-byte Reload
	movq	16(%rbp), %rbx
	vmovdqa	1360(%rsp), %xmm10              ## 16-byte Reload
	vmovdqa	1344(%rsp), %xmm15              ## 16-byte Reload
	movq	88(%rbp), %r13
	testq	%rax, %rax
	jne	LBB0_11
	jmp	LBB0_8
LBB0_6:                                 ## %_halide_buffer_is_bounds_query.exit749
	cmpq	$0, (%r10)
	movq	128(%rbp), %r14
	movq	104(%rbp), %r15
	movq	32(%rbp), %r8
	movq	24(%rbp), %rsi
	movq	816(%rsp), %rdi                 ## 8-byte Reload
	movq	16(%rbp), %rbx
	vmovdqa	1360(%rsp), %xmm10              ## 16-byte Reload
	vmovdqa	1344(%rsp), %xmm15              ## 16-byte Reload
	movq	88(%rbp), %r13
	je	LBB0_10
## %bb.7:                               ## %after_bb
	testq	%rax, %rax
	jne	LBB0_11
LBB0_8:                                 ## %_halide_buffer_is_bounds_query.exit763
	movq	40(%rbp), %rax
	cmpq	$0, (%rax)
	jne	LBB0_11
## %bb.9:                               ## %_halide_buffer_init.exit765
	vmovaps	LCPI0_6(%rip), %ymm0            ## ymm0 = [0,32,1,0,0,32,32,0]
	vmovups	%ymm0, 6816(%rsp)
	movq	40(%rbp), %rdx
	movq	40(%rdx), %rax
	vxorps	%xmm0, %xmm0, %xmm0
	vmovups	%xmm0, (%rdx)
	movq	$0, 16(%rdx)
	movabsq	$8590008322, %rcx               ## imm = 0x200012002
	movq	%rcx, 32(%rdx)
	vmovaps	6816(%rsp), %xmm0
	vmovups	%xmm0, (%rax)
	movq	40(%rdx), %rax
	vmovaps	6832(%rsp), %xmm0
	vmovups	%xmm0, 16(%rax)
	movq	$0, 24(%rdx)
	jmp	LBB0_11
LBB0_10:                                ## %then_bb
	movq	48(%rbp), %rdx
	movq	40(%rdx), %rax
	vxorps	%xmm0, %xmm0, %xmm0
	vmovups	%xmm0, (%rdx)
	movq	$0, 16(%rdx)
	movabsq	$4295041026, %rcx               ## imm = 0x100012002
	movq	%rcx, 32(%rdx)
	vmovaps	LCPI0_5(%rip), %xmm0            ## xmm0 = [0,32,1,0]
	vmovups	%xmm0, (%rax)
	movq	$0, 24(%rdx)
	movq	40(%rbp), %rax
	movq	16(%rax), %rax
	testq	%rax, %rax
	je	LBB0_8
LBB0_11:                                ## %after_bb17
	cmpq	$0, 16(%rbx)
	jne	LBB0_13
## %bb.12:                              ## %_halide_buffer_is_bounds_query.exit764
	cmpq	$0, (%rbx)
	je	LBB0_16
LBB0_13:                                ## %after_bb20
	cmpq	$0, 16(%rdi)
	jne	LBB0_17
LBB0_14:                                ## %_halide_buffer_is_bounds_query.exit766
	cmpq	$0, (%rdi)
	jne	LBB0_17
## %bb.15:                              ## %then_bb24
	movq	40(%rdi), %rax
	vxorps	%xmm0, %xmm0, %xmm0
	vmovups	%xmm0, (%rdi)
	movq	$0, 16(%rdi)
	movabsq	$12884975618, %rcx              ## imm = 0x300012002
	movq	%rcx, 32(%rdi)
	vmovaps	LCPI0_7(%rip), %xmm0            ## xmm0 = [0,8,1,0]
	vmovups	%xmm0, (%rax)
	movq	40(%rdi), %rax
	vmovaps	LCPI0_8(%rip), %xmm0            ## xmm0 = [0,40,8,0]
	vmovups	%xmm0, 16(%rax)
	movq	40(%rdi), %rax
	vmovaps	LCPI0_9(%rip), %xmm0            ## xmm0 = [0,7,320,0]
	vmovups	%xmm0, 32(%rax)
	movq	$0, 24(%rdi)
	jmp	LBB0_17
LBB0_16:                                ## %then_bb21
	movq	40(%rbx), %rax
	vxorps	%xmm0, %xmm0, %xmm0
	vmovups	%xmm0, (%rbx)
	movq	$0, 16(%rbx)
	movabsq	$4295041026, %rcx               ## imm = 0x100012002
	movq	%rcx, 32(%rbx)
	vmovaps	LCPI0_7(%rip), %xmm0            ## xmm0 = [0,8,1,0]
	vmovups	%xmm0, (%rax)
	movq	$0, 24(%rbx)
	cmpq	$0, 16(%rdi)
	je	LBB0_14
LBB0_17:                                ## %after_bb23
	cmpq	$0, 16(%r8)
	movq	136(%rbp), %rbx
	jne	LBB0_19
## %bb.18:                              ## %_halide_buffer_is_bounds_query.exit768
	cmpq	$0, (%r8)
	je	LBB0_22
LBB0_19:                                ## %after_bb26
	cmpq	$0, 16(%rsi)
	movq	1208(%rsp), %rdi                ## 8-byte Reload
	jne	LBB0_23
LBB0_20:                                ## %_halide_buffer_is_bounds_query.exit771
	cmpq	$0, (%rsi)
	jne	LBB0_23
## %bb.21:                              ## %_halide_buffer_init.exit775
	vmovaps	LCPI0_11(%rip), %ymm0           ## ymm0 = [0,24,1,0,0,39,24,0]
	vmovups	%ymm0, 6848(%rsp)
	movq	40(%rsi), %rax
	vxorps	%xmm0, %xmm0, %xmm0
	vmovups	%xmm0, (%rsi)
	movq	$0, 16(%rsi)
	movabsq	$8590008322, %rcx               ## imm = 0x200012002
	movq	%rcx, 32(%rsi)
	vmovaps	6848(%rsp), %xmm0
	vmovups	%xmm0, (%rax)
	movq	40(%rsi), %rax
	vmovaps	6864(%rsp), %xmm0
	vmovups	%xmm0, 16(%rax)
	movq	$0, 24(%rsi)
	jmp	LBB0_23
LBB0_22:                                ## %then_bb27
	movq	40(%r8), %rax
	vxorps	%xmm0, %xmm0, %xmm0
	vmovups	%xmm0, (%r8)
	movq	$0, 16(%r8)
	movabsq	$4295041026, %rcx               ## imm = 0x100012002
	movq	%rcx, 32(%r8)
	vmovaps	LCPI0_10(%rip), %xmm0           ## xmm0 = [0,24,1,0]
	vmovups	%xmm0, (%rax)
	movq	$0, 24(%r8)
	cmpq	$0, 16(%rsi)
	movq	1208(%rsp), %rdi                ## 8-byte Reload
	je	LBB0_20
LBB0_23:                                ## %after_bb29
	cmpq	$0, 16(%rbx)
	jne	LBB0_26
## %bb.24:                              ## %_halide_buffer_is_bounds_query.exit773
	cmpq	$0, (%rbx)
	jne	LBB0_26
## %bb.25:                              ## %then_bb33
	vxorps	%xmm0, %xmm0, %xmm0
	vmovups	%xmm0, 16(%rbx)
	vmovups	%xmm0, (%rbx)
	movq	$73730, 32(%rbx)                ## imm = 0x12002
LBB0_26:                                ## %after_bb32
	movq	80(%rsp), %rax                  ## 8-byte Reload
                                        ## kill: def $eax killed $eax killed $rax def $rax
	subl	104(%rsp), %eax                 ## 4-byte Folded Reload
	movq	%rax, 1048(%rsp)                ## 8-byte Spill
	cmpq	$0, 16(%rdi)
	movq	80(%rbp), %rsi
	jne	LBB0_28
## %bb.27:                              ## %_halide_buffer_is_bounds_query.exit776
	cmpq	$0, (%rdi)
	je	LBB0_31
LBB0_28:                                ## %after_bb35
	cmpq	$0, 16(%r14)
	jne	LBB0_32
LBB0_29:                                ## %_halide_buffer_is_bounds_query.exit777
	cmpq	$0, (%r14)
	jne	LBB0_32
## %bb.30:                              ## %then_bb39
	movq	40(%r14), %rax
	vxorps	%xmm0, %xmm0, %xmm0
	vmovups	%xmm0, (%r14)
	movq	$0, 16(%r14)
	movabsq	$4295041026, %rcx               ## imm = 0x100012002
	movq	%rcx, 32(%r14)
	movq	848(%rsp), %rcx                 ## 8-byte Reload
	movl	%ecx, (%rax)
	movq	1152(%rsp), %rcx                ## 8-byte Reload
	movl	%ecx, 4(%rax)
	movq	$1, 8(%rax)
	movq	$0, 24(%r14)
	jmp	LBB0_32
LBB0_31:                                ## %then_bb36
	movq	40(%rdi), %rax
	movq	104(%rsp), %rcx                 ## 8-byte Reload
	decl	%ecx
	vxorps	%xmm0, %xmm0, %xmm0
	vmovups	%xmm0, (%rdi)
	movq	1048(%rsp), %rdx                ## 8-byte Reload
	leal	1(%rdx), %edx
	movq	$0, 16(%rdi)
	movabsq	$12884975618, %rsi              ## imm = 0x300012002
	movq	%rsi, 32(%rdi)
	movq	80(%rbp), %rsi
	vmovaps	LCPI0_12(%rip), %xmm0           ## xmm0 = [0,40,1,0]
	vmovups	%xmm0, (%rax)
	movq	40(%rdi), %rax
	vmovaps	LCPI0_13(%rip), %xmm0           ## xmm0 = [0,7,40,0]
	vmovups	%xmm0, 16(%rax)
	movq	40(%rdi), %rax
	movl	%ecx, 32(%rax)
	movl	%edx, 36(%rax)
	movq	$280, 40(%rax)                  ## imm = 0x118
	movq	$0, 24(%rdi)
	cmpq	$0, 16(%r14)
	je	LBB0_29
LBB0_32:                                ## %after_bb38
	movl	%r11d, 1340(%rsp)               ## 4-byte Spill
	movq	24(%rsp), %r10                  ## 8-byte Reload
	cmpq	$0, 16(%r10)
	jne	LBB0_34
## %bb.33:                              ## %_halide_buffer_is_bounds_query.exit780
	cmpq	$0, (%r10)
	je	LBB0_37
LBB0_34:                                ## %after_bb41
	movq	72(%rbp), %r11
	cmpq	$0, 16(%r11)
	jne	LBB0_38
LBB0_35:                                ## %_halide_buffer_is_bounds_query.exit782
	cmpq	$0, (%r11)
	jne	LBB0_38
## %bb.36:                              ## %then_bb45
	movq	40(%r11), %rax
	vxorps	%xmm0, %xmm0, %xmm0
	vmovups	%xmm0, (%r11)
	movq	$0, 16(%r11)
	movabsq	$4295041026, %rcx               ## imm = 0x100012002
	movq	%rcx, 32(%r11)
	movl	$0, (%rax)
	movl	504(%rsp), %ecx                 ## 4-byte Reload
	movl	%ecx, 4(%rax)
	movq	$1, 8(%rax)
	movq	$0, 24(%r11)
	jmp	LBB0_38
LBB0_37:                                ## %then_bb42
	movl	824(%rsp), %eax                 ## 4-byte Reload
	movq	232(%rsp), %rbx                 ## 8-byte Reload
	subl	%ebx, %eax
	movq	40(%r10), %rcx
	imull	$39, %eax, %edx
	vxorps	%xmm0, %xmm0, %xmm0
	vmovups	%xmm0, (%r10)
	movq	$0, 16(%r10)
	movabsq	$12884975618, %rsi              ## imm = 0x300012002
	movq	%rsi, 32(%r10)
	movl	%ebx, (%rcx)
	movq	104(%rbp), %r15
	movq	136(%rbp), %rbx
	movl	%eax, 4(%rcx)
	movq	$1, 8(%rcx)
	movq	40(%r10), %rcx
	movabsq	$167503724544, %rsi             ## imm = 0x2700000000
	movq	%rsi, 16(%rcx)
	movq	80(%rbp), %rsi
	movl	%eax, 24(%rcx)
	movl	$0, 28(%rcx)
	movq	40(%r10), %rax
	movl	$0, 32(%rax)
	movq	80(%rsp), %rcx                  ## 8-byte Reload
	movl	%ecx, 36(%rax)
	movl	%edx, 40(%rax)
	movl	$0, 44(%rax)
	movq	$0, 24(%r10)
	movq	72(%rbp), %r11
	cmpq	$0, 16(%r11)
	je	LBB0_35
LBB0_38:                                ## %after_bb44
	movq	120(%rbp), %rax
	cmpq	$0, 16(%rax)
	jne	LBB0_40
## %bb.39:                              ## %_halide_buffer_is_bounds_query.exit785
	cmpq	$0, (%rax)
	je	LBB0_43
LBB0_40:                                ## %after_bb47
	movq	112(%rbp), %rdx
	cmpq	$0, 16(%rdx)
	jne	LBB0_44
LBB0_41:                                ## %_halide_buffer_is_bounds_query.exit787
	cmpq	$0, (%rdx)
	jne	LBB0_44
## %bb.42:                              ## %then_bb51
	movq	40(%rdx), %rax
	vxorps	%xmm0, %xmm0, %xmm0
	vmovups	%xmm0, (%rdx)
	movq	$0, 16(%rdx)
	movabsq	$12884975618, %rcx              ## imm = 0x300012002
	movq	%rcx, 32(%rdx)
	vmovaps	LCPI0_5(%rip), %xmm0            ## xmm0 = [0,32,1,0]
	vmovups	%xmm0, (%rax)
	movq	40(%rdx), %rax
	vmovaps	LCPI0_15(%rip), %xmm0           ## xmm0 = [0,32,32,0]
	vmovups	%xmm0, 16(%rax)
	movq	40(%rdx), %rax
	vmovaps	LCPI0_16(%rip), %xmm0           ## xmm0 = [0,4,1024,0]
	vmovups	%xmm0, 32(%rax)
	movq	120(%rbp), %rax
	movq	$0, 24(%rdx)
	jmp	LBB0_44
LBB0_43:                                ## %_halide_buffer_init.exit789
	vmovaps	LCPI0_14(%rip), %ymm0           ## ymm0 = [0,32,1,0,0,4,32,0]
	vmovups	%ymm0, 6880(%rsp)
	movq	120(%rbp), %rax
	movq	40(%rax), %rax
	vxorps	%xmm0, %xmm0, %xmm0
	movq	120(%rbp), %rcx
	vmovups	%xmm0, (%rcx)
	movq	120(%rbp), %rcx
	movq	$0, 16(%rcx)
	movabsq	$8590008322, %rcx               ## imm = 0x200012002
	movq	120(%rbp), %r10
	movq	%rcx, 32(%r10)
	vmovaps	6880(%rsp), %xmm0
	vmovups	%xmm0, (%rax)
	movq	120(%rbp), %rax
	movq	40(%rax), %rax
	vmovaps	6896(%rsp), %xmm0
	vmovups	%xmm0, 16(%rax)
	movq	120(%rbp), %rax
	movq	$0, 24(%rax)
	movq	112(%rbp), %rdx
	cmpq	$0, 16(%rdx)
	je	LBB0_41
LBB0_44:                                ## %after_bb50
	cmpq	$0, 16(%r13)
	jne	LBB0_46
## %bb.45:                              ## %_halide_buffer_is_bounds_query.exit790
	cmpq	$0, (%r13)
	je	LBB0_49
LBB0_46:                                ## %after_bb53
	cmpq	$0, 16(%rsi)
	jne	LBB0_50
LBB0_47:                                ## %_halide_buffer_is_bounds_query.exit793
	cmpq	$0, (%rsi)
	jne	LBB0_50
## %bb.48:                              ## %then_bb57
	vmovaps	LCPI0_18(%rip), %ymm0           ## ymm0 = [0,8,1,0,0,40,8,0]
	vmovups	%ymm0, 6976(%rsp)
	vmovaps	LCPI0_19(%rip), %ymm0           ## ymm0 = [0,7,320,0,0,4,2240,0]
	vmovups	%ymm0, 7008(%rsp)
	movq	40(%rsi), %rax
	vxorps	%xmm0, %xmm0, %xmm0
	vmovups	%xmm0, (%rsi)
	movq	$0, 16(%rsi)
	movabsq	$17179942914, %rcx              ## imm = 0x400012002
	movq	%rcx, 32(%rsi)
	vmovups	6976(%rsp), %xmm0
	vmovups	%xmm0, (%rax)
	movq	40(%rsi), %rax
	vmovups	6992(%rsp), %xmm0
	vmovups	%xmm0, 16(%rax)
	movq	40(%rsi), %rax
	vmovups	7008(%rsp), %xmm0
	vmovups	%xmm0, 32(%rax)
	movq	40(%rsi), %rax
	vmovups	7024(%rsp), %xmm0
	vmovups	%xmm0, 48(%rax)
	movq	120(%rbp), %rax
	movq	$0, 24(%rsi)
	jmp	LBB0_50
LBB0_49:                                ## %_halide_buffer_init.exit795
	vmovaps	LCPI0_17(%rip), %ymm0           ## ymm0 = [0,8,1,0,0,4,8,0]
	vmovups	%ymm0, 6912(%rsp)
	movq	40(%r13), %rax
	vxorps	%xmm0, %xmm0, %xmm0
	vmovups	%xmm0, (%r13)
	movq	$0, 16(%r13)
	movabsq	$8590008322, %rcx               ## imm = 0x200012002
	movq	%rcx, 32(%r13)
	vmovaps	6912(%rsp), %xmm0
	vmovups	%xmm0, (%rax)
	movq	40(%r13), %rax
	vmovaps	6928(%rsp), %xmm0
	vmovups	%xmm0, 16(%rax)
	movq	120(%rbp), %rax
	movq	$0, 24(%r13)
	cmpq	$0, 16(%rsi)
	je	LBB0_47
LBB0_50:                                ## %after_bb56
	cmpq	$0, 16(%r15)
	jne	LBB0_52
## %bb.51:                              ## %_halide_buffer_is_bounds_query.exit796
	cmpq	$0, (%r15)
	je	LBB0_54
LBB0_52:                                ## %after_bb59
	cmpq	$0, 16(%r12)
	je	LBB0_55
LBB0_53:
	movl	$0, 328(%rsp)                   ## 4-byte Folded Spill
	cmpq	$0, 16(%r15)
	je	LBB0_74
LBB0_57:
	movl	$0, 544(%rsp)                   ## 4-byte Folded Spill
	cmpq	$0, 16(%rsi)
	je	LBB0_75
LBB0_58:
	movl	$0, 536(%rsp)                   ## 4-byte Folded Spill
	cmpq	$0, 16(%r13)
	je	LBB0_76
LBB0_59:
	movl	$0, 528(%rsp)                   ## 4-byte Folded Spill
	cmpq	$0, 16(%rdx)
	je	LBB0_77
LBB0_60:
	movl	$0, 520(%rsp)                   ## 4-byte Folded Spill
	movq	%r9, 136(%rsp)                  ## 8-byte Spill
	cmpq	$0, 16(%rax)
	je	LBB0_78
LBB0_61:
	xorl	%r12d, %r12d
	movq	16(%rbp), %r9
	cmpq	$0, 16(%r11)
	je	LBB0_79
LBB0_62:
	xorl	%r13d, %r13d
	movq	24(%rsp), %rax                  ## 8-byte Reload
	cmpq	$0, 16(%rax)
	je	LBB0_80
LBB0_63:
	xorl	%r15d, %r15d
	cmpq	$0, 16(%r14)
	je	LBB0_81
LBB0_64:
	xorl	%edx, %edx
	movq	48(%rbp), %r11
	cmpq	$0, 16(%rdi)
	je	LBB0_82
LBB0_65:
	xorl	%esi, %esi
	cmpq	$0, 16(%rbx)
	je	LBB0_83
LBB0_66:
	xorl	%edi, %edi
	movq	24(%rbp), %rax
	cmpq	$0, 16(%rax)
	je	LBB0_84
LBB0_67:
	xorl	%ebx, %ebx
	cmpq	$0, 16(%r8)
	je	LBB0_85
LBB0_68:
	xorl	%eax, %eax
	movq	816(%rsp), %rcx                 ## 8-byte Reload
	cmpq	$0, 16(%rcx)
	je	LBB0_86
LBB0_69:
	xorl	%r8d, %r8d
	cmpq	$0, 16(%r9)
	je	LBB0_87
LBB0_70:
	xorl	%r9d, %r9d
	cmpq	$0, 16(%r11)
	je	LBB0_88
LBB0_71:
	xorl	%r10d, %r10d
	jmp	LBB0_89
LBB0_54:                                ## %_halide_buffer_init.exit801
	vmovaps	LCPI0_20(%rip), %ymm0           ## ymm0 = [0,24,1,0,0,4,24,0]
	vmovups	%ymm0, 6944(%rsp)
	movq	40(%r15), %rax
	vxorps	%xmm0, %xmm0, %xmm0
	vmovups	%xmm0, (%r15)
	movq	$0, 16(%r15)
	movabsq	$8590008322, %rcx               ## imm = 0x200012002
	movq	%rcx, 32(%r15)
	vmovaps	6944(%rsp), %xmm0
	vmovups	%xmm0, (%rax)
	movq	40(%r15), %rax
	vmovaps	6960(%rsp), %xmm0
	vmovups	%xmm0, 16(%rax)
	movq	120(%rbp), %rax
	movq	$0, 24(%r15)
	cmpq	$0, 16(%r12)
	jne	LBB0_53
LBB0_55:                                ## %_halide_buffer_is_bounds_query.exit799
	cmpq	$0, (%r12)
	je	LBB0_72
LBB0_56:                                ## %after_bb62.thread
	cmpq	$0, (%r12)
	sete	%cl
	movl	%ecx, 328(%rsp)                 ## 4-byte Spill
	cmpq	$0, 16(%r15)
	jne	LBB0_57
LBB0_74:
	cmpq	$0, (%r15)
	sete	%cl
	movl	%ecx, 544(%rsp)                 ## 4-byte Spill
	cmpq	$0, 16(%rsi)
	jne	LBB0_58
LBB0_75:
	cmpq	$0, (%rsi)
	sete	%cl
	movl	%ecx, 536(%rsp)                 ## 4-byte Spill
	cmpq	$0, 16(%r13)
	jne	LBB0_59
LBB0_76:
	cmpq	$0, (%r13)
	sete	%sil
	movl	%esi, 528(%rsp)                 ## 4-byte Spill
	cmpq	$0, 16(%rdx)
	jne	LBB0_60
LBB0_77:
	cmpq	$0, (%rdx)
	sete	%dl
	movl	%edx, 520(%rsp)                 ## 4-byte Spill
	movq	%r9, 136(%rsp)                  ## 8-byte Spill
	cmpq	$0, 16(%rax)
	jne	LBB0_61
LBB0_78:
	cmpq	$0, (%rax)
	sete	%r12b
	movq	16(%rbp), %r9
	cmpq	$0, 16(%r11)
	jne	LBB0_62
LBB0_79:
	cmpq	$0, (%r11)
	sete	%r13b
	movq	24(%rsp), %rax                  ## 8-byte Reload
	cmpq	$0, 16(%rax)
	jne	LBB0_63
LBB0_80:
	cmpq	$0, (%rax)
	sete	%r15b
	cmpq	$0, 16(%r14)
	jne	LBB0_64
LBB0_81:
	cmpq	$0, (%r14)
	sete	%dl
	movq	48(%rbp), %r11
	cmpq	$0, 16(%rdi)
	jne	LBB0_65
LBB0_82:
	cmpq	$0, (%rdi)
	sete	%sil
	cmpq	$0, 16(%rbx)
	jne	LBB0_66
LBB0_83:
	cmpq	$0, (%rbx)
	sete	%dil
	movq	24(%rbp), %rax
	cmpq	$0, 16(%rax)
	jne	LBB0_67
LBB0_84:
	cmpq	$0, (%rax)
	sete	%bl
	cmpq	$0, 16(%r8)
	jne	LBB0_68
LBB0_85:
	cmpq	$0, (%r8)
	sete	%al
	movq	816(%rsp), %rcx                 ## 8-byte Reload
	cmpq	$0, 16(%rcx)
	jne	LBB0_69
LBB0_86:
	cmpq	$0, (%rcx)
	sete	%r8b
	cmpq	$0, 16(%r9)
	jne	LBB0_70
LBB0_87:
	cmpq	$0, (%r9)
	sete	%r9b
	cmpq	$0, 16(%r11)
	jne	LBB0_71
LBB0_88:
	cmpq	$0, (%r11)
	sete	%r10b
LBB0_89:                                ## %_halide_buffer_is_bounds_query.exit817
	xorl	%r14d, %r14d
	movq	40(%rbp), %rcx
	cmpq	$0, 16(%rcx)
	movl	$0, %r11d
	jne	LBB0_91
## %bb.90:
	movq	40(%rbp), %rcx
	cmpq	$0, (%rcx)
	sete	%r11b
LBB0_91:                                ## %_halide_buffer_is_bounds_query.exit818
	orb	%r11b, %r10b
	orb	%r10b, %r9b
	orb	%r9b, %r8b
	orb	%r8b, %al
	orb	%al, %bl
	orb	%bl, %dil
	orb	%dil, %sil
	orb	%sil, %dl
	orb	%dl, %r15b
	orb	%r15b, %r13b
	orb	%r13b, %r12b
	movl	520(%rsp), %eax                 ## 4-byte Reload
	orb	%r12b, %al
	movl	528(%rsp), %ecx                 ## 4-byte Reload
	orb	%al, %cl
	movl	536(%rsp), %eax                 ## 4-byte Reload
	orb	%cl, %al
	movl	544(%rsp), %ecx                 ## 4-byte Reload
	orb	%al, %cl
	movl	328(%rsp), %eax                 ## 4-byte Reload
	orb	%cl, %al
	testb	$1, %al
	jne	LBB0_235
## %bb.92:                              ## %then_bb66
	xorl	%eax, %eax
	cmpl	$73730, 1040(%rsp)              ## 4-byte Folded Reload
                                        ## imm = 0x12002
	setne	%al
	movq	%rax, 328(%rsp)                 ## 8-byte Spill
	xorl	%eax, %eax
	cmpl	$1, 1032(%rsp)                  ## 4-byte Folded Reload
	sete	%al
	movl	%eax, 520(%rsp)                 ## 4-byte Spill
	xorl	%eax, %eax
	cmpl	$73730, 420(%rsp)               ## 4-byte Folded Reload
                                        ## imm = 0x12002
	sete	%al
	movl	%eax, 1324(%rsp)                ## 4-byte Spill
	xorl	%eax, %eax
	cmpl	$2, 1024(%rsp)                  ## 4-byte Folded Reload
	sete	%al
	movl	%eax, 1320(%rsp)                ## 4-byte Spill
	xorl	%eax, %eax
	cmpl	$73730, 416(%rsp)               ## 4-byte Folded Reload
                                        ## imm = 0x12002
	sete	%al
	movl	%eax, 1304(%rsp)                ## 4-byte Spill
	xorl	%eax, %eax
	cmpl	$1, 1016(%rsp)                  ## 4-byte Folded Reload
	sete	%al
	movl	%eax, 1292(%rsp)                ## 4-byte Spill
	xorl	%r8d, %r8d
	cmpl	$73730, 412(%rsp)               ## 4-byte Folded Reload
                                        ## imm = 0x12002
	sete	%r8b
	xorl	%r10d, %r10d
	cmpl	$3, 1008(%rsp)                  ## 4-byte Folded Reload
	sete	%r10b
	xorl	%eax, %eax
	cmpl	$73730, 1000(%rsp)              ## 4-byte Folded Reload
                                        ## imm = 0x12002
	sete	%al
	movl	%eax, 1284(%rsp)                ## 4-byte Spill
	xorl	%eax, %eax
	cmpl	$1, 992(%rsp)                   ## 4-byte Folded Reload
	sete	%al
	movl	%eax, 1276(%rsp)                ## 4-byte Spill
	xorl	%eax, %eax
	cmpl	$73730, 984(%rsp)               ## 4-byte Folded Reload
                                        ## imm = 0x12002
	sete	%al
	movl	%eax, 1272(%rsp)                ## 4-byte Spill
	xorl	%eax, %eax
	cmpl	$2, 976(%rsp)                   ## 4-byte Folded Reload
	sete	%al
	movl	%eax, 1268(%rsp)                ## 4-byte Spill
	xorl	%eax, %eax
	cmpl	$73730, 968(%rsp)               ## 4-byte Folded Reload
                                        ## imm = 0x12002
	sete	%al
	movl	%eax, 1296(%rsp)                ## 4-byte Spill
	xorl	%eax, %eax
	cmpl	$0, 408(%rsp)                   ## 4-byte Folded Reload
	sete	%al
	movl	%eax, 1288(%rsp)                ## 4-byte Spill
	xorl	%eax, %eax
	cmpl	$73730, 404(%rsp)               ## 4-byte Folded Reload
                                        ## imm = 0x12002
	sete	%al
	movl	%eax, 1280(%rsp)                ## 4-byte Spill
	xorl	%r9d, %r9d
	cmpl	$3, 400(%rsp)                   ## 4-byte Folded Reload
	sete	%r9b
	xorl	%eax, %eax
	cmpl	$73730, 396(%rsp)               ## 4-byte Folded Reload
                                        ## imm = 0x12002
	sete	%al
	movl	%eax, 1316(%rsp)                ## 4-byte Spill
	xorl	%eax, %eax
	cmpl	$1, 960(%rsp)                   ## 4-byte Folded Reload
	sete	%al
	movl	%eax, 1300(%rsp)                ## 4-byte Spill
	xorl	%eax, %eax
	cmpl	$73730, 952(%rsp)               ## 4-byte Folded Reload
                                        ## imm = 0x12002
	sete	%al
	movl	%eax, 1308(%rsp)                ## 4-byte Spill
	xorl	%eax, %eax
	cmpl	$3, 392(%rsp)                   ## 4-byte Folded Reload
	sete	%al
	movl	%eax, 1312(%rsp)                ## 4-byte Spill
	xorl	%eax, %eax
	cmpl	$73730, 388(%rsp)               ## 4-byte Folded Reload
                                        ## imm = 0x12002
	setne	%al
	shlq	$20, %rax
	movq	%rax, 536(%rsp)                 ## 8-byte Spill
	xorl	%eax, %eax
	cmpl	$1, 384(%rsp)                   ## 4-byte Folded Reload
	setne	%al
	shlq	$21, %rax
	movq	%rax, 528(%rsp)                 ## 8-byte Spill
	xorl	%eax, %eax
	cmpl	$73730, 380(%rsp)               ## 4-byte Folded Reload
                                        ## imm = 0x12002
	setne	%al
	shlq	$22, %rax
	xorl	%ecx, %ecx
	cmpl	$2, 376(%rsp)                   ## 4-byte Folded Reload
	setne	%cl
	shlq	$23, %rcx
	movq	%rcx, 1632(%rsp)                ## 8-byte Spill
	xorl	%ecx, %ecx
	cmpl	$73730, 372(%rsp)               ## 4-byte Folded Reload
                                        ## imm = 0x12002
	setne	%cl
	shlq	$24, %rcx
	movq	%rcx, 1624(%rsp)                ## 8-byte Spill
	xorl	%ecx, %ecx
	cmpl	$3, 368(%rsp)                   ## 4-byte Folded Reload
	setne	%cl
	shlq	$25, %rcx
	movq	%rcx, 1616(%rsp)                ## 8-byte Spill
	xorl	%ecx, %ecx
	cmpl	$73730, 364(%rsp)               ## 4-byte Folded Reload
                                        ## imm = 0x12002
	setne	%cl
	shlq	$26, %rcx
	movq	%rcx, 1608(%rsp)                ## 8-byte Spill
	xorl	%ecx, %ecx
	cmpl	$2, 360(%rsp)                   ## 4-byte Folded Reload
	setne	%cl
	shlq	$27, %rcx
	movq	%rcx, 1600(%rsp)                ## 8-byte Spill
	xorl	%ecx, %ecx
	cmpl	$73730, 356(%rsp)               ## 4-byte Folded Reload
                                        ## imm = 0x12002
	setne	%cl
	shlq	$28, %rcx
	movq	%rcx, 1592(%rsp)                ## 8-byte Spill
	xorl	%ecx, %ecx
	cmpl	$4, 352(%rsp)                   ## 4-byte Folded Reload
	setne	%cl
	shlq	$29, %rcx
	movq	%rcx, 1584(%rsp)                ## 8-byte Spill
	xorl	%ecx, %ecx
	cmpl	$73730, 880(%rsp)               ## 4-byte Folded Reload
                                        ## imm = 0x12002
	setne	%cl
	shlq	$30, %rcx
	movq	%rcx, 1576(%rsp)                ## 8-byte Spill
	xorl	%ecx, %ecx
	cmpl	$2, 876(%rsp)                   ## 4-byte Folded Reload
	setne	%cl
	shlq	$31, %rcx
	movq	%rcx, 1568(%rsp)                ## 8-byte Spill
	xorl	%ecx, %ecx
	cmpl	$73730, 872(%rsp)               ## 4-byte Folded Reload
                                        ## imm = 0x12002
	setne	%cl
	shlq	$32, %rcx
	movq	%rcx, 1560(%rsp)                ## 8-byte Spill
	xorl	%ecx, %ecx
	cmpl	$3, 868(%rsp)                   ## 4-byte Folded Reload
	setne	%cl
	shlq	$33, %rcx
	movq	%rcx, 1552(%rsp)                ## 8-byte Spill
	movq	808(%rsp), %rdx                 ## 8-byte Reload
	testl	%edx, %edx
	setg	%cl
	movq	800(%rsp), %rsi                 ## 8-byte Reload
	addl	%esi, %edx
	movl	%edx, 1264(%rsp)                ## 4-byte Spill
	cmpl	$32, %edx
	setl	%dl
	orb	%cl, %dl
	movb	%dl, 231(%rsp)                  ## 1-byte Spill
                                        ## kill: def $esi killed $esi killed $rsi def $rsi
	andl	$-2147483648, %esi              ## imm = 0x80000000
	shlq	$4, %rsi
	movq	%rsi, 1544(%rsp)                ## 8-byte Spill
	movq	760(%rsp), %rsi                 ## 8-byte Reload
	testl	%esi, %esi
	setg	%cl
	vmovd	%xmm10, %edx
	addl	%edx, %esi
	movl	%esi, 1260(%rsp)                ## 4-byte Spill
	cmpl	$32, %esi
	setl	%bl
	orb	%cl, %bl
	movb	%bl, 230(%rsp)                  ## 1-byte Spill
	movq	%rdx, 544(%rsp)                 ## 8-byte Spill
                                        ## kill: def $edx killed $edx killed $rdx def $rdx
	shrl	$31, %edx
	shlq	$37, %rdx
	movq	%rdx, 1536(%rsp)                ## 8-byte Spill
	movq	776(%rsp), %rsi                 ## 8-byte Reload
	cmpl	488(%rsp), %esi                 ## 4-byte Folded Reload
	setg	%cl
	movq	1144(%rsp), %rdx                ## 8-byte Reload
	addl	$24, %edx
	movq	744(%rsp), %rdi                 ## 8-byte Reload
	addl	%edi, %esi
	movl	%esi, 1256(%rsp)                ## 4-byte Spill
	cmpl	%esi, %edx
	vmovd	%r8d, %xmm0
	vpinsrb	$4, %r10d, %xmm0, %xmm0
	setg	%dl
	orb	%cl, %dl
                                        ## kill: def $edi killed $edi killed $rdi def $rdi
	andl	$-2147483648, %edi              ## imm = 0x80000000
	shlq	$8, %rdi
	movq	%rdi, 1528(%rsp)                ## 8-byte Spill
	movq	720(%rsp), %rsi                 ## 8-byte Reload
	testl	%esi, %esi
	setg	%cl
	movq	688(%rsp), %rdi                 ## 8-byte Reload
	addl	%edi, %esi
	movl	%esi, 1252(%rsp)                ## 4-byte Spill
	cmpl	$8, %esi
	setl	%r15b
	orb	%cl, %r15b
                                        ## kill: def $edi killed $edi killed $rdi def $rdi
	andl	$-2147483648, %edi              ## imm = 0x80000000
	shlq	$10, %rdi
	movq	%rdi, 1520(%rsp)                ## 8-byte Spill
	movq	672(%rsp), %rsi                 ## 8-byte Reload
	testl	%esi, %esi
	setg	%cl
	movq	40(%rsp), %rdi                  ## 8-byte Reload
	addl	%edi, %esi
	movl	%esi, 1248(%rsp)                ## 4-byte Spill
	cmpl	$8, %esi
	setl	%bl
	orb	%cl, %bl
	movzbl	%bl, %ecx
	vpinsrb	$8, %ecx, %xmm0, %xmm0
	movzbl	%dl, %ecx
	vpinsrb	$12, %ecx, %xmm0, %xmm0
	vmovd	%r9d, %xmm1
                                        ## kill: def $edi killed $edi killed $rdi def $rdi
	andl	$-2147483648, %edi              ## imm = 0x80000000
	shlq	$12, %rdi
	movq	%rdi, 1512(%rsp)                ## 8-byte Spill
	movq	664(%rsp), %rdx                 ## 8-byte Reload
	testl	%edx, %edx
	setg	%cl
	movq	640(%rsp), %rsi                 ## 8-byte Reload
	addl	%esi, %edx
	movl	%edx, 1244(%rsp)                ## 4-byte Spill
	cmpl	$40, %edx
	setl	%r12b
	orb	%cl, %r12b
                                        ## kill: def $esi killed $esi killed $rsi def $rsi
	andl	$-2147483648, %esi              ## imm = 0x80000000
	shlq	$14, %rsi
	movq	%rsi, 1504(%rsp)                ## 8-byte Spill
	movq	656(%rsp), %rcx                 ## 8-byte Reload
	testl	%ecx, %ecx
	setg	%dl
	movq	632(%rsp), %rsi                 ## 8-byte Reload
	addl	%esi, %ecx
	movl	%ecx, 1240(%rsp)                ## 4-byte Spill
	cmpl	$7, %ecx
	setl	%dil
	orb	%dl, %dil
                                        ## kill: def $esi killed $esi killed $rsi def $rsi
	andl	$-2147483648, %esi              ## imm = 0x80000000
	shlq	$16, %rsi
	movq	%rsi, 1496(%rsp)                ## 8-byte Spill
	movq	608(%rsp), %rcx                 ## 8-byte Reload
	testl	%ecx, %ecx
	setg	%dl
	movq	584(%rsp), %rbx                 ## 8-byte Reload
	addl	%ebx, %ecx
	movl	%ecx, 1232(%rsp)                ## 4-byte Spill
	cmpl	$24, %ecx
	setl	%sil
	orb	%dl, %sil
                                        ## kill: def $ebx killed $ebx killed $rbx def $rbx
	andl	$-2147483648, %ebx              ## imm = 0x80000000
	shlq	$18, %rbx
	movq	%rbx, 1488(%rsp)                ## 8-byte Spill
	movq	576(%rsp), %rcx                 ## 8-byte Reload
	testl	%ecx, %ecx
	setg	%dl
	movq	552(%rsp), %r14                 ## 8-byte Reload
	addl	%r14d, %ecx
	movl	%ecx, 1224(%rsp)                ## 4-byte Spill
	cmpl	$24, %ecx
	setl	%cl
	orb	%dl, %cl
                                        ## kill: def $r14d killed $r14d killed $r14 def $r14
	andl	$-2147483648, %r14d             ## imm = 0x80000000
	shlq	$20, %r14
	movq	344(%rsp), %rdx                 ## 8-byte Reload
	testl	%edx, %edx
	setg	%r8b
	movq	336(%rsp), %r11                 ## 8-byte Reload
	addl	%r11d, %edx
	movl	%edx, 1220(%rsp)                ## 4-byte Spill
	cmpl	$39, %edx
	setl	%r9b
	orb	%r8b, %r9b
                                        ## kill: def $r11d killed $r11d killed $r11 def $r11
	andl	$-2147483648, %r11d             ## imm = 0x80000000
	shlq	$22, %r11
	movq	1112(%rsp), %rdx                ## 8-byte Reload
	testl	%edx, %edx
	setg	%r8b
	movq	160(%rsp), %r10                 ## 8-byte Reload
	addl	%r10d, %edx
	movl	%edx, 1216(%rsp)                ## 4-byte Spill
	cmpl	$40, %edx
	setl	%bl
	orb	%r8b, %bl
	movzbl	%bl, %ebx
	vpinsrb	$4, %ebx, %xmm1, %xmm1
                                        ## kill: def $r10d killed $r10d killed $r10 def $r10
	andl	$-2147483648, %r10d             ## imm = 0x80000000
	shlq	$24, %r10
	movq	1104(%rsp), %rdx                ## 8-byte Reload
	testl	%edx, %edx
	setg	%bl
	movq	464(%rsp), %r8                  ## 8-byte Reload
	addl	%r8d, %edx
	movl	%edx, 1236(%rsp)                ## 4-byte Spill
	cmpl	$7, %edx
	setl	%dl
	orb	%bl, %dl
	movzbl	%dl, %edx
	vpinsrb	$8, %edx, %xmm1, %xmm1
                                        ## kill: def $r8d killed $r8d killed $r8 def $r8
	andl	$-2147483648, %r8d              ## imm = 0x80000000
	shlq	$26, %r8
	movq	840(%rsp), %rbx                 ## 8-byte Reload
	cmpl	104(%rsp), %ebx                 ## 4-byte Folded Reload
	setge	%dl
	movq	472(%rsp), %r13                 ## 8-byte Reload
	addl	%r13d, %ebx
	movl	%ebx, 1228(%rsp)                ## 4-byte Spill
	cmpl	%ebx, 80(%rsp)                  ## 4-byte Folded Reload
	setg	%bl
	orb	%dl, %bl
	movzbl	%bl, %edx
	vpinsrb	$12, %edx, %xmm1, %xmm1
	movzbl	%sil, %edx
	vmovd	%edx, %xmm2
	vpinsrb	$4, 1272(%rsp), %xmm2, %xmm2    ## 4-byte Folded Reload
	vpinsrb	$8, 1268(%rsp), %xmm2, %xmm2    ## 4-byte Folded Reload
	movzbl	%cl, %ecx
	vpinsrb	$12, %ecx, %xmm2, %xmm2
	movzbl	230(%rsp), %ecx                 ## 1-byte Folded Reload
	vmovd	%ecx, %xmm3
	vpinsrb	$4, 1304(%rsp), %xmm3, %xmm3    ## 4-byte Folded Reload
	vpinsrb	$8, 1292(%rsp), %xmm3, %xmm3    ## 4-byte Folded Reload
	movzbl	%r15b, %ecx
	vpinsrb	$12, %ecx, %xmm3, %xmm3
	movzbl	%r12b, %ecx
	vmovd	%ecx, %xmm4
	movzbl	%dil, %ecx
	vpinsrb	$4, %ecx, %xmm4, %xmm4
	vpinsrb	$8, 1284(%rsp), %xmm4, %xmm4    ## 4-byte Folded Reload
	vpinsrb	$12, 1276(%rsp), %xmm4, %xmm4   ## 4-byte Folded Reload
	vmovd	520(%rsp), %xmm5                ## 4-byte Folded Reload
                                        ## xmm5 = mem[0],zero,zero,zero
	movzbl	231(%rsp), %ecx                 ## 1-byte Folded Reload
	vpinsrb	$4, %ecx, %xmm5, %xmm5
	vpinsrb	$8, 1324(%rsp), %xmm5, %xmm5    ## 4-byte Folded Reload
	vpinsrb	$12, 1320(%rsp), %xmm5, %xmm5   ## 4-byte Folded Reload
	movzbl	%r9b, %ecx
	vmovd	%ecx, %xmm6
	vpinsrb	$4, 1296(%rsp), %xmm6, %xmm6    ## 4-byte Folded Reload
	vpinsrb	$8, 1288(%rsp), %xmm6, %xmm6    ## 4-byte Folded Reload
	vpinsrb	$12, 1280(%rsp), %xmm6, %xmm6   ## 4-byte Folded Reload
	vpslld	$31, %xmm0, %xmm0
	vpmovsxdq	%xmm0, %ymm0
	vmovapd	LCPI0_24(%rip), %ymm7           ## ymm7 = [64,128,0,0]
	vblendvpd	%ymm0, LCPI0_23(%rip), %ymm7, %ymm0
	vmovd	1316(%rsp), %xmm7               ## 4-byte Folded Reload
                                        ## xmm7 = mem[0],zero,zero,zero
	vpslld	$31, %xmm1, %xmm1
	vpmovsxdq	%xmm1, %ymm1
	vmovapd	LCPI0_25(%rip), %xmm8           ## xmm8 = [32768,0,0,0]
	vblendvpd	%ymm1, LCPI0_26(%rip), %ymm8, %ymm1
	vpinsrb	$4, 1300(%rsp), %xmm7, %xmm7    ## 4-byte Folded Reload
	vpslld	$31, %xmm2, %xmm2
	vpmovsxdq	%xmm2, %ymm2
	vmovapd	LCPI0_28(%rip), %ymm8           ## ymm8 = [0,1024,2048,0]
	vblendvpd	%ymm2, LCPI0_27(%rip), %ymm8, %ymm2
	vpinsrb	$8, 1308(%rsp), %xmm7, %xmm7    ## 4-byte Folded Reload
	vpslld	$31, %xmm3, %xmm3
	vpmovsxdq	%xmm3, %ymm3
	vmovapd	LCPI0_30(%rip), %ymm8           ## ymm8 = [0,16,32,0]
	vblendvpd	%ymm3, LCPI0_29(%rip), %ymm8, %ymm3
	vpinsrb	$12, 1312(%rsp), %xmm7, %xmm7   ## 4-byte Folded Reload
	vpslld	$31, %xmm4, %xmm4
	vpmovsxdq	%xmm4, %ymm4
	vmovapd	LCPI0_32(%rip), %ymm8           ## ymm8 = [0,0,256,512]
	vblendvpd	%ymm4, LCPI0_31(%rip), %ymm8, %ymm4
	vpslld	$31, %xmm6, %xmm6
	vpmovsxdq	%xmm6, %ymm6
	vmovapd	LCPI0_35(%rip), %xmm8           ## xmm8 = [4503599627370496,0]
	vmovapd	LCPI0_36(%rip), %ymm9           ## ymm9 = [0,4096,8192,16384]
	vblendvpd	%ymm6, %ymm8, %ymm9, %ymm6
	vpslld	$31, %xmm5, %xmm5
	vpmovsxdq	%xmm5, %ymm5
	vmovapd	LCPI0_34(%rip), %ymm8           ## ymm8 = [2,0,4,8]
	vblendvpd	%ymm5, LCPI0_33(%rip), %ymm8, %ymm5
	vorpd	%ymm1, %ymm0, %ymm0
	vorpd	%ymm0, %ymm5, %ymm0
	vpslld	$31, %xmm7, %xmm1
	vpsrad	$31, %xmm1, %xmm1
	vpmovsxdq	%xmm1, %ymm1
	vpandn	LCPI0_37(%rip), %ymm1, %ymm1
	vorpd	%ymm0, %ymm2, %ymm0
	vorpd	%ymm1, %ymm4, %ymm1
	vorpd	%ymm1, %ymm6, %ymm1
	vorpd	%ymm1, %ymm3, %ymm1
	vorpd	%ymm1, %ymm0, %ymm0
                                        ## kill: def $r13d killed $r13d killed $r13 def $r13
	andl	$-2147483648, %r13d             ## imm = 0x80000000
	shlq	$28, %r13
	movq	832(%rsp), %r12                 ## 8-byte Reload
                                        ## kill: def $r12d killed $r12d killed $r12 def $r12
	andl	$-2147483648, %r12d             ## imm = 0x80000000
	shlq	$30, %r12
	movq	320(%rsp), %rdx                 ## 8-byte Reload
	cmpl	232(%rsp), %edx                 ## 4-byte Folded Reload
	setg	%cl
	movq	208(%rsp), %rsi                 ## 8-byte Reload
	leal	(%rsi,%rdx), %edi
	cmpl	%edi, 824(%rsp)                 ## 4-byte Folded Reload
	vextractf128	$1, %ymm0, %xmm1
	vorpd	%xmm1, %xmm0, %xmm0
	vpermilps	$238, %xmm0, %xmm1      ## xmm1 = xmm0[2,3,2,3]
	vorpd	%xmm1, %xmm0, %xmm0
	setg	%dl
	vmovq	%xmm0, %r15
	orq	536(%rsp), %r15                 ## 8-byte Folded Reload
	orb	%cl, %dl
	movzbl	%dl, %ecx
	shlq	$62, %rcx
	orq	%rcx, %r15
	orq	528(%rsp), %rax                 ## 8-byte Folded Reload
	orq	1632(%rsp), %rax                ## 8-byte Folded Reload
	orq	1624(%rsp), %rax                ## 8-byte Folded Reload
	orq	1616(%rsp), %rax                ## 8-byte Folded Reload
	orq	1608(%rsp), %rax                ## 8-byte Folded Reload
	orq	1600(%rsp), %rax                ## 8-byte Folded Reload
	orq	1592(%rsp), %rax                ## 8-byte Folded Reload
	orq	1584(%rsp), %rax                ## 8-byte Folded Reload
	orq	1576(%rsp), %rax                ## 8-byte Folded Reload
	orq	1568(%rsp), %rax                ## 8-byte Folded Reload
	orq	1560(%rsp), %rax                ## 8-byte Folded Reload
	orq	1552(%rsp), %rax                ## 8-byte Folded Reload
	orq	328(%rsp), %rax                 ## 8-byte Folded Reload
	orq	1544(%rsp), %rax                ## 8-byte Folded Reload
	orq	1536(%rsp), %rax                ## 8-byte Folded Reload
	orq	1528(%rsp), %rax                ## 8-byte Folded Reload
	orq	1520(%rsp), %rax                ## 8-byte Folded Reload
	orq	1512(%rsp), %rax                ## 8-byte Folded Reload
	orq	1504(%rsp), %rax                ## 8-byte Folded Reload
	orq	1496(%rsp), %rax                ## 8-byte Folded Reload
	orq	1488(%rsp), %rax                ## 8-byte Folded Reload
	orq	%r14, %rax
	orq	%r11, %rax
	orq	%r10, %rax
	orq	%r8, %rax
	orq	%r13, %rax
	orq	%r12, %rax
	movabsq	$-9223372036854775808, %rcx     ## imm = 0x8000000000000000
	orq	%rcx, %rax
	orq	%r15, %rax
	tzcntq	%rax, %rax
	cmpl	$62, %eax
	jbe	LBB0_285
## %bb.93:                              ## %no_errors_bb68
	shrq	$31, %rsi
	movq	%rsi, 536(%rsp)                 ## 8-byte Spill
	movq	288(%rsp), %rcx                 ## 8-byte Reload
	testl	%ecx, %ecx
	setg	%al
	movq	448(%rsp), %rdx                 ## 8-byte Reload
	addl	%edx, %ecx
	movl	%ecx, 392(%rsp)                 ## 4-byte Spill
	cmpl	$39, %ecx
	setl	%bl
	orb	%al, %bl
	shrq	$29, %rdx
	andl	$-4, %edx
	movq	%rdx, 528(%rsp)                 ## 8-byte Spill
	movq	440(%rsp), %rcx                 ## 8-byte Reload
	testl	%ecx, %ecx
	setg	%al
	movq	456(%rsp), %rdx                 ## 8-byte Reload
	addl	%edx, %ecx
	movl	%ecx, 388(%rsp)                 ## 4-byte Spill
	cmpl	%ecx, 80(%rsp)                  ## 4-byte Folded Reload
	setg	%cl
	orb	%al, %cl
	movb	%cl, 420(%rsp)                  ## 1-byte Spill
	shrq	$27, %rdx
	andl	$-16, %edx
	movq	%rdx, 520(%rsp)                 ## 8-byte Spill
	movq	1128(%rsp), %rcx                ## 8-byte Reload
	testl	%ecx, %ecx
	setg	%al
	movq	480(%rsp), %rdx                 ## 8-byte Reload
	addl	%edx, %ecx
	movl	%ecx, 384(%rsp)                 ## 4-byte Spill
	cmpl	%ecx, 504(%rsp)                 ## 4-byte Folded Reload
	setg	%cl
	orb	%al, %cl
	movb	%cl, 416(%rsp)                  ## 1-byte Spill
	shrq	$25, %rdx
	andl	$-64, %edx
	movq	%rdx, 824(%rsp)                 ## 8-byte Spill
	movq	792(%rsp), %rcx                 ## 8-byte Reload
	testl	%ecx, %ecx
	setg	%al
	movq	768(%rsp), %rdx                 ## 8-byte Reload
	addl	%edx, %ecx
	movl	%ecx, 380(%rsp)                 ## 4-byte Spill
	cmpl	$32, %ecx
	setl	%cl
	orb	%al, %cl
	movb	%cl, 412(%rsp)                  ## 1-byte Spill
	shrq	$23, %rdx
	andl	$256, %edx                      ## imm = 0x100
	movq	%rdx, 1040(%rsp)                ## 8-byte Spill
	movq	784(%rsp), %rcx                 ## 8-byte Reload
	testl	%ecx, %ecx
	setg	%al
	movq	752(%rsp), %rdx                 ## 8-byte Reload
	addl	%edx, %ecx
	movl	%ecx, 376(%rsp)                 ## 4-byte Spill
	cmpl	$4, %ecx
	setl	%cl
	orb	%al, %cl
	movb	%cl, 408(%rsp)                  ## 1-byte Spill
	shrq	$21, %rdx
	andl	$1024, %edx                     ## imm = 0x400
	movq	%rdx, 1032(%rsp)                ## 8-byte Spill
	movq	736(%rsp), %rcx                 ## 8-byte Reload
	testl	%ecx, %ecx
	setg	%al
	movq	712(%rsp), %rdx                 ## 8-byte Reload
	addl	%edx, %ecx
	movl	%ecx, 372(%rsp)                 ## 4-byte Spill
	cmpl	$32, %ecx
	setl	%cl
	orb	%al, %cl
	movb	%cl, 404(%rsp)                  ## 1-byte Spill
	shrq	$19, %rdx
	andl	$4096, %edx                     ## imm = 0x1000
	movq	%rdx, 1024(%rsp)                ## 8-byte Spill
	movq	728(%rsp), %rcx                 ## 8-byte Reload
	testl	%ecx, %ecx
	setg	%al
	movq	696(%rsp), %rdx                 ## 8-byte Reload
	addl	%edx, %ecx
	movl	%ecx, 368(%rsp)                 ## 4-byte Spill
	cmpl	$32, %ecx
	setl	%cl
	orb	%al, %cl
	movb	%cl, 400(%rsp)                  ## 1-byte Spill
	shrq	$17, %rdx
	andl	$16384, %edx                    ## imm = 0x4000
	movq	%rdx, 1016(%rsp)                ## 8-byte Spill
	movq	704(%rsp), %rcx                 ## 8-byte Reload
	testl	%ecx, %ecx
	setg	%al
	movq	680(%rsp), %rdx                 ## 8-byte Reload
	addl	%edx, %ecx
	movl	%ecx, 364(%rsp)                 ## 4-byte Spill
	cmpl	$4, %ecx
	setl	%cl
	orb	%al, %cl
	movb	%cl, 396(%rsp)                  ## 1-byte Spill
	shrq	$15, %rdx
	andl	$65536, %edx                    ## imm = 0x10000
	movq	%rdx, 1008(%rsp)                ## 8-byte Spill
	movq	264(%rsp), %rax                 ## 8-byte Reload
	shrq	$13, %rax
	andl	$262144, %eax                   ## imm = 0x40000
	movq	%rax, 1000(%rsp)                ## 8-byte Spill
	movq	648(%rsp), %rax                 ## 8-byte Reload
	shrq	$11, %rax
	andl	$1048576, %eax                  ## imm = 0x100000
	movq	%rax, 992(%rsp)                 ## 8-byte Spill
	movq	600(%rsp), %rax                 ## 8-byte Reload
	shrq	$9, %rax
	andl	$4194304, %eax                  ## imm = 0x400000
	movq	%rax, 984(%rsp)                 ## 8-byte Spill
	movq	256(%rsp), %rax                 ## 8-byte Reload
	shrq	$7, %rax
	andl	$16777216, %eax                 ## imm = 0x1000000
	movq	%rax, 976(%rsp)                 ## 8-byte Spill
	movq	248(%rsp), %rax                 ## 8-byte Reload
	shrq	$5, %rax
	andl	$67108864, %eax                 ## imm = 0x4000000
	movq	%rax, 968(%rsp)                 ## 8-byte Spill
	movq	48(%rsp), %r13                  ## 8-byte Reload
	shrq	$3, %r13
	andl	$268435456, %r13d               ## imm = 0x10000000
	vmovd	%xmm15, %eax
	movl	%eax, 328(%rsp)                 ## 4-byte Spill
	movl	%eax, %r12d
	shrl	%r12d
	andl	$1073741824, %r12d              ## imm = 0x40000000
	movq	296(%rsp), %r15                 ## 8-byte Reload
                                        ## kill: def $r15d killed $r15d killed $r15 def $r15
	andl	$-2147483648, %r15d             ## imm = 0x80000000
	addq	%r15, %r15
	movq	1096(%rsp), %rcx                ## 8-byte Reload
	movq	1072(%rsp), %r11                ## 8-byte Reload
	leal	(%r11,%rcx), %eax
	xorl	%esi, %esi
	movl	%eax, 360(%rsp)                 ## 4-byte Spill
	cmpl	$24, %eax
	movabsq	$8589934592, %rax               ## imm = 0x200000000
	cmovlq	%rax, %rsi
	testl	%ecx, %ecx
	cmovgq	%rax, %rsi
                                        ## kill: def $r11d killed $r11d killed $r11 def $r11
	andl	$-2147483648, %r11d             ## imm = 0x80000000
	shlq	$3, %r11
	movq	1088(%rsp), %rcx                ## 8-byte Reload
	testl	%ecx, %ecx
	setg	%al
	movq	1064(%rsp), %r14                ## 8-byte Reload
	addl	%r14d, %ecx
	movl	%ecx, 356(%rsp)                 ## 4-byte Spill
	cmpl	$39, %ecx
	setl	%cl
	orb	%al, %cl
	movzbl	%cl, %eax
	shlq	$35, %rax
	movq	%rax, 960(%rsp)                 ## 8-byte Spill
                                        ## kill: def $r14d killed $r14d killed $r14 def $r14
	andl	$-2147483648, %r14d             ## imm = 0x80000000
	shlq	$5, %r14
	movq	1080(%rsp), %rcx                ## 8-byte Reload
	testl	%ecx, %ecx
	setg	%al
	movq	1056(%rsp), %rdx                ## 8-byte Reload
	addl	%edx, %ecx
	movl	%ecx, 352(%rsp)                 ## 4-byte Spill
	cmpl	$4, %ecx
	setl	%cl
	orb	%al, %cl
	movzbl	%cl, %eax
	shlq	$37, %rax
	movq	%rax, 952(%rsp)                 ## 8-byte Spill
                                        ## kill: def $edx killed $edx killed $rdx def $rdx
	andl	$-2147483648, %edx              ## imm = 0x80000000
	shlq	$7, %rdx
	xorl	%r10d, %r10d
	cmpl	$1, 920(%rsp)                   ## 4-byte Folded Reload
	sete	%r10b
	xorl	%r9d, %r9d
	cmpl	$0, 808(%rsp)                   ## 4-byte Folded Reload
	sete	%r9b
	xorl	%r8d, %r8d
	cmpl	$32, 800(%rsp)                  ## 4-byte Folded Reload
	vmovd	656(%rsp), %xmm0                ## 4-byte Folded Reload
                                        ## xmm0 = mem[0],zero,zero,zero
	vpinsrd	$1, 632(%rsp), %xmm0, %xmm0     ## 4-byte Folded Reload
	vpinsrd	$2, 608(%rsp), %xmm0, %xmm0     ## 4-byte Folded Reload
	sete	%r8b
	vpinsrd	$3, 584(%rsp), %xmm0, %xmm0     ## 4-byte Folded Reload
	xorl	%edi, %edi
	cmpl	$0, 760(%rsp)                   ## 4-byte Folded Reload
	vmovd	40(%rsp), %xmm1                 ## 4-byte Folded Reload
                                        ## xmm1 = mem[0],zero,zero,zero
	vpinsrd	$1, 900(%rsp), %xmm1, %xmm1     ## 4-byte Folded Reload
	vpinsrd	$2, 664(%rsp), %xmm1, %xmm1     ## 4-byte Folded Reload
	vpinsrd	$3, 640(%rsp), %xmm1, %xmm1     ## 4-byte Folded Reload
	sete	%dil
	vmovd	720(%rsp), %xmm2                ## 4-byte Folded Reload
                                        ## xmm2 = mem[0],zero,zero,zero
	vpinsrd	$1, 688(%rsp), %xmm2, %xmm2     ## 4-byte Folded Reload
	vpinsrd	$2, 908(%rsp), %xmm2, %xmm2     ## 4-byte Folded Reload
	vpinsrd	$3, 672(%rsp), %xmm2, %xmm2     ## 4-byte Folded Reload
	vmovd	544(%rsp), %xmm3                ## 4-byte Folded Reload
                                        ## xmm3 = mem[0],zero,zero,zero
	vpblendd	$2, %xmm10, %xmm3, %xmm3        ## xmm3 = xmm3[0],xmm10[1],xmm3[2,3]
	vpinsrd	$2, 776(%rsp), %xmm3, %xmm3     ## 4-byte Folded Reload
	vinserti128	$1, %xmm0, %ymm1, %ymm0
	vpinsrd	$3, 744(%rsp), %xmm3, %xmm1     ## 4-byte Folded Reload
	vinserti128	$1, %xmm2, %ymm1, %ymm1
	vpcmpeqd	LCPI0_41(%rip), %ymm1, %ymm1
	vpcmpeqd	LCPI0_42(%rip), %ymm0, %ymm0
	vpackssdw	%ymm0, %ymm1, %ymm0
	vextracti128	$1, %ymm0, %xmm1
	vpacksswb	%xmm1, %xmm0, %xmm0
	vpshufd	$216, %xmm0, %xmm0              ## xmm0 = xmm0[0,2,1,3]
	vpmovmskb	%xmm0, %ecx
	movl	%ecx, %eax
	shrl	$8, %eax
	andl	$1, %eax
	vmovd	%eax, %xmm0
	movl	%ecx, %eax
	shrl	$9, %eax
	andl	$1, %eax
	vpinsrb	$4, %eax, %xmm0, %xmm0
	movl	%ecx, %eax
	shrl	$10, %eax
	vpinsrb	$8, %eax, %xmm0, %xmm0
	movl	%ecx, %eax
	shrl	$11, %eax
	vpinsrb	$12, %eax, %xmm0, %xmm8
	movl	%ecx, %eax
	andl	$2, %eax
	shrl	%eax
	vmovd	%ecx, %xmm0
	vpinsrb	$4, %eax, %xmm0, %xmm0
	movl	%ecx, %eax
	shrl	$2, %eax
	vpinsrb	$8, %eax, %xmm0, %xmm0
	movl	%ecx, %eax
	shrl	$3, %eax
	vpinsrb	$12, %eax, %xmm0, %xmm9
	movl	%ecx, %eax
	shrl	$12, %eax
	andl	$1, %eax
	vmovd	%eax, %xmm0
	movl	%ecx, %eax
	shrl	$13, %eax
	andl	$1, %eax
	vpinsrb	$4, %eax, %xmm0, %xmm0
	movl	%ecx, %eax
	shrl	$14, %eax
	vpinsrb	$8, %eax, %xmm0, %xmm0
	movl	%ecx, %eax
	shrl	$15, %eax
	vpinsrb	$12, %eax, %xmm0, %xmm10
	movl	%ecx, %eax
	andl	$16, %eax
	shrl	$4, %eax
	vmovd	%eax, %xmm0
	movl	%ecx, %eax
	andl	$32, %eax
	shrl	$5, %eax
	vpinsrb	$4, %eax, %xmm0, %xmm0
	movl	%ecx, %eax
	shrl	$6, %eax
	vpinsrb	$8, %eax, %xmm0, %xmm0
	shrl	$7, %ecx
	vpinsrb	$12, %ecx, %xmm0, %xmm11
	movzbl	408(%rsp), %eax                 ## 1-byte Folded Reload
	vmovd	%eax, %xmm0
	movzbl	404(%rsp), %eax                 ## 1-byte Folded Reload
	vpinsrb	$4, %eax, %xmm0, %xmm0
	movzbl	400(%rsp), %eax                 ## 1-byte Folded Reload
	vpinsrb	$8, %eax, %xmm0, %xmm0
	movzbl	396(%rsp), %eax                 ## 1-byte Folded Reload
	vpinsrb	$12, %eax, %xmm0, %xmm12
	vmovd	892(%rsp), %xmm0                ## 4-byte Folded Reload
                                        ## xmm0 = mem[0],zero,zero,zero
	vpinsrd	$1, 576(%rsp), %xmm0, %xmm0     ## 4-byte Folded Reload
	vpinsrd	$2, 552(%rsp), %xmm0, %xmm0     ## 4-byte Folded Reload
	vpinsrd	$3, 888(%rsp), %xmm0, %xmm0     ## 4-byte Folded Reload
	vpcmpeqd	LCPI0_43(%rip), %xmm0, %xmm0
	vmovmskps	%xmm0, %eax
	movl	%eax, %ecx
	andb	$2, %cl
	shrb	%cl
	vmovd	%eax, %xmm0
	movzbl	%cl, %ecx
	vpinsrb	$4, %ecx, %xmm0, %xmm0
	movl	%eax, %ecx
	andb	$4, %cl
	shrb	$2, %cl
	movzbl	%cl, %ecx
	vpinsrb	$8, %ecx, %xmm0, %xmm0
                                        ## kill: def $al killed $al killed $eax
	shrb	$3, %al
	movzbl	%al, %eax
	vpinsrb	$12, %eax, %xmm0, %xmm13
	movzbl	%bl, %eax
	vmovd	%eax, %xmm0
	movzbl	420(%rsp), %eax                 ## 1-byte Folded Reload
	vpinsrb	$4, %eax, %xmm0, %xmm0
	movzbl	416(%rsp), %eax                 ## 1-byte Folded Reload
	vpinsrb	$8, %eax, %xmm0, %xmm0
	movzbl	412(%rsp), %eax                 ## 1-byte Folded Reload
	vpinsrb	$12, %eax, %xmm0, %xmm7
	vmovd	%r9d, %xmm0
	vpinsrb	$4, %r8d, %xmm0, %xmm0
	vpinsrb	$8, %r10d, %xmm0, %xmm0
	vpinsrb	$12, %edi, %xmm0, %xmm0
	vmovd	616(%rsp), %xmm6                ## 4-byte Folded Reload
                                        ## xmm6 = mem[0],zero,zero,zero
	vpinsrd	$1, 592(%rsp), %xmm6, %xmm6     ## 4-byte Folded Reload
	vpinsrd	$2, 568(%rsp), %xmm6, %xmm6     ## 4-byte Folded Reload
	vpinsrd	$3, 560(%rsp), %xmm6, %xmm6     ## 4-byte Folded Reload
	movl	500(%rsp), %r8d                 ## 4-byte Reload
	vmovd	%r8d, %xmm1
	movl	496(%rsp), %ebx                 ## 4-byte Reload
	vpinsrd	$1, %ebx, %xmm1, %xmm1
	movl	492(%rsp), %edi                 ## 4-byte Reload
	vpinsrd	$2, %edi, %xmm1, %xmm1
	vpinsrd	$3, 624(%rsp), %xmm1, %xmm1     ## 4-byte Folded Reload
	vmovd	248(%rsp), %xmm2                ## 4-byte Folded Reload
                                        ## xmm2 = mem[0],zero,zero,zero
	vpinsrd	$1, 48(%rsp), %xmm2, %xmm2      ## 4-byte Folded Reload
	vpinsrd	$2, 328(%rsp), %xmm2, %xmm2     ## 4-byte Folded Reload
	vpinsrd	$3, 296(%rsp), %xmm2, %xmm2     ## 4-byte Folded Reload
	vmovd	264(%rsp), %xmm3                ## 4-byte Folded Reload
                                        ## xmm3 = mem[0],zero,zero,zero
	movq	648(%rsp), %rax                 ## 8-byte Reload
	movq	%rax, %r9
	vpinsrd	$1, %eax, %xmm3, %xmm3
	movq	600(%rsp), %rax                 ## 8-byte Reload
	movq	%rax, %r10
	vpinsrd	$2, %eax, %xmm3, %xmm3
	vpinsrd	$3, 256(%rsp), %xmm3, %xmm3     ## 4-byte Folded Reload
	vinserti128	$1, %xmm6, %ymm1, %ymm1
	vinserti128	$1, %xmm2, %ymm3, %ymm2
	vpxor	%xmm3, %xmm3, %xmm3
	vpcmpgtd	%ymm3, %ymm1, %ymm3
	vpaddd	%ymm1, %ymm2, %ymm14
	vmovdqa	LCPI0_38(%rip), %ymm1           ## ymm1 = [8,4,8,40,7,4,24,4]
	vpcmpgtd	%ymm14, %ymm1, %ymm1
	vpor	%ymm1, %ymm3, %ymm1
	vextracti128	$1, %ymm1, %xmm2
	vpmovzxdq	%xmm2, %ymm2            ## ymm2 = xmm2[0],zero,xmm2[1],zero,xmm2[2],zero,xmm2[3],zero
	vpand	LCPI0_39(%rip), %ymm2, %ymm2
	vpmovzxdq	%xmm1, %ymm1            ## ymm1 = xmm1[0],zero,xmm1[1],zero,xmm1[2],zero,xmm1[3],zero
	vpand	LCPI0_40(%rip), %ymm1, %ymm1
	vpor	%ymm2, %ymm1, %ymm1
	vextracti128	$1, %ymm1, %xmm2
	vpor	%xmm2, %xmm1, %xmm1
	vpshufd	$238, %xmm1, %xmm2              ## xmm2 = xmm1[2,3,2,3]
	vpor	%xmm2, %xmm1, %xmm1
	vmovq	%xmm1, %rax
	orq	%rsi, %rax
	orq	960(%rsp), %rax                 ## 8-byte Folded Reload
	orq	952(%rsp), %rax                 ## 8-byte Folded Reload
	orq	536(%rsp), %rax                 ## 8-byte Folded Reload
	orq	528(%rsp), %rax                 ## 8-byte Folded Reload
	orq	520(%rsp), %rax                 ## 8-byte Folded Reload
	orq	824(%rsp), %rax                 ## 8-byte Folded Reload
	orq	1040(%rsp), %rax                ## 8-byte Folded Reload
	orq	1032(%rsp), %rax                ## 8-byte Folded Reload
	orq	1024(%rsp), %rax                ## 8-byte Folded Reload
	orq	1016(%rsp), %rax                ## 8-byte Folded Reload
	orq	1008(%rsp), %rax                ## 8-byte Folded Reload
	orq	1000(%rsp), %rax                ## 8-byte Folded Reload
	orq	992(%rsp), %rax                 ## 8-byte Folded Reload
	orq	984(%rsp), %rax                 ## 8-byte Folded Reload
	orq	976(%rsp), %rax                 ## 8-byte Folded Reload
	orq	968(%rsp), %rax                 ## 8-byte Folded Reload
	orq	%r13, %rax
	orq	%r12, %rax
	orq	%r15, %rax
	vpslld	$31, %xmm8, %xmm1
	vpsrad	$31, %xmm1, %xmm1
	vpmovsxdq	%xmm1, %ymm1
	vpandn	LCPI0_44(%rip), %ymm1, %ymm1
	vpslld	$31, %xmm9, %xmm2
	vpsrad	$31, %xmm2, %xmm2
	vpmovsxdq	%xmm2, %ymm2
	vpandn	LCPI0_45(%rip), %ymm2, %ymm2
	vpslld	$31, %xmm10, %xmm3
	vpsrad	$31, %xmm3, %xmm3
	vpmovsxdq	%xmm3, %ymm3
	vpandn	LCPI0_46(%rip), %ymm3, %ymm3
	orq	%r11, %rax
	vpslld	$31, %xmm11, %xmm4
	vpsrad	$31, %xmm4, %xmm4
	vpmovsxdq	%xmm4, %ymm4
	vpandn	LCPI0_47(%rip), %ymm4, %ymm4
	vpslld	$31, %xmm12, %xmm5
	vpsrad	$31, %xmm5, %xmm5
	vpmovzxdq	%xmm5, %ymm5            ## ymm5 = xmm5[0],zero,xmm5[1],zero,xmm5[2],zero,xmm5[3],zero
	vpand	LCPI0_48(%rip), %ymm5, %ymm5
	vpslld	$31, %xmm13, %xmm6
	vpsrad	$31, %xmm6, %xmm6
	vpmovsxdq	%xmm6, %ymm6
	vpandn	LCPI0_49(%rip), %ymm6, %ymm6
	vpor	%ymm5, %ymm1, %ymm1
	vpor	%ymm6, %ymm2, %ymm2
	vpor	%ymm1, %ymm2, %ymm1
	vpslld	$31, %xmm7, %xmm2
	vpsrad	$31, %xmm2, %xmm2
	vpmovzxdq	%xmm2, %ymm2            ## ymm2 = xmm2[0],zero,xmm2[1],zero,xmm2[2],zero,xmm2[3],zero
	vpand	LCPI0_50(%rip), %ymm2, %ymm2
	vpor	%ymm2, %ymm4, %ymm2
	vpslld	$31, %xmm0, %xmm0
	vpmovsxdq	%xmm0, %ymm0
	vmovapd	LCPI0_0(%rip), %xmm4            ## xmm4 = [9223372036854775808,0]
	vmovapd	LCPI0_51(%rip), %ymm5           ## ymm5 = [9223373136366403584,2199023255552,549755813888,8796093022208]
	vblendvpd	%ymm0, %ymm4, %ymm5, %ymm0
	vorpd	%ymm3, %ymm0, %ymm0
	vorpd	%ymm2, %ymm0, %ymm0
	vorpd	%ymm1, %ymm0, %ymm0
	vextractf128	$1, %ymm0, %xmm1
	vorpd	%xmm1, %xmm0, %xmm0
	vpermilps	$238, %xmm0, %xmm1      ## xmm1 = xmm0[2,3,2,3]
	vorpd	%xmm1, %xmm0, %xmm0
	vmovq	%xmm0, %rcx
	orq	%rcx, %rax
	orq	%r14, %rdx
	orq	%rax, %rdx
	xorl	%eax, %eax
	tzcntq	%rdx, %rax
	cmpl	$62, %eax
	jbe	LBB0_287
## %bb.94:                              ## %no_errors_bb132
	xorl	%eax, %eax
	cmpl	$4, 1056(%rsp)                  ## 4-byte Folded Reload
	setne	%al
	vmovd	%ebx, %xmm0
	vpinsrd	$1, %r9d, %xmm0, %xmm0
	vpinsrd	$2, %edi, %xmm0, %xmm0
	xorl	%ecx, %ecx
	vpinsrd	$3, %r10d, %xmm0, %xmm0
	vmovd	680(%rsp), %xmm1                ## 4-byte Folded Reload
                                        ## xmm1 = mem[0],zero,zero,zero
	vpinsrd	$1, %r8d, %xmm1, %xmm1
	vpinsrd	$2, 264(%rsp), %xmm1, %xmm1     ## 4-byte Folded Reload
	cmpl	$39, 1064(%rsp)                 ## 4-byte Folded Reload
	vpinsrd	$3, 904(%rsp), %xmm1, %xmm1     ## 4-byte Folded Reload
	vmovd	924(%rsp), %xmm2                ## 4-byte Folded Reload
                                        ## xmm2 = mem[0],zero,zero,zero
	vpinsrd	$1, 792(%rsp), %xmm2, %xmm2     ## 4-byte Folded Reload
	vpinsrd	$2, 768(%rsp), %xmm2, %xmm2     ## 4-byte Folded Reload
	setne	%cl
	vpinsrd	$3, 916(%rsp), %xmm2, %xmm2     ## 4-byte Folded Reload
	vmovd	336(%rsp), %xmm3                ## 4-byte Folded Reload
                                        ## xmm3 = mem[0],zero,zero,zero
	vpinsrd	$1, 936(%rsp), %xmm3, %xmm3     ## 4-byte Folded Reload
	vpinsrd	$2, 932(%rsp), %xmm3, %xmm3     ## 4-byte Folded Reload
	xorl	%edx, %edx
	vpinsrd	$3, 928(%rsp), %xmm3, %xmm3     ## 4-byte Folded Reload
	vmovd	1096(%rsp), %xmm4               ## 4-byte Folded Reload
                                        ## xmm4 = mem[0],zero,zero,zero
	vpinsrd	$1, 1072(%rsp), %xmm4, %xmm4    ## 4-byte Folded Reload
	vpinsrd	$2, 884(%rsp), %xmm4, %xmm4     ## 4-byte Folded Reload
	vpinsrd	$3, 1088(%rsp), %xmm4, %xmm4    ## 4-byte Folded Reload
	movl	328(%rsp), %ebx                 ## 4-byte Reload
	vmovd	%ebx, %xmm5
	vpblendd	$2, %xmm15, %xmm5, %xmm5        ## xmm5 = xmm5[0],xmm15[1],xmm5[2,3]
	vpinsrd	$2, 560(%rsp), %xmm5, %xmm5     ## 4-byte Folded Reload
	vpinsrd	$3, 296(%rsp), %xmm5, %xmm5     ## 4-byte Folded Reload
	cmpl	$0, 344(%rsp)                   ## 4-byte Folded Reload
	vinserti128	$1, %xmm4, %ymm5, %ymm4
	vpcmpeqd	LCPI0_64(%rip), %ymm4, %ymm4
	setne	%dl
	vpmovsxdq	%xmm4, %ymm5
	vextracti128	$1, %ymm4, %xmm4
	vpmovsxdq	%xmm4, %ymm4
	vpandn	LCPI0_65(%rip), %ymm4, %ymm4
	vpandn	LCPI0_66(%rip), %ymm5, %ymm5
	vpor	%ymm4, %ymm5, %ymm4
	shlq	$41, %rcx
	xorl	%esi, %esi
	cmpl	$0, 1080(%rsp)                  ## 4-byte Folded Reload
	setne	%sil
	shlq	$43, %rax
	orq	%rdx, %rax
	vextracti128	$1, %ymm4, %xmm5
	vpor	%xmm5, %xmm4, %xmm4
	vpshufd	$238, %xmm4, %xmm5              ## xmm5 = xmm4[2,3,2,3]
	vpor	%xmm5, %xmm4, %xmm4
	vmovq	%xmm4, %rdx
	orq	%rcx, %rdx
	vmovd	248(%rsp), %xmm4                ## 4-byte Folded Reload
                                        ## xmm4 = mem[0],zero,zero,zero
	vpinsrd	$1, 592(%rsp), %xmm4, %xmm4     ## 4-byte Folded Reload
	vpinsrd	$2, 48(%rsp), %xmm4, %xmm4      ## 4-byte Folded Reload
	vpinsrd	$3, 568(%rsp), %xmm4, %xmm4     ## 4-byte Folded Reload
	vinserti128	$1, %xmm0, %ymm1, %ymm0
	vmovd	896(%rsp), %xmm1                ## 4-byte Folded Reload
                                        ## xmm1 = mem[0],zero,zero,zero
	vpinsrd	$1, 624(%rsp), %xmm1, %xmm1     ## 4-byte Folded Reload
	vpinsrd	$2, 256(%rsp), %xmm1, %xmm1     ## 4-byte Folded Reload
	vpinsrd	$3, 616(%rsp), %xmm1, %xmm1     ## 4-byte Folded Reload
	vinserti128	$1, %xmm2, %ymm3, %ymm2
	vmovd	912(%rsp), %xmm3                ## 4-byte Folded Reload
                                        ## xmm3 = mem[0],zero,zero,zero
	vpinsrd	$1, 728(%rsp), %xmm3, %xmm3     ## 4-byte Folded Reload
	vpinsrd	$2, 696(%rsp), %xmm3, %xmm3     ## 4-byte Folded Reload
	vpinsrd	$3, 704(%rsp), %xmm3, %xmm3     ## 4-byte Folded Reload
	vinserti128	$1, %xmm4, %ymm1, %ymm1
	vmovd	784(%rsp), %xmm4                ## 4-byte Folded Reload
                                        ## xmm4 = mem[0],zero,zero,zero
	vpinsrd	$1, 752(%rsp), %xmm4, %xmm4     ## 4-byte Folded Reload
	vpinsrd	$2, 736(%rsp), %xmm4, %xmm4     ## 4-byte Folded Reload
	vpinsrd	$3, 712(%rsp), %xmm4, %xmm4     ## 4-byte Folded Reload
	vinserti128	$1, %xmm3, %ymm4, %ymm3
	vpcmpeqd	LCPI0_52(%rip), %ymm3, %ymm3
	vpmovsxdq	%xmm3, %ymm4
	vpcmpeqd	LCPI0_53(%rip), %ymm1, %ymm1
	vpcmpeqd	LCPI0_54(%rip), %ymm2, %ymm2
	vpmovsxdq	%xmm1, %ymm5
	vpmovsxdq	%xmm2, %ymm6
	vpcmpeqd	LCPI0_55(%rip), %ymm0, %ymm0
	vpmovsxdq	%xmm0, %ymm7
	vextracti128	$1, %ymm3, %xmm3
	vpmovsxdq	%xmm3, %ymm3
	vextracti128	$1, %ymm1, %xmm1
	vextracti128	$1, %ymm2, %xmm2
	vpmovsxdq	%xmm2, %ymm2
	vextracti128	$1, %ymm0, %xmm0
	vpmovsxdq	%xmm0, %ymm0
	vpandn	LCPI0_56(%rip), %ymm0, %ymm0
	vpandn	LCPI0_57(%rip), %ymm2, %ymm2
	vpmovsxdq	%xmm1, %ymm1
	vpor	%ymm0, %ymm2, %ymm0
	vpandn	LCPI0_58(%rip), %ymm1, %ymm1
	vpandn	LCPI0_59(%rip), %ymm3, %ymm2
	vpor	%ymm1, %ymm2, %ymm1
	vpor	%ymm1, %ymm0, %ymm0
	vpandn	LCPI0_60(%rip), %ymm7, %ymm1
	vpandn	LCPI0_61(%rip), %ymm6, %ymm2
	vpandn	LCPI0_62(%rip), %ymm5, %ymm3
	vpandn	LCPI0_63(%rip), %ymm4, %ymm4
	vpor	%ymm1, %ymm2, %ymm1
	vpor	%ymm3, %ymm4, %ymm2
	vpor	%ymm2, %ymm1, %ymm1
	vpor	%ymm0, %ymm1, %ymm0
	shlq	$42, %rsi
	vextracti128	$1, %ymm0, %xmm1
	vpor	%xmm1, %xmm0, %xmm0
	vpshufd	$238, %xmm0, %xmm1              ## xmm1 = xmm0[2,3,2,3]
	vpor	%xmm1, %xmm0, %xmm0
	orq	%rsi, %rdx
	vmovq	%xmm0, %rcx
	orq	%rcx, %rdx
	orq	%rax, %rdx
	movabsq	$-9223372036854775808, %rax     ## imm = 0x8000000000000000
	orq	%rax, %rdx
	xorl	%eax, %eax
	tzcntq	%rdx, %rax
	cmpl	$43, %eax
	movl	240(%rsp), %edi                 ## 4-byte Reload
	movl	428(%rsp), %r10d                ## 4-byte Reload
	jbe	LBB0_289
## %bb.95:                              ## %no_errors_bb196
	movslq	464(%rsp), %rcx                 ## 4-byte Folded Reload
	movq	160(%rsp), %r8                  ## 8-byte Reload
	movslq	%r8d, %rax
	movq	%rcx, %rsi
	movq	%rax, 704(%rsp)                 ## 8-byte Spill
	imulq	%rax, %rsi
	movslq	472(%rsp), %rdx                 ## 4-byte Folded Reload
	movq	%rsi, %rax
	imulq	%rdx, %rax
	movslq	448(%rsp), %r15                 ## 4-byte Folded Reload
	movq	208(%rsp), %r14                 ## 8-byte Reload
	movslq	%r14d, %rbx
	movq	%r15, %r13
	movq	%rbx, 680(%rsp)                 ## 8-byte Spill
	imulq	%rbx, %r13
	movslq	456(%rsp), %r12                 ## 4-byte Folded Reload
	movq	%r13, %r11
	imulq	%r12, %r11
	movslq	%edi, %r9
	movq	%r9, %rbx
	shlq	$5, %rbx
	movq	%rbx, %rdi
	negq	%rdi
	testl	%r9d, %r9d
	cmovnsq	%rbx, %rdi
	xorl	%ebx, %ebx
	movq	%rdi, 664(%rsp)                 ## 8-byte Spill
	cmpq	$2147483647, %rdi               ## imm = 0x7FFFFFFF
	seta	%bl
	movq	%rbx, 40(%rsp)                  ## 8-byte Spill
	movslq	%r10d, %r9
	leaq	(,%r9,8), %rbx
	leaq	(%rbx,%rbx,4), %rdi
	movq	%rdi, %rbx
	negq	%rbx
	testl	%r9d, %r9d
	cmovnsq	%rdi, %rbx
	xorl	%edi, %edi
	movq	%rbx, 656(%rsp)                 ## 8-byte Spill
	cmpq	$2147483647, %rbx               ## imm = 0x7FFFFFFF
	seta	%dil
	movl	%edi, 256(%rsp)                 ## 4-byte Spill
	movslq	424(%rsp), %r9                  ## 4-byte Folded Reload
	leaq	(,%r9,8), %rdi
	subq	%r9, %rdi
	movq	%rdi, %rbx
	negq	%rbx
	testl	%r9d, %r9d
	cmovnsq	%rdi, %rbx
	xorl	%edi, %edi
	movq	%rbx, 648(%rsp)                 ## 8-byte Spill
	cmpq	$2147483647, %rbx               ## imm = 0x7FFFFFFF
	seta	%dil
	movl	%edi, 48(%rsp)                  ## 4-byte Spill
	movslq	516(%rsp), %rdi                 ## 4-byte Folded Reload
	imulq	$39, %rdi, %r9
	movq	%r9, %rbx
	negq	%rbx
	testl	%edi, %edi
	cmovnsq	%r9, %rbx
	xorl	%edi, %edi
	movq	%rbx, 640(%rsp)                 ## 8-byte Spill
	cmpq	$2147483647, %rbx               ## imm = 0x7FFFFFFF
	seta	%dil
	movl	%edi, 248(%rsp)                 ## 4-byte Spill
	movslq	512(%rsp), %rdi                 ## 4-byte Folded Reload
	imulq	%rcx, %rdi
	shrl	$27, %r8d
	andl	$-16, %r8d
	movq	%r8, 160(%rsp)                  ## 8-byte Spill
	movq	%rdi, %rcx
	negq	%rcx
	cmovlq	%rdi, %rcx
	xorl	%ebx, %ebx
	movq	%rcx, 632(%rsp)                 ## 8-byte Spill
	cmpq	$2147483647, %rcx               ## imm = 0x7FFFFFFF
	seta	%bl
	xorl	%ecx, %ecx
	movq	%rsi, 696(%rsp)                 ## 8-byte Spill
	cmpq	$2147483647, %rsi               ## imm = 0x7FFFFFFF
	setg	%cl
	movl	%ecx, 344(%rsp)                 ## 4-byte Spill
	movslq	508(%rsp), %rsi                 ## 4-byte Folded Reload
	imulq	%rdx, %rsi
	movq	%rsi, %rcx
	negq	%rcx
	cmovlq	%rsi, %rcx
	xorl	%edx, %edx
	movq	%rcx, 624(%rsp)                 ## 8-byte Spill
	cmpq	$2147483647, %rcx               ## imm = 0x7FFFFFFF
	seta	%dl
	movl	%edx, 464(%rsp)                 ## 4-byte Spill
	xorl	%esi, %esi
	movq	%rax, 688(%rsp)                 ## 8-byte Spill
	cmpq	$2147483647, %rax               ## imm = 0x7FFFFFFF
	setg	%sil
	movslq	216(%rsp), %rdx                 ## 4-byte Folded Reload
	imulq	%r15, %rdx
	movq	832(%rsp), %rax                 ## 8-byte Reload
	shrq	$22, %rax
	andl	$512, %eax                      ## imm = 0x200
	movq	%rax, 264(%rsp)                 ## 8-byte Spill
	shrl	$21, %r14d
	andl	$1024, %r14d                    ## imm = 0x400
	movq	%r14, 208(%rsp)                 ## 8-byte Spill
	movq	%rdx, %rax
	negq	%rax
	cmovlq	%rdx, %rax
	xorl	%ecx, %ecx
	movq	%rax, 616(%rsp)                 ## 8-byte Spill
	cmpq	$2147483647, %rax               ## imm = 0x7FFFFFFF
	seta	%cl
	movl	%ecx, 336(%rsp)                 ## 4-byte Spill
	xorl	%eax, %eax
	movq	%r13, 1360(%rsp)                ## 8-byte Spill
	cmpq	$2147483647, %r13               ## imm = 0x7FFFFFFF
	setg	%al
	movl	%eax, 472(%rsp)                 ## 4-byte Spill
	movslq	32(%rsp), %rcx                  ## 4-byte Folded Reload
	imulq	%r12, %rcx
	movq	%rcx, %rax
	negq	%rax
	cmovlq	%rcx, %rax
	xorl	%ecx, %ecx
	movq	%rax, 608(%rsp)                 ## 8-byte Spill
	cmpq	$2147483647, %rax               ## imm = 0x7FFFFFFF
	seta	%cl
	movl	%ecx, 456(%rsp)                 ## 4-byte Spill
	xorl	%eax, %eax
	movq	%r11, 672(%rsp)                 ## 8-byte Spill
	cmpq	$2147483647, %r11               ## imm = 0x7FFFFFFF
	vmovd	1456(%rsp), %xmm0               ## 4-byte Folded Reload
                                        ## xmm0 = mem[0],zero,zero,zero
	vpinsrd	$1, 948(%rsp), %xmm0, %xmm0     ## 4-byte Folded Reload
	setg	%al
	movl	%eax, 448(%rsp)                 ## 4-byte Spill
	vpmovsxdq	%xmm0, %xmm1
	vpsllvq	LCPI0_67(%rip), %xmm1, %xmm1
	movq	480(%rsp), %rax                 ## 8-byte Reload
	shrq	$16, %rax
	vpcmpeqd	%xmm2, %xmm2, %xmm2
	vpcmpgtd	%xmm2, %xmm0, %xmm0
	vpmovsxdq	%xmm0, %xmm0
	vpxor	%xmm0, %xmm1, %xmm1
	vpsubq	%xmm1, %xmm0, %xmm8
	vpxor	LCPI0_68(%rip), %xmm8, %xmm0
	andl	$32768, %eax                    ## imm = 0x8000
	movq	%rax, 296(%rsp)                 ## 8-byte Spill
	vpcmpgtq	LCPI0_69(%rip), %xmm0, %xmm0
	vmovmskpd	%xmm0, %eax
	movl	%eax, 760(%rsp)                 ## 4-byte Spill
	movl	%eax, %edi
	shrb	%dil
	movslq	1440(%rsp), %rax                ## 4-byte Folded Reload
	leaq	(,%rax,4), %rcx
	movq	%rcx, %rdx
	negq	%rdx
	testl	%eax, %eax
	cmovnsq	%rcx, %rdx
	xorl	%eax, %eax
	movq	%rdx, 600(%rsp)                 ## 8-byte Spill
	cmpq	$2147483647, %rdx               ## imm = 0x7FFFFFFF
	seta	%al
	movl	%eax, 792(%rsp)                 ## 4-byte Spill
	movq	1392(%rsp), %rcx                ## 8-byte Reload
	leaq	(,%rcx,4), %rax
	movq	%rax, %rdx
	negq	%rdx
	testl	%ecx, %ecx
	cmovnsq	%rax, %rdx
	xorl	%eax, %eax
	movq	%rdx, 592(%rsp)                 ## 8-byte Spill
	cmpq	$2147483647, %rdx               ## imm = 0x7FFFFFFF
	seta	%al
	movl	%eax, 784(%rsp)                 ## 4-byte Spill
	movslq	940(%rsp), %rax                 ## 4-byte Folded Reload
	leaq	(,%rax,8), %rcx
	leaq	(%rcx,%rcx,4), %rcx
	movq	%rcx, %rdx
	negq	%rdx
	testl	%eax, %eax
	cmovnsq	%rcx, %rdx
	xorl	%eax, %eax
	movq	%rdx, 584(%rsp)                 ## 8-byte Spill
	cmpq	$2147483647, %rdx               ## imm = 0x7FFFFFFF
	seta	%al
	movl	%eax, 768(%rsp)                 ## 4-byte Spill
	movslq	1376(%rsp), %rax                ## 4-byte Folded Reload
	leaq	(,%rax,8), %rcx
	subq	%rax, %rcx
	movq	%rcx, %rdx
	negq	%rdx
	testl	%eax, %eax
	cmovnsq	%rcx, %rdx
	xorl	%eax, %eax
	movq	%rdx, 576(%rsp)                 ## 8-byte Spill
	cmpq	$2147483647, %rdx               ## imm = 0x7FFFFFFF
	seta	%al
	movl	%eax, 752(%rsp)                 ## 4-byte Spill
	movslq	1120(%rsp), %rax                ## 4-byte Folded Reload
	leaq	(,%rax,4), %rcx
	movq	%rcx, %rdx
	negq	%rdx
	testl	%eax, %eax
	cmovnsq	%rcx, %rdx
	movq	%rdx, 568(%rsp)                 ## 8-byte Spill
	cmpq	$2147483647, %rdx               ## imm = 0x7FFFFFFF
	movl	$0, %eax
	seta	%al
	shlq	$22, %rax
	movq	%rax, 808(%rsp)                 ## 8-byte Spill
	movslq	1424(%rsp), %rax                ## 4-byte Folded Reload
	leaq	(,%rax,4), %rcx
	movq	%rcx, %rdx
	negq	%rdx
	testl	%eax, %eax
	cmovnsq	%rcx, %rdx
	movq	%rdx, 560(%rsp)                 ## 8-byte Spill
	cmpq	$2147483647, %rdx               ## imm = 0x7FFFFFFF
	movl	$0, %eax
	seta	%al
	shlq	$23, %rax
	movq	%rax, 800(%rsp)                 ## 8-byte Spill
	movslq	944(%rsp), %rax                 ## 4-byte Folded Reload
	imulq	$39, %rax, %rcx
	movq	%rcx, %rdx
	negq	%rdx
	testl	%eax, %eax
	cmovnsq	%rcx, %rdx
	movq	%rdx, 552(%rsp)                 ## 8-byte Spill
	cmpq	$2147483647, %rdx               ## imm = 0x7FFFFFFF
	movl	$0, %eax
	seta	%al
	shlq	$24, %rax
	movq	%rax, 776(%rsp)                 ## 8-byte Spill
	movslq	1400(%rsp), %rax                ## 4-byte Folded Reload
	leaq	(,%rax,4), %rcx
	movq	%rcx, %rdx
	negq	%rdx
	testl	%eax, %eax
	cmovnsq	%rcx, %rdx
	xorl	%eax, %eax
	movq	%rdx, 1344(%rsp)                ## 8-byte Spill
	cmpq	$2147483647, %rdx               ## imm = 0x7FFFFFFF
	seta	%al
	movq	%rax, %rcx
	movq	56(%rsp), %rdx                  ## 8-byte Reload
	shlq	$32, %rdx
	movabsq	$8589934592, %rax               ## imm = 0x200000000
	andq	%rax, %rdx
	movq	%rdx, 56(%rsp)                  ## 8-byte Spill
	movq	200(%rsp), %rdx                 ## 8-byte Reload
	shlq	$34, %rdx
	movabsq	$34359738368, %rax              ## imm = 0x800000000
	andq	%rax, %rdx
	movq	%rdx, 200(%rsp)                 ## 8-byte Spill
	movq	312(%rsp), %rdx                 ## 8-byte Reload
	shlq	$36, %rdx
	movabsq	$137438953472, %rax             ## imm = 0x2000000000
	andq	%rax, %rdx
	movq	%rdx, 312(%rsp)                 ## 8-byte Spill
	movq	432(%rsp), %rdx                 ## 8-byte Reload
	shlq	$40, %rdx
	movabsq	$2199023255552, %rax            ## imm = 0x20000000000
	andq	%rax, %rdx
	movq	%rdx, 432(%rsp)                 ## 8-byte Spill
	movq	64(%rsp), %rdx                  ## 8-byte Reload
	shlq	$41, %rdx
	movabsq	$4398046511104, %rax            ## imm = 0x40000000000
	andq	%rax, %rdx
	movq	%rdx, 64(%rsp)                  ## 8-byte Spill
	vmovd	%ebx, %xmm0
	vpinsrb	$4, %esi, %xmm0, %xmm0
	shlq	$25, %rcx
	movq	%rcx, 744(%rsp)                 ## 8-byte Spill
	movq	1704(%rsp), %r8                 ## 8-byte Reload
	andl	$2, %r8d
	shlq	$25, %r8
	movq	1696(%rsp), %r10                ## 8-byte Reload
	andl	$2, %r10d
	shlq	$26, %r10
	movq	1688(%rsp), %r11                ## 8-byte Reload
	andl	$2, %r11d
	shlq	$27, %r11
	movq	1680(%rsp), %rbx                ## 8-byte Reload
	andl	$2, %ebx
	shlq	$28, %rbx
	movq	1656(%rsp), %r14                ## 8-byte Reload
	andl	$2, %r14d
	shlq	$29, %r14
	movq	1648(%rsp), %r12                ## 8-byte Reload
	andl	$2, %r12d
	shlq	$30, %r12
	movq	1640(%rsp), %r13                ## 8-byte Reload
	shlq	$31, %r13
	movabsq	$4294967296, %rax               ## imm = 0x100000000
	andq	%rax, %r13
	movq	168(%rsp), %rax                 ## 8-byte Reload
	andl	$2, %eax
	shlq	$33, %rax
	movq	%rax, 168(%rsp)                 ## 8-byte Spill
	movq	128(%rsp), %rax                 ## 8-byte Reload
	andl	$2, %eax
	shlq	$35, %rax
	movq	%rax, 128(%rsp)                 ## 8-byte Spill
	movq	304(%rsp), %rax                 ## 8-byte Reload
	andl	$2, %eax
	shlq	$37, %rax
	movq	%rax, 304(%rsp)                 ## 8-byte Spill
	movq	280(%rsp), %rax                 ## 8-byte Reload
	andl	$2, %eax
	shlq	$38, %rax
	movq	%rax, 280(%rsp)                 ## 8-byte Spill
	movq	272(%rsp), %rax                 ## 8-byte Reload
	andl	$2, %eax
	shlq	$39, %rax
	movq	%rax, 272(%rsp)                 ## 8-byte Spill
	xorl	%eax, %eax
	cmpq	$0, 1192(%rsp)                  ## 8-byte Folded Reload
	sete	%al
	movl	%eax, 720(%rsp)                 ## 4-byte Spill
	xorl	%eax, %eax
	cmpq	$0, 856(%rsp)                   ## 8-byte Folded Reload
	sete	%al
	movl	%eax, 712(%rsp)                 ## 4-byte Spill
	xorl	%eax, %eax
	cmpq	$0, 1168(%rsp)                  ## 8-byte Folded Reload
	sete	%al
	movl	%eax, 728(%rsp)                 ## 4-byte Spill
	xorl	%eax, %eax
	cmpq	$0, 1136(%rsp)                  ## 8-byte Folded Reload
	sete	%al
	movl	%eax, 736(%rsp)                 ## 4-byte Spill
	xorl	%edx, %edx
	cmpq	$0, 1184(%rsp)                  ## 8-byte Folded Reload
	sete	%dl
	xorl	%esi, %esi
	cmpq	$0, 1176(%rsp)                  ## 8-byte Folded Reload
	sete	%sil
	xorl	%r15d, %r15d
	cmpq	$0, 1480(%rsp)                  ## 8-byte Folded Reload
	sete	%r15b
	xorl	%r9d, %r9d
	cmpq	$0, 1160(%rsp)                  ## 8-byte Folded Reload
	sete	%r9b
	xorl	%eax, %eax
	cmpq	$0, 1472(%rsp)                  ## 8-byte Folded Reload
	sete	%al
	vpinsrb	$8, %eax, %xmm0, %xmm0
	vpinsrb	$12, 464(%rsp), %xmm0, %xmm1    ## 4-byte Folded Reload
	vmovd	760(%rsp), %xmm0                ## 4-byte Folded Reload
                                        ## xmm0 = mem[0],zero,zero,zero
	movzbl	%dil, %eax
	vpinsrb	$4, %eax, %xmm0, %xmm0
	xorl	%edi, %edi
	cmpq	$0, 184(%rsp)                   ## 8-byte Folded Reload
	sete	%dil
	xorl	%eax, %eax
	cmpq	$0, 1200(%rsp)                  ## 8-byte Folded Reload
	sete	%al
	vmovd	%eax, %xmm2
	vpinsrb	$4, 456(%rsp), %xmm2, %xmm2     ## 4-byte Folded Reload
	vpxor	%xmm3, %xmm3, %xmm3
	vmovq	1448(%rsp), %xmm4               ## 8-byte Folded Reload
                                        ## xmm4 = mem[0],zero
	vmovq	1464(%rsp), %xmm5               ## 8-byte Folded Reload
                                        ## xmm5 = mem[0],zero
	vpunpcklqdq	%xmm4, %xmm5, %xmm4     ## xmm4 = xmm5[0],xmm4[0]
	vpcmpeqq	%xmm3, %xmm4, %xmm3
	vmovmskpd	%xmm3, %eax
	vpinsrb	$8, %eax, %xmm2, %xmm4
	movl	%eax, %ecx
	shrb	%cl
	xorl	%eax, %eax
	cmpq	$0, 1408(%rsp)                  ## 8-byte Folded Reload
	sete	%al
	vpinsrb	$8, %eax, %xmm0, %xmm0
	vpinsrb	$12, 792(%rsp), %xmm0, %xmm2    ## 4-byte Folded Reload
	vmovd	%edi, %xmm0
	vpinsrb	$4, 472(%rsp), %xmm0, %xmm0     ## 4-byte Folded Reload
	vpinsrb	$8, 336(%rsp), %xmm0, %xmm0     ## 4-byte Folded Reload
	vpinsrb	$12, 448(%rsp), %xmm0, %xmm3    ## 4-byte Folded Reload
	xorl	%eax, %eax
	cmpq	$0, 1384(%rsp)                  ## 8-byte Folded Reload
	sete	%al
	vmovd	%eax, %xmm0
	vpinsrb	$4, 784(%rsp), %xmm0, %xmm0     ## 4-byte Folded Reload
	vpinsrb	$8, 768(%rsp), %xmm0, %xmm0     ## 4-byte Folded Reload
	vpinsrb	$12, 752(%rsp), %xmm0, %xmm5    ## 4-byte Folded Reload
	movzbl	%cl, %eax
	vpinsrb	$12, %eax, %xmm4, %xmm4
	vmovd	256(%rsp), %xmm0                ## 4-byte Folded Reload
                                        ## xmm0 = mem[0],zero,zero,zero
	vpinsrb	$4, %edx, %xmm0, %xmm0
	vpinsrb	$8, 48(%rsp), %xmm0, %xmm0      ## 4-byte Folded Reload
	vpinsrb	$12, %esi, %xmm0, %xmm6
	vmovd	%r15d, %xmm0
	vpinsrb	$4, 248(%rsp), %xmm0, %xmm0     ## 4-byte Folded Reload
	vpinsrb	$8, %r9d, %xmm0, %xmm0
	vpinsrb	$12, 344(%rsp), %xmm0, %xmm7    ## 4-byte Folded Reload
	vmovd	720(%rsp), %xmm0                ## 4-byte Folded Reload
                                        ## xmm0 = mem[0],zero,zero,zero
	vpinsrb	$4, 712(%rsp), %xmm0, %xmm0     ## 4-byte Folded Reload
	vpinsrb	$8, 728(%rsp), %xmm0, %xmm0     ## 4-byte Folded Reload
	vpinsrb	$12, 736(%rsp), %xmm0, %xmm0    ## 4-byte Folded Reload
	xorl	%eax, %eax
	cmpq	$0, 1432(%rsp)                  ## 8-byte Folded Reload
	sete	%al
	shlq	$58, %rax
	xorl	%ecx, %ecx
	cmpq	$0, 1416(%rsp)                  ## 8-byte Folded Reload
	sete	%cl
	orq	808(%rsp), %rax                 ## 8-byte Folded Reload
	shlq	$59, %rcx
	orq	%rcx, %rax
	orq	800(%rsp), %rax                 ## 8-byte Folded Reload
	orq	776(%rsp), %rax                 ## 8-byte Folded Reload
	orq	744(%rsp), %rax                 ## 8-byte Folded Reload
	orq	%r8, %rax
	orq	%r10, %rax
	orq	40(%rsp), %rax                  ## 8-byte Folded Reload
	orq	%r11, %rax
	orq	%rbx, %rax
	orq	%r14, %rax
	orq	%r12, %rax
	orq	%r13, %rax
	orq	56(%rsp), %rax                  ## 8-byte Folded Reload
	orq	160(%rsp), %rax                 ## 8-byte Folded Reload
	orq	168(%rsp), %rax                 ## 8-byte Folded Reload
	orq	264(%rsp), %rax                 ## 8-byte Folded Reload
	orq	200(%rsp), %rax                 ## 8-byte Folded Reload
	orq	208(%rsp), %rax                 ## 8-byte Folded Reload
	orq	128(%rsp), %rax                 ## 8-byte Folded Reload
	orq	296(%rsp), %rax                 ## 8-byte Folded Reload
	orq	312(%rsp), %rax                 ## 8-byte Folded Reload
	orq	304(%rsp), %rax                 ## 8-byte Folded Reload
	orq	280(%rsp), %rax                 ## 8-byte Folded Reload
	orq	272(%rsp), %rax                 ## 8-byte Folded Reload
	orq	432(%rsp), %rax                 ## 8-byte Folded Reload
	vpslld	$31, %xmm1, %xmm1
	vpsrad	$31, %xmm1, %xmm1
	vpmovsxdq	%xmm1, %ymm1
	vpand	LCPI0_70(%rip), %ymm1, %ymm1
	orq	64(%rsp), %rax                  ## 8-byte Folded Reload
	vpslld	$31, %xmm5, %xmm5
	vpsrad	$31, %xmm5, %xmm5
	vpmovsxdq	%xmm5, %ymm5
	vpand	LCPI0_73(%rip), %ymm5, %ymm5
	vpslld	$31, %xmm4, %xmm4
	vpsrad	$31, %xmm4, %xmm4
	vpmovsxdq	%xmm4, %ymm4
	vpand	LCPI0_74(%rip), %ymm4, %ymm4
	vpslld	$31, %xmm6, %xmm6
	vpsrad	$31, %xmm6, %xmm6
	vpmovsxdq	%xmm6, %ymm6
	vpand	LCPI0_75(%rip), %ymm6, %ymm6
	vpor	%ymm5, %ymm1, %ymm1
	vpor	%ymm4, %ymm6, %ymm4
	vpslld	$31, %xmm0, %xmm0
	vpmovsxdq	%xmm0, %ymm0
	vmovapd	LCPI0_0(%rip), %xmm5            ## xmm5 = [9223372036854775808,0]
	vblendvpd	%ymm0, LCPI0_77(%rip), %ymm5, %ymm0
	vpor	%ymm1, %ymm4, %ymm1
	vpslld	$31, %xmm2, %xmm2
	vpsrad	$31, %xmm2, %xmm2
	vpmovsxdq	%xmm2, %ymm2
	vpand	LCPI0_71(%rip), %ymm2, %ymm2
	vpslld	$31, %xmm3, %xmm3
	vpsrad	$31, %xmm3, %xmm3
	vpmovsxdq	%xmm3, %ymm3
	vpand	LCPI0_72(%rip), %ymm3, %ymm3
	vpslld	$31, %xmm7, %xmm4
	vpsrad	$31, %xmm4, %xmm4
	vpmovsxdq	%xmm4, %ymm4
	vpand	LCPI0_76(%rip), %ymm4, %ymm4
	vpor	%ymm2, %ymm4, %ymm2
	vorpd	%ymm3, %ymm0, %ymm0
	vorpd	%ymm1, %ymm0, %ymm0
	vpor	%ymm0, %ymm2, %ymm0
	vextracti128	$1, %ymm0, %xmm1
	vpor	%xmm1, %xmm0, %xmm0
	vpshufd	$238, %xmm0, %xmm1              ## xmm1 = xmm0[2,3,2,3]
	vpor	%xmm1, %xmm0, %xmm0
	vmovq	%xmm0, %rcx
	orq	%rcx, %rax
	tzcntq	%rax, %rax
	cmpl	$59, %eax
	jbe	LBB0_291
## %bb.96:                              ## %"produce squashed_head1_filter"
	movl	428(%rsp), %eax                 ## 4-byte Reload
	movl	%eax, 6096(%rsp)
	movl	424(%rsp), %eax                 ## 4-byte Reload
	movl	%eax, 6100(%rsp)
	movq	1136(%rsp), %rax                ## 8-byte Reload
	movq	%rax, 6104(%rsp)
	movq	816(%rsp), %rax                 ## 8-byte Reload
	movq	%rax, 6112(%rsp)
	leaq	7040(%rsp), %r12
	movq	%r12, 6120(%rsp)
	movq	$0, 6128(%rsp)
	leaq	_train_cost_model.par_for.squashed_head1_filter.s0.s.s.s(%rip), %rsi
	leaq	6096(%rsp), %r8
	xorl	%edi, %edi
	xorl	%edx, %edx
	movl	$16, %ecx
	vzeroupper
	callq	_halide_do_par_for
	movl	%eax, %r14d
	testl	%eax, %eax
	movq	80(%rsp), %rax                  ## 8-byte Reload
	jne	LBB0_235
## %bb.97:                              ## %"assert succeeded303"
	movl	%eax, %r13d
	movq	%r13, %rdx
	shlq	$5, %rdx
	cmpl	$67108864, %eax                 ## imm = 0x4000000
	jae	LBB0_293
## %bb.98:                              ## %"assert succeeded305"
	movq	%rdx, 168(%rsp)                 ## 8-byte Spill
	movq	%rdx, %rsi
	orq	$12, %rsi
	xorl	%edi, %edi
	callq	_halide_malloc
	testq	%rax, %rax
	je	LBB0_294
## %bb.99:                              ## %"assert succeeded307"
	movq	%rax, %rbx
	movq	1168(%rsp), %rax                ## 8-byte Reload
	movq	%rax, 6664(%rsp)
	movq	16(%rbp), %rax
	movq	%rax, 6672(%rsp)
	movq	%rbx, 6680(%rsp)
	movq	$0, 6688(%rsp)
	leaq	_train_cost_model.par_for.head1_conv.s0.w(%rip), %rsi
	leaq	6664(%rsp), %r8
	xorl	%edi, %edi
	xorl	%edx, %edx
	movq	80(%rsp), %r15                  ## 8-byte Reload
	movl	%r15d, %ecx
	callq	_halide_do_par_for
	testl	%eax, %eax
	movq	%rbx, 200(%rsp)                 ## 8-byte Spill
	jne	LBB0_273
## %bb.100:                             ## %"consume squashed_head1_filter"
	movq	%r13, 40(%rsp)                  ## 8-byte Spill
	movq	840(%rsp), %rax                 ## 8-byte Reload
	movl	508(%rsp), %edx                 ## 4-byte Reload
	imull	%edx, %eax
	movl	512(%rsp), %ecx                 ## 4-byte Reload
	movl	%ecx, %esi
	imull	1104(%rsp), %esi                ## 4-byte Folded Reload
	addl	1112(%rsp), %esi                ## 4-byte Folded Reload
	movq	%rax, 840(%rsp)                 ## 8-byte Spill
	addl	%eax, %esi
	movl	%ecx, 6696(%rsp)
	movl	%edx, 6700(%rsp)
	movl	%esi, 464(%rsp)                 ## 4-byte Spill
	movl	%esi, 6704(%rsp)
	movq	%rbx, 6712(%rsp)
	movq	$0, 6720(%rsp)
	movq	1160(%rsp), %rax                ## 8-byte Reload
	movq	%rax, 6728(%rsp)
	movq	1208(%rsp), %rax                ## 8-byte Reload
	movq	%rax, 6736(%rsp)
	movq	%r12, 6744(%rsp)
	movq	$0, 6752(%rsp)
	leaq	_train_cost_model.par_for.head1_conv.s1.w(%rip), %rsi
	leaq	6696(%rsp), %r8
	xorl	%edi, %edi
	xorl	%edx, %edx
	movl	%r15d, %ecx
	callq	_halide_do_par_for
	testl	%eax, %eax
	jne	LBB0_273
## %bb.101:                             ## %"assert succeeded311"
	movq	1664(%rsp), %r13                ## 8-byte Reload
	movl	%r13d, %r9d
	sarl	$3, %r9d
	movq	120(%rsp), %r14                 ## 8-byte Reload
	cmpl	$2, %r14d
	movl	$2, %r10d
	cmovll	%r14d, %r10d
	testl	%r10d, %r10d
	movl	$1, %r8d
	cmovlel	%r8d, %r10d
	movq	152(%rsp), %r12                 ## 8-byte Reload
	leal	39(%r12), %eax
	movl	%eax, %edx
	sarl	$31, %edx
	xorl	%edx, %eax
	imulq	$1717986919, %rax, %rdi         ## imm = 0x66666667
	shrq	$36, %rdi
	xorl	%edx, %edi
	leal	1(%r15), %esi
	movl	%esi, %eax
	shrl	%eax
	movl	%edi, %ecx
	movl	%eax, 56(%rsp)                  ## 4-byte Spill
	imull	%eax, %ecx
	xorl	%edx, %edx
	testl	%ecx, %ecx
	setle	%dl
	movq	%rcx, 128(%rsp)                 ## 8-byte Spill
	leal	(%rdx,%rcx,2), %eax
	decl	%eax
	movl	%edx, %r11d
	negl	%r11d
	xorl	%ecx, %ecx
	testl	%edi, %edi
	sete	%cl
	xorl	%ebx, %ebx
	cmpl	$-39, %r12d
	setl	%bl
	cltd
	movq	%rdi, 208(%rsp)                 ## 8-byte Spill
	addl	%ecx, %edi
	idivl	%edi
	leal	(%rbx,%rbx), %edx
	decl	%edx
	andl	%r11d, %edx
	addl	%eax, %edx
	decl	%ecx
	andl	%edx, %ecx
	andl	$2147483646, %esi               ## imm = 0x7FFFFFFE
	decl	%esi
	cmpl	%ecx, %esi
	cmovgl	%ecx, %esi
	andl	$-2, %esi
	addl	%r10d, %esi
	testl	%esi, %esi
	cmovgl	%r8d, %esi
	movl	$2147483648, %r10d              ## imm = 0x80000000
	cmpl	%r14d, %esi
	cmovlel	%r14d, %esi
	cmpl	$7, %r14d
	movl	$7, %ecx
	cmovll	%r14d, %ecx
	testl	%ecx, %ecx
	cmovlel	%r8d, %ecx
	leal	-1(%r15), %ebx
	movl	%ebx, %eax
	sarl	$31, %eax
	movl	%eax, %edx
	xorl	%ebx, %edx
	leaq	306783379(%r10), %r14
	imulq	%r14, %rdx
	shrq	$34, %rdx
	xorl	%eax, %edx
	leal	(,%rdx,8), %eax
	subl	%edx, %eax
	addl	%ecx, %eax
	cmpl	%r15d, %eax
	cmovgl	%r15d, %eax
	cmpl	%eax, %esi
	cmovgl	%esi, %eax
	cmpl	$4, %r9d
	movl	$4, %ecx
	cmovll	%r9d, %ecx
	movl	%r13d, %edx
	sarl	$31, %edx
	xorl	%edx, %r13d
	imulq	$1717986919, %r13, %rsi         ## imm = 0x66666667
	shrq	$36, %rsi
	xorl	%edx, %esi
	leal	(%rsi,%rsi,4), %edx
	movq	232(%rsp), %rsi                 ## 8-byte Reload
	leal	(%rsi,%rdx,8), %edx
	movl	1332(%rsp), %edi                ## 4-byte Reload
	cmpl	%edx, %edi
	cmovgl	%edx, %edi
	movq	1672(%rsp), %rdx                ## 8-byte Reload
	leal	(%rdx,%rcx,8), %r12d
	addl	%edi, %r12d
	movq	112(%rsp), %rcx                 ## 8-byte Reload
	cmpl	%ecx, %r12d
	cmovgl	%ecx, %r12d
	movq	136(%rsp), %rcx                 ## 8-byte Reload
	cmpl	%ecx, %r12d
	movl	%ecx, %r13d
	cmovgl	%r12d, %r13d
	movl	144(%rsp), %ecx                 ## 4-byte Reload
	cmpl	%ecx, %r13d
	cmovlel	%ecx, %r13d
	subl	%esi, %r13d
	movl	%r13d, %ecx
	sarl	$31, %ecx
	andnl	%r13d, %ecx, %ecx
	movl	%eax, %edx
	sarl	$31, %edx
	andnl	%eax, %edx, %eax
	imulq	$156, %rcx, %rsi
	movq	%rsi, %rdx
	imulq	%rax, %rdx
	cmpq	%r10, %rdx
	jae	LBB0_274
## %bb.102:                             ## %"assert succeeded311"
	movl	%ecx, %edi
	shll	$2, %edi
	imulq	$39, %rdi, %rdi
	shrq	$32, %rdi
	shrq	$30, %rcx
	imulq	$39, %rcx, %rcx
	addq	%rdi, %rcx
	movl	%esi, %esi
	imulq	%rax, %rsi
	shrq	$32, %rsi
	imulq	%rax, %rcx
	addq	%rsi, %rcx
	movabsq	$4393751543808, %rax            ## imm = 0x3FF00000000
	andq	%rcx, %rax
	jne	LBB0_274
## %bb.103:                             ## %"assert succeeded313"
	movl	%ebx, 280(%rsp)                 ## 4-byte Spill
	addq	$12, %rdx
	xorl	%edi, %edi
	movq	%rdx, %rsi
	callq	_halide_malloc
	testq	%rax, %rax
	je	LBB0_295
## %bb.104:                             ## %"assert succeeded315"
	movq	40(%rsp), %rsi                  ## 8-byte Reload
	imulq	%r14, %rsi
	shrq	$31, %rsi
	andl	$-8, %esi
	movq	80(%rsp), %r9                   ## 8-byte Reload
	leal	6(%r9), %ebx
	imulq	%r14, %rbx
	shrq	$31, %rbx
	andl	$-8, %ebx
	movl	32(%rsp), %r8d                  ## 4-byte Reload
	movl	%r8d, %edi
	imull	440(%rsp), %edi                 ## 4-byte Folded Reload
	movq	216(%rsp), %r10                 ## 8-byte Reload
	movl	%r10d, %ecx
	imull	288(%rsp), %ecx                 ## 4-byte Folded Reload
	movq	320(%rsp), %rdx                 ## 8-byte Reload
	movq	%rcx, 256(%rsp)                 ## 8-byte Spill
	addl	%ecx, %edx
	movq	%rdx, 312(%rsp)                 ## 8-byte Spill
	movq	%rdi, 304(%rsp)                 ## 8-byte Spill
	leal	(%rdx,%rdi), %edx
	movl	%r13d, 1752(%rsp)
	movl	%r10d, 1756(%rsp)
	movl	%r8d, 1760(%rsp)
	movq	%rsi, 40(%rsp)                  ## 8-byte Spill
	movl	%esi, 1764(%rsp)
	movq	152(%rsp), %rcx                 ## 8-byte Reload
	movl	%ecx, 1768(%rsp)
	movl	%edx, 160(%rsp)                 ## 4-byte Spill
	movl	%edx, 1772(%rsp)
	movl	%ecx, 1776(%rsp)
	movl	%r9d, 1780(%rsp)
	movq	232(%rsp), %r15                 ## 8-byte Reload
	movl	%r15d, 1784(%rsp)
	movq	112(%rsp), %rcx                 ## 8-byte Reload
	movl	%ecx, 1788(%rsp)
	movq	%rax, 48(%rsp)                  ## 8-byte Spill
	movq	%rax, 1792(%rsp)
	movq	$0, 1800(%rsp)
	movq	184(%rsp), %rax                 ## 8-byte Reload
	movq	%rax, 1808(%rsp)
	movq	24(%rsp), %rax                  ## 8-byte Reload
	movq	%rax, 1816(%rsp)
	leaq	_train_cost_model.par_for.normalized_schedule_features.s0.c.c.c(%rip), %rsi
	leaq	1752(%rsp), %r8
	xorl	%edi, %edi
	xorl	%edx, %edx
	movq	%rbx, 272(%rsp)                 ## 8-byte Spill
	movl	%ebx, %ecx
	callq	_halide_do_par_for
	testl	%eax, %eax
	jne	LBB0_298
## %bb.105:                             ## %"assert succeeded317"
	movl	144(%rsp), %eax                 ## 4-byte Reload
	cmpl	%eax, %r12d
	cmovlel	%eax, %r12d
	movq	136(%rsp), %rsi                 ## 8-byte Reload
	cmpl	$5, %esi
	movl	$5, %eax
	cmovll	%esi, %eax
	movl	224(%rsp), %edi                 ## 4-byte Reload
	movl	%edi, %ecx
	sarl	$31, %ecx
	movl	%ecx, %edx
	xorl	%edi, %edx
	imulq	$1717986919, %rdx, %rdx         ## imm = 0x66666667
	shrq	$33, %rdx
	xorl	%ecx, %edx
	leal	(%rdx,%rdx,4), %ecx
	addl	%eax, %ecx
	cmpl	%esi, %ecx
	cmovgel	%esi, %ecx
	cmpl	%ecx, %r12d
	cmovgl	%r12d, %ecx
	testl	%ecx, %ecx
	movl	$1, %ebx
	cmovgl	%ecx, %ebx
	subl	%r15d, %ebx
	movq	1048(%rsp), %rax                ## 8-byte Reload
	testl	%eax, %eax
	movl	$-1, %r15d
	cmovnsl	%eax, %r15d
	movl	%ebx, %eax
	sarl	$31, %eax
	andnl	%ebx, %eax, %eax
	incl	%r15d
	movq	%r15, %rcx
	shlq	$5, %rcx
	leaq	(%rcx,%rcx,2), %rcx
	movq	%rcx, %rsi
	imulq	%rax, %rsi
	movl	$2147483648, %edx               ## imm = 0x80000000
	cmpq	%rdx, %rsi
	jae	LBB0_275
## %bb.106:                             ## %"assert succeeded317"
	movabsq	$545460846592, %rdi             ## imm = 0x7F00000000
	movq	%rcx, %rdx
	shrq	$32, %rdx
	movl	%ecx, %ecx
	imulq	%rax, %rcx
	shrq	$32, %rcx
	imulq	%rax, %rdx
	addq	%rcx, %rdx
	andq	%rdi, %rdx
	jne	LBB0_275
## %bb.107:                             ## %"assert succeeded319"
	movl	%r13d, 296(%rsp)                ## 4-byte Spill
	orq	$12, %rsi
	xorl	%edi, %edi
	callq	_halide_malloc
	testq	%rax, %rax
	movq	128(%rsp), %r14                 ## 8-byte Reload
	movq	208(%rsp), %rcx                 ## 8-byte Reload
	je	LBB0_299
## %bb.108:                             ## %"assert succeeded321"
	movq	%rax, %rdx
	addl	%r14d, %r14d
	movl	%ebx, 3008(%rsp)
	movq	232(%rsp), %rax                 ## 8-byte Reload
	movl	%eax, 3012(%rsp)
	movl	%ecx, 3016(%rsp)
	movq	80(%rsp), %rax                  ## 8-byte Reload
	movl	%eax, 3020(%rsp)
	movq	112(%rsp), %r12                 ## 8-byte Reload
	movl	%r12d, 3024(%rsp)
	movq	104(%rsp), %r13                 ## 8-byte Reload
	movl	%r13d, 3028(%rsp)
	movq	1184(%rsp), %rax                ## 8-byte Reload
	movq	%rax, 3032(%rsp)
	movq	32(%rbp), %rax
	movq	%rax, 3040(%rsp)
	movq	%rdx, 3048(%rsp)
	movq	$0, 3056(%rsp)
	movq	%rdx, 64(%rsp)                  ## 8-byte Spill
	leaq	_train_cost_model.par_for.head2_conv.s0.n.n.n(%rip), %rsi
	leaq	3008(%rsp), %r8
	xorl	%edi, %edi
	xorl	%edx, %edx
	movl	%r14d, %ecx
	callq	_halide_do_par_for
	testl	%eax, %eax
	jne	LBB0_276
## %bb.109:                             ## %"consume normalized_schedule_features"
	movq	%r14, %rcx
	movl	%ebx, 344(%rsp)                 ## 4-byte Spill
	movl	%ebx, 4064(%rsp)
	movl	516(%rsp), %eax                 ## 4-byte Reload
	movl	%eax, 4068(%rsp)
	movl	296(%rsp), %eax                 ## 4-byte Reload
	movl	%eax, 4072(%rsp)
	movq	232(%rsp), %rbx                 ## 8-byte Reload
	movl	%ebx, 4076(%rsp)
	movq	208(%rsp), %rax                 ## 8-byte Reload
	movl	%eax, 4080(%rsp)
	movq	80(%rsp), %rax                  ## 8-byte Reload
	movl	%eax, 4084(%rsp)
	movl	%r12d, 4088(%rsp)
	movl	%r13d, 4092(%rsp)
	movq	64(%rsp), %rax                  ## 8-byte Reload
	movq	%rax, 4096(%rsp)
	movq	$0, 4104(%rsp)
	movq	1176(%rsp), %rax                ## 8-byte Reload
	movq	%rax, 4112(%rsp)
	movq	24(%rbp), %rax
	movq	%rax, 4120(%rsp)
	movq	48(%rsp), %rax                  ## 8-byte Reload
	movq	%rax, 4128(%rsp)
	movq	$0, 4136(%rsp)
	leaq	_train_cost_model.par_for.head2_conv.s1.n.n.n(%rip), %rsi
	leaq	4064(%rsp), %r8
	xorl	%edi, %edi
	xorl	%edx, %edx
                                        ## kill: def $ecx killed $ecx killed $rcx
	callq	_halide_do_par_for
	testl	%eax, %eax
	jne	LBB0_276
## %bb.110:                             ## %"assert succeeded325"
	movq	136(%rsp), %rax                 ## 8-byte Reload
	movl	144(%rsp), %ecx                 ## 4-byte Reload
	cmpl	%eax, %ecx
	movl	%eax, %r14d
	cmovgl	%ecx, %r14d
	subl	%ebx, %r14d
	movl	%r14d, %eax
	sarl	$31, %eax
	andnl	%r14d, %eax, %eax
	movq	168(%rsp), %rcx                 ## 8-byte Reload
	leaq	(%rcx,%rcx,2), %rcx
	movq	%rcx, %rdx
	imulq	%rax, %rdx
	movl	$2147483648, %esi               ## imm = 0x80000000
	cmpq	%rsi, %rdx
	jae	LBB0_277
## %bb.111:                             ## %"assert succeeded325"
	movq	%rcx, %rsi
	shrq	$32, %rsi
	movl	%ecx, %ecx
	imulq	%rax, %rcx
	shrq	$32, %rcx
	imulq	%rax, %rsi
	addq	%rcx, %rsi
	movabsq	$545460846592, %rax             ## imm = 0x7F00000000
	andq	%rax, %rsi
	jne	LBB0_277
## %bb.112:                             ## %"assert succeeded327"
	movq	%r13, %r12
	movq	%rbx, %r13
	orq	$12, %rdx
	xorl	%edi, %edi
	movq	%rdx, %rsi
	callq	_halide_malloc
	testq	%rax, %rax
	movq	112(%rsp), %rcx                 ## 8-byte Reload
	movq	80(%rsp), %rbx                  ## 8-byte Reload
	je	LBB0_302
## %bb.113:                             ## %"assert succeeded329"
	movl	344(%rsp), %edx                 ## 4-byte Reload
	movl	%edx, 3400(%rsp)
	movl	%r14d, 480(%rsp)                ## 4-byte Spill
	movl	%r14d, 3404(%rsp)
	movq	40(%rsp), %rdx                  ## 8-byte Reload
	movl	%edx, 3408(%rsp)
	movq	152(%rsp), %rdx                 ## 8-byte Reload
	movl	%edx, 3412(%rsp)
	movl	%r12d, 3416(%rsp)
	movl	%edx, 3420(%rsp)
	movl	%ebx, 3424(%rsp)
	movl	%r13d, 3428(%rsp)
	movl	%ecx, 3432(%rsp)
	movq	64(%rsp), %rcx                  ## 8-byte Reload
	movq	%rcx, 3440(%rsp)
	movq	$0, 3448(%rsp)
	movq	%rax, 128(%rsp)                 ## 8-byte Spill
	movq	%rax, 3456(%rsp)
	movq	$0, 3464(%rsp)
	leaq	_train_cost_model.par_for.head2_relu.s0.c.c.c(%rip), %rsi
	leaq	3400(%rsp), %r8
	xorl	%edi, %edi
	xorl	%edx, %edx
	movq	272(%rsp), %rcx                 ## 8-byte Reload
                                        ## kill: def $ecx killed $ecx killed $rcx
	callq	_halide_do_par_for
	testl	%eax, %eax
	jne	LBB0_305
## %bb.114:                             ## %"assert succeeded331"
	movl	224(%rsp), %eax                 ## 4-byte Reload
	andl	$-8, %eax
	movq	176(%rsp), %rcx                 ## 8-byte Reload
	addl	%eax, %ecx
	movq	136(%rsp), %rax                 ## 8-byte Reload
	cmpl	%eax, %ecx
	cmovgel	%eax, %ecx
	movl	144(%rsp), %edx                 ## 4-byte Reload
	cmpl	%ecx, %edx
	movq	%rcx, 176(%rsp)                 ## 8-byte Spill
	movl	%ecx, %eax
	cmovgl	%edx, %eax
	testl	%eax, %eax
	movl	$1, %r14d
	cmovgl	%eax, %r14d
	subl	%r13d, %r14d
	movl	%r14d, %eax
	sarl	$31, %eax
	andnl	%r14d, %eax, %eax
	movq	%r15, %rcx
	shlq	$7, %rcx
	movq	%rcx, %rsi
	imulq	%rax, %rsi
	movl	$2147483648, %edx               ## imm = 0x80000000
	cmpq	%rdx, %rsi
	jae	LBB0_278
## %bb.115:                             ## %"assert succeeded331"
	shrq	$25, %r15
	movl	%ecx, %ecx
	imulq	%rax, %rcx
	shrq	$32, %rcx
	imulq	%rax, %r15
	addq	%rcx, %r15
	movabsq	$545460846592, %rcx             ## imm = 0x7F00000000
	andq	%rcx, %r15
	jne	LBB0_278
## %bb.116:                             ## %"assert succeeded333"
	orq	$12, %rsi
	xorl	%edi, %edi
	callq	_halide_malloc
	testq	%rax, %rax
	je	LBB0_306
## %bb.117:                             ## %"assert succeeded335"
	movq	%rax, %rdx
	movl	%ebx, %eax
	andl	$2147483644, %eax               ## imm = 0x7FFFFFFC
	movl	%r14d, 5648(%rsp)
	movl	240(%rsp), %ecx                 ## 4-byte Reload
	movl	%ecx, 5652(%rsp)
	movl	%eax, 5656(%rsp)
	movl	280(%rsp), %eax                 ## 4-byte Reload
	sarl	%eax
	movq	152(%rsp), %r15                 ## 8-byte Reload
	movl	%r15d, 5660(%rsp)
	movl	%r12d, 5664(%rsp)
	movl	%r15d, 5668(%rsp)
	movl	%eax, 5672(%rsp)
	leal	3(%rbx), %ecx
	andl	$2147483644, %ecx               ## imm = 0x7FFFFFFC
	movl	%ebx, 5676(%rsp)
	movq	232(%rsp), %r12                 ## 8-byte Reload
	movl	%r12d, 5680(%rsp)
	movq	112(%rsp), %rax                 ## 8-byte Reload
	movl	%eax, 5684(%rsp)
	movq	1192(%rsp), %rax                ## 8-byte Reload
	movq	%rax, 5688(%rsp)
	movq	48(%rbp), %rax
	movq	%rax, 5696(%rsp)
	movq	%rdx, 5704(%rsp)
	movq	$0, 5712(%rsp)
	movq	856(%rsp), %r13                 ## 8-byte Reload
	movq	%r13, 5720(%rsp)
	movq	40(%rbp), %rax
	movq	%rax, 5728(%rsp)
	movq	200(%rsp), %rax                 ## 8-byte Reload
	movq	%rax, 5736(%rsp)
	movq	$0, 5744(%rsp)
	leaq	_train_cost_model.par_for.conv1_stage2.s0.c.c.c(%rip), %rsi
	leaq	5648(%rsp), %r8
	xorl	%edi, %edi
	movq	%rdx, 168(%rsp)                 ## 8-byte Spill
	xorl	%edx, %edx
	callq	_halide_do_par_for
	movq	168(%rsp), %rdx                 ## 8-byte Reload
	testl	%eax, %eax
	jne	LBB0_279
## %bb.118:                             ## %"consume head2_relu"
	leal	(%rbx,%rbx), %eax
	andl	$2147483644, %eax               ## imm = 0x7FFFFFFC
	movl	%r14d, 264(%rsp)                ## 4-byte Spill
	movl	%r14d, 3472(%rsp)
	movl	240(%rsp), %ecx                 ## 4-byte Reload
	movl	%ecx, 3476(%rsp)
	movl	480(%rsp), %ecx                 ## 4-byte Reload
	movl	%ecx, 3480(%rsp)
	movl	%eax, 3484(%rsp)
	movl	%r15d, 3488(%rsp)
	movq	104(%rsp), %rax                 ## 8-byte Reload
	movl	%eax, 3492(%rsp)
	movl	%r15d, 3496(%rsp)
	movl	%ebx, 3500(%rsp)
	movl	56(%rsp), %ecx                  ## 4-byte Reload
	shll	$2, %ecx
	movl	%r12d, 3504(%rsp)
	movq	112(%rsp), %rbx                 ## 8-byte Reload
	movl	%ebx, 3508(%rsp)
	movq	%rdx, 3512(%rsp)
	movq	$0, 3520(%rsp)
	movq	%r13, 3528(%rsp)
	movq	40(%rbp), %rax
	movq	%rax, 3536(%rsp)
	movq	128(%rsp), %rax                 ## 8-byte Reload
	movq	%rax, 3544(%rsp)
	movq	$0, 3552(%rsp)
	leaq	_train_cost_model.par_for.conv1_stage2.s1.c.c.c(%rip), %rsi
	leaq	3472(%rsp), %r8
	xorl	%edi, %edi
	xorl	%edx, %edx
	callq	_halide_do_par_for
	testl	%eax, %eax
	jne	LBB0_279
## %bb.119:                             ## %"assert succeeded339"
	movq	1152(%rsp), %rcx                ## 8-byte Reload
	addl	848(%rsp), %ecx                 ## 4-byte Folded Reload
	movq	136(%rsp), %rax                 ## 8-byte Reload
	cmpl	%eax, %ecx
	cmovlel	%eax, %ecx
	movq	1712(%rsp), %rax                ## 8-byte Reload
	cmpl	%eax, %ecx
	cmovlel	%eax, %ecx
	movl	144(%rsp), %eax                 ## 4-byte Reload
	cmpl	%eax, %ecx
	cmovlel	%eax, %ecx
	testl	%ecx, %ecx
	movl	$1, %eax
	cmovgl	%ecx, %eax
	subl	%r12d, %eax
	movl	%eax, %ecx
	sarl	$31, %ecx
	andnl	%eax, %ecx, %eax
	leaq	(,%rax,4), %rdx
	cmpl	$536870912, %eax                ## imm = 0x20000000
	jae	LBB0_309
## %bb.120:                             ## %"assert succeeded341"
	movq	%rbx, %r13
	addq	$12, %rdx
	xorl	%edi, %edi
	movq	%rdx, %rsi
	callq	_halide_malloc
	testq	%rax, %rax
	je	LBB0_310
## %bb.121:                             ## %"assert succeeded343"
	leal	7(%r15), %ebx
	sarl	$3, %ebx
	sarl	$3, %r15d
	movl	%r15d, 4144(%rsp)
	movl	%r12d, 4148(%rsp)
	movl	%r13d, 4152(%rsp)
	movq	%rax, 4160(%rsp)
	movq	$0, 4168(%rsp)
	movq	%rax, 56(%rsp)                  ## 8-byte Spill
	leaq	_train_cost_model.par_for.f1.s0.n.n(%rip), %rsi
	leaq	4144(%rsp), %r8
	xorl	%edi, %edi
	xorl	%edx, %edx
	movl	%ebx, %ecx
	callq	_halide_do_par_for
	testl	%eax, %eax
	jne	LBB0_313
## %bb.122:                             ## %"consume conv1_stage2"
	movq	%r12, %rcx
	vcvtsi2ssl	1336(%rsp), %xmm0, %xmm0 ## 4-byte Folded Reload
	movl	264(%rsp), %eax                 ## 4-byte Reload
	movl	%eax, 5856(%rsp)
	movq	120(%rsp), %r12                 ## 8-byte Reload
	movl	%r12d, 5860(%rsp)
	movq	216(%rsp), %rax                 ## 8-byte Reload
	movl	%eax, 5864(%rsp)
	movl	32(%rsp), %eax                  ## 4-byte Reload
	movl	%eax, 5868(%rsp)
	movl	%r15d, 5872(%rsp)
	movl	%ecx, 5876(%rsp)
	movl	160(%rsp), %eax                 ## 4-byte Reload
	movl	%eax, 5880(%rsp)
	vmovss	%xmm0, 432(%rsp)                ## 4-byte Spill
	vmovss	%xmm0, 5884(%rsp)
	movl	%r13d, 5888(%rsp)
	movq	168(%rsp), %rax                 ## 8-byte Reload
	movq	%rax, 5896(%rsp)
	movq	$0, 5904(%rsp)
	movq	56(%rsp), %r13                  ## 8-byte Reload
	movq	%r13, 5912(%rsp)
	movq	$0, 5920(%rsp)
	movq	184(%rsp), %rax                 ## 8-byte Reload
	movq	%rax, 5928(%rsp)
	movq	24(%rsp), %rax                  ## 8-byte Reload
	movq	%rax, 5936(%rsp)
	leaq	_train_cost_model.par_for.f1.s1.n.n(%rip), %rsi
	xorl	%r15d, %r15d
	leaq	5856(%rsp), %r8
	xorl	%edi, %edi
	xorl	%edx, %edx
	movl	%ebx, %ecx
	callq	_halide_do_par_for
	testl	%eax, %eax
	jne	LBB0_314
## %bb.123:                             ## %"produce sum$1_1_d_def__"
	movl	$1065353216, 7040(%rsp)         ## imm = 0x3F800000
	movq	136(%rsp), %r8                  ## 8-byte Reload
	leal	7(%r8), %r13d
	sarl	$3, %r13d
	leal	1(%r12), %eax
	sarl	%eax
	imull	%r13d, %eax
	xorl	%ecx, %ecx
	testl	%eax, %eax
	setle	%cl
	movq	%rax, 104(%rsp)                 ## 8-byte Spill
	addl	%ecx, %eax
	decl	%eax
	movl	%ecx, %esi
	negl	%esi
	xorl	%ecx, %ecx
	testl	%r13d, %r13d
	sete	%cl
	xorl	%edi, %edi
	cmpl	$-7, %r8d
	setl	%dil
	leal	(%rcx,%r13), %ebx
	cltd
	idivl	%ebx
	leal	(%rdi,%rdi), %edx
	decl	%edx
	andl	%esi, %edx
	addl	%eax, %edx
	decl	%ecx
	andl	%edx, %ecx
	testl	%r8d, %r8d
	movl	%ecx, %r14d
	cmovgl	%r15d, %r14d
	cmovlel	%r15d, %ecx
	leal	(%r14,%r14), %esi
	movl	%r12d, %eax
	subl	%esi, %eax
	cmpl	$2, %eax
	movl	$2, %edx
	cmovll	%eax, %edx
	leal	-1(%r12), %eax
	sarl	%eax
	cmpl	%ecx, %eax
	cmovgl	%ecx, %eax
	leal	(%rdx,%rax,2), %eax
	cmpl	%r12d, %eax
	cmovgel	%r12d, %eax
	movl	%esi, 336(%rsp)                 ## 4-byte Spill
	subl	%esi, %eax
	movl	196(%rsp), %edx                 ## 4-byte Reload
	movl	%edx, %ecx
	sarl	$31, %ecx
	andnl	%edx, %ecx, %ecx
	movl	%eax, %edx
	sarl	$31, %edx
	andnl	%eax, %edx, %eax
	movq	%rax, %rsi
	imulq	%rcx, %rsi
	leaq	(,%rcx,4), %rdx
	cmpq	$536870911, %rsi                ## imm = 0x1FFFFFFF
	ja	LBB0_280
## %bb.124:                             ## %"produce sum$1_1_d_def__"
	shrq	$30, %rcx
	movl	%edx, %edi
	imulq	%rax, %rdi
	shrq	$32, %rdi
	imulq	%rax, %rcx
	addq	%rdi, %rcx
	movabsq	$4294967296, %rdi               ## imm = 0x100000000
	andq	%rdi, %rcx
	jne	LBB0_280
## %bb.125:                             ## %"assert succeeded349"
	leaq	12(,%rsi,4), %rsi
	xorl	%edi, %edi
	callq	_halide_malloc
	testq	%rax, %rax
	movq	176(%rsp), %r15                 ## 8-byte Reload
	je	LBB0_315
## %bb.126:                             ## %"assert succeeded351"
	movq	%rax, %rsi
	movl	64(%rbp), %eax
	movq	136(%rsp), %rdx                 ## 8-byte Reload
	cmpl	%eax, %edx
	movl	224(%rsp), %ecx                 ## 4-byte Reload
	cmovgl	%eax, %ecx
	movl	%ecx, %eax
	sarl	$31, %eax
	andnl	%ecx, %eax, %edi
	movq	1128(%rsp), %rax                ## 8-byte Reload
	subl	%eax, %edi
	movl	%edx, 6000(%rsp)
	movl	196(%rsp), %ecx                 ## 4-byte Reload
	movl	%ecx, 6004(%rsp)
	movl	%r14d, 6008(%rsp)
	movl	%r12d, 6012(%rsp)
	movl	%r13d, 6016(%rsp)
	movl	%edi, 472(%rsp)                 ## 4-byte Spill
	movl	%edi, 6020(%rsp)
	movq	232(%rsp), %rcx                 ## 8-byte Reload
	movl	%ecx, 6024(%rsp)
	movl	%eax, 6028(%rsp)
	movq	%rsi, 40(%rsp)                  ## 8-byte Spill
	movq	%rsi, 6032(%rsp)
	movq	$0, 6040(%rsp)
	movq	56(%rsp), %rax                  ## 8-byte Reload
	movq	%rax, 6048(%rsp)
	movq	$0, 6056(%rsp)
	leaq	7040(%rsp), %rax
	movq	%rax, 6064(%rsp)
	movq	$0, 6072(%rsp)
	movq	1200(%rsp), %rax                ## 8-byte Reload
	movq	%rax, 6080(%rsp)
	movq	72(%rbp), %rax
	movq	%rax, 6088(%rsp)
	leaq	_train_cost_model.par_for.f0_0_d_def__.s0.n.n.n(%rip), %rsi
	leaq	6000(%rsp), %r8
	xorl	%edi, %edi
	xorl	%edx, %edx
	movq	104(%rsp), %rcx                 ## 8-byte Reload
                                        ## kill: def $ecx killed $ecx killed $rcx
	callq	_halide_do_par_for
	testl	%eax, %eax
	jne	LBB0_317
## %bb.127:                             ## %"assert succeeded353"
	movq	%r14, %rdi
	movq	%r13, 112(%rsp)                 ## 8-byte Spill
	movl	%edi, %eax
	sarl	$31, %eax
	andl	%edi, %eax
	movq	%rax, 144(%rsp)                 ## 8-byte Spill
	leal	(%rax,%rax), %ecx
	movl	%r12d, %eax
	movl	%ecx, 248(%rsp)                 ## 4-byte Spill
	subl	%ecx, %eax
	movl	%eax, %ecx
	sarl	$31, %ecx
	andnl	%eax, %ecx, %eax
	movq	%r15, %rdx
	movl	%edx, %ecx
	sarl	$31, %ecx
	andnl	%edx, %ecx, %ebx
	movq	%rbx, %rsi
	imulq	%rax, %rsi
	shlq	$7, %rsi
	movq	%rax, %rdx
	shlq	$7, %rdx
	movl	$2147483648, %ecx               ## imm = 0x80000000
	cmpq	%rcx, %rsi
	jae	LBB0_281
## %bb.128:                             ## %"assert succeeded353"
	movabsq	$270582939648, %r8              ## imm = 0x3F00000000
	shrq	$25, %rax
	movl	%edx, %ecx
	imulq	%rbx, %rcx
	shrq	$32, %rcx
	imulq	%rbx, %rax
	addq	%rcx, %rax
	andq	%r8, %rax
	jne	LBB0_281
## %bb.129:                             ## %"assert succeeded355"
	movq	%rbx, 456(%rsp)                 ## 8-byte Spill
	movq	%rdi, 152(%rsp)                 ## 8-byte Spill
	orq	$12, %rsi
	xorl	%edi, %edi
	callq	_halide_malloc
	movq	%rax, 80(%rsp)                  ## 8-byte Spill
	testq	%rax, %rax
	movq	184(%rsp), %r12                 ## 8-byte Reload
	je	LBB0_318
## %bb.130:                             ## %"assert succeeded357"
	movq	136(%rsp), %rax                 ## 8-byte Reload
	movl	%eax, 2584(%rsp)
	movq	120(%rsp), %rax                 ## 8-byte Reload
	movl	%eax, 2588(%rsp)
	movl	%r15d, 2592(%rsp)
	movq	112(%rsp), %rax                 ## 8-byte Reload
	movl	%eax, 2596(%rsp)
	movq	144(%rsp), %rax                 ## 8-byte Reload
	movl	%eax, 2600(%rsp)
	movq	80(%rsp), %rax                  ## 8-byte Reload
	movq	%rax, 2608(%rsp)
	movq	$0, 2616(%rsp)
	leaq	_train_cost_model.par_for.relu1_0_d_def__.s0.n.n.n(%rip), %rsi
	leaq	2584(%rsp), %r8
	xorl	%edi, %edi
	xorl	%edx, %edx
	movq	104(%rsp), %rcx                 ## 8-byte Reload
                                        ## kill: def $ecx killed $ecx killed $rcx
	callq	_halide_do_par_for
	testl	%eax, %eax
	jne	LBB0_264
## %bb.131:                             ## %"consume f0_0_d_def__"
	movl	$21, %eax
	subl	288(%rsp), %eax                 ## 4-byte Folded Reload
	movq	216(%rsp), %rbx                 ## 8-byte Reload
	imull	%ebx, %eax
	subl	320(%rsp), %eax                 ## 4-byte Folded Reload
	movq	304(%rsp), %rcx                 ## 8-byte Reload
	subl	%ecx, %eax
	movq	136(%rsp), %rsi                 ## 8-byte Reload
	movl	%esi, 3560(%rsp)
	movl	196(%rsp), %edx                 ## 4-byte Reload
	movl	%edx, 3564(%rsp)
	movq	152(%rsp), %rdx                 ## 8-byte Reload
	movl	%edx, 3568(%rsp)
	movq	120(%rsp), %rdx                 ## 8-byte Reload
	movl	%edx, 3572(%rsp)
	movl	%r15d, 3576(%rsp)
	movl	32(%rsp), %edx                  ## 4-byte Reload
	movl	%edx, 3580(%rsp)
	movq	112(%rsp), %rdx                 ## 8-byte Reload
	movl	%edx, 3584(%rsp)
	movl	%eax, 3588(%rsp)
	leal	(%rbx,%rbx,4), %eax
	leal	(%rbx,%rax,4), %eax
	addl	%esi, %eax
	subl	312(%rsp), %eax                 ## 4-byte Folded Reload
	subl	%ecx, %eax
	movl	%eax, 3592(%rsp)
	movq	144(%rsp), %rax                 ## 8-byte Reload
	movl	%eax, 3596(%rsp)
	movq	40(%rsp), %rax                  ## 8-byte Reload
	movq	%rax, 3600(%rsp)
	movq	$0, 3608(%rsp)
	movq	80(%rsp), %rax                  ## 8-byte Reload
	movq	%rax, 3616(%rsp)
	movq	$0, 3624(%rsp)
	movq	%r12, 3632(%rsp)
	movq	24(%rsp), %r15                  ## 8-byte Reload
	movq	%r15, 3640(%rsp)
	leaq	_train_cost_model.par_for.relu1_0_d_def__.s6.n.n.n(%rip), %rsi
	leaq	3560(%rsp), %r8
	xorl	%edi, %edi
	xorl	%edx, %edx
	movq	104(%rsp), %rcx                 ## 8-byte Reload
                                        ## kill: def $ecx killed $ecx killed $rcx
	callq	_halide_do_par_for
	testl	%eax, %eax
	jne	LBB0_264
## %bb.132:                             ## %"consume f0_0_d_def__362"
	movq	%r12, %rcx
	leal	(%rbx,%rbx,8), %r12d
	movq	136(%rsp), %rax                 ## 8-byte Reload
	movl	%eax, 4176(%rsp)
	movl	196(%rsp), %eax                 ## 4-byte Reload
	movl	%eax, 4180(%rsp)
	movq	152(%rsp), %rax                 ## 8-byte Reload
	movl	%eax, 4184(%rsp)
	movq	120(%rsp), %rax                 ## 8-byte Reload
	movl	%eax, 4188(%rsp)
	movq	176(%rsp), %rax                 ## 8-byte Reload
	movl	%eax, 4192(%rsp)
	movl	%ebx, 4196(%rsp)
	movl	32(%rsp), %eax                  ## 4-byte Reload
	movl	%eax, 4200(%rsp)
	movq	112(%rsp), %rax                 ## 8-byte Reload
	movl	%eax, 4204(%rsp)
	movq	144(%rsp), %rax                 ## 8-byte Reload
	movl	%eax, 4208(%rsp)
	movl	160(%rsp), %eax                 ## 4-byte Reload
	movl	%eax, 4212(%rsp)
	movl	%r12d, 4216(%rsp)
	movq	40(%rsp), %rax                  ## 8-byte Reload
	movq	%rax, 4224(%rsp)
	movq	$0, 4232(%rsp)
	movq	80(%rsp), %rax                  ## 8-byte Reload
	movq	%rax, 4240(%rsp)
	movq	$0, 4248(%rsp)
	movq	%rcx, %r13
	movq	%rcx, 4256(%rsp)
	movq	%r15, 4264(%rsp)
	leaq	_train_cost_model.par_for.relu1_0_d_def__.s7.n.n.n(%rip), %rsi
	leaq	4176(%rsp), %r8
	xorl	%edi, %edi
	xorl	%edx, %edx
	movq	104(%rsp), %rcx                 ## 8-byte Reload
                                        ## kill: def $ecx killed $ecx killed $rcx
	callq	_halide_do_par_for
	testl	%eax, %eax
	jne	LBB0_264
## %bb.133:                             ## %"consume f0_0_d_def__365"
	movq	136(%rsp), %rax                 ## 8-byte Reload
	movl	%eax, 4272(%rsp)
	movl	196(%rsp), %eax                 ## 4-byte Reload
	movl	%eax, 4276(%rsp)
	movq	152(%rsp), %rax                 ## 8-byte Reload
	movl	%eax, 4280(%rsp)
	movq	120(%rsp), %rax                 ## 8-byte Reload
	movl	%eax, 4284(%rsp)
	movq	176(%rsp), %rax                 ## 8-byte Reload
	movl	%eax, 4288(%rsp)
	movl	%ebx, 4292(%rsp)
	movl	32(%rsp), %eax                  ## 4-byte Reload
	movl	%eax, 4296(%rsp)
	movq	112(%rsp), %rax                 ## 8-byte Reload
	movl	%eax, 4300(%rsp)
	movq	144(%rsp), %rax                 ## 8-byte Reload
	movl	%eax, 4304(%rsp)
	movl	160(%rsp), %eax                 ## 4-byte Reload
	movl	%eax, 4308(%rsp)
	movl	%r12d, 208(%rsp)                ## 4-byte Spill
	movl	%r12d, 4312(%rsp)
	movq	40(%rsp), %r12                  ## 8-byte Reload
	movq	%r12, 4320(%rsp)
	movq	$0, 4328(%rsp)
	movq	80(%rsp), %rax                  ## 8-byte Reload
	movq	%rax, 4336(%rsp)
	movq	$0, 4344(%rsp)
	movq	%r13, 4352(%rsp)
	movq	%r15, 4360(%rsp)
	leaq	_train_cost_model.par_for.relu1_0_d_def__.s8.n.n.n(%rip), %rsi
	leaq	4272(%rsp), %r8
	xorl	%edi, %edi
	xorl	%edx, %edx
	movq	104(%rsp), %rcx                 ## 8-byte Reload
                                        ## kill: def $ecx killed $ecx killed $rcx
	callq	_halide_do_par_for
	testl	%eax, %eax
	jne	LBB0_264
## %bb.134:                             ## %"consume f0_0_d_def__368"
	movq	%r13, %rbx
	movq	136(%rsp), %rcx                 ## 8-byte Reload
	movl	%ecx, %eax
	subl	320(%rsp), %eax                 ## 4-byte Folded Reload
	subl	256(%rsp), %eax                 ## 4-byte Folded Reload
	subl	304(%rsp), %eax                 ## 4-byte Folded Reload
	movl	%ecx, 2624(%rsp)
	movl	196(%rsp), %r13d                ## 4-byte Reload
	movl	%r13d, 2628(%rsp)
	movq	152(%rsp), %rcx                 ## 8-byte Reload
	movl	%ecx, 2632(%rsp)
	movq	120(%rsp), %rcx                 ## 8-byte Reload
	movl	%ecx, 2636(%rsp)
	movq	176(%rsp), %rcx                 ## 8-byte Reload
	movl	%ecx, 2640(%rsp)
	movq	440(%rsp), %rcx                 ## 8-byte Reload
	movl	%ecx, 2644(%rsp)
	movl	32(%rsp), %ecx                  ## 4-byte Reload
	movl	%ecx, 2648(%rsp)
	movq	112(%rsp), %rcx                 ## 8-byte Reload
	movl	%ecx, 2652(%rsp)
	movq	312(%rsp), %rcx                 ## 8-byte Reload
	movl	%ecx, 2656(%rsp)
	movl	160(%rsp), %ecx                 ## 4-byte Reload
	movl	%ecx, 2660(%rsp)
	movl	%eax, 2664(%rsp)
	movq	144(%rsp), %rax                 ## 8-byte Reload
	movl	%eax, 2668(%rsp)
	movq	%r12, 2672(%rsp)
	movq	$0, 2680(%rsp)
	movq	80(%rsp), %rax                  ## 8-byte Reload
	movq	%rax, 2688(%rsp)
	movq	$0, 2696(%rsp)
	movq	%rbx, 2704(%rsp)
	movq	%r15, 2712(%rsp)
	leaq	_train_cost_model.par_for.relu1_0_d_def__.s9.n.n.n(%rip), %rsi
	leaq	2624(%rsp), %r8
	xorl	%edi, %edi
	xorl	%edx, %edx
	movq	104(%rsp), %rcx                 ## 8-byte Reload
                                        ## kill: def $ecx killed $ecx killed $rcx
	callq	_halide_do_par_for
	testl	%eax, %eax
	jne	LBB0_264
## %bb.135:                             ## %"consume f0_0_d_def__371"
	movq	%rbx, %rdx
	movq	216(%rsp), %rcx                 ## 8-byte Reload
	leal	(%rcx,%rcx), %eax
	movq	%rax, 448(%rsp)                 ## 8-byte Spill
	leal	(%rax,%rax,4), %r12d
	movl	%ecx, %r15d
	shll	$5, %r15d
	leal	(%r15,%rcx,2), %eax
	movq	136(%rsp), %rcx                 ## 8-byte Reload
	movl	%ecx, 2720(%rsp)
	movl	%r13d, 2724(%rsp)
	movq	152(%rsp), %rcx                 ## 8-byte Reload
	movl	%ecx, 2728(%rsp)
	movq	120(%rsp), %rcx                 ## 8-byte Reload
	movl	%ecx, 2732(%rsp)
	movq	176(%rsp), %rcx                 ## 8-byte Reload
	movl	%ecx, 2736(%rsp)
	movl	32(%rsp), %ecx                  ## 4-byte Reload
	movl	%ecx, 2740(%rsp)
	movq	112(%rsp), %rcx                 ## 8-byte Reload
	movl	%ecx, 2744(%rsp)
	movq	144(%rsp), %rcx                 ## 8-byte Reload
	movl	%ecx, 2748(%rsp)
	movl	160(%rsp), %ecx                 ## 4-byte Reload
	movl	%ecx, 2752(%rsp)
	movl	208(%rsp), %ebx                 ## 4-byte Reload
	movl	%ebx, 2756(%rsp)
	movl	%r12d, 2760(%rsp)
	movl	%eax, 2764(%rsp)
	movq	40(%rsp), %rax                  ## 8-byte Reload
	movq	%rax, 2768(%rsp)
	movq	$0, 2776(%rsp)
	movq	80(%rsp), %rax                  ## 8-byte Reload
	movq	%rax, 2784(%rsp)
	movq	$0, 2792(%rsp)
	movq	%rdx, 2800(%rsp)
	movq	24(%rsp), %rax                  ## 8-byte Reload
	movq	%rax, 2808(%rsp)
	leaq	_train_cost_model.par_for.relu1_0_d_def__.s10.n.n.n(%rip), %rsi
	leaq	2720(%rsp), %r8
	xorl	%edi, %edi
	xorl	%edx, %edx
	movq	104(%rsp), %rcx                 ## 8-byte Reload
                                        ## kill: def $ecx killed $ecx killed $rcx
	callq	_halide_do_par_for
	testl	%eax, %eax
	jne	LBB0_264
## %bb.136:                             ## %"consume f0_0_d_def__374"
	movq	216(%rsp), %rax                 ## 8-byte Reload
	addl	%r15d, %eax
	movq	136(%rsp), %rcx                 ## 8-byte Reload
	movl	%ecx, 2816(%rsp)
	movl	%r13d, 2820(%rsp)
	movq	152(%rsp), %rcx                 ## 8-byte Reload
	movl	%ecx, 2824(%rsp)
	movq	120(%rsp), %rcx                 ## 8-byte Reload
	movl	%ecx, 2828(%rsp)
	movq	176(%rsp), %rcx                 ## 8-byte Reload
	movl	%ecx, 2832(%rsp)
	movl	32(%rsp), %ecx                  ## 4-byte Reload
	movl	%ecx, 2836(%rsp)
	movq	112(%rsp), %rcx                 ## 8-byte Reload
	movl	%ecx, 2840(%rsp)
	movq	144(%rsp), %rcx                 ## 8-byte Reload
	movl	%ecx, 2844(%rsp)
	movl	160(%rsp), %ecx                 ## 4-byte Reload
	movl	%ecx, 2848(%rsp)
	movl	%ebx, 2852(%rsp)
	movl	%r12d, 224(%rsp)                ## 4-byte Spill
	movl	%r12d, 2856(%rsp)
	movl	%eax, 2860(%rsp)
	movq	40(%rsp), %rax                  ## 8-byte Reload
	movq	%rax, 2864(%rsp)
	movq	$0, 2872(%rsp)
	movq	80(%rsp), %rax                  ## 8-byte Reload
	movq	%rax, 2880(%rsp)
	movq	$0, 2888(%rsp)
	movq	184(%rsp), %r12                 ## 8-byte Reload
	movq	%r12, 2896(%rsp)
	movq	24(%rsp), %rbx                  ## 8-byte Reload
	movq	%rbx, 2904(%rsp)
	leaq	_train_cost_model.par_for.relu1_0_d_def__.s11.n.n.n(%rip), %rsi
	leaq	2816(%rsp), %r8
	xorl	%edi, %edi
	xorl	%edx, %edx
	movq	104(%rsp), %rcx                 ## 8-byte Reload
                                        ## kill: def $ecx killed $ecx killed $rcx
	callq	_halide_do_par_for
	testl	%eax, %eax
	jne	LBB0_264
## %bb.137:                             ## %"consume f0_0_d_def__377"
	movq	216(%rsp), %rax                 ## 8-byte Reload
	leal	(,%rax,8), %eax
	leal	(%rax,%rax,2), %ecx
	movq	136(%rsp), %rax                 ## 8-byte Reload
	movl	%eax, 4368(%rsp)
	movl	%r13d, 4372(%rsp)
	movq	152(%rsp), %rax                 ## 8-byte Reload
	movl	%eax, 4376(%rsp)
	movq	120(%rsp), %rax                 ## 8-byte Reload
	movl	%eax, 4380(%rsp)
	movq	176(%rsp), %rax                 ## 8-byte Reload
	movl	%eax, 4384(%rsp)
	movl	32(%rsp), %eax                  ## 4-byte Reload
	movl	%eax, 4388(%rsp)
	movq	112(%rsp), %rax                 ## 8-byte Reload
	movl	%eax, 4392(%rsp)
	movq	144(%rsp), %rax                 ## 8-byte Reload
	movl	%eax, 4396(%rsp)
	movl	160(%rsp), %eax                 ## 4-byte Reload
	movl	%eax, 4400(%rsp)
	movl	%ecx, 288(%rsp)                 ## 4-byte Spill
	movl	%ecx, 4404(%rsp)
	movl	%r15d, 4408(%rsp)
	movq	40(%rsp), %rax                  ## 8-byte Reload
	movq	%rax, 4416(%rsp)
	movq	$0, 4424(%rsp)
	movq	80(%rsp), %rax                  ## 8-byte Reload
	movq	%rax, 4432(%rsp)
	movq	$0, 4440(%rsp)
	movq	%r12, 4448(%rsp)
	movq	%rbx, 4456(%rsp)
	leaq	_train_cost_model.par_for.relu1_0_d_def__.s12.n.n.n(%rip), %rsi
	leaq	4368(%rsp), %r8
	xorl	%edi, %edi
	xorl	%edx, %edx
	movq	104(%rsp), %rcx                 ## 8-byte Reload
                                        ## kill: def $ecx killed $ecx killed $rcx
	callq	_halide_do_par_for
	testl	%eax, %eax
	jne	LBB0_264
## %bb.138:                             ## %"consume f0_0_d_def__380"
	movq	216(%rsp), %rax                 ## 8-byte Reload
	leal	(%rax,%rax,4), %eax
	leal	(%rax,%rax,4), %ecx
	movq	136(%rsp), %rax                 ## 8-byte Reload
	movl	%eax, 4464(%rsp)
	movl	%r13d, 4468(%rsp)
	movq	152(%rsp), %rax                 ## 8-byte Reload
	movl	%eax, 4472(%rsp)
	movq	120(%rsp), %rax                 ## 8-byte Reload
	movl	%eax, 4476(%rsp)
	movq	176(%rsp), %rax                 ## 8-byte Reload
	movl	%eax, 4480(%rsp)
	movl	32(%rsp), %eax                  ## 4-byte Reload
	movl	%eax, 4484(%rsp)
	movq	112(%rsp), %rax                 ## 8-byte Reload
	movl	%eax, 4488(%rsp)
	movq	144(%rsp), %rax                 ## 8-byte Reload
	movl	%eax, 4492(%rsp)
	movl	160(%rsp), %eax                 ## 4-byte Reload
	movl	%eax, 4496(%rsp)
	movl	%ecx, 280(%rsp)                 ## 4-byte Spill
	movl	%ecx, 4500(%rsp)
	movl	%r15d, 4504(%rsp)
	movq	40(%rsp), %rax                  ## 8-byte Reload
	movq	%rax, 4512(%rsp)
	movq	$0, 4520(%rsp)
	movq	80(%rsp), %rax                  ## 8-byte Reload
	movq	%rax, 4528(%rsp)
	movq	$0, 4536(%rsp)
	movq	%r12, 4544(%rsp)
	movq	%rbx, 4552(%rsp)
	leaq	_train_cost_model.par_for.relu1_0_d_def__.s13.n.n.n(%rip), %rsi
	leaq	4464(%rsp), %r8
	xorl	%edi, %edi
	xorl	%edx, %edx
	movq	104(%rsp), %rcx                 ## 8-byte Reload
                                        ## kill: def $ecx killed $ecx killed $rcx
	callq	_halide_do_par_for
	testl	%eax, %eax
	jne	LBB0_264
## %bb.139:                             ## %"consume f0_0_d_def__383"
	subl	216(%rsp), %r15d                ## 4-byte Folded Reload
	movq	136(%rsp), %rax                 ## 8-byte Reload
	movl	%eax, 4560(%rsp)
	movl	%r13d, 4564(%rsp)
	movq	152(%rsp), %rax                 ## 8-byte Reload
	movl	%eax, 4568(%rsp)
	movq	120(%rsp), %rax                 ## 8-byte Reload
	movl	%eax, 4572(%rsp)
	movq	176(%rsp), %r12                 ## 8-byte Reload
	movl	%r12d, 4576(%rsp)
	movl	32(%rsp), %eax                  ## 4-byte Reload
	movl	%eax, 4580(%rsp)
	movq	112(%rsp), %rax                 ## 8-byte Reload
	movl	%eax, 4584(%rsp)
	movq	144(%rsp), %rax                 ## 8-byte Reload
	movl	%eax, 4588(%rsp)
	movl	160(%rsp), %eax                 ## 4-byte Reload
	movl	%eax, 4592(%rsp)
	movl	288(%rsp), %eax                 ## 4-byte Reload
	movl	%eax, 4596(%rsp)
	movl	%r15d, 4600(%rsp)
	movq	40(%rsp), %rax                  ## 8-byte Reload
	movq	%rax, 4608(%rsp)
	movq	$0, 4616(%rsp)
	movq	80(%rsp), %rax                  ## 8-byte Reload
	movq	%rax, 4624(%rsp)
	movq	$0, 4632(%rsp)
	movq	184(%rsp), %rax                 ## 8-byte Reload
	movq	%rax, 4640(%rsp)
	movq	24(%rsp), %rax                  ## 8-byte Reload
	movq	%rax, 4648(%rsp)
	leaq	_train_cost_model.par_for.relu1_0_d_def__.s14.n.n.n(%rip), %rsi
	leaq	4560(%rsp), %r8
	xorl	%edi, %edi
	xorl	%edx, %edx
	movq	104(%rsp), %rcx                 ## 8-byte Reload
                                        ## kill: def $ecx killed $ecx killed $rcx
	callq	_halide_do_par_for
	testl	%eax, %eax
	jne	LBB0_264
## %bb.140:                             ## %"consume f0_0_d_def__386"
	movq	136(%rsp), %rax                 ## 8-byte Reload
	movl	%eax, 4656(%rsp)
	movl	%r13d, 4660(%rsp)
	movq	152(%rsp), %rax                 ## 8-byte Reload
	movl	%eax, 4664(%rsp)
	movq	120(%rsp), %rax                 ## 8-byte Reload
	movl	%eax, 4668(%rsp)
	movl	%r12d, 4672(%rsp)
	movl	32(%rsp), %eax                  ## 4-byte Reload
	movl	%eax, 4676(%rsp)
	movq	112(%rsp), %rax                 ## 8-byte Reload
	movl	%eax, 4680(%rsp)
	movq	144(%rsp), %rax                 ## 8-byte Reload
	movl	%eax, 4684(%rsp)
	movl	160(%rsp), %eax                 ## 4-byte Reload
	movl	%eax, 4688(%rsp)
	movl	280(%rsp), %eax                 ## 4-byte Reload
	movl	%eax, 4692(%rsp)
	movq	%r15, 272(%rsp)                 ## 8-byte Spill
	movl	%r15d, 4696(%rsp)
	movq	40(%rsp), %rax                  ## 8-byte Reload
	movq	%rax, 4704(%rsp)
	movq	$0, 4712(%rsp)
	movq	80(%rsp), %rax                  ## 8-byte Reload
	movq	%rax, 4720(%rsp)
	movq	$0, 4728(%rsp)
	movq	184(%rsp), %rax                 ## 8-byte Reload
	movq	%rax, 4736(%rsp)
	movq	24(%rsp), %r15                  ## 8-byte Reload
	movq	%r15, 4744(%rsp)
	leaq	_train_cost_model.par_for.relu1_0_d_def__.s15.n.n.n(%rip), %rsi
	leaq	4656(%rsp), %r8
	xorl	%edi, %edi
	xorl	%edx, %edx
	movq	104(%rsp), %rcx                 ## 8-byte Reload
                                        ## kill: def $ecx killed $ecx killed $rcx
	callq	_halide_do_par_for
	testl	%eax, %eax
	jne	LBB0_264
## %bb.141:                             ## %"consume f0_0_d_def__389"
	movq	216(%rsp), %rbx                 ## 8-byte Reload
	leal	(%rbx,%rbx,8), %eax
	leal	(%rax,%rax,2), %eax
	movq	136(%rsp), %rcx                 ## 8-byte Reload
	movl	%ecx, 4752(%rsp)
	movl	%r13d, 4756(%rsp)
	movq	152(%rsp), %rcx                 ## 8-byte Reload
	movl	%ecx, 4760(%rsp)
	movq	120(%rsp), %rcx                 ## 8-byte Reload
	movl	%ecx, 4764(%rsp)
	movl	%r12d, 4768(%rsp)
	movl	32(%rsp), %ecx                  ## 4-byte Reload
	movl	%ecx, 4772(%rsp)
	movq	112(%rsp), %rcx                 ## 8-byte Reload
	movl	%ecx, 4776(%rsp)
	movq	144(%rsp), %rcx                 ## 8-byte Reload
	movl	%ecx, 4780(%rsp)
	movl	160(%rsp), %ecx                 ## 4-byte Reload
	movl	%ecx, 4784(%rsp)
	movl	288(%rsp), %ecx                 ## 4-byte Reload
	movl	%ecx, 4788(%rsp)
	movl	%eax, 4792(%rsp)
	movq	40(%rsp), %rax                  ## 8-byte Reload
	movq	%rax, 4800(%rsp)
	movq	$0, 4808(%rsp)
	movq	80(%rsp), %rax                  ## 8-byte Reload
	movq	%rax, 4816(%rsp)
	movq	$0, 4824(%rsp)
	movq	184(%rsp), %rax                 ## 8-byte Reload
	movq	%rax, 4832(%rsp)
	movq	%r15, 4840(%rsp)
	leaq	_train_cost_model.par_for.relu1_0_d_def__.s16.n.n.n(%rip), %rsi
	leaq	4752(%rsp), %r8
	xorl	%edi, %edi
	xorl	%edx, %edx
	movq	104(%rsp), %rcx                 ## 8-byte Reload
                                        ## kill: def $ecx killed $ecx killed $rcx
	callq	_halide_do_par_for
	testl	%eax, %eax
	jne	LBB0_264
## %bb.142:                             ## %"consume f0_0_d_def__392"
	leal	(%rbx,%rbx,8), %eax
	leal	(%rax,%rax,2), %eax
	addl	%ebx, %eax
	movq	136(%rsp), %rcx                 ## 8-byte Reload
	movl	%ecx, 4848(%rsp)
	movl	%r13d, 4852(%rsp)
	movq	152(%rsp), %rcx                 ## 8-byte Reload
	movl	%ecx, 4856(%rsp)
	movq	120(%rsp), %rcx                 ## 8-byte Reload
	movl	%ecx, 4860(%rsp)
	movl	%r12d, 4864(%rsp)
	movl	32(%rsp), %ecx                  ## 4-byte Reload
	movl	%ecx, 4868(%rsp)
	movq	112(%rsp), %rcx                 ## 8-byte Reload
	movl	%ecx, 4872(%rsp)
	movq	144(%rsp), %rcx                 ## 8-byte Reload
	movl	%ecx, 4876(%rsp)
	movl	160(%rsp), %ecx                 ## 4-byte Reload
	movl	%ecx, 4880(%rsp)
	movl	280(%rsp), %ecx                 ## 4-byte Reload
	movl	%ecx, 4884(%rsp)
	movl	%eax, 4888(%rsp)
	movq	40(%rsp), %rax                  ## 8-byte Reload
	movq	%rax, 4896(%rsp)
	movq	$0, 4904(%rsp)
	movq	80(%rsp), %rax                  ## 8-byte Reload
	movq	%rax, 4912(%rsp)
	movq	$0, 4920(%rsp)
	movq	184(%rsp), %rax                 ## 8-byte Reload
	movq	%rax, 4928(%rsp)
	movq	%r15, 4936(%rsp)
	leaq	_train_cost_model.par_for.relu1_0_d_def__.s17.n.n.n(%rip), %rsi
	leaq	4848(%rsp), %r8
	xorl	%edi, %edi
	xorl	%edx, %edx
	movq	104(%rsp), %rcx                 ## 8-byte Reload
                                        ## kill: def $ecx killed $ecx killed $rcx
	callq	_halide_do_par_for
	testl	%eax, %eax
	jne	LBB0_264
## %bb.143:                             ## %"consume f0_0_d_def__395"
	leal	(%rbx,%rbx,4), %eax
	leal	(%rax,%rax,4), %eax
	addl	%ebx, %eax
	movq	136(%rsp), %rcx                 ## 8-byte Reload
	movl	%ecx, 4944(%rsp)
	movl	%r13d, %ebx
	movl	%r13d, 4948(%rsp)
	movq	152(%rsp), %rcx                 ## 8-byte Reload
	movl	%ecx, 4952(%rsp)
	movq	120(%rsp), %rcx                 ## 8-byte Reload
	movl	%ecx, 4956(%rsp)
	movl	%r12d, 4960(%rsp)
	movl	32(%rsp), %ecx                  ## 4-byte Reload
	movl	%ecx, 4964(%rsp)
	movq	112(%rsp), %rcx                 ## 8-byte Reload
	movl	%ecx, 4968(%rsp)
	movq	144(%rsp), %rcx                 ## 8-byte Reload
	movl	%ecx, 4972(%rsp)
	movl	160(%rsp), %ecx                 ## 4-byte Reload
	movl	%ecx, 4976(%rsp)
	movl	288(%rsp), %ecx                 ## 4-byte Reload
	movl	%ecx, 4980(%rsp)
	movl	%eax, 4984(%rsp)
	movq	40(%rsp), %r12                  ## 8-byte Reload
	movq	%r12, 4992(%rsp)
	movq	$0, 5000(%rsp)
	movq	80(%rsp), %rax                  ## 8-byte Reload
	movq	%rax, 5008(%rsp)
	movq	$0, 5016(%rsp)
	movq	184(%rsp), %rax                 ## 8-byte Reload
	movq	%rax, 5024(%rsp)
	movq	24(%rsp), %r13                  ## 8-byte Reload
	movq	%r13, 5032(%rsp)
	leaq	_train_cost_model.par_for.relu1_0_d_def__.s18.n.n.n(%rip), %rsi
	leaq	4944(%rsp), %r8
	xorl	%edi, %edi
	xorl	%edx, %edx
	movq	104(%rsp), %rcx                 ## 8-byte Reload
                                        ## kill: def $ecx killed $ecx killed $rcx
	callq	_halide_do_par_for
	testl	%eax, %eax
	jne	LBB0_264
## %bb.144:                             ## %"consume f0_0_d_def__398"
	movq	%r12, %rsi
	movq	448(%rsp), %rax                 ## 8-byte Reload
	leal	(%rax,%rax,8), %eax
	movl	%eax, %ecx
	movq	304(%rsp), %r12                 ## 8-byte Reload
	subl	%r12d, %ecx
	movq	136(%rsp), %rdx                 ## 8-byte Reload
	movl	%edx, 1960(%rsp)
	movl	%ebx, 1964(%rsp)
	movq	152(%rsp), %rdx                 ## 8-byte Reload
	movl	%edx, 1968(%rsp)
	movq	120(%rsp), %rdx                 ## 8-byte Reload
	movl	%edx, 1972(%rsp)
	movq	176(%rsp), %rdx                 ## 8-byte Reload
	movl	%edx, 1976(%rsp)
	movq	320(%rsp), %rdx                 ## 8-byte Reload
	movl	%edx, 1980(%rsp)
	movq	440(%rsp), %rbx                 ## 8-byte Reload
	movl	%ebx, 1984(%rsp)
	movl	32(%rsp), %edx                  ## 4-byte Reload
	movl	%edx, 1988(%rsp)
	movq	112(%rsp), %rdx                 ## 8-byte Reload
	movl	%edx, 1992(%rsp)
	movq	144(%rsp), %rdx                 ## 8-byte Reload
	movl	%edx, 1996(%rsp)
	movl	%ecx, 2000(%rsp)
	movl	160(%rsp), %ecx                 ## 4-byte Reload
	movl	%ecx, 2004(%rsp)
	movq	256(%rsp), %r15                 ## 8-byte Reload
	movl	%r15d, 2008(%rsp)
	movl	%eax, 2012(%rsp)
	movq	%rsi, 2016(%rsp)
	movq	$0, 2024(%rsp)
	movq	80(%rsp), %rax                  ## 8-byte Reload
	movq	%rax, 2032(%rsp)
	movq	$0, 2040(%rsp)
	movq	184(%rsp), %rax                 ## 8-byte Reload
	movq	%rax, 2048(%rsp)
	movq	%r13, 2056(%rsp)
	leaq	_train_cost_model.par_for.relu1_0_d_def__.s19.n.n.n(%rip), %rsi
	leaq	1960(%rsp), %r8
	xorl	%edi, %edi
	xorl	%edx, %edx
	movq	104(%rsp), %rcx                 ## 8-byte Reload
                                        ## kill: def $ecx killed $ecx killed $rcx
	callq	_halide_do_par_for
	testl	%eax, %eax
	jne	LBB0_264
## %bb.145:                             ## %"consume f0_0_d_def__401"
	movq	216(%rsp), %r13                 ## 8-byte Reload
	leal	(%r13,%r13,8), %eax
	leal	(%r13,%rax,2), %eax
	movl	%eax, %ecx
	subl	%r12d, %ecx
	movq	136(%rsp), %rdx                 ## 8-byte Reload
	movl	%edx, 2064(%rsp)
	movl	196(%rsp), %edx                 ## 4-byte Reload
	movl	%edx, 2068(%rsp)
	movq	152(%rsp), %rdx                 ## 8-byte Reload
	movl	%edx, 2072(%rsp)
	movq	120(%rsp), %rdx                 ## 8-byte Reload
	movl	%edx, 2076(%rsp)
	movq	176(%rsp), %rdx                 ## 8-byte Reload
	movl	%edx, 2080(%rsp)
	movq	320(%rsp), %rdx                 ## 8-byte Reload
	movl	%edx, 2084(%rsp)
	movl	%ebx, 2088(%rsp)
	movl	32(%rsp), %edx                  ## 4-byte Reload
	movl	%edx, 2092(%rsp)
	movq	112(%rsp), %r12                 ## 8-byte Reload
	movl	%r12d, 2096(%rsp)
	movq	144(%rsp), %rdx                 ## 8-byte Reload
	movl	%edx, 2100(%rsp)
	movl	%ecx, 2104(%rsp)
	movl	160(%rsp), %ecx                 ## 4-byte Reload
	movl	%ecx, 2108(%rsp)
	movl	%r15d, 2112(%rsp)
	movl	%eax, 2116(%rsp)
	movq	40(%rsp), %rbx                  ## 8-byte Reload
	movq	%rbx, 2120(%rsp)
	movq	$0, 2128(%rsp)
	movq	80(%rsp), %rax                  ## 8-byte Reload
	movq	%rax, 2136(%rsp)
	movq	$0, 2144(%rsp)
	movq	184(%rsp), %rax                 ## 8-byte Reload
	movq	%rax, 2152(%rsp)
	movq	24(%rsp), %rax                  ## 8-byte Reload
	movq	%rax, 2160(%rsp)
	leaq	_train_cost_model.par_for.relu1_0_d_def__.s20.n.n.n(%rip), %rsi
	leaq	2064(%rsp), %r8
	xorl	%edi, %edi
	xorl	%edx, %edx
	movq	104(%rsp), %rcx                 ## 8-byte Reload
                                        ## kill: def $ecx killed $ecx killed $rcx
	callq	_halide_do_par_for
	testl	%eax, %eax
	jne	LBB0_264
## %bb.146:                             ## %"consume f0_0_d_def__404"
	movq	%r12, %rdx
	leal	(,%r13,4), %eax
	leal	(%rax,%rax,2), %eax
	movq	136(%rsp), %rcx                 ## 8-byte Reload
	movl	%ecx, 3648(%rsp)
	movl	196(%rsp), %r12d                ## 4-byte Reload
	movl	%r12d, 3652(%rsp)
	movq	152(%rsp), %rcx                 ## 8-byte Reload
	movl	%ecx, 3656(%rsp)
	movq	120(%rsp), %rcx                 ## 8-byte Reload
	movl	%ecx, 3660(%rsp)
	movq	176(%rsp), %rcx                 ## 8-byte Reload
	movl	%ecx, 3664(%rsp)
	movl	32(%rsp), %ecx                  ## 4-byte Reload
	movl	%ecx, 3668(%rsp)
	movl	%edx, 3672(%rsp)
	movq	144(%rsp), %rcx                 ## 8-byte Reload
	movl	%ecx, 3676(%rsp)
	movl	160(%rsp), %ecx                 ## 4-byte Reload
	movl	%ecx, 3680(%rsp)
	movl	208(%rsp), %ecx                 ## 4-byte Reload
	movl	%ecx, 3684(%rsp)
	movl	224(%rsp), %ecx                 ## 4-byte Reload
	movl	%ecx, 3688(%rsp)
	movl	%eax, 3692(%rsp)
	movq	272(%rsp), %r15                 ## 8-byte Reload
	subl	%r13d, %r15d
	movl	%r15d, 3696(%rsp)
	movq	%rbx, %r13
	movq	%rbx, 3704(%rsp)
	movq	$0, 3712(%rsp)
	movq	80(%rsp), %rax                  ## 8-byte Reload
	movq	%rax, 3720(%rsp)
	movq	$0, 3728(%rsp)
	movq	184(%rsp), %rax                 ## 8-byte Reload
	movq	%rax, 3736(%rsp)
	movq	24(%rsp), %rax                  ## 8-byte Reload
	movq	%rax, 3744(%rsp)
	leaq	_train_cost_model.par_for.relu1_0_d_def__.s21.n.n.n(%rip), %rsi
	leaq	3648(%rsp), %r8
	xorl	%edi, %edi
	xorl	%edx, %edx
	movq	104(%rsp), %rcx                 ## 8-byte Reload
                                        ## kill: def $ecx killed $ecx killed $rcx
	callq	_halide_do_par_for
	testl	%eax, %eax
	jne	LBB0_264
## %bb.147:                             ## %"consume f0_0_d_def__407"
	movq	136(%rsp), %rax                 ## 8-byte Reload
	movl	%eax, 3752(%rsp)
	movl	%r12d, 3756(%rsp)
	movq	152(%rsp), %rax                 ## 8-byte Reload
	movl	%eax, 3760(%rsp)
	movq	120(%rsp), %rax                 ## 8-byte Reload
	movl	%eax, 3764(%rsp)
	movq	176(%rsp), %rax                 ## 8-byte Reload
	movl	%eax, 3768(%rsp)
	movl	32(%rsp), %eax                  ## 4-byte Reload
	movl	%eax, 3772(%rsp)
	movq	112(%rsp), %rax                 ## 8-byte Reload
	movl	%eax, 3776(%rsp)
	movq	144(%rsp), %rax                 ## 8-byte Reload
	movl	%eax, 3780(%rsp)
	movl	160(%rsp), %eax                 ## 4-byte Reload
	movl	%eax, 3784(%rsp)
	movl	208(%rsp), %eax                 ## 4-byte Reload
	movl	%eax, 3788(%rsp)
	movl	288(%rsp), %eax                 ## 4-byte Reload
	movl	%eax, 3792(%rsp)
	movl	280(%rsp), %eax                 ## 4-byte Reload
	movl	%eax, 3796(%rsp)
	movq	%r15, 272(%rsp)                 ## 8-byte Spill
	movl	%r15d, 3800(%rsp)
	movq	%r13, %r15
	movq	%r13, 3808(%rsp)
	movq	$0, 3816(%rsp)
	movq	80(%rsp), %rax                  ## 8-byte Reload
	movq	%rax, 3824(%rsp)
	movq	$0, 3832(%rsp)
	movq	184(%rsp), %rax                 ## 8-byte Reload
	movq	%rax, 3840(%rsp)
	movq	24(%rsp), %rax                  ## 8-byte Reload
	movq	%rax, 3848(%rsp)
	leaq	_train_cost_model.par_for.relu1_0_d_def__.s22.n.n.n(%rip), %rsi
	leaq	3752(%rsp), %r8
	xorl	%edi, %edi
	xorl	%edx, %edx
	movq	104(%rsp), %rcx                 ## 8-byte Reload
                                        ## kill: def $ecx killed $ecx killed $rcx
	callq	_halide_do_par_for
	testl	%eax, %eax
	jne	LBB0_264
## %bb.148:                             ## %"consume f0_0_d_def__410"
	movl	%r12d, %ebx
	movq	%r13, %rdx
	movq	216(%rsp), %rcx                 ## 8-byte Reload
	leal	(%rcx,%rcx,4), %eax
	leal	(%rcx,%rax,2), %r13d
	movq	136(%rsp), %rax                 ## 8-byte Reload
	movl	%eax, 3856(%rsp)
	movl	%r12d, 3860(%rsp)
	movq	120(%rsp), %rax                 ## 8-byte Reload
	movl	%eax, 3864(%rsp)
	movq	176(%rsp), %rax                 ## 8-byte Reload
	movl	%eax, 3868(%rsp)
	movq	440(%rsp), %rax                 ## 8-byte Reload
	movl	%eax, 3872(%rsp)
	movl	32(%rsp), %eax                  ## 4-byte Reload
	movl	%eax, 3876(%rsp)
	movq	112(%rsp), %rax                 ## 8-byte Reload
	movl	%eax, 3880(%rsp)
	movl	248(%rsp), %eax                 ## 4-byte Reload
	movl	%eax, 3884(%rsp)
	movl	160(%rsp), %eax                 ## 4-byte Reload
	movl	%eax, 3888(%rsp)
	movq	312(%rsp), %rax                 ## 8-byte Reload
	movl	%eax, 3892(%rsp)
	movl	336(%rsp), %eax                 ## 4-byte Reload
	movl	%eax, 3896(%rsp)
	movl	208(%rsp), %eax                 ## 4-byte Reload
	movl	%eax, 3900(%rsp)
	movl	%r13d, 3904(%rsp)
	movq	%rdx, 3912(%rsp)
	movq	$0, 3920(%rsp)
	movq	80(%rsp), %rax                  ## 8-byte Reload
	movq	%rax, 3928(%rsp)
	movq	$0, 3936(%rsp)
	movq	184(%rsp), %rax                 ## 8-byte Reload
	movq	%rax, 3944(%rsp)
	movq	24(%rsp), %rax                  ## 8-byte Reload
	movq	%rax, 3952(%rsp)
	leaq	_train_cost_model.par_for.relu1_0_d_def__.s23.n.n.n(%rip), %rsi
	leaq	3856(%rsp), %r8
	xorl	%edi, %edi
	xorl	%edx, %edx
	movq	104(%rsp), %rcx                 ## 8-byte Reload
                                        ## kill: def $ecx killed $ecx killed $rcx
	callq	_halide_do_par_for
	movl	%eax, %r14d
	testl	%eax, %eax
	movq	320(%rsp), %rax                 ## 8-byte Reload
	movq	304(%rsp), %rcx                 ## 8-byte Reload
	jne	LBB0_265
## %bb.149:                             ## %"consume f0_0_d_def__413"
	negl	%ecx
	subl	256(%rsp), %ecx                 ## 4-byte Folded Reload
	subl	%eax, %ecx
	movq	136(%rsp), %r12                 ## 8-byte Reload
	movl	%r12d, 5040(%rsp)
	movl	%ebx, 5044(%rsp)
	movq	152(%rsp), %rbx                 ## 8-byte Reload
	movl	%ebx, 5048(%rsp)
	movq	120(%rsp), %r15                 ## 8-byte Reload
	movl	%r15d, 5052(%rsp)
	movq	176(%rsp), %rax                 ## 8-byte Reload
	movl	%eax, 5056(%rsp)
	movq	112(%rsp), %rax                 ## 8-byte Reload
	movl	%eax, 5060(%rsp)
	movq	144(%rsp), %rax                 ## 8-byte Reload
	movl	%eax, 5064(%rsp)
	movl	160(%rsp), %eax                 ## 4-byte Reload
	movl	%eax, 5068(%rsp)
	movq	%rcx, 304(%rsp)                 ## 8-byte Spill
	movl	%ecx, 5072(%rsp)
	movl	208(%rsp), %eax                 ## 4-byte Reload
	movl	%eax, 5076(%rsp)
	movl	%r13d, 320(%rsp)                ## 4-byte Spill
	movl	%r13d, 5080(%rsp)
	movq	40(%rsp), %rax                  ## 8-byte Reload
	movq	%rax, 5088(%rsp)
	movq	$0, 5096(%rsp)
	movq	80(%rsp), %rax                  ## 8-byte Reload
	movq	%rax, 5104(%rsp)
	movq	$0, 5112(%rsp)
	movq	184(%rsp), %rax                 ## 8-byte Reload
	movq	%rax, 5120(%rsp)
	movq	24(%rsp), %rax                  ## 8-byte Reload
	movq	%rax, 5128(%rsp)
	leaq	_train_cost_model.par_for.relu1_0_d_def__.s24.n.n.n(%rip), %rsi
	leaq	5040(%rsp), %r8
	xorl	%edi, %edi
	xorl	%edx, %edx
	movq	104(%rsp), %rcx                 ## 8-byte Reload
                                        ## kill: def $ecx killed $ecx killed $rcx
	callq	_halide_do_par_for
	testl	%eax, %eax
	jne	LBB0_264
## %bb.150:                             ## %"consume f0_0_d_def__416"
	movl	%r12d, 3960(%rsp)
	movl	196(%rsp), %edx                 ## 4-byte Reload
	movl	%edx, 3964(%rsp)
	movl	%ebx, 3968(%rsp)
	movl	%r15d, 3972(%rsp)
	movq	176(%rsp), %rdx                 ## 8-byte Reload
	movl	%edx, 3976(%rsp)
	movq	440(%rsp), %r13                 ## 8-byte Reload
	movl	%r13d, 3980(%rsp)
	movl	32(%rsp), %edx                  ## 4-byte Reload
	movl	%edx, 3984(%rsp)
	movq	112(%rsp), %rdx                 ## 8-byte Reload
	movl	%edx, 3988(%rsp)
	movq	144(%rsp), %rdx                 ## 8-byte Reload
	movl	%edx, 3992(%rsp)
	movl	160(%rsp), %edx                 ## 4-byte Reload
	movl	%edx, 3996(%rsp)
	movq	312(%rsp), %rax                 ## 8-byte Reload
	movl	%eax, 4000(%rsp)
	movl	208(%rsp), %eax                 ## 4-byte Reload
	movl	%eax, 4004(%rsp)
	movl	320(%rsp), %eax                 ## 4-byte Reload
	movl	%eax, 4008(%rsp)
	movq	40(%rsp), %rbx                  ## 8-byte Reload
	movq	%rbx, 4016(%rsp)
	movq	$0, 4024(%rsp)
	movq	80(%rsp), %rax                  ## 8-byte Reload
	movq	%rax, 4032(%rsp)
	movq	$0, 4040(%rsp)
	movq	184(%rsp), %rax                 ## 8-byte Reload
	movq	%rax, 4048(%rsp)
	movq	24(%rsp), %rax                  ## 8-byte Reload
	movq	%rax, 4056(%rsp)
	leaq	_train_cost_model.par_for.relu1_0_d_def__.s25.n.n.n(%rip), %rsi
	leaq	3960(%rsp), %r8
	xorl	%edi, %edi
	xorl	%edx, %edx
	movq	104(%rsp), %rcx                 ## 8-byte Reload
                                        ## kill: def $ecx killed $ecx killed $rcx
	callq	_halide_do_par_for
	testl	%eax, %eax
	jne	LBB0_264
## %bb.151:                             ## %"consume f0_0_d_def__419"
	movq	%rbx, %rdx
	movq	136(%rsp), %rax                 ## 8-byte Reload
	movl	%eax, 2168(%rsp)
	movl	196(%rsp), %r12d                ## 4-byte Reload
	movl	%r12d, 2172(%rsp)
	movq	120(%rsp), %rax                 ## 8-byte Reload
	movl	%eax, 2176(%rsp)
	movq	176(%rsp), %r15                 ## 8-byte Reload
	movl	%r15d, 2180(%rsp)
	movl	%r13d, 2184(%rsp)
	movl	32(%rsp), %eax                  ## 4-byte Reload
	movl	%eax, 2188(%rsp)
	movq	112(%rsp), %rax                 ## 8-byte Reload
	movl	%eax, 2192(%rsp)
	movl	248(%rsp), %eax                 ## 4-byte Reload
	movl	%eax, 2196(%rsp)
	movl	160(%rsp), %eax                 ## 4-byte Reload
	movl	%eax, 2200(%rsp)
	movq	312(%rsp), %rax                 ## 8-byte Reload
	movl	%eax, 2204(%rsp)
	movl	336(%rsp), %eax                 ## 4-byte Reload
	movl	%eax, 2208(%rsp)
	movq	216(%rsp), %rcx                 ## 8-byte Reload
	leal	(%rcx,%rcx,8), %eax
	leal	(%rax,%rax,2), %ebx
	addl	%ecx, %ebx
	addl	%ecx, %ebx
	movl	208(%rsp), %eax                 ## 4-byte Reload
	movl	%eax, 2212(%rsp)
	movl	%ebx, 2216(%rsp)
	movq	272(%rsp), %rax                 ## 8-byte Reload
	movl	%eax, 2220(%rsp)
	movq	%rdx, 2224(%rsp)
	movq	$0, 2232(%rsp)
	movq	80(%rsp), %rax                  ## 8-byte Reload
	movq	%rax, 2240(%rsp)
	movq	$0, 2248(%rsp)
	movq	184(%rsp), %rax                 ## 8-byte Reload
	movq	%rax, 2256(%rsp)
	movq	24(%rsp), %rax                  ## 8-byte Reload
	movq	%rax, 2264(%rsp)
	leaq	_train_cost_model.par_for.relu1_0_d_def__.s26.n.n.n(%rip), %rsi
	leaq	2168(%rsp), %r8
	xorl	%edi, %edi
	xorl	%edx, %edx
	movq	104(%rsp), %r13                 ## 8-byte Reload
	movl	%r13d, %ecx
	callq	_halide_do_par_for
	testl	%eax, %eax
	jne	LBB0_264
## %bb.152:                             ## %"consume f0_0_d_def__422"
	movq	%r15, %rcx
	movq	%r13, %r15
	movq	136(%rsp), %rax                 ## 8-byte Reload
	movl	%eax, 2912(%rsp)
	movl	%r12d, 2916(%rsp)
	movq	152(%rsp), %rax                 ## 8-byte Reload
	movl	%eax, 2920(%rsp)
	movq	120(%rsp), %rax                 ## 8-byte Reload
	movl	%eax, 2924(%rsp)
	movl	%ecx, 2928(%rsp)
	movq	112(%rsp), %rax                 ## 8-byte Reload
	movl	%eax, 2932(%rsp)
	movq	144(%rsp), %rax                 ## 8-byte Reload
	movl	%eax, 2936(%rsp)
	movl	160(%rsp), %r13d                ## 4-byte Reload
	movl	%r13d, 2940(%rsp)
	movq	304(%rsp), %rax                 ## 8-byte Reload
	movl	%eax, 2944(%rsp)
	movl	208(%rsp), %eax                 ## 4-byte Reload
	movl	%eax, 2948(%rsp)
	movl	%ebx, 2952(%rsp)
	movq	272(%rsp), %rax                 ## 8-byte Reload
	movl	%eax, 2956(%rsp)
	movq	40(%rsp), %rax                  ## 8-byte Reload
	movq	%rax, 2960(%rsp)
	movq	$0, 2968(%rsp)
	movq	80(%rsp), %rax                  ## 8-byte Reload
	movq	%rax, 2976(%rsp)
	movq	$0, 2984(%rsp)
	movq	184(%rsp), %rax                 ## 8-byte Reload
	movq	%rax, 2992(%rsp)
	movq	24(%rsp), %rax                  ## 8-byte Reload
	movq	%rax, 3000(%rsp)
	leaq	_train_cost_model.par_for.relu1_0_d_def__.s27.n.n.n(%rip), %rsi
	leaq	2912(%rsp), %r8
	xorl	%edi, %edi
	xorl	%edx, %edx
	movl	%r15d, %ecx
	callq	_halide_do_par_for
	testl	%eax, %eax
	jne	LBB0_264
## %bb.153:                             ## %"consume f0_0_d_def__425"
	movq	136(%rsp), %rax                 ## 8-byte Reload
	movl	%eax, 2272(%rsp)
	movl	%r12d, 2276(%rsp)
	movq	152(%rsp), %rax                 ## 8-byte Reload
	movl	%eax, 2280(%rsp)
	movq	120(%rsp), %rax                 ## 8-byte Reload
	movl	%eax, 2284(%rsp)
	movq	176(%rsp), %rax                 ## 8-byte Reload
	movl	%eax, 2288(%rsp)
	movq	440(%rsp), %rax                 ## 8-byte Reload
	movl	%eax, 2292(%rsp)
	movl	32(%rsp), %eax                  ## 4-byte Reload
	movl	%eax, 2296(%rsp)
	movq	112(%rsp), %rax                 ## 8-byte Reload
	movl	%eax, 2300(%rsp)
	movq	144(%rsp), %rax                 ## 8-byte Reload
	movl	%eax, 2304(%rsp)
	movl	%r13d, 2308(%rsp)
	movq	312(%rsp), %rax                 ## 8-byte Reload
	movl	%eax, 2312(%rsp)
	movl	208(%rsp), %eax                 ## 4-byte Reload
	movl	%eax, 2316(%rsp)
	movl	%ebx, 2320(%rsp)
	movq	272(%rsp), %rax                 ## 8-byte Reload
	movl	%eax, 2324(%rsp)
	movq	40(%rsp), %rax                  ## 8-byte Reload
	movq	%rax, 2328(%rsp)
	movq	$0, 2336(%rsp)
	movq	80(%rsp), %rax                  ## 8-byte Reload
	movq	%rax, 2344(%rsp)
	movq	$0, 2352(%rsp)
	movq	184(%rsp), %rax                 ## 8-byte Reload
	movq	%rax, 2360(%rsp)
	movq	24(%rsp), %rax                  ## 8-byte Reload
	movq	%rax, 2368(%rsp)
	leaq	_train_cost_model.par_for.relu1_0_d_def__.s28.n.n.n(%rip), %rsi
	leaq	2272(%rsp), %r8
	xorl	%edi, %edi
	xorl	%edx, %edx
	movl	%r15d, %ecx
	callq	_halide_do_par_for
	testl	%eax, %eax
	jne	LBB0_264
## %bb.154:                             ## %"consume f0_0_d_def__428"
	movl	%r13d, %ecx
	movq	216(%rsp), %r13                 ## 8-byte Reload
	movl	%r13d, %r15d
	shll	$4, %r15d
	addl	%r13d, %r15d
	movq	136(%rsp), %rax                 ## 8-byte Reload
	movl	%eax, 2376(%rsp)
	movl	%r12d, 2380(%rsp)
	movq	152(%rsp), %rax                 ## 8-byte Reload
	movl	%eax, 2384(%rsp)
	movq	120(%rsp), %rax                 ## 8-byte Reload
	movl	%eax, 2388(%rsp)
	movq	176(%rsp), %r12                 ## 8-byte Reload
	movl	%r12d, 2392(%rsp)
	movl	32(%rsp), %eax                  ## 4-byte Reload
	movl	%eax, 2396(%rsp)
	movq	112(%rsp), %rax                 ## 8-byte Reload
	movl	%eax, 2400(%rsp)
	movq	144(%rsp), %rax                 ## 8-byte Reload
	movl	%eax, 2404(%rsp)
	movl	%ecx, 2408(%rsp)
	vmovss	432(%rsp), %xmm0                ## 4-byte Reload
                                        ## xmm0 = mem[0],zero,zero,zero
	vmovss	%xmm0, 2412(%rsp)
	movl	208(%rsp), %eax                 ## 4-byte Reload
	movl	%eax, 2416(%rsp)
	movl	224(%rsp), %eax                 ## 4-byte Reload
	movl	%eax, 2420(%rsp)
	movl	%r15d, 2424(%rsp)
	movl	280(%rsp), %eax                 ## 4-byte Reload
	movl	%eax, 2428(%rsp)
	movq	40(%rsp), %rax                  ## 8-byte Reload
	movq	%rax, 2432(%rsp)
	movq	$0, 2440(%rsp)
	movq	80(%rsp), %rax                  ## 8-byte Reload
	movq	%rax, 2448(%rsp)
	movq	$0, 2456(%rsp)
	movq	184(%rsp), %rax                 ## 8-byte Reload
	movq	%rax, 2464(%rsp)
	movq	24(%rsp), %rbx                  ## 8-byte Reload
	movq	%rbx, 2472(%rsp)
	leaq	_train_cost_model.par_for.relu1_0_d_def__.s29.n.n.n(%rip), %rsi
	leaq	2376(%rsp), %r8
	xorl	%edi, %edi
	xorl	%edx, %edx
	movq	104(%rsp), %rcx                 ## 8-byte Reload
                                        ## kill: def $ecx killed $ecx killed $rcx
	callq	_halide_do_par_for
	testl	%eax, %eax
	movq	%rbx, %rdx
	movq	120(%rsp), %rcx                 ## 8-byte Reload
	movl	196(%rsp), %edi                 ## 4-byte Reload
	movq	136(%rsp), %rbx                 ## 8-byte Reload
	jne	LBB0_264
## %bb.155:                             ## %"consume f0_0_d_def__431"
	movq	%r12, %rsi
	leal	(%r13,%r13,4), %eax
	leal	(%r13,%rax,4), %r12d
	movl	%ebx, 3064(%rsp)
	movl	%edi, 3068(%rsp)
	movq	152(%rsp), %rax                 ## 8-byte Reload
	movl	%eax, 3072(%rsp)
	movl	%ecx, 3076(%rsp)
	movl	%esi, 3080(%rsp)
	movl	32(%rsp), %eax                  ## 4-byte Reload
	movl	%eax, 3084(%rsp)
	movq	112(%rsp), %rax                 ## 8-byte Reload
	movl	%eax, 3088(%rsp)
	movq	144(%rsp), %rax                 ## 8-byte Reload
	movl	%eax, 3092(%rsp)
	movl	160(%rsp), %eax                 ## 4-byte Reload
	movl	%eax, 3096(%rsp)
	vmovss	432(%rsp), %xmm0                ## 4-byte Reload
                                        ## xmm0 = mem[0],zero,zero,zero
	vmovss	%xmm0, 3100(%rsp)
	addl	%r13d, %r12d
	movl	208(%rsp), %eax                 ## 4-byte Reload
	movl	%eax, 3104(%rsp)
	movl	224(%rsp), %eax                 ## 4-byte Reload
	movl	%eax, 3108(%rsp)
	movl	%r15d, 3112(%rsp)
	movl	%r12d, 3116(%rsp)
	movl	288(%rsp), %eax                 ## 4-byte Reload
	movl	%eax, 3120(%rsp)
	movq	40(%rsp), %rax                  ## 8-byte Reload
	movq	%rax, 3128(%rsp)
	movq	$0, 3136(%rsp)
	movq	80(%rsp), %rax                  ## 8-byte Reload
	movq	%rax, 3144(%rsp)
	movq	$0, 3152(%rsp)
	movq	184(%rsp), %rax                 ## 8-byte Reload
	movq	%rax, 3160(%rsp)
	movq	%rdx, 3168(%rsp)
	leaq	_train_cost_model.par_for.relu1_0_d_def__.s30.n.n.n(%rip), %rsi
	leaq	3064(%rsp), %r8
	xorl	%edi, %edi
	xorl	%edx, %edx
	movq	104(%rsp), %r13                 ## 8-byte Reload
	movl	%r13d, %ecx
	callq	_halide_do_par_for
	testl	%eax, %eax
	jne	LBB0_264
## %bb.156:                             ## %"consume f0_0_d_def__434"
	movq	%r13, %rcx
	movl	%ebx, 2480(%rsp)
	movl	196(%rsp), %eax                 ## 4-byte Reload
	movl	%eax, 2484(%rsp)
	movq	152(%rsp), %rax                 ## 8-byte Reload
	movl	%eax, 2488(%rsp)
	movq	120(%rsp), %rax                 ## 8-byte Reload
	movl	%eax, 2492(%rsp)
	movq	176(%rsp), %rax                 ## 8-byte Reload
	movl	%eax, 2496(%rsp)
	movl	32(%rsp), %eax                  ## 4-byte Reload
	movl	%eax, 2500(%rsp)
	movq	112(%rsp), %rax                 ## 8-byte Reload
	movl	%eax, 2504(%rsp)
	movq	144(%rsp), %rax                 ## 8-byte Reload
	movl	%eax, 2508(%rsp)
	movl	160(%rsp), %r13d                ## 4-byte Reload
	movl	%r13d, 2512(%rsp)
	vmovss	432(%rsp), %xmm0                ## 4-byte Reload
                                        ## xmm0 = mem[0],zero,zero,zero
	vmovss	%xmm0, 2516(%rsp)
	movl	208(%rsp), %eax                 ## 4-byte Reload
	movl	%eax, 2520(%rsp)
	movl	224(%rsp), %eax                 ## 4-byte Reload
	movl	%eax, 2524(%rsp)
	movl	%r15d, 2528(%rsp)
	movl	280(%rsp), %eax                 ## 4-byte Reload
	movl	%eax, 2532(%rsp)
	movq	40(%rsp), %rax                  ## 8-byte Reload
	movq	%rax, 2536(%rsp)
	movq	$0, 2544(%rsp)
	movq	80(%rsp), %rax                  ## 8-byte Reload
	movq	%rax, 2552(%rsp)
	movq	$0, 2560(%rsp)
	movq	184(%rsp), %rax                 ## 8-byte Reload
	movq	%rax, 2568(%rsp)
	movq	24(%rsp), %rax                  ## 8-byte Reload
	movq	%rax, 2576(%rsp)
	leaq	_train_cost_model.par_for.relu1_0_d_def__.s31.n.n.n(%rip), %rsi
	leaq	2480(%rsp), %r8
	xorl	%edi, %edi
	xorl	%edx, %edx
                                        ## kill: def $ecx killed $ecx killed $rcx
	callq	_halide_do_par_for
	testl	%eax, %eax
	jne	LBB0_264
## %bb.157:                             ## %"consume f0_0_d_def__437"
	movl	%r13d, %eax
	movl	%ebx, 3176(%rsp)
	movl	196(%rsp), %ecx                 ## 4-byte Reload
	movl	%ecx, 3180(%rsp)
	movq	152(%rsp), %rcx                 ## 8-byte Reload
	movl	%ecx, 3184(%rsp)
	movq	120(%rsp), %r13                 ## 8-byte Reload
	movl	%r13d, 3188(%rsp)
	movq	176(%rsp), %rcx                 ## 8-byte Reload
	movl	%ecx, 3192(%rsp)
	movl	32(%rsp), %ecx                  ## 4-byte Reload
	movl	%ecx, 3196(%rsp)
	movq	112(%rsp), %rcx                 ## 8-byte Reload
	movl	%ecx, 3200(%rsp)
	movq	144(%rsp), %rcx                 ## 8-byte Reload
	movl	%ecx, 3204(%rsp)
	movl	%eax, 3208(%rsp)
	vmovss	432(%rsp), %xmm0                ## 4-byte Reload
                                        ## xmm0 = mem[0],zero,zero,zero
	vmovss	%xmm0, 3212(%rsp)
	movl	208(%rsp), %eax                 ## 4-byte Reload
	movl	%eax, 3216(%rsp)
	movl	224(%rsp), %eax                 ## 4-byte Reload
	movl	%eax, 3220(%rsp)
	movl	%r15d, 3224(%rsp)
	movl	%r12d, 3228(%rsp)
	movl	288(%rsp), %eax                 ## 4-byte Reload
	movl	%eax, 3232(%rsp)
	movq	40(%rsp), %rbx                  ## 8-byte Reload
	movq	%rbx, 3240(%rsp)
	movq	$0, 3248(%rsp)
	movq	80(%rsp), %rax                  ## 8-byte Reload
	movq	%rax, 3256(%rsp)
	movq	$0, 3264(%rsp)
	movq	184(%rsp), %rax                 ## 8-byte Reload
	movq	%rax, 3272(%rsp)
	movq	24(%rsp), %rax                  ## 8-byte Reload
	movq	%rax, 3280(%rsp)
	leaq	_train_cost_model.par_for.relu1_0_d_def__.s32.n.n.n(%rip), %rsi
	leaq	3176(%rsp), %r8
	xorl	%edi, %edi
	xorl	%edx, %edx
	movq	104(%rsp), %rcx                 ## 8-byte Reload
                                        ## kill: def $ecx killed $ecx killed $rcx
	callq	_halide_do_par_for
	testl	%eax, %eax
	jne	LBB0_264
## %bb.158:                             ## %call_destructor.exit821
	movq	%rbx, %rsi
	movq	456(%rsp), %rbx                 ## 8-byte Reload
	movl	%ebx, %r14d
	xorl	%edi, %edi
	callq	_halide_free
	shlq	$2, %rbx
	cmpl	$536870912, %r14d               ## imm = 0x20000000
	jae	LBB0_321
## %bb.159:                             ## %"assert succeeded441"
	addq	$12, %rbx
	xorl	%edi, %edi
	movq	%rbx, %rsi
	callq	_halide_malloc
	testq	%rax, %rax
	movq	136(%rsp), %rbx                 ## 8-byte Reload
	movq	112(%rsp), %rdx                 ## 8-byte Reload
	je	LBB0_322
## %bb.160:                             ## %"assert succeeded443"
	movq	%rax, %rsi
	movl	%edx, %eax
	sarl	$31, %eax
	andnl	%edx, %eax, %ecx
	movl	%ebx, %r12d
	sarl	$3, %r12d
	movl	%ebx, %r15d
	sarl	$31, %r15d
	andnl	%r12d, %r15d, %eax
	movl	%ebx, 6136(%rsp)
	movl	%eax, 144(%rsp)                 ## 4-byte Spill
	movl	%eax, 6140(%rsp)
	leaq	7040(%rsp), %rax
	movq	%rax, 6144(%rsp)
	movq	$0, 6152(%rsp)
	movq	%rsi, 24(%rsp)                  ## 8-byte Spill
	movq	%rsi, 6160(%rsp)
	movq	$0, 6168(%rsp)
	leaq	_train_cost_model.par_for.sum_1_d_def__.s0.n.n(%rip), %rsi
	leaq	6136(%rsp), %r8
	xorl	%edi, %edi
	xorl	%edx, %edx
	movl	%ecx, 160(%rsp)                 ## 4-byte Spill
	callq	_halide_do_par_for
	testl	%eax, %eax
	jne	LBB0_324
## %bb.161:                             ## %"assert succeeded446"
	movl	%r13d, %eax
	sarl	$31, %eax
	movl	%eax, 216(%rsp)                 ## 4-byte Spill
	andnl	%r13d, %eax, %r13d
	movl	%r15d, 40(%rsp)                 ## 4-byte Spill
	andnl	%ebx, %r15d, %edx
	movq	%rdx, %rdi
	imulq	%r13, %rdi
	movq	%rdi, %rsi
	shlq	$7, %rsi
	movq	%r13, %r15
	shlq	$7, %r15
	movl	$2147483648, %eax               ## imm = 0x80000000
	cmpq	%rax, %rsi
	jae	LBB0_282
## %bb.162:                             ## %"assert succeeded446"
	movq	%r13, %rax
	shrq	$25, %rax
	movl	%r15d, %ecx
	imulq	%rdx, %rcx
	shrq	$32, %rcx
	imulq	%rdx, %rax
	addq	%rcx, %rax
	movabsq	$270582939648, %rcx             ## imm = 0x3F00000000
	andq	%rcx, %rax
	jne	LBB0_282
## %bb.163:                             ## %"assert succeeded448"
	movq	%rdi, 152(%rsp)                 ## 8-byte Spill
	movq	%rdx, 184(%rsp)                 ## 8-byte Spill
	orq	$12, %rsi
	xorl	%edi, %edi
	callq	_halide_malloc
	testq	%rax, %rax
	movq	232(%rsp), %rdx                 ## 8-byte Reload
	movl	264(%rsp), %esi                 ## 4-byte Reload
	movl	144(%rsp), %ebx                 ## 4-byte Reload
	je	LBB0_325
## %bb.164:                             ## %"assert succeeded450"
	movq	%rax, %rdi
	movq	112(%rsp), %rcx                 ## 8-byte Reload
	subl	%ebx, %ecx
	movl	%ecx, %eax
	sarl	$31, %eax
	andnl	%ecx, %eax, %eax
	movq	136(%rsp), %rcx                 ## 8-byte Reload
	movl	%ecx, 5752(%rsp)
	movl	%esi, 5756(%rsp)
	movq	120(%rsp), %rcx                 ## 8-byte Reload
	movl	%ecx, 5760(%rsp)
	movq	176(%rsp), %rcx                 ## 8-byte Reload
	movl	%ecx, 5764(%rsp)
	movl	%r13d, 5768(%rsp)
	movl	%edx, 5772(%rsp)
	movl	248(%rsp), %ecx                 ## 4-byte Reload
	movl	%ecx, 5776(%rsp)
	movl	%eax, 5780(%rsp)
	movl	%r12d, 5784(%rsp)
	movl	%ebx, 5788(%rsp)
	movq	168(%rsp), %rax                 ## 8-byte Reload
	movq	%rax, 5792(%rsp)
	movq	$0, 5800(%rsp)
	movq	%rdi, 104(%rsp)                 ## 8-byte Spill
	movq	%rdi, 5808(%rsp)
	movq	$0, 5816(%rsp)
	movq	80(%rsp), %rbx                  ## 8-byte Reload
	movq	%rbx, 5824(%rsp)
	movq	$0, 5832(%rsp)
	movq	24(%rsp), %r12                  ## 8-byte Reload
	movq	%r12, 5840(%rsp)
	movq	$0, 5848(%rsp)
	leaq	_train_cost_model.par_for.conv1_stage2_1_d_def__.s0.c.c(%rip), %rsi
	leaq	5752(%rsp), %r8
	xorl	%edi, %edi
	xorl	%edx, %edx
	movl	$16, %ecx
	callq	_halide_do_par_for
	testl	%eax, %eax
	jne	LBB0_331
## %bb.165:                             ## %call_destructor.exit823
	xorl	%edi, %edi
	movq	%rbx, %rsi
	callq	_halide_free
	xorl	%edi, %edi
	movq	%r12, %rsi
	callq	_halide_free
	cmpl	$16777216, %r13d                ## imm = 0x1000000
	jae	LBB0_332
## %bb.166:                             ## %"assert succeeded455"
	movq	%r13, 24(%rsp)                  ## 8-byte Spill
	orq	$12, %r15
	xorl	%edi, %edi
	movq	%r15, %rsi
	callq	_halide_malloc
	testq	%rax, %rax
	movq	136(%rsp), %rbx                 ## 8-byte Reload
	je	LBB0_333
## %bb.167:                             ## %"assert succeeded457"
	movq	%rax, %r15
	movq	120(%rsp), %r13                 ## 8-byte Reload
	movl	%r13d, %eax
	sarl	$3, %eax
	movl	216(%rsp), %ecx                 ## 4-byte Reload
	andnl	%eax, %ecx, %r12d
	leal	7(%r13), %eax
	sarl	$3, %eax
	subl	%r12d, %eax
	movl	%eax, %ecx
	sarl	$31, %ecx
	andnl	%eax, %ecx, %eax
	movl	%r13d, 5136(%rsp)
	movl	%eax, 80(%rsp)                  ## 4-byte Spill
	movl	%eax, 5140(%rsp)
	movl	%r12d, 5144(%rsp)
	movq	%r15, 5152(%rsp)
	movq	$0, 5160(%rsp)
	leaq	_train_cost_model.par_for.conv1_stage1_1_d_def__.s0.c(%rip), %rsi
	leaq	5136(%rsp), %r8
	xorl	%edi, %edi
	xorl	%edx, %edx
	movl	$32, %ecx
	callq	_halide_do_par_for
	testl	%eax, %eax
	movq	%r15, 32(%rsp)                  ## 8-byte Spill
	jne	LBB0_334
## %bb.168:                             ## %"consume conv1_stage2_1_d_def__"
	leal	1(%rbx), %eax
	movl	%eax, %ecx
	sarl	%ecx
	movl	%ebx, %edx
	sarl	%edx
	movl	40(%rsp), %esi                  ## 4-byte Reload
	andnl	%edx, %esi, %edx
	sarl	$31, %eax
	andnl	%ecx, %eax, %eax
	subl	%edx, %ecx
	movl	%ecx, %esi
	sarl	$31, %esi
	andnl	%ecx, %esi, %ecx
	movl	%ebx, 5168(%rsp)
	movl	%r13d, 5172(%rsp)
	movl	%ecx, 5176(%rsp)
	movl	80(%rsp), %ecx                  ## 4-byte Reload
	movl	%ecx, 5180(%rsp)
	movl	%eax, 5184(%rsp)
	movl	%edx, 5188(%rsp)
	movl	%r12d, 5192(%rsp)
	movq	%r15, 5200(%rsp)
	movq	$0, 5208(%rsp)
	movq	104(%rsp), %rax                 ## 8-byte Reload
	movq	%rax, 5216(%rsp)
	movq	$0, 5224(%rsp)
	leaq	_train_cost_model.par_for.conv1_stage1_1_d_def__.s1.c(%rip), %rsi
	leaq	5168(%rsp), %r8
	xorl	%edi, %edi
	xorl	%edx, %edx
	movl	$32, %ecx
	callq	_halide_do_par_for
	testl	%eax, %eax
	jne	LBB0_334
## %bb.169:                             ## %"assert succeeded463"
	movq	24(%rsp), %rcx                  ## 8-byte Reload
	shlq	$5, %rcx
	movq	%rcx, 176(%rsp)                 ## 8-byte Spill
	leaq	12(%rcx), %rsi
	xorl	%edi, %edi
	callq	_halide_malloc
	testq	%rax, %rax
	movq	40(%rbp), %r12
	je	LBB0_335
## %bb.170:                             ## %"assert succeeded465"
	movq	%rax, 5232(%rsp)
	movq	$0, 5240(%rsp)
	leaq	_train_cost_model.par_for.head1_conv_1_d_def__.s0.w(%rip), %rsi
	movq	%rax, %r13
	leaq	5232(%rsp), %r8
	xorl	%edi, %edi
	xorl	%edx, %edx
	movq	120(%rsp), %rbx                 ## 8-byte Reload
	movl	%ebx, %ecx
	callq	_halide_do_par_for
	testl	%eax, %eax
	movq	%r13, 216(%rsp)                 ## 8-byte Spill
	jne	LBB0_345
## %bb.171:                             ## %"consume conv1_stage1_1_d_def__"
	movl	240(%rsp), %eax                 ## 4-byte Reload
	movl	%eax, 6608(%rsp)
	movl	%ebx, 6612(%rsp)
	movq	%r15, 6616(%rsp)
	movq	$0, 6624(%rsp)
	movq	856(%rsp), %rax                 ## 8-byte Reload
	movq	%rax, 6632(%rsp)
	movq	%r12, 6640(%rsp)
	movq	%r13, 6648(%rsp)
	movq	$0, 6656(%rsp)
	leaq	_train_cost_model.par_for.head1_conv_1_d_def__.s1.w(%rip), %rsi
	leaq	6608(%rsp), %r8
	xorl	%edi, %edi
	xorl	%edx, %edx
	movl	%ebx, %ecx
	callq	_halide_do_par_for
	testl	%eax, %eax
	movq	24(%rsp), %rsi                  ## 8-byte Reload
	jne	LBB0_345
## %bb.172:                             ## %"consume head1_conv_1_d_def__"
	movl	$6, %eax
	subl	1104(%rsp), %eax                ## 4-byte Folded Reload
	movl	512(%rsp), %edx                 ## 4-byte Reload
	imull	%edx, %eax
	subl	1112(%rsp), %eax                ## 4-byte Folded Reload
	subl	840(%rsp), %eax                 ## 4-byte Folded Reload
	movl	428(%rsp), %ecx                 ## 4-byte Reload
	movl	%ecx, 5248(%rsp)
	movl	424(%rsp), %ecx                 ## 4-byte Reload
	movl	%ecx, 5252(%rsp)
	movl	%ebx, 5256(%rsp)
	movl	%edx, 5260(%rsp)
	movl	508(%rsp), %ecx                 ## 4-byte Reload
	movl	%ecx, 5264(%rsp)
	movl	%esi, 5268(%rsp)
	movl	464(%rsp), %ecx                 ## 4-byte Reload
	movl	%ecx, 5272(%rsp)
	movq	1120(%rsp), %rbx                ## 8-byte Reload
	leal	(%rbx,%rbx,2), %ecx
	movl	%ecx, 80(%rsp)                  ## 4-byte Spill
	movl	%ecx, 5276(%rsp)
	movl	%eax, 5280(%rsp)
	movq	1376(%rsp), %r12                ## 8-byte Reload
	leal	(%rbx,%r12,2), %eax
	leal	(%rax,%rax,2), %eax
	movl	%eax, 5284(%rsp)
	movl	940(%rsp), %r15d                ## 4-byte Reload
	movl	%r15d, 5288(%rsp)
	movl	%r12d, 5292(%rsp)
	movq	%r13, 5296(%rsp)
	movq	$0, 5304(%rsp)
	movq	1136(%rsp), %rax                ## 8-byte Reload
	movq	%rax, 5312(%rsp)
	movq	816(%rsp), %rax                 ## 8-byte Reload
	movq	%rax, 5320(%rsp)
	movq	1160(%rsp), %rax                ## 8-byte Reload
	movq	%rax, 5328(%rsp)
	movq	1208(%rsp), %rax                ## 8-byte Reload
	movq	%rax, 5336(%rsp)
	movq	1384(%rsp), %r13                ## 8-byte Reload
	movq	%r13, 5344(%rsp)
	movq	80(%rbp), %rax
	movq	%rax, 5352(%rsp)
	leaq	_train_cost_model.par_for.updated_head1_filter.s1.v235.v235.v235(%rip), %rsi
	leaq	5248(%rsp), %r8
	xorl	%edi, %edi
	xorl	%edx, %edx
	movl	$80, %ecx
	callq	_halide_do_par_for
	testl	%eax, %eax
	jne	LBB0_345
## %bb.173:                             ## %"assert succeeded471"
	movl	%r15d, 5360(%rsp)
	movl	%r12d, 5364(%rsp)
	movl	%ebx, 5368(%rsp)
	movq	%r13, 5376(%rsp)
	movq	80(%rbp), %rax
	movq	%rax, 5384(%rsp)
	leaq	_train_cost_model.par_for.updated_head1_filter.s2.v235.v235.v235(%rip), %rsi
	leaq	5360(%rsp), %r8
	xorl	%edi, %edi
	xorl	%edx, %edx
	movl	$80, %ecx
	callq	_halide_do_par_for
	testl	%eax, %eax
	jne	LBB0_345
## %bb.174:                             ## %"assert succeeded473"
	leal	(%rbx,%rbx), %eax
	movl	80(%rsp), %ecx                  ## 4-byte Reload
	movl	%ecx, 1720(%rsp)
	movl	%eax, 1724(%rsp)
	movl	%r15d, 1728(%rsp)
	movl	%r12d, 1732(%rsp)
	movq	%r13, 1736(%rsp)
	movq	80(%rbp), %rax
	movq	%rax, 1744(%rsp)
	leaq	_train_cost_model.par_for.updated_head1_filter.s3.v235.v235.v235(%rip), %rsi
	leaq	1720(%rsp), %r8
	xorl	%edi, %edi
	xorl	%edx, %edx
	movl	$80, %ecx
	callq	_halide_do_par_for
	testl	%eax, %eax
	jne	LBB0_345
## %bb.175:                             ## %"assert succeeded475"
	movq	%r13, %rdx
	movl	56(%rbp), %eax
	incl	%eax
	vcvtsi2ss	%eax, %xmm0, %xmm3
	vmulss	LCPI0_78(%rip), %xmm3, %xmm0
	vroundss	$9, %xmm0, %xmm0, %xmm4
	vcvttss2si	%xmm4, %eax
	vmovss	LCPI0_81(%rip), %xmm1           ## xmm1 = mem[0],zero,zero,zero
	vmovss	LCPI0_84(%rip), %xmm2           ## xmm2 = mem[0],zero,zero,zero
	vmovss	LCPI0_85(%rip), %xmm0           ## xmm0 = mem[0],zero,zero,zero
	cmpl	$127, %eax
	movq	24(%rsp), %r13                  ## 8-byte Reload
	jg	LBB0_177
## %bb.176:                             ## %select.true.sink
	vmulss	LCPI0_79(%rip), %xmm4, %xmm4
	vfmadd231ss	LCPI0_80(%rip), %xmm3, %xmm4 ## xmm4 = (xmm3 * mem) + xmm4
	vmulss	%xmm4, %xmm4, %xmm5
	movl	%eax, %ecx
	shll	$23, %ecx
	addl	$1065353216, %ecx               ## imm = 0x3F800000
	vmovss	LCPI0_82(%rip), %xmm6           ## xmm6 = mem[0],zero,zero,zero
	vfmadd231ss	%xmm1, %xmm5, %xmm6     ## xmm6 = (xmm5 * xmm1) + xmm6
	vfmadd213ss	LCPI0_83(%rip), %xmm5, %xmm6 ## xmm6 = (xmm5 * xmm6) + mem
	vmovss	LCPI0_86(%rip), %xmm7           ## xmm7 = mem[0],zero,zero,zero
	vfmadd231ss	%xmm0, %xmm5, %xmm7     ## xmm7 = (xmm5 * xmm0) + xmm7
	vfmadd213ss	LCPI0_87(%rip), %xmm5, %xmm7 ## xmm7 = (xmm5 * xmm7) + mem
	vfmadd213ss	%xmm2, %xmm5, %xmm6     ## xmm6 = (xmm5 * xmm6) + xmm2
	vfmadd213ss	%xmm2, %xmm5, %xmm7     ## xmm7 = (xmm5 * xmm7) + xmm2
	vfmadd231ss	%xmm6, %xmm4, %xmm7     ## xmm7 = (xmm4 * xmm6) + xmm7
	vmovd	%ecx, %xmm4
	vfmadd213ss	LCPI0_88(%rip), %xmm7, %xmm4 ## xmm4 = (xmm7 * xmm4) + mem
	vmovss	LCPI0_88(%rip), %xmm5           ## xmm5 = mem[0],zero,zero,zero
	vdivss	%xmm4, %xmm5, %xmm4
	jmp	LBB0_178
LBB0_177:
	vmovss	LCPI0_89(%rip), %xmm4           ## xmm4 = mem[0],zero,zero,zero
LBB0_178:                               ## %select.end
	vmovaps	%xmm4, 80(%rsp)                 ## 16-byte Spill
	movq	152(%rsp), %rbx                 ## 8-byte Reload
	vmovss	LCPI0_88(%rip), %xmm4           ## xmm4 = mem[0],zero,zero,zero
	cmpl	$-127, %eax
	jg	LBB0_180
## %bb.179:                             ## %select.end
	vmovaps	%xmm4, %xmm5
	vmovaps	%xmm4, 80(%rsp)                 ## 16-byte Spill
LBB0_180:                               ## %select.end
	vmulss	LCPI0_90(%rip), %xmm3, %xmm5
	vroundss	$9, %xmm5, %xmm5, %xmm5
	vcvttss2si	%xmm5, %eax
	cmpl	$127, %eax
	jg	LBB0_182
## %bb.181:                             ## %select.true.sink237
	vmulss	LCPI0_79(%rip), %xmm5, %xmm5
	vfmadd231ss	LCPI0_91(%rip), %xmm3, %xmm5 ## xmm5 = (xmm3 * mem) + xmm5
	vmulss	%xmm5, %xmm5, %xmm3
	movl	%eax, %ecx
	shll	$23, %ecx
	vfmadd213ss	LCPI0_82(%rip), %xmm3, %xmm1 ## xmm1 = (xmm3 * xmm1) + mem
	vfmadd213ss	LCPI0_83(%rip), %xmm3, %xmm1 ## xmm1 = (xmm3 * xmm1) + mem
	addl	$1065353216, %ecx               ## imm = 0x3F800000
	vfmadd213ss	%xmm2, %xmm3, %xmm1     ## xmm1 = (xmm3 * xmm1) + xmm2
	vfmadd213ss	LCPI0_86(%rip), %xmm3, %xmm0 ## xmm0 = (xmm3 * xmm0) + mem
	vfmadd213ss	LCPI0_87(%rip), %xmm3, %xmm0 ## xmm0 = (xmm3 * xmm0) + mem
	vfmadd213ss	%xmm2, %xmm3, %xmm0     ## xmm0 = (xmm3 * xmm0) + xmm2
	vfmadd231ss	%xmm1, %xmm5, %xmm0     ## xmm0 = (xmm5 * xmm1) + xmm0
	vmovd	%ecx, %xmm1
	vfmadd213ss	%xmm4, %xmm0, %xmm1     ## xmm1 = (xmm0 * xmm1) + xmm4
	vmovss	LCPI0_88(%rip), %xmm0           ## xmm0 = mem[0],zero,zero,zero
	vdivss	%xmm1, %xmm0, %xmm0
	vmovss	%xmm0, 112(%rsp)                ## 4-byte Spill
	movq	1120(%rsp), %rcx                ## 8-byte Reload
	cmpl	$-127, %eax
	jle	LBB0_183
	jmp	LBB0_184
LBB0_182:
	vmovss	LCPI0_89(%rip), %xmm0           ## xmm0 = mem[0],zero,zero,zero
	vmovss	%xmm0, 112(%rsp)                ## 4-byte Spill
	movq	1120(%rsp), %rcx                ## 8-byte Reload
	cmpl	$-127, %eax
	jg	LBB0_184
LBB0_183:                               ## %select.end236
	vmovss	LCPI0_88(%rip), %xmm0           ## xmm0 = mem[0],zero,zero,zero
	vmovss	%xmm0, 112(%rsp)                ## 4-byte Spill
LBB0_184:                               ## %select.end236
	movl	428(%rsp), %eax                 ## 4-byte Reload
	movl	%eax, 1896(%rsp)
	movl	424(%rsp), %eax                 ## 4-byte Reload
	movl	%eax, 1900(%rsp)
	vmovss	244(%rsp), %xmm0                ## 4-byte Reload
                                        ## xmm0 = mem[0],zero,zero,zero
	vmovss	%xmm0, 1904(%rsp)
	vmovss	112(%rsp), %xmm0                ## 4-byte Reload
                                        ## xmm0 = mem[0],zero,zero,zero
	vmovss	%xmm0, 1908(%rsp)
	vmovaps	80(%rsp), %xmm0                 ## 16-byte Reload
	vmovss	%xmm0, 1912(%rsp)
	movl	%r15d, 1916(%rsp)
	movl	%r12d, 1920(%rsp)
	movl	%ecx, 1924(%rsp)
	movq	1136(%rsp), %rax                ## 8-byte Reload
	movq	%rax, 1928(%rsp)
	movq	816(%rsp), %rax                 ## 8-byte Reload
	movq	%rax, 1936(%rsp)
	movq	%rdx, 1944(%rsp)
	movq	80(%rbp), %rax
	movq	%rax, 1952(%rsp)
	leaq	_train_cost_model.par_for.updated_head1_filter.s4.v235.v235.v235(%rip), %rsi
	leaq	1896(%rsp), %r8
	xorl	%edi, %edi
	xorl	%edx, %edx
	movl	$80, %ecx
	callq	_halide_do_par_for
	testl	%eax, %eax
	jne	LBB0_345
## %bb.185:                             ## %"produce head1_bias_im_0_d_def__"
	vxorps	%xmm0, %xmm0, %xmm0
	vmovaps	%ymm0, 7040(%rsp)
	cmpl	$0, 120(%rsp)                   ## 4-byte Folded Reload
	movq	216(%rsp), %rsi                 ## 8-byte Reload
	jle	LBB0_194
## %bb.186:                             ## %"for head1_bias_im_0_d_def__.s1.r1240$x.preheader"
	movl	120(%rsp), %edx                 ## 4-byte Reload
	leaq	-1(%rdx), %rcx
	movl	%edx, %eax
	andl	$7, %eax
	cmpq	$7, %rcx
	jae	LBB0_188
## %bb.187:
	xorl	%ecx, %ecx
	jmp	LBB0_190
LBB0_188:                               ## %"for head1_bias_im_0_d_def__.s1.r1240$x.preheader.new"
	andl	$-8, %edx
	leaq	224(%rsi), %rdi
	vxorps	%xmm0, %xmm0, %xmm0
	xorl	%ecx, %ecx
	.p2align	4, 0x90
LBB0_189:                               ## %"for head1_bias_im_0_d_def__.s1.r1240$x"
                                        ## =>This Inner Loop Header: Depth=1
	vaddps	-224(%rdi), %ymm0, %ymm0
	vaddps	-192(%rdi), %ymm0, %ymm0
	vaddps	-160(%rdi), %ymm0, %ymm0
	vaddps	-128(%rdi), %ymm0, %ymm0
	vaddps	-96(%rdi), %ymm0, %ymm0
	vaddps	-64(%rdi), %ymm0, %ymm0
	vaddps	-32(%rdi), %ymm0, %ymm0
	vaddps	(%rdi), %ymm0, %ymm0
	addq	$8, %rcx
	addq	$256, %rdi                      ## imm = 0x100
	cmpq	%rcx, %rdx
	jne	LBB0_189
LBB0_190:                               ## %call_destructor.exit826.loopexit.unr-lcssa
	testq	%rax, %rax
	je	LBB0_193
## %bb.191:                             ## %"for head1_bias_im_0_d_def__.s1.r1240$x.epil.preheader"
	shlq	$5, %rcx
	movq	%rsi, %rdx
	addq	%rcx, %rdx
	shlq	$5, %rax
	xorl	%ecx, %ecx
	.p2align	4, 0x90
LBB0_192:                               ## %"for head1_bias_im_0_d_def__.s1.r1240$x.epil"
                                        ## =>This Inner Loop Header: Depth=1
	vaddps	(%rdx,%rcx), %ymm0, %ymm0
	addq	$32, %rcx
	cmpq	%rcx, %rax
	jne	LBB0_192
LBB0_193:                               ## %call_destructor.exit826.loopexit
	vmovaps	%ymm0, 7040(%rsp)
LBB0_194:                               ## %call_destructor.exit826
	xorl	%edi, %edi
	vzeroupper
	callq	_halide_free
	vmovaps	7040(%rsp), %ymm0
	movq	1392(%rsp), %rdx                ## 8-byte Reload
	leaq	(%rdx,%rdx,2), %rax
	movq	1408(%rsp), %rcx                ## 8-byte Reload
	vmovups	%ymm0, (%rcx,%rax,4)
	vbroadcastss	LCPI0_92(%rip), %ymm1   ## ymm1 = [8.99999976E-1,8.99999976E-1,8.99999976E-1,8.99999976E-1,8.99999976E-1,8.99999976E-1,8.99999976E-1,8.99999976E-1]
	vmulps	(%rcx,%rdx,4), %ymm1, %ymm1
	vbroadcastss	LCPI0_93(%rip), %ymm2   ## ymm2 = [1.00000001E-1,1.00000001E-1,1.00000001E-1,1.00000001E-1,1.00000001E-1,1.00000001E-1,1.00000001E-1,1.00000001E-1]
	vfmadd213ps	%ymm1, %ymm0, %ymm2     ## ymm2 = (ymm0 * ymm2) + ymm1
	vmovups	%ymm2, (%rcx,%rdx,4)
	vmovups	(%rcx,%rax,4), %ymm0
	vbroadcastss	LCPI0_94(%rip), %ymm1   ## ymm1 = [9.99000012E-1,9.99000012E-1,9.99000012E-1,9.99000012E-1,9.99000012E-1,9.99000012E-1,9.99000012E-1,9.99000012E-1]
	vmulps	%ymm0, %ymm0, %ymm0
	vbroadcastss	LCPI0_95(%rip), %ymm2   ## ymm2 = [1.00000005E-3,1.00000005E-3,1.00000005E-3,1.00000005E-3,1.00000005E-3,1.00000005E-3,1.00000005E-3,1.00000005E-3]
	vmulps	%ymm2, %ymm0, %ymm0
	vfmadd231ps	(%rcx,%rdx,8), %ymm1, %ymm0 ## ymm0 = (ymm1 * mem) + ymm0
	vmovups	%ymm0, (%rcx,%rdx,8)
	vmovss	112(%rsp), %xmm1                ## 4-byte Reload
                                        ## xmm1 = mem[0],zero,zero,zero
	vmulss	244(%rsp), %xmm1, %xmm1         ## 4-byte Folded Reload
	vbroadcastss	%xmm1, %ymm1
	vmulps	(%rcx,%rdx,4), %ymm1, %ymm1
	vbroadcastss	80(%rsp), %ymm2         ## 16-byte Folded Reload
	vmulps	%ymm0, %ymm2, %ymm0
	vsqrtps	%ymm0, %ymm0
	vbroadcastss	LCPI0_96(%rip), %ymm2   ## ymm2 = [-9.99999974E-6,-9.99999974E-6,-9.99999974E-6,-9.99999974E-6,-9.99999974E-6,-9.99999974E-6,-9.99999974E-6,-9.99999974E-6]
	vsubps	%ymm0, %ymm2, %ymm0
	vdivps	%ymm0, %ymm1, %ymm0
	movq	1168(%rsp), %rax                ## 8-byte Reload
	vaddps	(%rax), %ymm0, %ymm0
	vmovups	%ymm0, (%rcx)
	shlq	$5, %rbx
	leaq	(%rbx,%rbx,2), %rsi
	movq	176(%rsp), %rax                 ## 8-byte Reload
	leaq	(%rax,%rax,2), %rdx
	movl	$2147483648, %eax               ## imm = 0x80000000
	cmpq	%rax, %rsi
	jae	LBB0_283
## %bb.195:                             ## %call_destructor.exit826
	movq	%rdx, %rax
	shrq	$32, %rax
	movl	%edx, %ecx
	movq	184(%rsp), %rdi                 ## 8-byte Reload
	imulq	%rdi, %rcx
	shrq	$32, %rcx
	imulq	%rdi, %rax
	addq	%rcx, %rax
	movabsq	$545460846592, %rcx             ## imm = 0x7F00000000
	andq	%rcx, %rax
	jne	LBB0_283
## %bb.196:                             ## %"assert succeeded480"
	orq	$12, %rsi
	xorl	%edi, %edi
	vzeroupper
	callq	_halide_malloc
	testq	%rax, %rax
	movq	120(%rsp), %r12                 ## 8-byte Reload
	movq	1400(%rsp), %r14                ## 8-byte Reload
	je	LBB0_348
## %bb.197:                             ## %"assert succeeded482"
	movq	%rax, %rsi
	movq	136(%rsp), %r15                 ## 8-byte Reload
	leal	4(%r15), %eax
	movl	%eax, %ecx
	sarl	$31, %ecx
	xorl	%ecx, %eax
	imulq	$1717986919, %rax, %rax         ## imm = 0x66666667
	shrq	$33, %rax
	xorl	%ecx, %eax
	andnl	%eax, %ecx, %ecx
	movl	40(%rsp), %edx                  ## 4-byte Reload
	movl	%edx, %eax
	xorl	%r15d, %eax
	imulq	$1717986919, %rax, %rax         ## imm = 0x66666667
	shrq	$33, %rax
	xorl	%edx, %eax
	andnl	%eax, %edx, %eax
	movl	%r15d, 6320(%rsp)
	movl	240(%rsp), %edx                 ## 4-byte Reload
	movl	%edx, 6324(%rsp)
	movl	344(%rsp), %edx                 ## 4-byte Reload
	movl	%edx, 6328(%rsp)
	movl	%r12d, 6332(%rsp)
	movl	%eax, 6336(%rsp)
	movl	%r13d, 6340(%rsp)
	movq	232(%rsp), %r13                 ## 8-byte Reload
	movl	%r13d, 6344(%rsp)
	movq	104(%rsp), %rax                 ## 8-byte Reload
	movq	%rax, 6352(%rsp)
	movq	$0, 6360(%rsp)
	movq	856(%rsp), %rax                 ## 8-byte Reload
	movq	%rax, 6368(%rsp)
	movq	40(%rbp), %rax
	movq	%rax, 6376(%rsp)
	movq	64(%rsp), %rbx                  ## 8-byte Reload
	movq	%rbx, 6384(%rsp)
	movq	$0, 6392(%rsp)
	movq	%rsi, 6400(%rsp)
	movq	$0, 6408(%rsp)
	movq	%rsi, 24(%rsp)                  ## 8-byte Spill
	leaq	_train_cost_model.par_for.head2_conv_1_d_def__.s0.n.n(%rip), %rsi
	leaq	6320(%rsp), %r8
	xorl	%edi, %edi
	xorl	%edx, %edx
	callq	_halide_do_par_for
	testl	%eax, %eax
	jne	LBB0_267
## %bb.198:                             ## %call_destructor.exit827
	xorl	%edi, %edi
	movq	%rbx, %rsi
	callq	_halide_free
	leal	(%r14,%r14,2), %edi
	movl	296(%rsp), %edx                 ## 4-byte Reload
	imull	$38, %edx, %eax
	subl	%r13d, %eax
	movl	944(%rsp), %esi                 ## 4-byte Reload
	imull	$38, %esi, %ecx
	addl	%edi, %ecx
	movl	%r15d, 5392(%rsp)
	movl	%edx, 5396(%rsp)
	movl	%r12d, 5400(%rsp)
	movl	%esi, %r12d
	movl	%eax, 5404(%rsp)
	movl	%ecx, 5408(%rsp)
	movl	%edi, 176(%rsp)                 ## 4-byte Spill
	movl	%edi, 5412(%rsp)
	movl	%r13d, 5416(%rsp)
	movl	%esi, 5420(%rsp)
	movq	24(%rsp), %rax                  ## 8-byte Reload
	movq	%rax, 5424(%rsp)
	movq	$0, 5432(%rsp)
	movq	48(%rsp), %rbx                  ## 8-byte Reload
	movq	%rbx, 5440(%rsp)
	movq	$0, 5448(%rsp)
	movq	1416(%rsp), %r15                ## 8-byte Reload
	movq	%r15, 5456(%rsp)
	movq	96(%rbp), %r13
	movq	%r13, 5464(%rsp)
	leaq	_train_cost_model.par_for.updated_head2_filter.s1.v240.v240.v240(%rip), %rsi
	leaq	5392(%rsp), %r8
	xorl	%edi, %edi
	xorl	%edx, %edx
	movl	$60, %ecx
	callq	_halide_do_par_for
	testl	%eax, %eax
	jne	LBB0_349
## %bb.199:                             ## %call_destructor.exit828
	xorl	%edi, %edi
	movq	%rbx, %rsi
	callq	_halide_free
	movl	%r12d, 1824(%rsp)
	movl	%r14d, 1828(%rsp)
	movq	%r15, 1832(%rsp)
	movq	%r13, 1840(%rsp)
	leaq	_train_cost_model.par_for.updated_head2_filter.s2.v240.v240.v240(%rip), %rsi
	leaq	1824(%rsp), %r8
	xorl	%edi, %edi
	xorl	%edx, %edx
	movl	$60, %ecx
	callq	_halide_do_par_for
	testl	%eax, %eax
	jne	LBB0_266
## %bb.200:                             ## %"assert succeeded491"
	leal	(%r14,%r14), %eax
	movl	176(%rsp), %ecx                 ## 4-byte Reload
	movl	%ecx, 5472(%rsp)
	movl	%eax, 5476(%rsp)
	movl	%r12d, 5480(%rsp)
	movq	%r15, 5488(%rsp)
	movq	%r13, 5496(%rsp)
	leaq	_train_cost_model.par_for.updated_head2_filter.s3.v240.v240.v240(%rip), %rsi
	leaq	5472(%rsp), %r8
	xorl	%edi, %edi
	xorl	%edx, %edx
	movl	$60, %ecx
	callq	_halide_do_par_for
	testl	%eax, %eax
	jne	LBB0_266
## %bb.201:                             ## %"assert succeeded493"
	movl	516(%rsp), %eax                 ## 4-byte Reload
	movl	%eax, 3288(%rsp)
	vmovss	244(%rsp), %xmm0                ## 4-byte Reload
                                        ## xmm0 = mem[0],zero,zero,zero
	vmovss	%xmm0, 3292(%rsp)
	vmovss	112(%rsp), %xmm0                ## 4-byte Reload
                                        ## xmm0 = mem[0],zero,zero,zero
	vmovss	%xmm0, 3296(%rsp)
	vmovaps	80(%rsp), %xmm0                 ## 16-byte Reload
	vmovss	%xmm0, 3300(%rsp)
	movl	%r12d, 3304(%rsp)
	movl	%r14d, 3308(%rsp)
	movq	1176(%rsp), %rax                ## 8-byte Reload
	movq	%rax, 3312(%rsp)
	movq	24(%rbp), %rax
	movq	%rax, 3320(%rsp)
	movq	%r15, 3328(%rsp)
	movq	%r13, 3336(%rsp)
	leaq	_train_cost_model.par_for.updated_head2_filter.s4.v240.v240.v240(%rip), %rsi
	leaq	3288(%rsp), %r8
	xorl	%edi, %edi
	xorl	%edx, %edx
	movl	$60, %ecx
	callq	_halide_do_par_for
	testl	%eax, %eax
	jne	LBB0_266
## %bb.202:                             ## %"consume head2_conv_1_d_def__496"
	movq	1424(%rsp), %r14                ## 8-byte Reload
	leal	(%r14,%r14,2), %eax
	movq	136(%rsp), %rcx                 ## 8-byte Reload
	movl	%ecx, 6416(%rsp)
	movq	120(%rsp), %rcx                 ## 8-byte Reload
	movl	%ecx, 6420(%rsp)
	movl	%eax, 6424(%rsp)
	movq	24(%rsp), %rbx                  ## 8-byte Reload
	movq	%rbx, 6432(%rsp)
	movq	$0, 6440(%rsp)
	movq	1432(%rsp), %r13                ## 8-byte Reload
	movq	%r13, 6448(%rsp)
	movq	104(%rbp), %r12
	movq	%r12, 6456(%rsp)
	leaq	_train_cost_model.par_for.updated_head2_bias.s1.v243.v243(%rip), %rsi
	leaq	6416(%rsp), %r8
	xorl	%edi, %edi
	xorl	%edx, %edx
	movl	$3, %ecx
	callq	_halide_do_par_for
	testl	%eax, %eax
	jne	LBB0_266
## %bb.203:                             ## %call_destructor.exit829
	xorl	%edi, %edi
	movq	%rbx, %rsi
	callq	_halide_free
	movl	%r14d, 6464(%rsp)
	movq	%r13, 6472(%rsp)
	movq	%r12, 6480(%rsp)
	leaq	_train_cost_model.par_for.updated_head2_bias.s2.v243.v243(%rip), %rsi
	leaq	6464(%rsp), %r8
	xorl	%edi, %edi
	xorl	%edx, %edx
	movl	$3, %ecx
	callq	_halide_do_par_for
	testl	%eax, %eax
	jne	LBB0_268
## %bb.204:                             ## %"assert succeeded500"
	movl	%r14d, 6488(%rsp)
	movq	%r13, 6496(%rsp)
	movq	%r12, 6504(%rsp)
	leaq	_train_cost_model.par_for.updated_head2_bias.s3.v243.v243(%rip), %rsi
	leaq	6488(%rsp), %r8
	xorl	%edi, %edi
	xorl	%edx, %edx
	movl	$3, %ecx
	callq	_halide_do_par_for
	testl	%eax, %eax
	movq	136(%rsp), %r15                 ## 8-byte Reload
	jne	LBB0_268
## %bb.205:                             ## %"assert succeeded502"
	vmovss	244(%rsp), %xmm0                ## 4-byte Reload
                                        ## xmm0 = mem[0],zero,zero,zero
	vmovss	%xmm0, 5504(%rsp)
	vmovss	112(%rsp), %xmm0                ## 4-byte Reload
                                        ## xmm0 = mem[0],zero,zero,zero
	vmovss	%xmm0, 5508(%rsp)
	vmovaps	80(%rsp), %xmm0                 ## 16-byte Reload
	vmovss	%xmm0, 5512(%rsp)
	movl	%r14d, 5516(%rsp)
	movq	1184(%rsp), %rax                ## 8-byte Reload
	movq	%rax, 5520(%rsp)
	movq	32(%rbp), %rax
	movq	%rax, 5528(%rsp)
	movq	%r13, 5536(%rsp)
	movq	%r12, 5544(%rsp)
	leaq	_train_cost_model.par_for.updated_head2_bias.s4.v243.v243(%rip), %rsi
	leaq	5504(%rsp), %r8
	xorl	%edi, %edi
	xorl	%edx, %edx
	movl	$3, %ecx
	callq	_halide_do_par_for
	testl	%eax, %eax
	jne	LBB0_268
## %bb.206:                             ## %"produce filter1_im_0_d_def__"
	leaq	7040(%rsp), %r13
	movq	%r13, 5552(%rsp)
	movq	$0, 5560(%rsp)
	leaq	_train_cost_model.par_for.filter1_im_0_d_def__.s0.v12(%rip), %rsi
	leaq	5552(%rsp), %r8
	xorl	%edi, %edi
	xorl	%edx, %edx
	movl	$32, %ecx
	callq	_halide_do_par_for
	testl	%eax, %eax
	movq	128(%rsp), %rbx                 ## 8-byte Reload
	movl	480(%rsp), %ecx                 ## 4-byte Reload
	jne	LBB0_268
## %bb.207:                             ## %"consume head2_relu508"
	movl	%r15d, 6256(%rsp)
	movl	%ecx, 6260(%rsp)
	movq	120(%rsp), %r14                 ## 8-byte Reload
	movl	%r14d, 6264(%rsp)
	movq	232(%rsp), %rax                 ## 8-byte Reload
	movl	%eax, 6268(%rsp)
	movq	104(%rsp), %r15                 ## 8-byte Reload
	movq	%r15, 6272(%rsp)
	movq	$0, 6280(%rsp)
	movq	%r13, 6288(%rsp)
	movq	$0, 6296(%rsp)
	movq	%rbx, 6304(%rsp)
	movq	$0, 6312(%rsp)
	leaq	_train_cost_model.par_for.filter1_im_0_d_def__.s1.v12(%rip), %rsi
	leaq	6256(%rsp), %r8
	xorl	%edi, %edi
	xorl	%edx, %edx
	movl	$32, %ecx
	callq	_halide_do_par_for
	testl	%eax, %eax
	jne	LBB0_350
## %bb.208:                             ## %call_destructor.exit831
	xorl	%edi, %edi
	movq	%rbx, %rsi
	callq	_halide_free
	xorl	%edi, %edi
	movq	%r15, %rsi
	callq	_halide_free
	movl	%r14d, 6760(%rsp)
	movq	32(%rsp), %rax                  ## 8-byte Reload
	movq	%rax, 6768(%rsp)
	movq	$0, 6776(%rsp)
	movq	%r13, %r14
	movq	%r13, 6784(%rsp)
	movq	$0, 6792(%rsp)
	movq	200(%rsp), %rbx                 ## 8-byte Reload
	movq	%rbx, 6800(%rsp)
	movq	$0, 6808(%rsp)
	leaq	_train_cost_model.par_for.filter1_im_0_d_def__.s2.v12(%rip), %rsi
	leaq	6760(%rsp), %r8
	xorl	%edi, %edi
	xorl	%edx, %edx
	movl	$32, %ecx
	callq	_halide_do_par_for
	testl	%eax, %eax
	jne	LBB0_351
## %bb.209:                             ## %call_destructor.exit832
	xorl	%edi, %edi
	movq	%rbx, %rsi
	callq	_halide_free
	movq	1440(%rsp), %r15                ## 8-byte Reload
	leal	(%r15,%r15,2), %r13d
	movl	%r13d, 6176(%rsp)
	movl	948(%rsp), %r12d                ## 4-byte Reload
	movl	%r12d, 6180(%rsp)
	movq	%r14, 6184(%rsp)
	movq	$0, 6192(%rsp)
	movq	1448(%rsp), %rbx                ## 8-byte Reload
	movq	%rbx, 6200(%rsp)
	movq	112(%rbp), %r14
	movq	%r14, 6208(%rsp)
	leaq	_train_cost_model.par_for.updated_filter1.s1.v245.v245.v245(%rip), %rsi
	leaq	6176(%rsp), %r8
	xorl	%edi, %edi
	xorl	%edx, %edx
	movl	$32, %ecx
	callq	_halide_do_par_for
	testl	%eax, %eax
	jne	LBB0_269
## %bb.210:                             ## %"assert succeeded516"
	movl	%r12d, 1848(%rsp)
	movl	%r15d, 1852(%rsp)
	movq	%rbx, 1856(%rsp)
	movq	%r14, 1864(%rsp)
	leaq	_train_cost_model.par_for.updated_filter1.s2.v245.v245.v245(%rip), %rsi
	leaq	1848(%rsp), %r8
	xorl	%edi, %edi
	xorl	%edx, %edx
	movl	$32, %ecx
	callq	_halide_do_par_for
	testl	%eax, %eax
	movq	1456(%rsp), %r14                ## 8-byte Reload
	jne	LBB0_269
## %bb.211:                             ## %"assert succeeded518"
	leal	(%r15,%r15), %eax
	movl	%eax, 5568(%rsp)
	movl	%r13d, 5572(%rsp)
	movl	%r12d, 5576(%rsp)
	movq	%rbx, 5584(%rsp)
	movq	112(%rbp), %rax
	movq	%rax, 5592(%rsp)
	leaq	_train_cost_model.par_for.updated_filter1.s3.v245.v245.v245(%rip), %rsi
	leaq	5568(%rsp), %r8
	xorl	%edi, %edi
	xorl	%edx, %edx
	movl	$32, %ecx
	callq	_halide_do_par_for
	testl	%eax, %eax
	jne	LBB0_269
## %bb.212:                             ## %"assert succeeded520"
	movl	240(%rsp), %eax                 ## 4-byte Reload
	movl	%eax, 3344(%rsp)
	vmovss	244(%rsp), %xmm0                ## 4-byte Reload
                                        ## xmm0 = mem[0],zero,zero,zero
	vmovss	%xmm0, 3348(%rsp)
	vmovss	112(%rsp), %xmm0                ## 4-byte Reload
                                        ## xmm0 = mem[0],zero,zero,zero
	vmovss	%xmm0, 3352(%rsp)
	vmovaps	80(%rsp), %xmm0                 ## 16-byte Reload
	vmovss	%xmm0, 3356(%rsp)
	movl	%r12d, 3360(%rsp)
	movl	%r15d, 3364(%rsp)
	movq	856(%rsp), %rax                 ## 8-byte Reload
	movq	%rax, 3368(%rsp)
	movq	40(%rbp), %rax
	movq	%rax, 3376(%rsp)
	movq	%rbx, 3384(%rsp)
	movq	112(%rbp), %rax
	movq	%rax, 3392(%rsp)
	leaq	_train_cost_model.par_for.updated_filter1.s4.v245.v245.v245(%rip), %rsi
	leaq	3344(%rsp), %r8
	xorl	%edi, %edi
	xorl	%edx, %edx
	movl	$32, %ecx
	callq	_halide_do_par_for
	testl	%eax, %eax
	movq	120(%rbp), %r12
	jne	LBB0_269
## %bb.213:                             ## %"consume conv1_stage1_1_d_def__523"
	leal	(%r14,%r14,2), %eax
	movq	120(%rsp), %rcx                 ## 8-byte Reload
	movl	%ecx, 6216(%rsp)
	movl	%eax, 6220(%rsp)
	movq	32(%rsp), %rbx                  ## 8-byte Reload
	movq	%rbx, 6224(%rsp)
	movq	$0, 6232(%rsp)
	movq	1464(%rsp), %r15                ## 8-byte Reload
	movq	%r15, 6240(%rsp)
	movq	%r12, 6248(%rsp)
	leaq	_train_cost_model.par_for.updated_bias1.s1.v248.v248(%rip), %rsi
	leaq	6216(%rsp), %r8
	xorl	%edi, %edi
	xorl	%edx, %edx
	movl	$4, %ecx
	callq	_halide_do_par_for
	testl	%eax, %eax
	jne	LBB0_353
## %bb.214:                             ## %call_destructor.exit833
	xorl	%edi, %edi
	movq	%rbx, %rsi
	callq	_halide_free
	movl	%r14d, 6512(%rsp)
	movq	%r15, 6520(%rsp)
	movq	%r12, 6528(%rsp)
	leaq	_train_cost_model.par_for.updated_bias1.s2.v248.v248(%rip), %rsi
	leaq	6512(%rsp), %r8
	xorl	%edi, %edi
	xorl	%edx, %edx
	movl	$4, %ecx
	callq	_halide_do_par_for
	testl	%eax, %eax
	jne	LBB0_352
## %bb.215:                             ## %"assert succeeded527"
	movl	%r14d, 6536(%rsp)
	movq	%r15, 6544(%rsp)
	movq	%r12, 6552(%rsp)
	leaq	_train_cost_model.par_for.updated_bias1.s3.v248.v248(%rip), %rsi
	leaq	6536(%rsp), %r8
	xorl	%edi, %edi
	xorl	%edx, %edx
	movl	$4, %ecx
	callq	_halide_do_par_for
	testl	%eax, %eax
	jne	LBB0_270
## %bb.216:                             ## %"assert succeeded529"
	vmovss	244(%rsp), %xmm0                ## 4-byte Reload
                                        ## xmm0 = mem[0],zero,zero,zero
	vmovss	%xmm0, 5600(%rsp)
	vmovss	112(%rsp), %xmm0                ## 4-byte Reload
                                        ## xmm0 = mem[0],zero,zero,zero
	vmovss	%xmm0, 5604(%rsp)
	vmovaps	80(%rsp), %xmm0                 ## 16-byte Reload
	vmovss	%xmm0, 5608(%rsp)
	movl	%r14d, 5612(%rsp)
	movq	1192(%rsp), %rax                ## 8-byte Reload
	movq	%rax, 5616(%rsp)
	movq	48(%rbp), %rax
	movq	%rax, 5624(%rsp)
	movq	%r15, 5632(%rsp)
	movq	%r12, 5640(%rsp)
	leaq	_train_cost_model.par_for.updated_bias1.s4.v248.v248(%rip), %rsi
	leaq	5600(%rsp), %r8
	xorl	%edi, %edi
	xorl	%edx, %edx
	movl	$4, %ecx
	callq	_halide_do_par_for
	testl	%eax, %eax
	jne	LBB0_270
## %bb.217:                             ## %"consume f1532"
	movl	1340(%rsp), %eax                ## 4-byte Reload
	andnl	848(%rsp), %eax, %eax           ## 4-byte Folded Reload
	movq	832(%rsp), %rdx                 ## 8-byte Reload
	leal	7(%rdx), %ecx
	sarl	$3, %ecx
	movl	%edx, 6560(%rsp)
	sarl	$3, %edx
	movl	%edx, 6564(%rsp)
	movl	%eax, 6568(%rsp)
	movq	56(%rsp), %rax                  ## 8-byte Reload
	movq	%rax, 6576(%rsp)
	movq	$0, 6584(%rsp)
	movq	1472(%rsp), %rax                ## 8-byte Reload
	movq	%rax, 6592(%rsp)
	movq	128(%rbp), %rax
	movq	%rax, 6600(%rsp)
	leaq	_train_cost_model.par_for.prediction_output.s0.n.n(%rip), %rsi
	leaq	6560(%rsp), %r8
	xorl	%edi, %edi
	xorl	%edx, %edx
	callq	_halide_do_par_for
	testl	%eax, %eax
	movl	264(%rsp), %ebx                 ## 4-byte Reload
	movl	160(%rsp), %r14d                ## 4-byte Reload
	jne	LBB0_270
## %bb.218:                             ## %"assert succeeded534"
	movq	184(%rsp), %rdx                 ## 8-byte Reload
	movl	%edx, %eax
	shlq	$2, %rdx
	cmpl	$536870912, %eax                ## imm = 0x20000000
	jae	LBB0_354
## %bb.219:                             ## %"assert succeeded536"
	addq	$12, %rdx
	xorl	%edi, %edi
	movq	%rdx, %rsi
	callq	_halide_malloc
	testq	%rax, %rax
	je	LBB0_355
## %bb.220:                             ## %"assert succeeded538"
	movq	%rax, %r15
	movq	136(%rsp), %r13                 ## 8-byte Reload
	movl	%r13d, 1872(%rsp)
	movl	144(%rsp), %r12d                ## 4-byte Reload
	movl	%r12d, 1876(%rsp)
	movq	%rax, 1880(%rsp)
	movq	$0, 1888(%rsp)
	leaq	_train_cost_model.par_for.sum.s0.n.n(%rip), %rsi
	leaq	1872(%rsp), %r8
	xorl	%edi, %edi
	xorl	%edx, %edx
	movl	%r14d, %ecx
	callq	_halide_do_par_for
	testl	%eax, %eax
	jne	LBB0_284
## %bb.221:                             ## %"consume conv1_stage2541"
	movl	%r13d, 5944(%rsp)
	movl	%ebx, 5948(%rsp)
	movq	120(%rsp), %rax                 ## 8-byte Reload
	movl	%eax, 5952(%rsp)
	movl	%r12d, 5956(%rsp)
	movq	232(%rsp), %r12                 ## 8-byte Reload
	movl	%r12d, 5960(%rsp)
	movq	168(%rsp), %rbx                 ## 8-byte Reload
	movq	%rbx, 5968(%rsp)
	movq	$0, 5976(%rsp)
	movq	%r15, 5984(%rsp)
	movq	$0, 5992(%rsp)
	leaq	_train_cost_model.par_for.sum.s1.n.n(%rip), %rsi
	leaq	5944(%rsp), %r8
	xorl	%edi, %edi
	xorl	%edx, %edx
	movl	%r14d, %ecx
	callq	_halide_do_par_for
	testl	%eax, %eax
	jne	LBB0_284
## %bb.222:                             ## %call_destructor.exit834
	xorl	%r14d, %r14d
	xorl	%edi, %edi
	movq	%rbx, %rsi
	callq	_halide_free
	movl	$0, 7040(%rsp)
	testl	%r13d, %r13d
	movq	56(%rsp), %rsi                  ## 8-byte Reload
	jle	LBB0_234
## %bb.223:                             ## %"for sum$1.s1.r87$x.preheader"
	movslq	1128(%rsp), %r9                 ## 4-byte Folded Reload
	movslq	472(%rsp), %rcx                 ## 4-byte Folded Reload
	movslq	%r12d, %r8
	vmovss	LCPI0_88(%rip), %xmm0           ## xmm0 = mem[0],zero,zero,zero
	movq	1200(%rsp), %rax                ## 8-byte Reload
	vdivss	(%rax,%rcx,4), %xmm0, %xmm0
	movl	%r13d, %r11d
	cmpl	$8, %r13d
	jae	LBB0_225
## %bb.224:
	vxorps	%xmm1, %xmm1, %xmm1
	xorl	%ecx, %ecx
	jmp	LBB0_231
LBB0_225:                               ## %vector.ph
	movl	%r11d, %ecx
	andl	$-8, %ecx
	vbroadcastss	%xmm0, %ymm1
	leaq	-8(%rcx), %rdx
	movq	%rdx, %r10
	shrq	$3, %r10
	incq	%r10
	testq	%rdx, %rdx
	je	LBB0_263
## %bb.226:                             ## %vector.ph.new
	leaq	(,%r9,4), %rsi
	movq	%rax, %rdx
	subq	%rsi, %rdx
	addq	$32, %rdx
	leaq	(,%r8,4), %rdi
	movq	56(%rsp), %rsi                  ## 8-byte Reload
	subq	%rdi, %rsi
	addq	$32, %rsi
	movq	%r10, %rdi
	andq	$-2, %rdi
	negq	%rdi
	vxorps	%xmm2, %xmm2, %xmm2
	xorl	%ebx, %ebx
	vbroadcastss	LCPI0_97(%rip), %ymm3   ## ymm3 = [1.00000001E-10,1.00000001E-10,1.00000001E-10,1.00000001E-10,1.00000001E-10,1.00000001E-10,1.00000001E-10,1.00000001E-10]
	vbroadcastss	LCPI0_88(%rip), %ymm4   ## ymm4 = [1.0E+0,1.0E+0,1.0E+0,1.0E+0,1.0E+0,1.0E+0,1.0E+0,1.0E+0]
	vbroadcastss	LCPI0_98(%rip), %ymm5   ## ymm5 = [9.99999974E-6,9.99999974E-6,9.99999974E-6,9.99999974E-6,9.99999974E-6,9.99999974E-6,9.99999974E-6,9.99999974E-6]
	.p2align	4, 0x90
LBB0_227:                               ## %vector.body
                                        ## =>This Inner Loop Header: Depth=1
	vmulps	-32(%rsi,%rbx,4), %ymm1, %ymm6
	vmaxps	%ymm3, %ymm6, %ymm6
	vdivps	%ymm6, %ymm4, %ymm6
	vmulps	-32(%rdx,%rbx,4), %ymm1, %ymm7
	vdivps	%ymm7, %ymm4, %ymm7
	vmulps	(%rsi,%rbx,4), %ymm1, %ymm8
	vmaxps	%ymm3, %ymm8, %ymm8
	vdivps	%ymm8, %ymm4, %ymm8
	vsubps	%ymm7, %ymm6, %ymm6
	vmulps	(%rdx,%rbx,4), %ymm1, %ymm7
	vfmadd213ps	%ymm2, %ymm6, %ymm6     ## ymm6 = (ymm6 * ymm6) + ymm2
	vdivps	%ymm7, %ymm4, %ymm2
	vfmadd231ps	(%r15,%rbx,4), %ymm5, %ymm6 ## ymm6 = (ymm5 * mem) + ymm6
	vsubps	%ymm2, %ymm8, %ymm2
	vfmadd213ps	%ymm6, %ymm2, %ymm2     ## ymm2 = (ymm2 * ymm2) + ymm6
	vfmadd231ps	32(%r15,%rbx,4), %ymm5, %ymm2 ## ymm2 = (ymm5 * mem) + ymm2
	addq	$16, %rbx
	addq	$2, %rdi
	jne	LBB0_227
## %bb.228:                             ## %middle.block.unr-lcssa
	testb	$1, %r10b
	movq	56(%rsp), %rsi                  ## 8-byte Reload
	je	LBB0_230
LBB0_229:                               ## %vector.body.epil
	movq	%rbx, %rdx
	subq	%r8, %rdx
	vmulps	(%rsi,%rdx,4), %ymm1, %ymm3
	vbroadcastss	LCPI0_97(%rip), %ymm4   ## ymm4 = [1.00000001E-10,1.00000001E-10,1.00000001E-10,1.00000001E-10,1.00000001E-10,1.00000001E-10,1.00000001E-10,1.00000001E-10]
	vmaxps	%ymm4, %ymm3, %ymm3
	vbroadcastss	LCPI0_88(%rip), %ymm4   ## ymm4 = [1.0E+0,1.0E+0,1.0E+0,1.0E+0,1.0E+0,1.0E+0,1.0E+0,1.0E+0]
	vdivps	%ymm3, %ymm4, %ymm3
	movq	%rbx, %rdx
	subq	%r9, %rdx
	vmulps	(%rax,%rdx,4), %ymm1, %ymm1
	vdivps	%ymm1, %ymm4, %ymm1
	vsubps	%ymm1, %ymm3, %ymm1
	vbroadcastss	LCPI0_98(%rip), %ymm3   ## ymm3 = [9.99999974E-6,9.99999974E-6,9.99999974E-6,9.99999974E-6,9.99999974E-6,9.99999974E-6,9.99999974E-6,9.99999974E-6]
	vfmadd213ps	%ymm2, %ymm1, %ymm1     ## ymm1 = (ymm1 * ymm1) + ymm2
	vfmadd231ps	(%r15,%rbx,4), %ymm3, %ymm1 ## ymm1 = (ymm3 * mem) + ymm1
	vmovaps	%ymm1, %ymm2
LBB0_230:                               ## %middle.block
	vextractf128	$1, %ymm2, %xmm1
	vaddps	%xmm1, %xmm2, %xmm1
	vpermilpd	$1, %xmm1, %xmm2        ## xmm2 = xmm1[1,0]
	vaddps	%xmm2, %xmm1, %xmm1
	vmovshdup	%xmm1, %xmm2            ## xmm2 = xmm1[1,1,3,3]
	vaddss	%xmm2, %xmm1, %xmm1
	cmpq	%r11, %rcx
	je	LBB0_233
LBB0_231:                               ## %"for sum$1.s1.r87$x.preheader1288"
	shlq	$2, %r9
	subq	%r9, %rax
	shlq	$2, %r8
	movq	%rsi, %rdx
	subq	%r8, %rdx
	vmovss	LCPI0_97(%rip), %xmm2           ## xmm2 = mem[0],zero,zero,zero
	vbroadcastss	LCPI0_88(%rip), %xmm3   ## xmm3 = [1.0E+0,1.0E+0,1.0E+0,1.0E+0]
	vmovss	LCPI0_98(%rip), %xmm4           ## xmm4 = mem[0],zero,zero,zero
	vmovaps	%xmm1, %xmm5
	.p2align	4, 0x90
LBB0_232:                               ## %"for sum$1.s1.r87$x"
                                        ## =>This Inner Loop Header: Depth=1
	vmulss	(%rdx,%rcx,4), %xmm0, %xmm1
	vmaxss	%xmm2, %xmm1, %xmm1
	vmulss	(%rax,%rcx,4), %xmm0, %xmm6
	vinsertps	$16, %xmm6, %xmm1, %xmm1 ## xmm1 = xmm1[0],xmm6[0],xmm1[2,3]
	vdivps	%xmm1, %xmm3, %xmm1
	vmovshdup	%xmm1, %xmm6            ## xmm6 = xmm1[1,1,3,3]
	vsubss	%xmm6, %xmm1, %xmm1
	vfmadd213ss	%xmm5, %xmm1, %xmm1     ## xmm1 = (xmm1 * xmm1) + xmm5
	vfmadd231ss	(%r15,%rcx,4), %xmm4, %xmm1 ## xmm1 = (xmm4 * mem) + xmm1
	incq	%rcx
	vmovaps	%xmm1, %xmm5
	cmpq	%rcx, %r11
	jne	LBB0_232
LBB0_233:                               ## %"end for sum$1.s1.r87$x.thread.loopexit"
	vmovss	%xmm1, 7040(%rsp)
LBB0_234:                               ## %call_destructor.exit836
	xorl	%edi, %edi
	vzeroupper
	callq	_halide_free
	xorl	%edi, %edi
	movq	%r15, %rsi
	callq	_halide_free
	vmovss	7040(%rsp), %xmm0               ## xmm0 = mem[0],zero,zero,zero
	movq	1480(%rsp), %rax                ## 8-byte Reload
	vmovss	%xmm0, (%rax)
LBB0_235:
	xorl	%eax, %eax
	movq	%rax, 32(%rsp)                  ## 8-byte Spill
LBB0_236:                               ## %call_destructor.exit750.thread972
	xorl	%edx, %edx
	xorl	%eax, %eax
	movq	%rax, 24(%rsp)                  ## 8-byte Spill
	xorl	%eax, %eax
	movq	%rax, 80(%rsp)                  ## 8-byte Spill
	xorl	%eax, %eax
	movq	%rax, 40(%rsp)                  ## 8-byte Spill
	xorl	%ecx, %ecx
	xorl	%eax, %eax
	movq	%rax, 168(%rsp)                 ## 8-byte Spill
	xorl	%eax, %eax
	movq	%rax, 128(%rsp)                 ## 8-byte Spill
	xorl	%esi, %esi
	xorl	%r12d, %r12d
	xorl	%eax, %eax
	movq	%rax, 200(%rsp)                 ## 8-byte Spill
LBB0_237:                               ## %call_destructor.exit750.thread972
	movq	%rcx, 56(%rsp)                  ## 8-byte Spill
	testl	%r14d, %r14d
	setne	%r15b
	movq	%rsi, %r13
	testb	%r15b, %r15b
	je	LBB0_240
LBB0_238:                               ## %call_destructor.exit751
	cmpq	$0, 32(%rsp)                    ## 8-byte Folded Reload
	je	LBB0_240
## %bb.239:
	xorl	%edi, %edi
	movq	32(%rsp), %rsi                  ## 8-byte Reload
	movq	%rdx, %rbx
	vzeroupper
	callq	_halide_free
	movq	%rbx, %rdx
LBB0_240:                               ## %call_destructor.exit752
	testq	%rdx, %rdx
	sete	%al
	movl	%r15d, %ebx
	xorb	$1, %bl
	orb	%bl, %al
	jne	LBB0_242
## %bb.241:
	xorl	%edi, %edi
	movq	%rdx, %rsi
	vzeroupper
	callq	_halide_free
LBB0_242:                               ## %call_destructor.exit753
	movq	24(%rsp), %rsi                  ## 8-byte Reload
	testq	%rsi, %rsi
	sete	%al
	orb	%bl, %al
	jne	LBB0_244
## %bb.243:
	xorl	%edi, %edi
	vzeroupper
	callq	_halide_free
LBB0_244:                               ## %call_destructor.exit754
	movq	80(%rsp), %rsi                  ## 8-byte Reload
	testq	%rsi, %rsi
	sete	%al
	orb	%bl, %al
	jne	LBB0_246
## %bb.245:
	xorl	%edi, %edi
	vzeroupper
	callq	_halide_free
LBB0_246:                               ## %call_destructor.exit755
	movq	40(%rsp), %rsi                  ## 8-byte Reload
	testq	%rsi, %rsi
	sete	%al
	orb	%al, %bl
	jne	LBB0_248
LBB0_247:                               ## %call_destructor.exit756.sink.split
	xorl	%edi, %edi
	vzeroupper
	callq	_halide_free
	movb	$1, %r15b
LBB0_248:                               ## %call_destructor.exit756
	movq	56(%rsp), %rsi                  ## 8-byte Reload
	testq	%rsi, %rsi
	je	LBB0_251
## %bb.249:                             ## %call_destructor.exit756
	testb	%r15b, %r15b
	je	LBB0_251
## %bb.250:
	xorl	%edi, %edi
	vzeroupper
	callq	_halide_free
LBB0_251:                               ## %call_destructor.exit757
	movq	168(%rsp), %rsi                 ## 8-byte Reload
	testq	%rsi, %rsi
	sete	%al
	xorb	$1, %r15b
	orb	%r15b, %al
	jne	LBB0_253
## %bb.252:
	xorl	%edi, %edi
	vzeroupper
	callq	_halide_free
LBB0_253:                               ## %call_destructor.exit758
	movq	128(%rsp), %rsi                 ## 8-byte Reload
	testq	%rsi, %rsi
	sete	%al
	orb	%r15b, %al
	jne	LBB0_254
## %bb.261:
	xorl	%edi, %edi
	vzeroupper
	callq	_halide_free
	testq	%r13, %r13
	sete	%al
	orb	%r15b, %al
	je	LBB0_262
LBB0_255:                               ## %call_destructor.exit760
	testq	%r12, %r12
	sete	%al
	orb	%r15b, %al
	jne	LBB0_257
LBB0_256:
	xorl	%edi, %edi
	movq	%r12, %rsi
	vzeroupper
	callq	_halide_free
LBB0_257:                               ## %call_destructor.exit761
	movq	200(%rsp), %rsi                 ## 8-byte Reload
	testq	%rsi, %rsi
	sete	%al
	orb	%r15b, %al
	jne	LBB0_259
## %bb.258:
	xorl	%edi, %edi
	vzeroupper
	callq	_halide_free
LBB0_259:                               ## %call_destructor.exit762
	movl	%r14d, %eax
LBB0_260:                               ## %call_destructor.exit762
	leaq	-40(%rbp), %rsp
	popq	%rbx
	popq	%r12
	popq	%r13
	popq	%r14
	popq	%r15
	popq	%rbp
	vzeroupper
	retq
LBB0_254:                               ## %call_destructor.exit759
	testq	%r13, %r13
	sete	%al
	orb	%r15b, %al
	jne	LBB0_255
LBB0_262:
	xorl	%edi, %edi
	movq	%r13, %rsi
	vzeroupper
	callq	_halide_free
	testq	%r12, %r12
	sete	%al
	orb	%r15b, %al
	je	LBB0_256
	jmp	LBB0_257
LBB0_72:                                ## %after_bb62
	movq	40(%r12), %rax
	vxorps	%xmm0, %xmm0, %xmm0
	vmovups	%xmm0, (%r12)
	movq	$0, 16(%r12)
	movabsq	$12884975618, %rcx              ## imm = 0x300012002
	movq	%rcx, 32(%r12)
	vmovaps	LCPI0_10(%rip), %xmm0           ## xmm0 = [0,24,1,0]
	vmovups	%xmm0, (%rax)
	movq	40(%r12), %rax
	vmovaps	LCPI0_21(%rip), %xmm0           ## xmm0 = [0,39,24,0]
	vmovups	%xmm0, 16(%rax)
	movq	40(%r12), %rax
	vmovaps	LCPI0_22(%rip), %xmm0           ## xmm0 = [0,4,936,0]
	vmovups	%xmm0, 32(%rax)
	movq	120(%rbp), %rax
	movq	$0, 24(%r12)
	cmpq	$0, 16(%r12)
	jne	LBB0_53
	jmp	LBB0_56
LBB0_263:
	vxorps	%xmm2, %xmm2, %xmm2
	xorl	%ebx, %ebx
	testb	$1, %r10b
	movq	56(%rsp), %rsi                  ## 8-byte Reload
	jne	LBB0_229
	jmp	LBB0_230
LBB0_264:
	movl	%eax, %r14d
LBB0_265:
	xorl	%eax, %eax
	movq	%rax, 32(%rsp)                  ## 8-byte Spill
	xorl	%edx, %edx
	xorl	%eax, %eax
	movq	%rax, 24(%rsp)                  ## 8-byte Spill
	jmp	LBB0_329
LBB0_345:
	movl	%eax, %r14d
	xorl	%eax, %eax
	movq	%rax, 40(%rsp)                  ## 8-byte Spill
	xorl	%edi, %edi
	movq	216(%rsp), %rsi                 ## 8-byte Reload
	jmp	LBB0_347
LBB0_266:
	xorl	%ecx, %ecx
	movq	%rcx, 64(%rsp)                  ## 8-byte Spill
	xorl	%ecx, %ecx
	movq	%rcx, 48(%rsp)                  ## 8-byte Spill
LBB0_267:
	movl	%eax, %r14d
	xorl	%eax, %eax
	movq	%rax, 40(%rsp)                  ## 8-byte Spill
	xorl	%edi, %edi
	movq	24(%rsp), %rsi                  ## 8-byte Reload
LBB0_347:                               ## %call_destructor.exit751
	callq	_halide_free
	movb	$1, %r15b
	xorl	%eax, %eax
	movq	%rax, 80(%rsp)                  ## 8-byte Spill
	xorl	%eax, %eax
	movq	%rax, 24(%rsp)                  ## 8-byte Spill
	movq	48(%rsp), %r12                  ## 8-byte Reload
	movq	64(%rsp), %rsi                  ## 8-byte Reload
	movq	104(%rsp), %rdx                 ## 8-byte Reload
	movq	%rsi, %r13
	testb	%r15b, %r15b
	jne	LBB0_238
	jmp	LBB0_240
LBB0_268:
	xorl	%ecx, %ecx
	movq	%rcx, 24(%rsp)                  ## 8-byte Spill
	xorl	%ecx, %ecx
	movq	%rcx, 80(%rsp)                  ## 8-byte Spill
	xorl	%ecx, %ecx
	movq	%rcx, 40(%rsp)                  ## 8-byte Spill
	xorl	%esi, %esi
	xorl	%r12d, %r12d
	movl	%eax, %r14d
	movq	56(%rsp), %rcx                  ## 8-byte Reload
	movq	104(%rsp), %rdx                 ## 8-byte Reload
	jmp	LBB0_237
LBB0_269:
	xorl	%ecx, %ecx
	movq	%rcx, 24(%rsp)                  ## 8-byte Spill
	xorl	%ecx, %ecx
	movq	%rcx, 80(%rsp)                  ## 8-byte Spill
	xorl	%ecx, %ecx
	movq	%rcx, 40(%rsp)                  ## 8-byte Spill
	xorl	%ecx, %ecx
	movq	%rcx, 128(%rsp)                 ## 8-byte Spill
	xorl	%esi, %esi
	xorl	%r12d, %r12d
	xorl	%ecx, %ecx
	movq	%rcx, 200(%rsp)                 ## 8-byte Spill
	movl	%eax, %r14d
	movq	56(%rsp), %rcx                  ## 8-byte Reload
	xorl	%edx, %edx
	jmp	LBB0_237
LBB0_270:
	xorl	%ecx, %ecx
	movq	%rcx, 32(%rsp)                  ## 8-byte Spill
	jmp	LBB0_356
LBB0_271:                               ## %entry
	leaq	LJTI0_0(%rip), %rcx
	movslq	(%rcx,%rax,4), %rax
	addq	%rcx, %rax
	jmpq	*%rax
LBB0_272:                               ## %assert_failed
	leaq	l_str(%rip), %rsi
	jmp	LBB0_376
LBB0_273:
	movl	%eax, %r14d
	xorl	%eax, %eax
	movq	%rax, 32(%rsp)                  ## 8-byte Spill
	jmp	LBB0_297
LBB0_274:                               ## %"assert failed312"
	leaq	l_str.114(%rip), %rsi
	xorl	%eax, %eax
	movq	%rax, 32(%rsp)                  ## 8-byte Spill
	movl	$2147483647, %ecx               ## imm = 0x7FFFFFFF
	xorl	%edi, %edi
	callq	_halide_error_buffer_allocation_too_large
	jmp	LBB0_296
LBB0_275:                               ## %"assert failed318"
	movq	1048(%rsp), %rdx                ## 8-byte Reload
	leal	1(%rdx), %ecx
	xorl	%esi, %esi
	testl	%edx, %edx
	movl	$0, %edx
	movq	%rdx, 32(%rsp)                  ## 8-byte Spill
	cmovsl	%esi, %ecx
	imulq	%rcx, %rax
	shlq	$5, %rax
	leaq	(%rax,%rax,2), %rdx
	leaq	l_str.115(%rip), %rsi
	movl	$2147483647, %ecx               ## imm = 0x7FFFFFFF
	xorl	%edi, %edi
	callq	_halide_error_buffer_allocation_too_large
	jmp	LBB0_300
LBB0_276:
	movl	%eax, %r14d
	xorl	%eax, %eax
	movq	%rax, 32(%rsp)                  ## 8-byte Spill
	jmp	LBB0_304
LBB0_277:                               ## %"assert failed326"
	leaq	l_str.116(%rip), %rsi
	xorl	%eax, %eax
	movq	%rax, 32(%rsp)                  ## 8-byte Spill
	movl	$2147483647, %ecx               ## imm = 0x7FFFFFFF
	xorl	%edi, %edi
	callq	_halide_error_buffer_allocation_too_large
	jmp	LBB0_303
LBB0_278:                               ## %"assert failed332"
	movq	1048(%rsp), %rcx                ## 8-byte Reload
	leal	1(%rcx), %edx
	xorl	%esi, %esi
	testl	%ecx, %ecx
	movl	$0, %ecx
	movq	%rcx, 32(%rsp)                  ## 8-byte Spill
	cmovsl	%esi, %edx
	imulq	%rax, %rdx
	shlq	$7, %rdx
	leaq	l_str.117(%rip), %rsi
	movl	$2147483647, %ecx               ## imm = 0x7FFFFFFF
	xorl	%edi, %edi
	callq	_halide_error_buffer_allocation_too_large
	jmp	LBB0_307
LBB0_279:
	movl	%eax, %r14d
	xorl	%eax, %eax
	movq	%rax, 32(%rsp)                  ## 8-byte Spill
	jmp	LBB0_312
LBB0_280:                               ## %"assert failed348"
	imulq	%rax, %rdx
	leaq	l_str.119(%rip), %rsi
	xorl	%eax, %eax
	movq	%rax, 32(%rsp)                  ## 8-byte Spill
	movl	$2147483647, %ecx               ## imm = 0x7FFFFFFF
	xorl	%edi, %edi
	callq	_halide_error_buffer_allocation_too_large
	jmp	LBB0_316
LBB0_281:                               ## %"assert failed354"
	imulq	%rbx, %rdx
	leaq	l_str.121(%rip), %rsi
	xorl	%eax, %eax
	movq	%rax, 32(%rsp)                  ## 8-byte Spill
	movl	$2147483647, %ecx               ## imm = 0x7FFFFFFF
	xorl	%edi, %edi
	callq	_halide_error_buffer_allocation_too_large
	jmp	LBB0_319
LBB0_282:                               ## %"assert failed447"
	imulq	%rdx, %r15
	leaq	l_str.123(%rip), %rsi
	xorl	%eax, %eax
	movq	%rax, 32(%rsp)                  ## 8-byte Spill
	movl	$2147483647, %ecx               ## imm = 0x7FFFFFFF
	xorl	%edi, %edi
	movq	%r15, %rdx
	callq	_halide_error_buffer_allocation_too_large
	jmp	LBB0_326
LBB0_283:                               ## %"assert failed479"
	imulq	184(%rsp), %rdx                 ## 8-byte Folded Reload
	leaq	l_str.126(%rip), %rsi
	xorl	%eax, %eax
	movq	%rax, 24(%rsp)                  ## 8-byte Spill
	movl	$2147483647, %ecx               ## imm = 0x7FFFFFFF
	xorl	%edi, %edi
	vzeroupper
	callq	_halide_error_buffer_allocation_too_large
	jmp	LBB0_336
LBB0_284:
	xorl	%ecx, %ecx
	movq	%rcx, 128(%rsp)                 ## 8-byte Spill
	xorl	%r13d, %r13d
	xorl	%r12d, %r12d
	xorl	%ecx, %ecx
	movq	%rcx, 200(%rsp)                 ## 8-byte Spill
	movl	%eax, %r14d
	movq	%r15, %rsi
	jmp	LBB0_247
LBB0_285:                               ## %then_bb66
	movl	%edi, 120(%rsp)                 ## 4-byte Spill
	movl	1216(%rsp), %r13d               ## 4-byte Reload
	movl	1220(%rsp), %r12d               ## 4-byte Reload
	movl	1224(%rsp), %r15d               ## 4-byte Reload
	movl	1232(%rsp), %r14d               ## 4-byte Reload
	movl	1240(%rsp), %r11d               ## 4-byte Reload
	movl	1244(%rsp), %r10d               ## 4-byte Reload
	movl	1248(%rsp), %r8d                ## 4-byte Reload
	movl	1252(%rsp), %ebx                ## 4-byte Reload
	movl	1256(%rsp), %r9d                ## 4-byte Reload
	movl	1260(%rsp), %edi                ## 4-byte Reload
	movl	1264(%rsp), %esi                ## 4-byte Reload
	movq	544(%rsp), %rcx                 ## 8-byte Reload
	leaq	LJTI0_1(%rip), %rdx
	movslq	(%rdx,%rax,4), %rax
	addq	%rdx, %rax
	jmpq	*%rax
LBB0_286:                               ## %assert_failed69
	leaq	l_str.20(%rip), %rsi
	xorl	%edi, %edi
	movl	1040(%rsp), %edx                ## 4-byte Reload
	jmp	LBB0_461
LBB0_287:                               ## %no_errors_bb68
	movl	352(%rsp), %ecx                 ## 4-byte Reload
	movl	356(%rsp), %r13d                ## 4-byte Reload
	movl	360(%rsp), %r12d                ## 4-byte Reload
	movl	364(%rsp), %r15d                ## 4-byte Reload
	movl	368(%rsp), %r14d                ## 4-byte Reload
	movl	372(%rsp), %r11d                ## 4-byte Reload
	movl	376(%rsp), %r10d                ## 4-byte Reload
	movl	380(%rsp), %r8d                 ## 4-byte Reload
	movl	384(%rsp), %ebx                 ## 4-byte Reload
	movl	388(%rsp), %r9d                 ## 4-byte Reload
	movl	392(%rsp), %edi                 ## 4-byte Reload
	movq	544(%rsp), %rdx                 ## 8-byte Reload
	leaq	LJTI0_2(%rip), %rsi
	movslq	(%rsi,%rax,4), %rax
	addq	%rsi, %rax
	jmpq	*%rax
LBB0_288:                               ## %assert_failed133
	leaq	l_str.28(%rip), %rsi
	xorl	%edi, %edi
	xorl	%edx, %edx
	movq	208(%rsp), %rcx                 ## 8-byte Reload
                                        ## kill: def $ecx killed $ecx killed $rcx
	vzeroupper
	callq	_halide_error_buffer_extents_negative
	jmp	LBB0_260
LBB0_289:                               ## %no_errors_bb132
	leaq	LJTI0_3(%rip), %rcx
	movslq	(%rcx,%rax,4), %rax
	addq	%rcx, %rax
	jmpq	*%rax
LBB0_290:                               ## %assert_failed197
	leaq	l_str.67(%rip), %rsi
	leaq	l_str.39(%rip), %rcx
	xorl	%edi, %edi
	movq	344(%rsp), %rdx                 ## 8-byte Reload
	jmp	LBB0_557
LBB0_291:                               ## %no_errors_bb196
	movq	640(%rsp), %r13                 ## 8-byte Reload
	movq	648(%rsp), %r12                 ## 8-byte Reload
	movq	656(%rsp), %r15                 ## 8-byte Reload
	movq	664(%rsp), %r14                 ## 8-byte Reload
	movq	672(%rsp), %r11                 ## 8-byte Reload
	movq	1360(%rsp), %r10                ## 8-byte Reload
	movq	680(%rsp), %r9                  ## 8-byte Reload
	movq	688(%rsp), %r8                  ## 8-byte Reload
	movq	696(%rsp), %rbx                 ## 8-byte Reload
	movq	704(%rsp), %rdx                 ## 8-byte Reload
	leaq	LJTI0_4(%rip), %rcx
	movslq	(%rcx,%rax,4), %rax
	addq	%rcx, %rax
	jmpq	*%rax
LBB0_292:                               ## %assert_failed242
	leaq	l_str.17(%rip), %rsi
	movl	$2147483647, %ecx               ## imm = 0x7FFFFFFF
	xorl	%edi, %edi
	movq	%r14, %rdx
	vzeroupper
	callq	_halide_error_buffer_allocation_too_large
	jmp	LBB0_260
LBB0_293:                               ## %"assert failed304"
	leaq	l_str.113(%rip), %rsi
	xorl	%eax, %eax
	movq	%rax, 32(%rsp)                  ## 8-byte Spill
	movl	$2147483647, %ecx               ## imm = 0x7FFFFFFF
	xorl	%edi, %edi
	callq	_halide_error_buffer_allocation_too_large
	jmp	LBB0_359
LBB0_294:                               ## %"assert failed306"
	xorl	%eax, %eax
	movq	%rax, 32(%rsp)                  ## 8-byte Spill
	xorl	%edi, %edi
	callq	_halide_error_out_of_memory
	jmp	LBB0_359
LBB0_295:                               ## %"assert failed314"
	xorl	%eax, %eax
	movq	%rax, 32(%rsp)                  ## 8-byte Spill
	xorl	%edi, %edi
	callq	_halide_error_out_of_memory
LBB0_296:                               ## %call_destructor.exit750.thread972
	movl	%eax, %r14d
LBB0_297:                               ## %call_destructor.exit750.thread972
	xorl	%edx, %edx
	xorl	%eax, %eax
	movq	%rax, 24(%rsp)                  ## 8-byte Spill
	xorl	%eax, %eax
	movq	%rax, 80(%rsp)                  ## 8-byte Spill
	xorl	%eax, %eax
	movq	%rax, 40(%rsp)                  ## 8-byte Spill
	xorl	%ecx, %ecx
	xorl	%eax, %eax
	movq	%rax, 168(%rsp)                 ## 8-byte Spill
	xorl	%eax, %eax
	movq	%rax, 128(%rsp)                 ## 8-byte Spill
	xorl	%esi, %esi
	xorl	%r12d, %r12d
	jmp	LBB0_237
LBB0_298:
	movl	%eax, %r14d
	xorl	%eax, %eax
	movq	%rax, 32(%rsp)                  ## 8-byte Spill
	jmp	LBB0_301
LBB0_299:                               ## %"assert failed320"
	xorl	%eax, %eax
	movq	%rax, 32(%rsp)                  ## 8-byte Spill
	xorl	%edi, %edi
	callq	_halide_error_out_of_memory
LBB0_300:                               ## %call_destructor.exit750.thread972
	movl	%eax, %r14d
LBB0_301:                               ## %call_destructor.exit750.thread972
	xorl	%edx, %edx
	xorl	%eax, %eax
	movq	%rax, 24(%rsp)                  ## 8-byte Spill
	xorl	%eax, %eax
	movq	%rax, 80(%rsp)                  ## 8-byte Spill
	xorl	%eax, %eax
	movq	%rax, 40(%rsp)                  ## 8-byte Spill
	xorl	%ecx, %ecx
	xorl	%eax, %eax
	movq	%rax, 168(%rsp)                 ## 8-byte Spill
	xorl	%eax, %eax
	movq	%rax, 128(%rsp)                 ## 8-byte Spill
	xorl	%esi, %esi
	movq	48(%rsp), %r12                  ## 8-byte Reload
	jmp	LBB0_237
LBB0_302:                               ## %"assert failed328"
	xorl	%eax, %eax
	movq	%rax, 32(%rsp)                  ## 8-byte Spill
	xorl	%edi, %edi
	callq	_halide_error_out_of_memory
LBB0_303:                               ## %call_destructor.exit750.thread972
	movl	%eax, %r14d
LBB0_304:                               ## %call_destructor.exit750.thread972
	xorl	%edx, %edx
	xorl	%eax, %eax
	movq	%rax, 24(%rsp)                  ## 8-byte Spill
	xorl	%eax, %eax
	movq	%rax, 80(%rsp)                  ## 8-byte Spill
	xorl	%eax, %eax
	movq	%rax, 40(%rsp)                  ## 8-byte Spill
	xorl	%ecx, %ecx
	xorl	%eax, %eax
	movq	%rax, 168(%rsp)                 ## 8-byte Spill
	xorl	%eax, %eax
	movq	%rax, 128(%rsp)                 ## 8-byte Spill
	jmp	LBB0_330
LBB0_305:
	movl	%eax, %r14d
	xorl	%eax, %eax
	movq	%rax, 32(%rsp)                  ## 8-byte Spill
	jmp	LBB0_308
LBB0_306:                               ## %"assert failed334"
	xorl	%eax, %eax
	movq	%rax, 32(%rsp)                  ## 8-byte Spill
	xorl	%edi, %edi
	callq	_halide_error_out_of_memory
LBB0_307:                               ## %call_destructor.exit750.thread972
	movl	%eax, %r14d
LBB0_308:                               ## %call_destructor.exit750.thread972
	xorl	%edx, %edx
	xorl	%eax, %eax
	movq	%rax, 24(%rsp)                  ## 8-byte Spill
	xorl	%eax, %eax
	movq	%rax, 80(%rsp)                  ## 8-byte Spill
	xorl	%eax, %eax
	movq	%rax, 40(%rsp)                  ## 8-byte Spill
	xorl	%ecx, %ecx
	xorl	%eax, %eax
	movq	%rax, 168(%rsp)                 ## 8-byte Spill
	jmp	LBB0_330
LBB0_309:                               ## %"assert failed340"
	leaq	l_str.118(%rip), %rsi
	xorl	%eax, %eax
	movq	%rax, 32(%rsp)                  ## 8-byte Spill
	movl	$2147483647, %ecx               ## imm = 0x7FFFFFFF
	xorl	%edi, %edi
	callq	_halide_error_buffer_allocation_too_large
	jmp	LBB0_311
LBB0_310:                               ## %"assert failed342"
	xorl	%eax, %eax
	movq	%rax, 32(%rsp)                  ## 8-byte Spill
	xorl	%edi, %edi
	callq	_halide_error_out_of_memory
LBB0_311:                               ## %call_destructor.exit750.thread972
	movl	%eax, %r14d
LBB0_312:                               ## %call_destructor.exit750.thread972
	xorl	%edx, %edx
	xorl	%eax, %eax
	movq	%rax, 24(%rsp)                  ## 8-byte Spill
	xorl	%eax, %eax
	movq	%rax, 80(%rsp)                  ## 8-byte Spill
	xorl	%eax, %eax
	movq	%rax, 40(%rsp)                  ## 8-byte Spill
	xorl	%ecx, %ecx
	jmp	LBB0_330
LBB0_313:
	movl	%eax, %r14d
	xorl	%eax, %eax
	movq	%rax, 32(%rsp)                  ## 8-byte Spill
	xorl	%edx, %edx
	xorl	%eax, %eax
	movq	%rax, 24(%rsp)                  ## 8-byte Spill
	xorl	%eax, %eax
	movq	%rax, 80(%rsp)                  ## 8-byte Spill
	xorl	%eax, %eax
	movq	%rax, 40(%rsp)                  ## 8-byte Spill
	movq	48(%rsp), %r12                  ## 8-byte Reload
	movq	64(%rsp), %rsi                  ## 8-byte Reload
	jmp	LBB0_357
LBB0_314:
	movl	%eax, %r14d
	xorl	%eax, %eax
	movq	%rax, 32(%rsp)                  ## 8-byte Spill
	xorl	%edx, %edx
	xorl	%eax, %eax
	movq	%rax, 24(%rsp)                  ## 8-byte Spill
	xorl	%eax, %eax
	movq	%rax, 80(%rsp)                  ## 8-byte Spill
	xorl	%eax, %eax
	movq	%rax, 40(%rsp)                  ## 8-byte Spill
	movq	48(%rsp), %r12                  ## 8-byte Reload
	movq	64(%rsp), %rsi                  ## 8-byte Reload
	movq	%r13, %rcx
	jmp	LBB0_237
LBB0_315:                               ## %"assert failed350"
	xorl	%eax, %eax
	movq	%rax, 32(%rsp)                  ## 8-byte Spill
	xorl	%edi, %edi
	callq	_halide_error_out_of_memory
LBB0_316:                               ## %call_destructor.exit750.thread972
	movl	%eax, %r14d
	xorl	%edx, %edx
	xorl	%eax, %eax
	movq	%rax, 24(%rsp)                  ## 8-byte Spill
	xorl	%eax, %eax
	movq	%rax, 80(%rsp)                  ## 8-byte Spill
	jmp	LBB0_328
LBB0_317:
	movl	%eax, %r14d
	xorl	%eax, %eax
	movq	%rax, 32(%rsp)                  ## 8-byte Spill
	jmp	LBB0_320
LBB0_318:                               ## %"assert failed356"
	xorl	%eax, %eax
	movq	%rax, 32(%rsp)                  ## 8-byte Spill
	xorl	%edi, %edi
	callq	_halide_error_out_of_memory
LBB0_319:                               ## %call_destructor.exit750.thread972
	movl	%eax, %r14d
LBB0_320:                               ## %call_destructor.exit750.thread972
	xorl	%edx, %edx
	xorl	%eax, %eax
	movq	%rax, 24(%rsp)                  ## 8-byte Spill
	xorl	%eax, %eax
	movq	%rax, 80(%rsp)                  ## 8-byte Spill
	jmp	LBB0_329
LBB0_321:                               ## %"assert failed440"
	leaq	l_str.122(%rip), %rsi
	xorl	%eax, %eax
	movq	%rax, 32(%rsp)                  ## 8-byte Spill
	movl	$2147483647, %ecx               ## imm = 0x7FFFFFFF
	xorl	%edi, %edi
	movq	%rbx, %rdx
	callq	_halide_error_buffer_allocation_too_large
	jmp	LBB0_323
LBB0_322:                               ## %"assert failed442"
	xorl	%eax, %eax
	movq	%rax, 32(%rsp)                  ## 8-byte Spill
	xorl	%edi, %edi
	callq	_halide_error_out_of_memory
LBB0_323:                               ## %call_destructor.exit750.thread972
	movl	%eax, %r14d
	xorl	%edx, %edx
	xorl	%eax, %eax
	movq	%rax, 24(%rsp)                  ## 8-byte Spill
	jmp	LBB0_328
LBB0_324:
	movl	%eax, %r14d
	xorl	%eax, %eax
	movq	%rax, 32(%rsp)                  ## 8-byte Spill
	jmp	LBB0_327
LBB0_325:                               ## %"assert failed449"
	xorl	%eax, %eax
	movq	%rax, 32(%rsp)                  ## 8-byte Spill
	xorl	%edi, %edi
	callq	_halide_error_out_of_memory
LBB0_326:                               ## %call_destructor.exit750.thread972
	movl	%eax, %r14d
LBB0_327:                               ## %call_destructor.exit750.thread972
	xorl	%edx, %edx
LBB0_328:                               ## %call_destructor.exit750.thread972
	xorl	%eax, %eax
	movq	%rax, 40(%rsp)                  ## 8-byte Spill
LBB0_329:                               ## %call_destructor.exit750.thread972
	movq	56(%rsp), %rcx                  ## 8-byte Reload
LBB0_330:                               ## %call_destructor.exit750.thread972
	movq	48(%rsp), %r12                  ## 8-byte Reload
	movq	64(%rsp), %rsi                  ## 8-byte Reload
	jmp	LBB0_237
LBB0_331:
	movl	%eax, %r14d
	xorl	%eax, %eax
	movq	%rax, 32(%rsp)                  ## 8-byte Spill
	jmp	LBB0_338
LBB0_332:                               ## %"assert failed454"
	leaq	l_str.124(%rip), %rsi
	xorl	%eax, %eax
	movq	%rax, 32(%rsp)                  ## 8-byte Spill
	movl	$2147483647, %ecx               ## imm = 0x7FFFFFFF
	xorl	%edi, %edi
	movq	%r15, %rdx
	callq	_halide_error_buffer_allocation_too_large
	jmp	LBB0_334
LBB0_333:                               ## %"assert failed456"
	xorl	%eax, %eax
	movq	%rax, 32(%rsp)                  ## 8-byte Spill
	xorl	%edi, %edi
	callq	_halide_error_out_of_memory
LBB0_334:
	movl	%eax, %r14d
	xorl	%eax, %eax
	movq	%rax, 24(%rsp)                  ## 8-byte Spill
	jmp	LBB0_337
LBB0_335:                               ## %"assert failed464"
	xorl	%eax, %eax
	movq	%rax, 24(%rsp)                  ## 8-byte Spill
	xorl	%edi, %edi
	callq	_halide_error_out_of_memory
LBB0_336:                               ## %call_destructor.exit750.thread972
	movl	%eax, %r14d
LBB0_337:                               ## %call_destructor.exit750.thread972
	xorl	%eax, %eax
	movq	%rax, 80(%rsp)                  ## 8-byte Spill
LBB0_338:                               ## %call_destructor.exit750.thread972
	xorl	%eax, %eax
	movq	%rax, 40(%rsp)                  ## 8-byte Spill
LBB0_339:                               ## %call_destructor.exit750.thread972
	movq	56(%rsp), %rcx                  ## 8-byte Reload
	movq	48(%rsp), %r12                  ## 8-byte Reload
	movq	64(%rsp), %rsi                  ## 8-byte Reload
	movq	104(%rsp), %rdx                 ## 8-byte Reload
	jmp	LBB0_237
LBB0_348:                               ## %"assert failed481"
	xorl	%eax, %eax
	movq	%rax, 24(%rsp)                  ## 8-byte Spill
	xorl	%edi, %edi
	callq	_halide_error_out_of_memory
	xorl	%ecx, %ecx
	movq	%rcx, 80(%rsp)                  ## 8-byte Spill
	xorl	%ecx, %ecx
	movq	%rcx, 40(%rsp)                  ## 8-byte Spill
	movl	%eax, %r14d
	jmp	LBB0_339
LBB0_349:
	xorl	%ecx, %ecx
	movq	%rcx, 64(%rsp)                  ## 8-byte Spill
	jmp	LBB0_267
LBB0_350:
	xorl	%ecx, %ecx
	movq	%rcx, 24(%rsp)                  ## 8-byte Spill
	xorl	%ecx, %ecx
	movq	%rcx, 80(%rsp)                  ## 8-byte Spill
	xorl	%ecx, %ecx
	movq	%rcx, 40(%rsp)                  ## 8-byte Spill
	xorl	%esi, %esi
	xorl	%r12d, %r12d
	movl	%eax, %r14d
	movq	56(%rsp), %rcx                  ## 8-byte Reload
	movq	%r15, %rdx
	jmp	LBB0_237
LBB0_351:
	xorl	%ecx, %ecx
	movq	%rcx, 24(%rsp)                  ## 8-byte Spill
	xorl	%edx, %edx
	xorl	%ecx, %ecx
	movq	%rcx, 80(%rsp)                  ## 8-byte Spill
	xorl	%ecx, %ecx
	movq	%rcx, 40(%rsp)                  ## 8-byte Spill
	xorl	%ecx, %ecx
	movq	%rcx, 128(%rsp)                 ## 8-byte Spill
	xorl	%r12d, %r12d
	movl	%eax, %r14d
	xorl	%esi, %esi
	jmp	LBB0_357
LBB0_352:
	xorl	%ecx, %ecx
	movq	%rcx, 32(%rsp)                  ## 8-byte Spill
LBB0_353:
	xorl	%ecx, %ecx
	movq	%rcx, 24(%rsp)                  ## 8-byte Spill
	xorl	%ecx, %ecx
	movq	%rcx, 80(%rsp)                  ## 8-byte Spill
	xorl	%ecx, %ecx
	movq	%rcx, 40(%rsp)                  ## 8-byte Spill
	xorl	%ecx, %ecx
	movq	%rcx, 128(%rsp)                 ## 8-byte Spill
	xorl	%esi, %esi
	xorl	%r12d, %r12d
	xorl	%ecx, %ecx
	movq	%rcx, 200(%rsp)                 ## 8-byte Spill
	movl	%eax, %r14d
	xorl	%edx, %edx
	jmp	LBB0_357
LBB0_354:                               ## %"assert failed535"
	leaq	l_str.127(%rip), %rsi
	xorl	%eax, %eax
	movq	%rax, 32(%rsp)                  ## 8-byte Spill
	movl	$2147483647, %ecx               ## imm = 0x7FFFFFFF
	xorl	%edi, %edi
	callq	_halide_error_buffer_allocation_too_large
	jmp	LBB0_356
LBB0_355:                               ## %"assert failed537"
	xorl	%eax, %eax
	movq	%rax, 32(%rsp)                  ## 8-byte Spill
	xorl	%edi, %edi
	callq	_halide_error_out_of_memory
LBB0_356:                               ## %call_destructor.exit750.thread972
	xorl	%edx, %edx
	xorl	%ecx, %ecx
	movq	%rcx, 24(%rsp)                  ## 8-byte Spill
	xorl	%ecx, %ecx
	movq	%rcx, 80(%rsp)                  ## 8-byte Spill
	xorl	%ecx, %ecx
	movq	%rcx, 40(%rsp)                  ## 8-byte Spill
	xorl	%ecx, %ecx
	movq	%rcx, 128(%rsp)                 ## 8-byte Spill
	xorl	%esi, %esi
	xorl	%r12d, %r12d
	xorl	%ecx, %ecx
	movq	%rcx, 200(%rsp)                 ## 8-byte Spill
	movl	%eax, %r14d
LBB0_357:                               ## %call_destructor.exit750.thread972
	movq	56(%rsp), %rcx                  ## 8-byte Reload
	jmp	LBB0_237
LBB0_358:                               ## %"assert failed"
	movq	1144(%rsp), %rax                ## 8-byte Reload
	addl	$23, %eax
	movl	%eax, (%rsp)
	leaq	l_str.19(%rip), %rsi
	xorl	%eax, %eax
	movq	%rax, 32(%rsp)                  ## 8-byte Spill
	xorl	%edi, %edi
	movl	$1, %edx
	xorl	%ecx, %ecx
	movl	$31, %r8d
	movl	488(%rsp), %r9d                 ## 4-byte Reload
	callq	_halide_error_constraints_make_required_region_smaller
LBB0_359:                               ## %call_destructor.exit750.thread972
	movl	%eax, %r14d
	jmp	LBB0_236
LBB0_360:                               ## %assert_failed1
	leaq	l_str.3(%rip), %rsi
	jmp	LBB0_376
LBB0_361:                               ## %assert_failed2
	leaq	l_str.4(%rip), %rsi
	jmp	LBB0_376
LBB0_362:                               ## %assert_failed3
	leaq	l_str.5(%rip), %rsi
	jmp	LBB0_376
LBB0_363:                               ## %assert_failed4
	leaq	l_str.6(%rip), %rsi
	jmp	LBB0_376
LBB0_364:                               ## %assert_failed5
	leaq	l_str.7(%rip), %rsi
	jmp	LBB0_376
LBB0_365:                               ## %assert_failed6
	leaq	l_str.8(%rip), %rsi
	jmp	LBB0_376
LBB0_366:                               ## %assert_failed7
	leaq	l_str.9(%rip), %rsi
	jmp	LBB0_376
LBB0_367:                               ## %assert_failed8
	leaq	l_str.10(%rip), %rsi
	jmp	LBB0_376
LBB0_368:                               ## %assert_failed9
	leaq	l_str.11(%rip), %rsi
	jmp	LBB0_376
LBB0_369:                               ## %assert_failed10
	leaq	l_str.12(%rip), %rsi
	jmp	LBB0_376
LBB0_370:                               ## %assert_failed11
	leaq	l_str.13(%rip), %rsi
	jmp	LBB0_376
LBB0_371:                               ## %assert_failed12
	leaq	l_str.14(%rip), %rsi
	jmp	LBB0_376
LBB0_372:                               ## %assert_failed13
	leaq	l_str.15(%rip), %rsi
	jmp	LBB0_376
LBB0_373:                               ## %assert_failed14
	leaq	l_str.16(%rip), %rsi
	jmp	LBB0_376
LBB0_374:                               ## %assert_failed15
	leaq	l_str.17(%rip), %rsi
	jmp	LBB0_376
LBB0_375:                               ## %assert_failed16
	leaq	l_str.18(%rip), %rsi
LBB0_376:                               ## %assert_failed
	xorl	%edi, %edi
	vzeroupper
	callq	_halide_error_buffer_argument_is_null
	jmp	LBB0_260
LBB0_377:                               ## %assert_failed198
	leaq	l_str.68(%rip), %rsi
	leaq	l_str.69(%rip), %rcx
	xorl	%edi, %edi
	movq	336(%rsp), %rdx                 ## 8-byte Reload
                                        ## kill: def $edx killed $edx killed $rdx
	movl	$39, %r8d
	vzeroupper
	callq	_halide_error_constraint_violated
	jmp	LBB0_260
LBB0_378:                               ## %assert_failed199
	leaq	l_str.70(%rip), %rsi
	leaq	l_str.37(%rip), %rcx
	xorl	%edi, %edi
	movl	936(%rsp), %edx                 ## 4-byte Reload
	jmp	LBB0_555
LBB0_379:                               ## %assert_failed200
	leaq	l_str.71(%rip), %rsi
	leaq	l_str.37(%rip), %rcx
	xorl	%edi, %edi
	movl	932(%rsp), %edx                 ## 4-byte Reload
	jmp	LBB0_555
LBB0_380:                               ## %assert_failed201
	leaq	l_str.72(%rip), %rsi
	leaq	l_str.37(%rip), %rcx
	xorl	%edi, %edi
	movl	928(%rsp), %edx                 ## 4-byte Reload
	jmp	LBB0_555
LBB0_381:                               ## %assert_failed202
	leaq	l_str.73(%rip), %rsi
	leaq	l_str.37(%rip), %rcx
	xorl	%edi, %edi
	movl	924(%rsp), %edx                 ## 4-byte Reload
	jmp	LBB0_555
LBB0_382:                               ## %assert_failed203
	leaq	l_str.74(%rip), %rsi
	leaq	l_str.37(%rip), %rcx
	xorl	%edi, %edi
	movl	916(%rsp), %edx                 ## 4-byte Reload
	jmp	LBB0_555
LBB0_383:                               ## %assert_failed204
	leaq	l_str.75(%rip), %rsi
	leaq	l_str.39(%rip), %rcx
	xorl	%edi, %edi
	movq	792(%rsp), %rdx                 ## 8-byte Reload
	jmp	LBB0_423
LBB0_384:                               ## %assert_failed205
	leaq	l_str.76(%rip), %rsi
	leaq	l_str.41(%rip), %rcx
	xorl	%edi, %edi
	movq	768(%rsp), %rdx                 ## 8-byte Reload
	jmp	LBB0_539
LBB0_385:                               ## %assert_failed206
	leaq	l_str.77(%rip), %rsi
	leaq	l_str.39(%rip), %rcx
	xorl	%edi, %edi
	movq	784(%rsp), %rdx                 ## 8-byte Reload
	jmp	LBB0_423
LBB0_386:                               ## %assert_failed207
	leaq	l_str.78(%rip), %rsi
	leaq	l_str.79(%rip), %rcx
	xorl	%edi, %edi
	movq	752(%rsp), %rdx                 ## 8-byte Reload
	jmp	LBB0_426
LBB0_387:                               ## %assert_failed208
	leaq	l_str.80(%rip), %rsi
	leaq	l_str.37(%rip), %rcx
	xorl	%edi, %edi
	movl	912(%rsp), %edx                 ## 4-byte Reload
	jmp	LBB0_555
LBB0_388:                               ## %assert_failed209
	leaq	l_str.81(%rip), %rsi
	leaq	l_str.39(%rip), %rcx
	xorl	%edi, %edi
	movq	736(%rsp), %rdx                 ## 8-byte Reload
	jmp	LBB0_423
LBB0_389:                               ## %assert_failed210
	leaq	l_str.82(%rip), %rsi
	leaq	l_str.41(%rip), %rcx
	xorl	%edi, %edi
	movq	712(%rsp), %rdx                 ## 8-byte Reload
	jmp	LBB0_539
LBB0_390:                               ## %assert_failed211
	leaq	l_str.83(%rip), %rsi
	leaq	l_str.39(%rip), %rcx
	xorl	%edi, %edi
	movq	728(%rsp), %rdx                 ## 8-byte Reload
	jmp	LBB0_423
LBB0_391:                               ## %assert_failed212
	leaq	l_str.84(%rip), %rsi
	leaq	l_str.41(%rip), %rcx
	xorl	%edi, %edi
	movq	696(%rsp), %rdx                 ## 8-byte Reload
	jmp	LBB0_539
LBB0_393:                               ## %assert_failed213
	leaq	l_str.85(%rip), %rsi
	leaq	l_str.39(%rip), %rcx
	xorl	%edi, %edi
	movq	704(%rsp), %rdx                 ## 8-byte Reload
	jmp	LBB0_423
LBB0_394:                               ## %assert_failed214
	leaq	l_str.86(%rip), %rsi
	leaq	l_str.79(%rip), %rcx
	xorl	%edi, %edi
	movq	680(%rsp), %rdx                 ## 8-byte Reload
	jmp	LBB0_426
LBB0_395:                               ## %assert_failed215
	leaq	l_str.87(%rip), %rsi
	leaq	l_str.37(%rip), %rcx
	xorl	%edi, %edi
	movl	904(%rsp), %edx                 ## 4-byte Reload
	jmp	LBB0_555
LBB0_396:                               ## %assert_failed216
	leaq	l_str.88(%rip), %rsi
	leaq	l_str.39(%rip), %rcx
	xorl	%edi, %edi
	movl	500(%rsp), %edx                 ## 4-byte Reload
	jmp	LBB0_424
LBB0_397:                               ## %assert_failed217
	leaq	l_str.89(%rip), %rsi
	leaq	l_str.50(%rip), %rcx
	xorl	%edi, %edi
	movq	264(%rsp), %rdx                 ## 8-byte Reload
	jmp	LBB0_546
LBB0_398:                               ## %assert_failed218
	leaq	l_str.90(%rip), %rsi
	leaq	l_str.39(%rip), %rcx
	xorl	%edi, %edi
	movl	496(%rsp), %edx                 ## 4-byte Reload
	jmp	LBB0_424
LBB0_399:                               ## %assert_failed219
	leaq	l_str.91(%rip), %rsi
	leaq	l_str.79(%rip), %rcx
	xorl	%edi, %edi
	movq	648(%rsp), %rdx                 ## 8-byte Reload
	jmp	LBB0_426
LBB0_400:                               ## %assert_failed220
	leaq	l_str.92(%rip), %rsi
	leaq	l_str.37(%rip), %rcx
	xorl	%edi, %edi
	movl	896(%rsp), %edx                 ## 4-byte Reload
	jmp	LBB0_555
LBB0_401:                               ## %assert_failed221
	leaq	l_str.93(%rip), %rsi
	leaq	l_str.39(%rip), %rcx
	xorl	%edi, %edi
	movl	492(%rsp), %edx                 ## 4-byte Reload
	jmp	LBB0_424
LBB0_402:                               ## %assert_failed222
	leaq	l_str.94(%rip), %rsi
	leaq	l_str.50(%rip), %rcx
	xorl	%edi, %edi
	movq	600(%rsp), %rdx                 ## 8-byte Reload
	jmp	LBB0_546
LBB0_404:                               ## %assert_failed223
	leaq	l_str.95(%rip), %rsi
	leaq	l_str.39(%rip), %rcx
	xorl	%edi, %edi
	movl	624(%rsp), %edx                 ## 4-byte Reload
	jmp	LBB0_424
LBB0_405:                               ## %assert_failed224
	leaq	l_str.96(%rip), %rsi
	leaq	l_str.56(%rip), %rcx
	xorl	%edi, %edi
	movq	256(%rsp), %rdx                 ## 8-byte Reload
                                        ## kill: def $edx killed $edx killed $rdx
	movl	$40, %r8d
	vzeroupper
	callq	_halide_error_constraint_violated
	jmp	LBB0_260
LBB0_406:                               ## %assert_failed225
	leaq	l_str.97(%rip), %rsi
	leaq	l_str.39(%rip), %rcx
	xorl	%edi, %edi
	movl	616(%rsp), %edx                 ## 4-byte Reload
	jmp	LBB0_424
LBB0_407:                               ## %assert_failed226
	leaq	l_str.98(%rip), %rsi
	leaq	l_str.59(%rip), %rcx
	xorl	%edi, %edi
	movq	248(%rsp), %rdx                 ## 8-byte Reload
                                        ## kill: def $edx killed $edx killed $rdx
	movl	$7, %r8d
	vzeroupper
	callq	_halide_error_constraint_violated
	jmp	LBB0_260
LBB0_408:                               ## %assert_failed227
	leaq	l_str.99(%rip), %rsi
	leaq	l_str.39(%rip), %rcx
	xorl	%edi, %edi
	movl	592(%rsp), %edx                 ## 4-byte Reload
	jmp	LBB0_424
LBB0_409:                               ## %assert_failed228
	leaq	l_str.100(%rip), %rsi
	leaq	l_str.79(%rip), %rcx
	xorl	%edi, %edi
	movq	48(%rsp), %rdx                  ## 8-byte Reload
	jmp	LBB0_426
LBB0_410:                               ## %assert_failed229
	vmovapd	1344(%rsp), %xmm0               ## 16-byte Reload
	vextractps	$1, %xmm0, %edx
	leaq	l_str.101(%rip), %rsi
	leaq	l_str.37(%rip), %rcx
	xorl	%edi, %edi
	jmp	LBB0_555
LBB0_411:                               ## %assert_failed230
	leaq	l_str.102(%rip), %rsi
	leaq	l_str.39(%rip), %rcx
	xorl	%edi, %edi
	movl	568(%rsp), %edx                 ## 4-byte Reload
	jmp	LBB0_424
LBB0_412:                               ## %assert_failed231
	movl	%ebx, %edx
	leaq	l_str.103(%rip), %rsi
	leaq	l_str.63(%rip), %rcx
	xorl	%edi, %edi
	jmp	LBB0_419
LBB0_413:                               ## %assert_failed232
	leaq	l_str.104(%rip), %rsi
	leaq	l_str.39(%rip), %rcx
	xorl	%edi, %edi
	movl	560(%rsp), %edx                 ## 4-byte Reload
	jmp	LBB0_424
LBB0_414:                               ## %assert_failed233
	leaq	l_str.105(%rip), %rsi
	leaq	l_str.79(%rip), %rcx
	xorl	%edi, %edi
	movq	296(%rsp), %rdx                 ## 8-byte Reload
	jmp	LBB0_426
LBB0_415:                               ## %assert_failed234
	leaq	l_str.106(%rip), %rsi
	leaq	l_str.37(%rip), %rcx
	xorl	%edi, %edi
	movl	884(%rsp), %edx                 ## 4-byte Reload
	jmp	LBB0_555
LBB0_417:                               ## %assert_failed235
	leaq	l_str.107(%rip), %rsi
	leaq	l_str.39(%rip), %rcx
	xorl	%edi, %edi
	movq	1096(%rsp), %rdx                ## 8-byte Reload
	jmp	LBB0_423
LBB0_418:                               ## %assert_failed236
	leaq	l_str.108(%rip), %rsi
	leaq	l_str.63(%rip), %rcx
	xorl	%edi, %edi
	movq	1072(%rsp), %rdx                ## 8-byte Reload
                                        ## kill: def $edx killed $edx killed $rdx
LBB0_419:                               ## %assert_failed231
	movl	$24, %r8d
	vzeroupper
	callq	_halide_error_constraint_violated
	jmp	LBB0_260
LBB0_420:                               ## %assert_failed237
	leaq	l_str.109(%rip), %rsi
	leaq	l_str.39(%rip), %rcx
	xorl	%edi, %edi
	movq	1088(%rsp), %rdx                ## 8-byte Reload
	jmp	LBB0_423
LBB0_421:                               ## %assert_failed238
	leaq	l_str.110(%rip), %rsi
	leaq	l_str.69(%rip), %rcx
	xorl	%edi, %edi
	movq	1064(%rsp), %rdx                ## 8-byte Reload
                                        ## kill: def $edx killed $edx killed $rdx
	movl	$39, %r8d
	vzeroupper
	callq	_halide_error_constraint_violated
	jmp	LBB0_260
LBB0_422:                               ## %assert_failed239
	leaq	l_str.111(%rip), %rsi
	leaq	l_str.39(%rip), %rcx
	xorl	%edi, %edi
	movq	1080(%rsp), %rdx                ## 8-byte Reload
LBB0_423:                               ## %assert_failed204
                                        ## kill: def $edx killed $edx killed $rdx
LBB0_424:                               ## %assert_failed204
	xorl	%r8d, %r8d
	vzeroupper
	callq	_halide_error_constraint_violated
	jmp	LBB0_260
LBB0_425:                               ## %assert_failed240
	leaq	l_str.112(%rip), %rsi
	leaq	l_str.79(%rip), %rcx
	xorl	%edi, %edi
	movq	1056(%rsp), %rdx                ## 8-byte Reload
LBB0_426:                               ## %assert_failed207
                                        ## kill: def $edx killed $edx killed $rdx
	movl	$4, %r8d
	vzeroupper
	callq	_halide_error_constraint_violated
	jmp	LBB0_260
LBB0_427:                               ## %assert_failed70
	leaq	l_str.20(%rip), %rsi
	xorl	%edi, %edi
	movl	1032(%rsp), %edx                ## 4-byte Reload
	jmp	LBB0_448
LBB0_428:                               ## %assert_failed71
	leaq	l_str.19(%rip), %rsi
	xorl	%edi, %edi
	movl	420(%rsp), %edx                 ## 4-byte Reload
	jmp	LBB0_461
LBB0_429:                               ## %assert_failed72
	leaq	l_str.19(%rip), %rsi
	xorl	%edi, %edi
	movl	1024(%rsp), %edx                ## 4-byte Reload
	jmp	LBB0_459
LBB0_430:                               ## %assert_failed73
	leaq	l_str.21(%rip), %rsi
	xorl	%edi, %edi
	movl	416(%rsp), %edx                 ## 4-byte Reload
	jmp	LBB0_461
LBB0_431:                               ## %assert_failed74
	leaq	l_str.21(%rip), %rsi
	xorl	%edi, %edi
	movl	1016(%rsp), %edx                ## 4-byte Reload
	jmp	LBB0_448
LBB0_432:                               ## %assert_failed75
	leaq	l_str.22(%rip), %rsi
	xorl	%edi, %edi
	movl	412(%rsp), %edx                 ## 4-byte Reload
	jmp	LBB0_461
LBB0_433:                               ## %assert_failed76
	leaq	l_str.22(%rip), %rsi
	xorl	%edi, %edi
	movl	1008(%rsp), %edx                ## 4-byte Reload
	jmp	LBB0_463
LBB0_434:                               ## %assert_failed77
	leaq	l_str.23(%rip), %rsi
	xorl	%edi, %edi
	movl	1000(%rsp), %edx                ## 4-byte Reload
	jmp	LBB0_461
LBB0_435:                               ## %assert_failed78
	leaq	l_str.23(%rip), %rsi
	xorl	%edi, %edi
	movl	992(%rsp), %edx                 ## 4-byte Reload
	jmp	LBB0_448
LBB0_436:                               ## %assert_failed79
	leaq	l_str.24(%rip), %rsi
	xorl	%edi, %edi
	movl	984(%rsp), %edx                 ## 4-byte Reload
	jmp	LBB0_461
LBB0_437:                               ## %assert_failed80
	leaq	l_str.24(%rip), %rsi
	xorl	%edi, %edi
	movl	976(%rsp), %edx                 ## 4-byte Reload
	jmp	LBB0_459
LBB0_438:                               ## %assert_failed81
	leaq	l_str.25(%rip), %rsi
	xorl	%edi, %edi
	movl	968(%rsp), %edx                 ## 4-byte Reload
	jmp	LBB0_461
LBB0_439:                               ## %assert_failed82
	leaq	l_str.25(%rip), %rsi
	xorl	%edi, %edi
	movl	408(%rsp), %edx                 ## 4-byte Reload
	xorl	%ecx, %ecx
	vzeroupper
	callq	_halide_error_bad_dimensions
	jmp	LBB0_260
LBB0_440:                               ## %assert_failed83
	leaq	l_str.26(%rip), %rsi
	xorl	%edi, %edi
	movl	404(%rsp), %edx                 ## 4-byte Reload
	jmp	LBB0_461
LBB0_441:                               ## %assert_failed84
	leaq	l_str.26(%rip), %rsi
	xorl	%edi, %edi
	movl	400(%rsp), %edx                 ## 4-byte Reload
	jmp	LBB0_463
LBB0_442:                               ## %assert_failed85
	leaq	l_str.27(%rip), %rsi
	xorl	%edi, %edi
	movl	396(%rsp), %edx                 ## 4-byte Reload
	jmp	LBB0_461
LBB0_443:                               ## %assert_failed86
	leaq	l_str.27(%rip), %rsi
	xorl	%edi, %edi
	movl	960(%rsp), %edx                 ## 4-byte Reload
	jmp	LBB0_448
LBB0_444:                               ## %assert_failed87
	leaq	l_str.28(%rip), %rsi
	xorl	%edi, %edi
	movl	952(%rsp), %edx                 ## 4-byte Reload
	jmp	LBB0_461
LBB0_445:                               ## %assert_failed88
	leaq	l_str.28(%rip), %rsi
	xorl	%edi, %edi
	movl	392(%rsp), %edx                 ## 4-byte Reload
	jmp	LBB0_463
LBB0_446:                               ## %assert_failed89
	leaq	l_str.29(%rip), %rsi
	xorl	%edi, %edi
	movl	388(%rsp), %edx                 ## 4-byte Reload
	jmp	LBB0_461
LBB0_447:                               ## %assert_failed90
	leaq	l_str.29(%rip), %rsi
	xorl	%edi, %edi
	movl	384(%rsp), %edx                 ## 4-byte Reload
LBB0_448:                               ## %assert_failed70
	movl	$1, %ecx
	vzeroupper
	callq	_halide_error_bad_dimensions
	jmp	LBB0_260
LBB0_449:                               ## %assert_failed91
	leaq	l_str.30(%rip), %rsi
	xorl	%edi, %edi
	movl	380(%rsp), %edx                 ## 4-byte Reload
	jmp	LBB0_461
LBB0_450:                               ## %assert_failed92
	leaq	l_str.30(%rip), %rsi
	xorl	%edi, %edi
	movl	376(%rsp), %edx                 ## 4-byte Reload
	jmp	LBB0_459
LBB0_451:                               ## %assert_failed93
	leaq	l_str.31(%rip), %rsi
	xorl	%edi, %edi
	movl	372(%rsp), %edx                 ## 4-byte Reload
	jmp	LBB0_461
LBB0_452:                               ## %assert_failed94
	leaq	l_str.31(%rip), %rsi
	xorl	%edi, %edi
	movl	368(%rsp), %edx                 ## 4-byte Reload
	jmp	LBB0_463
LBB0_453:                               ## %assert_failed95
	leaq	l_str.32(%rip), %rsi
	xorl	%edi, %edi
	movl	364(%rsp), %edx                 ## 4-byte Reload
	jmp	LBB0_461
LBB0_454:                               ## %assert_failed96
	leaq	l_str.32(%rip), %rsi
	xorl	%edi, %edi
	movl	360(%rsp), %edx                 ## 4-byte Reload
	jmp	LBB0_459
LBB0_455:                               ## %assert_failed97
	leaq	l_str.33(%rip), %rsi
	xorl	%edi, %edi
	movl	356(%rsp), %edx                 ## 4-byte Reload
	jmp	LBB0_461
LBB0_456:                               ## %assert_failed98
	leaq	l_str.33(%rip), %rsi
	xorl	%edi, %edi
	movl	352(%rsp), %edx                 ## 4-byte Reload
	movl	$4, %ecx
	vzeroupper
	callq	_halide_error_bad_dimensions
	jmp	LBB0_260
LBB0_457:                               ## %assert_failed99
	leaq	l_str.34(%rip), %rsi
	xorl	%edi, %edi
	movl	880(%rsp), %edx                 ## 4-byte Reload
	jmp	LBB0_461
LBB0_458:                               ## %assert_failed100
	leaq	l_str.34(%rip), %rsi
	xorl	%edi, %edi
	movl	876(%rsp), %edx                 ## 4-byte Reload
LBB0_459:                               ## %assert_failed72
	movl	$2, %ecx
	vzeroupper
	callq	_halide_error_bad_dimensions
	jmp	LBB0_260
LBB0_460:                               ## %assert_failed101
	leaq	l_str.35(%rip), %rsi
	xorl	%edi, %edi
	movl	872(%rsp), %edx                 ## 4-byte Reload
LBB0_461:                               ## %assert_failed69
	movl	$73730, %ecx                    ## imm = 0x12002
	vzeroupper
	callq	_halide_error_bad_type
	jmp	LBB0_260
LBB0_462:                               ## %assert_failed102
	leaq	l_str.35(%rip), %rsi
	xorl	%edi, %edi
	movl	868(%rsp), %edx                 ## 4-byte Reload
LBB0_463:                               ## %assert_failed76
	movl	$3, %ecx
	vzeroupper
	callq	_halide_error_bad_dimensions
	jmp	LBB0_260
LBB0_464:                               ## %assert_failed103
	decl	%esi
	movl	%esi, (%rsp)
	leaq	l_str.20(%rip), %rsi
	xorl	%edi, %edi
	xorl	%edx, %edx
	xorl	%ecx, %ecx
	movl	$31, %r8d
	movq	808(%rsp), %r9                  ## 8-byte Reload
                                        ## kill: def $r9d killed $r9d killed $r9
	vzeroupper
	callq	_halide_error_access_out_of_bounds
	jmp	LBB0_260
LBB0_465:                               ## %assert_failed104
	leaq	l_str.20(%rip), %rsi
	xorl	%edi, %edi
	xorl	%edx, %edx
	movq	800(%rsp), %rcx                 ## 8-byte Reload
                                        ## kill: def $ecx killed $ecx killed $rcx
	vzeroupper
	callq	_halide_error_buffer_extents_negative
	jmp	LBB0_260
LBB0_466:                               ## %assert_failed105
	decl	%edi
	movl	%edi, (%rsp)
	leaq	l_str.19(%rip), %rsi
	xorl	%edi, %edi
	xorl	%edx, %edx
	xorl	%ecx, %ecx
	movl	$31, %r8d
	movq	760(%rsp), %r9                  ## 8-byte Reload
                                        ## kill: def $r9d killed $r9d killed $r9
	vzeroupper
	callq	_halide_error_access_out_of_bounds
	jmp	LBB0_260
LBB0_467:                               ## %assert_failed106
	leaq	l_str.19(%rip), %rsi
	xorl	%edi, %edi
	xorl	%edx, %edx
                                        ## kill: def $ecx killed $ecx killed $rcx
	vzeroupper
	callq	_halide_error_buffer_extents_negative
	jmp	LBB0_260
LBB0_468:                               ## %assert_failed107
	movq	1144(%rsp), %r8                 ## 8-byte Reload
	addl	$23, %r8d
	movl	%r9d, %ebx
	decl	%ebx
	movl	%ebx, (%rsp)
	leaq	l_str.19(%rip), %rsi
	xorl	%edi, %edi
	movl	$1, %edx
	movl	488(%rsp), %ecx                 ## 4-byte Reload
                                        ## kill: def $r8d killed $r8d killed $r8
	movq	776(%rsp), %r9                  ## 8-byte Reload
                                        ## kill: def $r9d killed $r9d killed $r9
	vzeroupper
	callq	_halide_error_access_out_of_bounds
	jmp	LBB0_260
LBB0_469:                               ## %assert_failed108
	leaq	l_str.19(%rip), %rsi
	xorl	%edi, %edi
	movl	$1, %edx
	movq	744(%rsp), %rcx                 ## 8-byte Reload
                                        ## kill: def $ecx killed $ecx killed $rcx
	vzeroupper
	callq	_halide_error_buffer_extents_negative
	jmp	LBB0_260
LBB0_470:                               ## %assert_failed109
	decl	%ebx
	movl	%ebx, (%rsp)
	leaq	l_str.21(%rip), %rsi
	xorl	%edi, %edi
	xorl	%edx, %edx
	xorl	%ecx, %ecx
	movl	$7, %r8d
	movq	720(%rsp), %r9                  ## 8-byte Reload
                                        ## kill: def $r9d killed $r9d killed $r9
	vzeroupper
	callq	_halide_error_access_out_of_bounds
	jmp	LBB0_260
LBB0_471:                               ## %assert_failed110
	leaq	l_str.21(%rip), %rsi
	xorl	%edi, %edi
	xorl	%edx, %edx
	movq	688(%rsp), %rcx                 ## 8-byte Reload
                                        ## kill: def $ecx killed $ecx killed $rcx
	vzeroupper
	callq	_halide_error_buffer_extents_negative
	jmp	LBB0_260
LBB0_472:                               ## %assert_failed111
	decl	%r8d
	movl	%r8d, (%rsp)
	leaq	l_str.22(%rip), %rsi
	xorl	%edi, %edi
	xorl	%edx, %edx
	xorl	%ecx, %ecx
	movl	$7, %r8d
	movq	672(%rsp), %r9                  ## 8-byte Reload
                                        ## kill: def $r9d killed $r9d killed $r9
	vzeroupper
	callq	_halide_error_access_out_of_bounds
	jmp	LBB0_260
LBB0_473:                               ## %assert_failed112
	leaq	l_str.22(%rip), %rsi
	xorl	%edi, %edi
	xorl	%edx, %edx
	movq	40(%rsp), %rcx                  ## 8-byte Reload
                                        ## kill: def $ecx killed $ecx killed $rcx
	vzeroupper
	callq	_halide_error_buffer_extents_negative
	jmp	LBB0_260
LBB0_474:                               ## %assert_failed113
	decl	%r10d
	movl	%r10d, (%rsp)
	leaq	l_str.22(%rip), %rsi
	xorl	%edi, %edi
	movl	$1, %edx
	xorl	%ecx, %ecx
	movl	$39, %r8d
	movq	664(%rsp), %r9                  ## 8-byte Reload
                                        ## kill: def $r9d killed $r9d killed $r9
	vzeroupper
	callq	_halide_error_access_out_of_bounds
	jmp	LBB0_260
LBB0_475:                               ## %assert_failed114
	leaq	l_str.22(%rip), %rsi
	xorl	%edi, %edi
	movl	$1, %edx
	movq	640(%rsp), %rcx                 ## 8-byte Reload
                                        ## kill: def $ecx killed $ecx killed $rcx
	vzeroupper
	callq	_halide_error_buffer_extents_negative
	jmp	LBB0_260
LBB0_476:                               ## %assert_failed115
	decl	%r11d
	movl	%r11d, (%rsp)
	leaq	l_str.22(%rip), %rsi
	xorl	%edi, %edi
	movl	$2, %edx
	xorl	%ecx, %ecx
	movl	$6, %r8d
	movq	656(%rsp), %r9                  ## 8-byte Reload
                                        ## kill: def $r9d killed $r9d killed $r9
	vzeroupper
	callq	_halide_error_access_out_of_bounds
	jmp	LBB0_260
LBB0_477:                               ## %assert_failed116
	leaq	l_str.22(%rip), %rsi
	xorl	%edi, %edi
	movl	$2, %edx
	movq	632(%rsp), %rcx                 ## 8-byte Reload
                                        ## kill: def $ecx killed $ecx killed $rcx
	vzeroupper
	callq	_halide_error_buffer_extents_negative
	jmp	LBB0_260
LBB0_478:                               ## %assert_failed117
	decl	%r14d
	movl	%r14d, (%rsp)
	leaq	l_str.23(%rip), %rsi
	xorl	%edi, %edi
	xorl	%edx, %edx
	xorl	%ecx, %ecx
	movl	$23, %r8d
	movq	608(%rsp), %r9                  ## 8-byte Reload
                                        ## kill: def $r9d killed $r9d killed $r9
	vzeroupper
	callq	_halide_error_access_out_of_bounds
	jmp	LBB0_260
LBB0_479:                               ## %assert_failed118
	leaq	l_str.23(%rip), %rsi
	xorl	%edi, %edi
	xorl	%edx, %edx
	movq	584(%rsp), %rcx                 ## 8-byte Reload
                                        ## kill: def $ecx killed $ecx killed $rcx
	vzeroupper
	callq	_halide_error_buffer_extents_negative
	jmp	LBB0_260
LBB0_480:                               ## %assert_failed119
	decl	%r15d
	movl	%r15d, (%rsp)
	leaq	l_str.24(%rip), %rsi
	xorl	%edi, %edi
	xorl	%edx, %edx
	xorl	%ecx, %ecx
	movl	$23, %r8d
	movq	576(%rsp), %r9                  ## 8-byte Reload
                                        ## kill: def $r9d killed $r9d killed $r9
	vzeroupper
	callq	_halide_error_access_out_of_bounds
	jmp	LBB0_260
LBB0_481:                               ## %assert_failed120
	leaq	l_str.24(%rip), %rsi
	xorl	%edi, %edi
	xorl	%edx, %edx
	movq	552(%rsp), %rcx                 ## 8-byte Reload
                                        ## kill: def $ecx killed $ecx killed $rcx
	vzeroupper
	callq	_halide_error_buffer_extents_negative
	jmp	LBB0_260
LBB0_482:                               ## %assert_failed121
	decl	%r12d
	movl	%r12d, (%rsp)
	leaq	l_str.24(%rip), %rsi
	xorl	%edi, %edi
	movl	$1, %edx
	xorl	%ecx, %ecx
	movl	$38, %r8d
	movq	344(%rsp), %r9                  ## 8-byte Reload
                                        ## kill: def $r9d killed $r9d killed $r9
	vzeroupper
	callq	_halide_error_access_out_of_bounds
	jmp	LBB0_260
LBB0_483:                               ## %assert_failed122
	leaq	l_str.24(%rip), %rsi
	xorl	%edi, %edi
	movl	$1, %edx
	movq	336(%rsp), %rcx                 ## 8-byte Reload
                                        ## kill: def $ecx killed $ecx killed $rcx
	vzeroupper
	callq	_halide_error_buffer_extents_negative
	jmp	LBB0_260
LBB0_484:                               ## %assert_failed123
	decl	%r13d
	movl	%r13d, (%rsp)
	leaq	l_str.26(%rip), %rsi
	xorl	%edi, %edi
	xorl	%edx, %edx
	xorl	%ecx, %ecx
	movl	$39, %r8d
	movq	1112(%rsp), %r9                 ## 8-byte Reload
                                        ## kill: def $r9d killed $r9d killed $r9
	vzeroupper
	callq	_halide_error_access_out_of_bounds
	jmp	LBB0_260
LBB0_485:                               ## %assert_failed124
	leaq	l_str.26(%rip), %rsi
	xorl	%edi, %edi
	xorl	%edx, %edx
	movq	160(%rsp), %rcx                 ## 8-byte Reload
                                        ## kill: def $ecx killed $ecx killed $rcx
	vzeroupper
	callq	_halide_error_buffer_extents_negative
	jmp	LBB0_260
LBB0_486:                               ## %assert_failed125
	movl	1236(%rsp), %eax                ## 4-byte Reload
	decl	%eax
	movl	%eax, (%rsp)
	leaq	l_str.26(%rip), %rsi
	xorl	%edi, %edi
	movl	$1, %edx
	xorl	%ecx, %ecx
	movl	$6, %r8d
	movq	1104(%rsp), %r9                 ## 8-byte Reload
                                        ## kill: def $r9d killed $r9d killed $r9
	vzeroupper
	callq	_halide_error_access_out_of_bounds
	jmp	LBB0_260
LBB0_487:                               ## %assert_failed126
	leaq	l_str.26(%rip), %rsi
	xorl	%edi, %edi
	movl	$1, %edx
	movq	464(%rsp), %rcx                 ## 8-byte Reload
                                        ## kill: def $ecx killed $ecx killed $rcx
	vzeroupper
	callq	_halide_error_buffer_extents_negative
	jmp	LBB0_260
LBB0_488:                               ## %assert_failed127
	movq	104(%rsp), %rcx                 ## 8-byte Reload
	decl	%ecx
	movq	80(%rsp), %r8                   ## 8-byte Reload
	decl	%r8d
	movl	1228(%rsp), %eax                ## 4-byte Reload
	decl	%eax
	movl	%eax, (%rsp)
	leaq	l_str.26(%rip), %rsi
	xorl	%edi, %edi
	movl	$2, %edx
                                        ## kill: def $ecx killed $ecx killed $rcx
                                        ## kill: def $r8d killed $r8d killed $r8
	movq	840(%rsp), %r9                  ## 8-byte Reload
                                        ## kill: def $r9d killed $r9d killed $r9
	vzeroupper
	callq	_halide_error_access_out_of_bounds
	jmp	LBB0_260
LBB0_489:                               ## %assert_failed128
	leaq	l_str.26(%rip), %rsi
	xorl	%edi, %edi
	movl	$2, %edx
	movq	472(%rsp), %rcx                 ## 8-byte Reload
                                        ## kill: def $ecx killed $ecx killed $rcx
	vzeroupper
	callq	_halide_error_buffer_extents_negative
	jmp	LBB0_260
LBB0_490:                               ## %assert_failed129
	movq	848(%rsp), %rcx                 ## 8-byte Reload
	movq	1152(%rsp), %rax                ## 8-byte Reload
	leal	(%rax,%rcx), %r8d
	decl	%r8d
	movl	1328(%rsp), %eax                ## 4-byte Reload
	decl	%eax
	movl	%eax, (%rsp)
	leaq	l_str.27(%rip), %rsi
	xorl	%edi, %edi
	xorl	%edx, %edx
	movl	%ecx, %r9d
	vzeroupper
	callq	_halide_error_access_out_of_bounds
	jmp	LBB0_260
LBB0_491:                               ## %assert_failed130
	leaq	l_str.27(%rip), %rsi
	xorl	%edi, %edi
	xorl	%edx, %edx
	movq	832(%rsp), %rcx                 ## 8-byte Reload
                                        ## kill: def $ecx killed $ecx killed $rcx
	vzeroupper
	callq	_halide_error_buffer_extents_negative
	jmp	LBB0_260
LBB0_492:                               ## %assert_failed131
	movl	824(%rsp), %r8d                 ## 4-byte Reload
	decl	%r8d
	movl	120(%rsp), %eax                 ## 4-byte Reload
	decl	%eax
	movl	%eax, (%rsp)
	leaq	l_str.28(%rip), %rsi
	xorl	%edi, %edi
	xorl	%edx, %edx
	movq	232(%rsp), %rcx                 ## 8-byte Reload
                                        ## kill: def $ecx killed $ecx killed $rcx
	movq	320(%rsp), %r9                  ## 8-byte Reload
                                        ## kill: def $r9d killed $r9d killed $r9
	vzeroupper
	callq	_halide_error_access_out_of_bounds
	jmp	LBB0_260
LBB0_493:                               ## %assert_failed134
	decl	%edi
	movl	%edi, (%rsp)
	leaq	l_str.28(%rip), %rsi
	xorl	%edi, %edi
	movl	$1, %edx
	xorl	%ecx, %ecx
	movl	$38, %r8d
	movq	288(%rsp), %r9                  ## 8-byte Reload
                                        ## kill: def $r9d killed $r9d killed $r9
	vzeroupper
	callq	_halide_error_access_out_of_bounds
	jmp	LBB0_260
LBB0_494:                               ## %assert_failed135
	leaq	l_str.28(%rip), %rsi
	xorl	%edi, %edi
	movl	$1, %edx
	movq	448(%rsp), %rcx                 ## 8-byte Reload
                                        ## kill: def $ecx killed $ecx killed $rcx
	vzeroupper
	callq	_halide_error_buffer_extents_negative
	jmp	LBB0_260
LBB0_495:                               ## %assert_failed136
	movq	80(%rsp), %r8                   ## 8-byte Reload
	decl	%r8d
	movl	%r9d, %ebx
	decl	%ebx
	movl	%ebx, (%rsp)
	leaq	l_str.28(%rip), %rsi
	xorl	%edi, %edi
	movl	$2, %edx
	xorl	%ecx, %ecx
                                        ## kill: def $r8d killed $r8d killed $r8
	movq	440(%rsp), %r9                  ## 8-byte Reload
                                        ## kill: def $r9d killed $r9d killed $r9
	vzeroupper
	callq	_halide_error_access_out_of_bounds
	jmp	LBB0_260
LBB0_496:                               ## %assert_failed137
	leaq	l_str.28(%rip), %rsi
	xorl	%edi, %edi
	movl	$2, %edx
	movq	456(%rsp), %rcx                 ## 8-byte Reload
                                        ## kill: def $ecx killed $ecx killed $rcx
	vzeroupper
	callq	_halide_error_buffer_extents_negative
	jmp	LBB0_260
LBB0_497:                               ## %assert_failed138
	movl	504(%rsp), %r8d                 ## 4-byte Reload
	decl	%r8d
	decl	%ebx
	movl	%ebx, (%rsp)
	leaq	l_str.29(%rip), %rsi
	xorl	%edi, %edi
	xorl	%edx, %edx
	xorl	%ecx, %ecx
	movq	1128(%rsp), %r9                 ## 8-byte Reload
                                        ## kill: def $r9d killed $r9d killed $r9
	vzeroupper
	callq	_halide_error_access_out_of_bounds
	jmp	LBB0_260
LBB0_498:                               ## %assert_failed139
	leaq	l_str.29(%rip), %rsi
	xorl	%edi, %edi
	xorl	%edx, %edx
	movq	480(%rsp), %rcx                 ## 8-byte Reload
                                        ## kill: def $ecx killed $ecx killed $rcx
	vzeroupper
	callq	_halide_error_buffer_extents_negative
	jmp	LBB0_260
LBB0_499:                               ## %assert_failed140
	decl	%r8d
	movl	%r8d, (%rsp)
	leaq	l_str.30(%rip), %rsi
	xorl	%edi, %edi
	xorl	%edx, %edx
	xorl	%ecx, %ecx
	movl	$31, %r8d
	movq	792(%rsp), %r9                  ## 8-byte Reload
                                        ## kill: def $r9d killed $r9d killed $r9
	vzeroupper
	callq	_halide_error_access_out_of_bounds
	jmp	LBB0_260
LBB0_500:                               ## %assert_failed141
	leaq	l_str.30(%rip), %rsi
	xorl	%edi, %edi
	xorl	%edx, %edx
	movq	768(%rsp), %rcx                 ## 8-byte Reload
                                        ## kill: def $ecx killed $ecx killed $rcx
	vzeroupper
	callq	_halide_error_buffer_extents_negative
	jmp	LBB0_260
LBB0_501:                               ## %assert_failed142
	decl	%r10d
	movl	%r10d, (%rsp)
	leaq	l_str.30(%rip), %rsi
	xorl	%edi, %edi
	movl	$1, %edx
	xorl	%ecx, %ecx
	movl	$3, %r8d
	movq	784(%rsp), %r9                  ## 8-byte Reload
                                        ## kill: def $r9d killed $r9d killed $r9
	vzeroupper
	callq	_halide_error_access_out_of_bounds
	jmp	LBB0_260
LBB0_502:                               ## %assert_failed143
	leaq	l_str.30(%rip), %rsi
	xorl	%edi, %edi
	movl	$1, %edx
	movq	752(%rsp), %rcx                 ## 8-byte Reload
                                        ## kill: def $ecx killed $ecx killed $rcx
	vzeroupper
	callq	_halide_error_buffer_extents_negative
	jmp	LBB0_260
LBB0_503:                               ## %assert_failed144
	decl	%r11d
	movl	%r11d, (%rsp)
	leaq	l_str.31(%rip), %rsi
	xorl	%edi, %edi
	xorl	%edx, %edx
	xorl	%ecx, %ecx
	movl	$31, %r8d
	movq	736(%rsp), %r9                  ## 8-byte Reload
                                        ## kill: def $r9d killed $r9d killed $r9
	vzeroupper
	callq	_halide_error_access_out_of_bounds
	jmp	LBB0_260
LBB0_504:                               ## %assert_failed145
	leaq	l_str.31(%rip), %rsi
	xorl	%edi, %edi
	xorl	%edx, %edx
	movq	712(%rsp), %rcx                 ## 8-byte Reload
                                        ## kill: def $ecx killed $ecx killed $rcx
	vzeroupper
	callq	_halide_error_buffer_extents_negative
	jmp	LBB0_260
LBB0_505:                               ## %assert_failed146
	decl	%r14d
	movl	%r14d, (%rsp)
	leaq	l_str.31(%rip), %rsi
	xorl	%edi, %edi
	movl	$1, %edx
	xorl	%ecx, %ecx
	movl	$31, %r8d
	movq	728(%rsp), %r9                  ## 8-byte Reload
                                        ## kill: def $r9d killed $r9d killed $r9
	vzeroupper
	callq	_halide_error_access_out_of_bounds
	jmp	LBB0_260
LBB0_506:                               ## %assert_failed147
	leaq	l_str.31(%rip), %rsi
	xorl	%edi, %edi
	movl	$1, %edx
	movq	696(%rsp), %rcx                 ## 8-byte Reload
                                        ## kill: def $ecx killed $ecx killed $rcx
	vzeroupper
	callq	_halide_error_buffer_extents_negative
	jmp	LBB0_260
LBB0_507:                               ## %assert_failed148
	decl	%r15d
	movl	%r15d, (%rsp)
	leaq	l_str.31(%rip), %rsi
	xorl	%edi, %edi
	movl	$2, %edx
	xorl	%ecx, %ecx
	movl	$3, %r8d
	movq	704(%rsp), %r9                  ## 8-byte Reload
                                        ## kill: def $r9d killed $r9d killed $r9
	vzeroupper
	callq	_halide_error_access_out_of_bounds
	jmp	LBB0_260
LBB0_508:                               ## %assert_failed149
	leaq	l_str.31(%rip), %rsi
	xorl	%edi, %edi
	movl	$2, %edx
	movq	680(%rsp), %rcx                 ## 8-byte Reload
                                        ## kill: def $ecx killed $ecx killed $rcx
	vzeroupper
	callq	_halide_error_buffer_extents_negative
	jmp	LBB0_260
LBB0_509:                               ## %assert_failed150
	vmovd	%xmm14, %eax
	decl	%eax
	movl	%eax, (%rsp)
	leaq	l_str.32(%rip), %rsi
	xorl	%edi, %edi
	xorl	%edx, %edx
	xorl	%ecx, %ecx
	movl	$7, %r8d
	movl	500(%rsp), %r9d                 ## 4-byte Reload
	vzeroupper
	callq	_halide_error_access_out_of_bounds
	jmp	LBB0_260
LBB0_510:                               ## %assert_failed151
	leaq	l_str.32(%rip), %rsi
	xorl	%edi, %edi
	xorl	%edx, %edx
	movq	264(%rsp), %rcx                 ## 8-byte Reload
                                        ## kill: def $ecx killed $ecx killed $rcx
	vzeroupper
	callq	_halide_error_buffer_extents_negative
	jmp	LBB0_260
LBB0_511:                               ## %assert_failed152
	vpextrd	$1, %xmm14, %eax
	decl	%eax
	movl	%eax, (%rsp)
	leaq	l_str.32(%rip), %rsi
	xorl	%edi, %edi
	movl	$1, %edx
	xorl	%ecx, %ecx
	movl	$3, %r8d
	movl	496(%rsp), %r9d                 ## 4-byte Reload
	vzeroupper
	callq	_halide_error_access_out_of_bounds
	jmp	LBB0_260
LBB0_512:                               ## %assert_failed153
	leaq	l_str.32(%rip), %rsi
	xorl	%edi, %edi
	movl	$1, %edx
	movq	648(%rsp), %rcx                 ## 8-byte Reload
                                        ## kill: def $ecx killed $ecx killed $rcx
	vzeroupper
	callq	_halide_error_buffer_extents_negative
	jmp	LBB0_260
LBB0_513:                               ## %assert_failed154
	vpextrd	$2, %xmm14, %eax
	decl	%eax
	movl	%eax, (%rsp)
	leaq	l_str.33(%rip), %rsi
	xorl	%edi, %edi
	xorl	%edx, %edx
	xorl	%ecx, %ecx
	movl	$7, %r8d
	movl	492(%rsp), %r9d                 ## 4-byte Reload
	vzeroupper
	callq	_halide_error_access_out_of_bounds
	jmp	LBB0_260
LBB0_514:                               ## %assert_failed155
	leaq	l_str.33(%rip), %rsi
	xorl	%edi, %edi
	xorl	%edx, %edx
	movq	600(%rsp), %rcx                 ## 8-byte Reload
                                        ## kill: def $ecx killed $ecx killed $rcx
	vzeroupper
	callq	_halide_error_buffer_extents_negative
	jmp	LBB0_260
LBB0_515:                               ## %assert_failed156
	vpextrd	$3, %xmm14, %eax
	decl	%eax
	movl	%eax, (%rsp)
	leaq	l_str.33(%rip), %rsi
	xorl	%edi, %edi
	movl	$1, %edx
	xorl	%ecx, %ecx
	movl	$39, %r8d
	movl	624(%rsp), %r9d                 ## 4-byte Reload
	vzeroupper
	callq	_halide_error_access_out_of_bounds
	jmp	LBB0_260
LBB0_516:                               ## %assert_failed157
	leaq	l_str.33(%rip), %rsi
	xorl	%edi, %edi
	movl	$1, %edx
	movq	256(%rsp), %rcx                 ## 8-byte Reload
                                        ## kill: def $ecx killed $ecx killed $rcx
	vzeroupper
	callq	_halide_error_buffer_extents_negative
	jmp	LBB0_260
LBB0_517:                               ## %assert_failed158
	vextracti128	$1, %ymm14, %xmm0
	vmovd	%xmm0, %eax
	decl	%eax
	movl	%eax, (%rsp)
	leaq	l_str.33(%rip), %rsi
	xorl	%edi, %edi
	movl	$2, %edx
	xorl	%ecx, %ecx
	movl	$6, %r8d
	movl	616(%rsp), %r9d                 ## 4-byte Reload
	vzeroupper
	callq	_halide_error_access_out_of_bounds
	jmp	LBB0_260
LBB0_518:                               ## %assert_failed159
	leaq	l_str.33(%rip), %rsi
	xorl	%edi, %edi
	movl	$2, %edx
	movq	248(%rsp), %rcx                 ## 8-byte Reload
                                        ## kill: def $ecx killed $ecx killed $rcx
	vzeroupper
	callq	_halide_error_buffer_extents_negative
	jmp	LBB0_260
LBB0_519:                               ## %assert_failed160
	vextracti128	$1, %ymm14, %xmm0
	vpextrd	$1, %xmm0, %eax
	decl	%eax
	movl	%eax, (%rsp)
	leaq	l_str.33(%rip), %rsi
	xorl	%edi, %edi
	movl	$3, %edx
	xorl	%ecx, %ecx
	movl	$3, %r8d
	movl	592(%rsp), %r9d                 ## 4-byte Reload
	vzeroupper
	callq	_halide_error_access_out_of_bounds
	jmp	LBB0_260
LBB0_520:                               ## %assert_failed161
	leaq	l_str.33(%rip), %rsi
	xorl	%edi, %edi
	movl	$3, %edx
	movq	48(%rsp), %rcx                  ## 8-byte Reload
                                        ## kill: def $ecx killed $ecx killed $rcx
	vzeroupper
	callq	_halide_error_buffer_extents_negative
	jmp	LBB0_260
LBB0_521:                               ## %assert_failed162
	vextracti128	$1, %ymm14, %xmm0
	vpextrd	$2, %xmm0, %eax
	decl	%eax
	movl	%eax, (%rsp)
	leaq	l_str.34(%rip), %rsi
	xorl	%edi, %edi
	xorl	%edx, %edx
	xorl	%ecx, %ecx
	movl	$23, %r8d
	movl	568(%rsp), %r9d                 ## 4-byte Reload
	vzeroupper
	callq	_halide_error_access_out_of_bounds
	jmp	LBB0_260
LBB0_522:                               ## %assert_failed163
	leaq	l_str.34(%rip), %rsi
	xorl	%edi, %edi
	xorl	%edx, %edx
	movl	328(%rsp), %ecx                 ## 4-byte Reload
	vzeroupper
	callq	_halide_error_buffer_extents_negative
	jmp	LBB0_260
LBB0_523:                               ## %assert_failed164
	vextracti128	$1, %ymm14, %xmm0
	vpextrd	$3, %xmm0, %eax
	decl	%eax
	movl	%eax, (%rsp)
	leaq	l_str.34(%rip), %rsi
	xorl	%edi, %edi
	movl	$1, %edx
	xorl	%ecx, %ecx
	movl	$3, %r8d
	movl	560(%rsp), %r9d                 ## 4-byte Reload
	vzeroupper
	callq	_halide_error_access_out_of_bounds
	jmp	LBB0_260
LBB0_524:                               ## %assert_failed165
	leaq	l_str.34(%rip), %rsi
	xorl	%edi, %edi
	movl	$1, %edx
	movq	296(%rsp), %rcx                 ## 8-byte Reload
                                        ## kill: def $ecx killed $ecx killed $rcx
	vzeroupper
	callq	_halide_error_buffer_extents_negative
	jmp	LBB0_260
LBB0_525:                               ## %assert_failed166
	decl	%r12d
	movl	%r12d, (%rsp)
	leaq	l_str.35(%rip), %rsi
	xorl	%edi, %edi
	xorl	%edx, %edx
	xorl	%ecx, %ecx
	movl	$23, %r8d
	movq	1096(%rsp), %r9                 ## 8-byte Reload
                                        ## kill: def $r9d killed $r9d killed $r9
	vzeroupper
	callq	_halide_error_access_out_of_bounds
	jmp	LBB0_260
LBB0_526:                               ## %assert_failed167
	leaq	l_str.35(%rip), %rsi
	xorl	%edi, %edi
	xorl	%edx, %edx
	movq	1072(%rsp), %rcx                ## 8-byte Reload
                                        ## kill: def $ecx killed $ecx killed $rcx
	vzeroupper
	callq	_halide_error_buffer_extents_negative
	jmp	LBB0_260
LBB0_527:                               ## %assert_failed168
	decl	%r13d
	movl	%r13d, (%rsp)
	leaq	l_str.35(%rip), %rsi
	xorl	%edi, %edi
	movl	$1, %edx
	xorl	%ecx, %ecx
	movl	$38, %r8d
	movq	1088(%rsp), %r9                 ## 8-byte Reload
                                        ## kill: def $r9d killed $r9d killed $r9
	vzeroupper
	callq	_halide_error_access_out_of_bounds
	jmp	LBB0_260
LBB0_528:                               ## %assert_failed169
	leaq	l_str.35(%rip), %rsi
	xorl	%edi, %edi
	movl	$1, %edx
	movq	1064(%rsp), %rcx                ## 8-byte Reload
                                        ## kill: def $ecx killed $ecx killed $rcx
	vzeroupper
	callq	_halide_error_buffer_extents_negative
	jmp	LBB0_260
LBB0_529:                               ## %assert_failed170
	decl	%ecx
	movl	%ecx, (%rsp)
	leaq	l_str.35(%rip), %rsi
	xorl	%edi, %edi
	movl	$2, %edx
	xorl	%ecx, %ecx
	movl	$3, %r8d
	movq	1080(%rsp), %r9                 ## 8-byte Reload
                                        ## kill: def $r9d killed $r9d killed $r9
	vzeroupper
	callq	_halide_error_access_out_of_bounds
	jmp	LBB0_260
LBB0_530:                               ## %assert_failed171
	leaq	l_str.35(%rip), %rsi
	xorl	%edi, %edi
	movl	$2, %edx
	movq	1056(%rsp), %rcx                ## 8-byte Reload
                                        ## kill: def $ecx killed $ecx killed $rcx
	vzeroupper
	callq	_halide_error_buffer_extents_negative
	jmp	LBB0_260
LBB0_531:                               ## %assert_failed172
	leaq	l_str.36(%rip), %rsi
	leaq	l_str.37(%rip), %rcx
	xorl	%edi, %edi
	movl	920(%rsp), %edx                 ## 4-byte Reload
	jmp	LBB0_555
LBB0_532:                               ## %assert_failed173
	leaq	l_str.38(%rip), %rsi
	leaq	l_str.39(%rip), %rcx
	xorl	%edi, %edi
	movq	808(%rsp), %rdx                 ## 8-byte Reload
	jmp	LBB0_557
LBB0_533:                               ## %assert_failed174
	leaq	l_str.40(%rip), %rsi
	leaq	l_str.41(%rip), %rcx
	xorl	%edi, %edi
	movq	800(%rsp), %rdx                 ## 8-byte Reload
	jmp	LBB0_539
LBB0_534:                               ## %assert_failed175
	vmovapd	1360(%rsp), %xmm0               ## 16-byte Reload
	vextractps	$1, %xmm0, %edx
	leaq	l_str.42(%rip), %rsi
	leaq	l_str.37(%rip), %rcx
	xorl	%edi, %edi
	jmp	LBB0_555
LBB0_535:                               ## %assert_failed176
	leaq	l_str.43(%rip), %rsi
	leaq	l_str.39(%rip), %rcx
	xorl	%edi, %edi
	movq	760(%rsp), %rdx                 ## 8-byte Reload
	jmp	LBB0_557
LBB0_536:                               ## %assert_failed177
	leaq	l_str.44(%rip), %rsi
	leaq	l_str.41(%rip), %rcx
	xorl	%edi, %edi
	jmp	LBB0_539
LBB0_537:                               ## %assert_failed178
	leaq	l_str.45(%rip), %rsi
	leaq	l_str.39(%rip), %rcx
	xorl	%edi, %edi
	movq	776(%rsp), %rdx                 ## 8-byte Reload
	jmp	LBB0_557
LBB0_538:                               ## %assert_failed179
	leaq	l_str.46(%rip), %rsi
	leaq	l_str.41(%rip), %rcx
	xorl	%edi, %edi
	movq	744(%rsp), %rdx                 ## 8-byte Reload
LBB0_539:                               ## %assert_failed177
                                        ## kill: def $edx killed $edx killed $rdx
	movl	$32, %r8d
	vzeroupper
	callq	_halide_error_constraint_violated
	jmp	LBB0_260
LBB0_540:                               ## %assert_failed180
	leaq	l_str.47(%rip), %rsi
	leaq	l_str.37(%rip), %rcx
	xorl	%edi, %edi
	movl	908(%rsp), %edx                 ## 4-byte Reload
	jmp	LBB0_555
LBB0_541:                               ## %assert_failed181
	leaq	l_str.48(%rip), %rsi
	leaq	l_str.39(%rip), %rcx
	xorl	%edi, %edi
	movq	720(%rsp), %rdx                 ## 8-byte Reload
	jmp	LBB0_557
LBB0_542:                               ## %assert_failed182
	leaq	l_str.49(%rip), %rsi
	leaq	l_str.50(%rip), %rcx
	xorl	%edi, %edi
	movq	688(%rsp), %rdx                 ## 8-byte Reload
	jmp	LBB0_546
LBB0_543:                               ## %assert_failed183
	leaq	l_str.51(%rip), %rsi
	leaq	l_str.37(%rip), %rcx
	xorl	%edi, %edi
	movl	900(%rsp), %edx                 ## 4-byte Reload
	jmp	LBB0_555
LBB0_544:                               ## %assert_failed184
	leaq	l_str.52(%rip), %rsi
	leaq	l_str.39(%rip), %rcx
	xorl	%edi, %edi
	movq	672(%rsp), %rdx                 ## 8-byte Reload
	jmp	LBB0_557
LBB0_545:                               ## %assert_failed185
	leaq	l_str.53(%rip), %rsi
	leaq	l_str.50(%rip), %rcx
	xorl	%edi, %edi
	movq	40(%rsp), %rdx                  ## 8-byte Reload
LBB0_546:                               ## %assert_failed182
                                        ## kill: def $edx killed $edx killed $rdx
	movl	$8, %r8d
	vzeroupper
	callq	_halide_error_constraint_violated
	jmp	LBB0_260
LBB0_547:                               ## %assert_failed186
	leaq	l_str.54(%rip), %rsi
	leaq	l_str.39(%rip), %rcx
	xorl	%edi, %edi
	movq	664(%rsp), %rdx                 ## 8-byte Reload
	jmp	LBB0_557
LBB0_548:                               ## %assert_failed187
	leaq	l_str.55(%rip), %rsi
	leaq	l_str.56(%rip), %rcx
	xorl	%edi, %edi
	movq	640(%rsp), %rdx                 ## 8-byte Reload
                                        ## kill: def $edx killed $edx killed $rdx
	movl	$40, %r8d
	vzeroupper
	callq	_halide_error_constraint_violated
	jmp	LBB0_260
LBB0_549:                               ## %assert_failed188
	leaq	l_str.57(%rip), %rsi
	leaq	l_str.39(%rip), %rcx
	xorl	%edi, %edi
	movq	656(%rsp), %rdx                 ## 8-byte Reload
	jmp	LBB0_557
LBB0_550:                               ## %assert_failed189
	leaq	l_str.58(%rip), %rsi
	leaq	l_str.59(%rip), %rcx
	xorl	%edi, %edi
	movq	632(%rsp), %rdx                 ## 8-byte Reload
                                        ## kill: def $edx killed $edx killed $rdx
	movl	$7, %r8d
	vzeroupper
	callq	_halide_error_constraint_violated
	jmp	LBB0_260
LBB0_551:                               ## %assert_failed190
	leaq	l_str.60(%rip), %rsi
	leaq	l_str.37(%rip), %rcx
	xorl	%edi, %edi
	movl	892(%rsp), %edx                 ## 4-byte Reload
	jmp	LBB0_555
LBB0_552:                               ## %assert_failed191
	leaq	l_str.61(%rip), %rsi
	leaq	l_str.39(%rip), %rcx
	xorl	%edi, %edi
	movq	608(%rsp), %rdx                 ## 8-byte Reload
	jmp	LBB0_557
LBB0_553:                               ## %assert_failed192
	leaq	l_str.62(%rip), %rsi
	leaq	l_str.63(%rip), %rcx
	xorl	%edi, %edi
	movq	584(%rsp), %rdx                 ## 8-byte Reload
	jmp	LBB0_559
LBB0_554:                               ## %assert_failed193
	leaq	l_str.64(%rip), %rsi
	leaq	l_str.37(%rip), %rcx
	xorl	%edi, %edi
	movl	888(%rsp), %edx                 ## 4-byte Reload
LBB0_555:                               ## %assert_failed172
	movl	$1, %r8d
	vzeroupper
	callq	_halide_error_constraint_violated
	jmp	LBB0_260
LBB0_556:                               ## %assert_failed194
	leaq	l_str.65(%rip), %rsi
	leaq	l_str.39(%rip), %rcx
	xorl	%edi, %edi
	movq	576(%rsp), %rdx                 ## 8-byte Reload
LBB0_557:                               ## %assert_failed173
                                        ## kill: def $edx killed $edx killed $rdx
	xorl	%r8d, %r8d
	vzeroupper
	callq	_halide_error_constraint_violated
	jmp	LBB0_260
LBB0_558:                               ## %assert_failed195
	leaq	l_str.66(%rip), %rsi
	leaq	l_str.63(%rip), %rcx
	xorl	%edi, %edi
	movq	552(%rsp), %rdx                 ## 8-byte Reload
LBB0_559:                               ## %assert_failed192
                                        ## kill: def $edx killed $edx killed $rdx
	movl	$24, %r8d
	vzeroupper
	callq	_halide_error_constraint_violated
	jmp	LBB0_260
LBB0_560:                               ## %assert_failed243
	leaq	l_str.15(%rip), %rsi
	movl	$2147483647, %ecx               ## imm = 0x7FFFFFFF
	xorl	%edi, %edi
	movq	%r15, %rdx
	vzeroupper
	callq	_halide_error_buffer_allocation_too_large
	jmp	LBB0_260
LBB0_561:                               ## %assert_failed244
	leaq	l_str.15(%rip), %rsi
	movl	$2147483647, %ecx               ## imm = 0x7FFFFFFF
	xorl	%edi, %edi
	movq	%r12, %rdx
	vzeroupper
	callq	_halide_error_buffer_allocation_too_large
	jmp	LBB0_260
LBB0_562:                               ## %assert_failed245
	leaq	l_str.13(%rip), %rsi
	movl	$2147483647, %ecx               ## imm = 0x7FFFFFFF
	xorl	%edi, %edi
	movq	%r13, %rdx
	vzeroupper
	callq	_halide_error_buffer_allocation_too_large
	jmp	LBB0_260
LBB0_563:                               ## %assert_failed246
	leaq	l_str.11(%rip), %rsi
	jmp	LBB0_577
LBB0_564:                               ## %assert_failed247
	leaq	l_str.11(%rip), %rsi
	movl	$2147483647, %ecx               ## imm = 0x7FFFFFFF
	xorl	%edi, %edi
	movq	632(%rsp), %rdx                 ## 8-byte Reload
	vzeroupper
	callq	_halide_error_buffer_allocation_too_large
	jmp	LBB0_260
LBB0_565:                               ## %assert_failed248
	leaq	l_str.11(%rip), %rsi
	movl	$2147483647, %ecx               ## imm = 0x7FFFFFFF
	xorl	%edi, %edi
	movq	%rbx, %rdx
	vzeroupper
	callq	_halide_error_buffer_extents_too_large
	jmp	LBB0_260
LBB0_566:                               ## %assert_failed249
	leaq	l_str.11(%rip), %rsi
	movl	$2147483647, %ecx               ## imm = 0x7FFFFFFF
	xorl	%edi, %edi
	movq	624(%rsp), %rdx                 ## 8-byte Reload
	vzeroupper
	callq	_halide_error_buffer_allocation_too_large
	jmp	LBB0_260
LBB0_567:                               ## %assert_failed250
	leaq	l_str.11(%rip), %rsi
	movl	$2147483647, %ecx               ## imm = 0x7FFFFFFF
	xorl	%edi, %edi
	movq	%r8, %rdx
	vzeroupper
	callq	_halide_error_buffer_extents_too_large
	jmp	LBB0_260
LBB0_568:                               ## %assert_failed251
	movslq	832(%rsp), %rdx                 ## 4-byte Folded Reload
	leaq	l_str.10(%rip), %rsi
	jmp	LBB0_577
LBB0_569:                               ## %assert_failed252
	leaq	l_str.9(%rip), %rsi
	movl	$2147483647, %ecx               ## imm = 0x7FFFFFFF
	xorl	%edi, %edi
	movq	%r9, %rdx
	vzeroupper
	callq	_halide_error_buffer_allocation_too_large
	jmp	LBB0_260
LBB0_570:                               ## %assert_failed253
	leaq	l_str.9(%rip), %rsi
	movl	$2147483647, %ecx               ## imm = 0x7FFFFFFF
	xorl	%edi, %edi
	movq	616(%rsp), %rdx                 ## 8-byte Reload
	vzeroupper
	callq	_halide_error_buffer_allocation_too_large
	jmp	LBB0_260
LBB0_571:                               ## %assert_failed254
	leaq	l_str.9(%rip), %rsi
	movl	$2147483647, %ecx               ## imm = 0x7FFFFFFF
	xorl	%edi, %edi
	movq	%r10, %rdx
	vzeroupper
	callq	_halide_error_buffer_extents_too_large
	jmp	LBB0_260
LBB0_572:                               ## %assert_failed255
	leaq	l_str.9(%rip), %rsi
	movl	$2147483647, %ecx               ## imm = 0x7FFFFFFF
	xorl	%edi, %edi
	movq	608(%rsp), %rdx                 ## 8-byte Reload
	vzeroupper
	callq	_halide_error_buffer_allocation_too_large
	jmp	LBB0_260
LBB0_573:                               ## %assert_failed256
	leaq	l_str.9(%rip), %rsi
	movl	$2147483647, %ecx               ## imm = 0x7FFFFFFF
	xorl	%edi, %edi
	movq	%r11, %rdx
	vzeroupper
	callq	_halide_error_buffer_extents_too_large
	jmp	LBB0_260
LBB0_574:                               ## %assert_failed257
	movslq	480(%rsp), %rdx                 ## 4-byte Folded Reload
	leaq	l_str.8(%rip), %rsi
	jmp	LBB0_577
LBB0_575:                               ## %assert_failed258
	vmovq	%xmm8, %rdx
	leaq	l_str.7(%rip), %rsi
	jmp	LBB0_577
LBB0_576:                               ## %assert_failed259
	vpextrq	$1, %xmm8, %rdx
	leaq	l_str.6(%rip), %rsi
LBB0_577:                               ## %assert_failed246
	movl	$2147483647, %ecx               ## imm = 0x7FFFFFFF
	xorl	%edi, %edi
	vzeroupper
	callq	_halide_error_buffer_allocation_too_large
	jmp	LBB0_260
LBB0_578:                               ## %assert_failed260
	leaq	l_str.6(%rip), %rsi
	movl	$2147483647, %ecx               ## imm = 0x7FFFFFFF
	xorl	%edi, %edi
	movq	600(%rsp), %rdx                 ## 8-byte Reload
	vzeroupper
	callq	_halide_error_buffer_allocation_too_large
	jmp	LBB0_260
LBB0_579:                               ## %assert_failed261
	leaq	l_str.5(%rip), %rsi
	movl	$2147483647, %ecx               ## imm = 0x7FFFFFFF
	xorl	%edi, %edi
	movq	592(%rsp), %rdx                 ## 8-byte Reload
	vzeroupper
	callq	_halide_error_buffer_allocation_too_large
	jmp	LBB0_260
LBB0_580:                               ## %assert_failed262
	leaq	l_str.4(%rip), %rsi
	movl	$2147483647, %ecx               ## imm = 0x7FFFFFFF
	xorl	%edi, %edi
	movq	584(%rsp), %rdx                 ## 8-byte Reload
	vzeroupper
	callq	_halide_error_buffer_allocation_too_large
	jmp	LBB0_260
LBB0_581:                               ## %assert_failed263
	leaq	l_str.4(%rip), %rsi
	movl	$2147483647, %ecx               ## imm = 0x7FFFFFFF
	xorl	%edi, %edi
	movq	576(%rsp), %rdx                 ## 8-byte Reload
	vzeroupper
	callq	_halide_error_buffer_allocation_too_large
	jmp	LBB0_260
LBB0_582:                               ## %assert_failed264
	leaq	l_str.4(%rip), %rsi
	movl	$2147483647, %ecx               ## imm = 0x7FFFFFFF
	xorl	%edi, %edi
	movq	568(%rsp), %rdx                 ## 8-byte Reload
	vzeroupper
	callq	_halide_error_buffer_allocation_too_large
	jmp	LBB0_260
LBB0_583:                               ## %assert_failed265
	leaq	l_str.3(%rip), %rsi
	movl	$2147483647, %ecx               ## imm = 0x7FFFFFFF
	xorl	%edi, %edi
	movq	560(%rsp), %rdx                 ## 8-byte Reload
	vzeroupper
	callq	_halide_error_buffer_allocation_too_large
	jmp	LBB0_260
LBB0_584:                               ## %assert_failed266
	leaq	l_str(%rip), %rsi
	movl	$2147483647, %ecx               ## imm = 0x7FFFFFFF
	xorl	%edi, %edi
	movq	552(%rsp), %rdx                 ## 8-byte Reload
	vzeroupper
	callq	_halide_error_buffer_allocation_too_large
	jmp	LBB0_260
LBB0_585:                               ## %assert_failed267
	leaq	l_str(%rip), %rsi
	movl	$2147483647, %ecx               ## imm = 0x7FFFFFFF
	xorl	%edi, %edi
	movq	1344(%rsp), %rdx                ## 8-byte Reload
	vzeroupper
	callq	_halide_error_buffer_allocation_too_large
	jmp	LBB0_260
LBB0_586:                               ## %assert_failed268
	leaq	l_str.20(%rip), %rsi
	jmp	LBB0_603
LBB0_587:                               ## %assert_failed269
	leaq	l_str.19(%rip), %rsi
	jmp	LBB0_603
LBB0_588:                               ## %assert_failed270
	leaq	l_str.21(%rip), %rsi
	jmp	LBB0_603
LBB0_589:                               ## %assert_failed271
	leaq	l_str.22(%rip), %rsi
	jmp	LBB0_603
LBB0_590:                               ## %assert_failed272
	leaq	l_str.23(%rip), %rsi
	jmp	LBB0_603
LBB0_591:                               ## %assert_failed273
	leaq	l_str.24(%rip), %rsi
	jmp	LBB0_603
LBB0_592:                               ## %assert_failed274
	leaq	l_str.25(%rip), %rsi
	jmp	LBB0_603
LBB0_593:                               ## %assert_failed275
	leaq	l_str.26(%rip), %rsi
	jmp	LBB0_603
LBB0_594:                               ## %assert_failed276
	leaq	l_str.27(%rip), %rsi
	jmp	LBB0_603
LBB0_595:                               ## %assert_failed277
	leaq	l_str.28(%rip), %rsi
	jmp	LBB0_603
LBB0_596:                               ## %assert_failed278
	leaq	l_str.29(%rip), %rsi
	jmp	LBB0_603
LBB0_597:                               ## %assert_failed279
	leaq	l_str.30(%rip), %rsi
	jmp	LBB0_603
LBB0_598:                               ## %assert_failed280
	leaq	l_str.31(%rip), %rsi
	jmp	LBB0_603
LBB0_599:                               ## %assert_failed281
	leaq	l_str.32(%rip), %rsi
	jmp	LBB0_603
LBB0_600:                               ## %assert_failed282
	leaq	l_str.33(%rip), %rsi
	jmp	LBB0_603
LBB0_601:                               ## %assert_failed283
	leaq	l_str.34(%rip), %rsi
	jmp	LBB0_603
LBB0_602:                               ## %assert_failed284
	leaq	l_str.35(%rip), %rsi
LBB0_603:                               ## %assert_failed268
	xorl	%edi, %edi
	vzeroupper
	callq	_halide_error_device_dirty_with_no_device_support
	jmp	LBB0_260
LBB0_604:                               ## %assert_failed285
	leaq	l_str.20(%rip), %rsi
	jmp	LBB0_621
LBB0_605:                               ## %assert_failed286
	leaq	l_str.19(%rip), %rsi
	jmp	LBB0_621
LBB0_606:                               ## %assert_failed287
	leaq	l_str.21(%rip), %rsi
	jmp	LBB0_621
LBB0_607:                               ## %assert_failed288
	leaq	l_str.22(%rip), %rsi
	jmp	LBB0_621
LBB0_608:                               ## %assert_failed289
	leaq	l_str.23(%rip), %rsi
	jmp	LBB0_621
LBB0_609:                               ## %assert_failed290
	leaq	l_str.24(%rip), %rsi
	jmp	LBB0_621
LBB0_610:                               ## %assert_failed291
	leaq	l_str.25(%rip), %rsi
	jmp	LBB0_621
LBB0_611:                               ## %assert_failed292
	leaq	l_str.26(%rip), %rsi
	jmp	LBB0_621
LBB0_612:                               ## %assert_failed293
	leaq	l_str.27(%rip), %rsi
	jmp	LBB0_621
LBB0_613:                               ## %assert_failed294
	leaq	l_str.28(%rip), %rsi
	jmp	LBB0_621
LBB0_614:                               ## %assert_failed295
	leaq	l_str.29(%rip), %rsi
	jmp	LBB0_621
LBB0_615:                               ## %assert_failed296
	leaq	l_str.30(%rip), %rsi
	jmp	LBB0_621
LBB0_616:                               ## %assert_failed297
	leaq	l_str.31(%rip), %rsi
	jmp	LBB0_621
LBB0_617:                               ## %assert_failed298
	leaq	l_str.32(%rip), %rsi
	jmp	LBB0_621
LBB0_618:                               ## %assert_failed299
	leaq	l_str.33(%rip), %rsi
	jmp	LBB0_621
LBB0_619:                               ## %assert_failed300
	leaq	l_str.34(%rip), %rsi
	jmp	LBB0_621
LBB0_620:                               ## %assert_failed301
	leaq	l_str.35(%rip), %rsi
LBB0_621:                               ## %assert_failed285
	xorl	%edi, %edi
	vzeroupper
	callq	_halide_error_host_is_null
	jmp	LBB0_260
	.cfi_endproc
	.p2align	2, 0x90
	.data_region jt32
.set L0_0_set_272, LBB0_272-LJTI0_0
.set L0_0_set_360, LBB0_360-LJTI0_0
.set L0_0_set_361, LBB0_361-LJTI0_0
.set L0_0_set_362, LBB0_362-LJTI0_0
.set L0_0_set_363, LBB0_363-LJTI0_0
.set L0_0_set_364, LBB0_364-LJTI0_0
.set L0_0_set_365, LBB0_365-LJTI0_0
.set L0_0_set_366, LBB0_366-LJTI0_0
.set L0_0_set_367, LBB0_367-LJTI0_0
.set L0_0_set_368, LBB0_368-LJTI0_0
.set L0_0_set_369, LBB0_369-LJTI0_0
.set L0_0_set_370, LBB0_370-LJTI0_0
.set L0_0_set_371, LBB0_371-LJTI0_0
.set L0_0_set_372, LBB0_372-LJTI0_0
.set L0_0_set_373, LBB0_373-LJTI0_0
.set L0_0_set_374, LBB0_374-LJTI0_0
.set L0_0_set_375, LBB0_375-LJTI0_0
LJTI0_0:
	.long	L0_0_set_272
	.long	L0_0_set_360
	.long	L0_0_set_361
	.long	L0_0_set_362
	.long	L0_0_set_363
	.long	L0_0_set_364
	.long	L0_0_set_365
	.long	L0_0_set_366
	.long	L0_0_set_367
	.long	L0_0_set_368
	.long	L0_0_set_369
	.long	L0_0_set_370
	.long	L0_0_set_371
	.long	L0_0_set_372
	.long	L0_0_set_373
	.long	L0_0_set_374
	.long	L0_0_set_375
.set L0_1_set_286, LBB0_286-LJTI0_1
.set L0_1_set_427, LBB0_427-LJTI0_1
.set L0_1_set_428, LBB0_428-LJTI0_1
.set L0_1_set_429, LBB0_429-LJTI0_1
.set L0_1_set_430, LBB0_430-LJTI0_1
.set L0_1_set_431, LBB0_431-LJTI0_1
.set L0_1_set_432, LBB0_432-LJTI0_1
.set L0_1_set_433, LBB0_433-LJTI0_1
.set L0_1_set_434, LBB0_434-LJTI0_1
.set L0_1_set_435, LBB0_435-LJTI0_1
.set L0_1_set_436, LBB0_436-LJTI0_1
.set L0_1_set_437, LBB0_437-LJTI0_1
.set L0_1_set_438, LBB0_438-LJTI0_1
.set L0_1_set_439, LBB0_439-LJTI0_1
.set L0_1_set_440, LBB0_440-LJTI0_1
.set L0_1_set_441, LBB0_441-LJTI0_1
.set L0_1_set_442, LBB0_442-LJTI0_1
.set L0_1_set_443, LBB0_443-LJTI0_1
.set L0_1_set_444, LBB0_444-LJTI0_1
.set L0_1_set_445, LBB0_445-LJTI0_1
.set L0_1_set_446, LBB0_446-LJTI0_1
.set L0_1_set_447, LBB0_447-LJTI0_1
.set L0_1_set_449, LBB0_449-LJTI0_1
.set L0_1_set_450, LBB0_450-LJTI0_1
.set L0_1_set_451, LBB0_451-LJTI0_1
.set L0_1_set_452, LBB0_452-LJTI0_1
.set L0_1_set_453, LBB0_453-LJTI0_1
.set L0_1_set_454, LBB0_454-LJTI0_1
.set L0_1_set_455, LBB0_455-LJTI0_1
.set L0_1_set_456, LBB0_456-LJTI0_1
.set L0_1_set_457, LBB0_457-LJTI0_1
.set L0_1_set_458, LBB0_458-LJTI0_1
.set L0_1_set_460, LBB0_460-LJTI0_1
.set L0_1_set_462, LBB0_462-LJTI0_1
.set L0_1_set_464, LBB0_464-LJTI0_1
.set L0_1_set_465, LBB0_465-LJTI0_1
.set L0_1_set_466, LBB0_466-LJTI0_1
.set L0_1_set_467, LBB0_467-LJTI0_1
.set L0_1_set_468, LBB0_468-LJTI0_1
.set L0_1_set_469, LBB0_469-LJTI0_1
.set L0_1_set_470, LBB0_470-LJTI0_1
.set L0_1_set_471, LBB0_471-LJTI0_1
.set L0_1_set_472, LBB0_472-LJTI0_1
.set L0_1_set_473, LBB0_473-LJTI0_1
.set L0_1_set_474, LBB0_474-LJTI0_1
.set L0_1_set_475, LBB0_475-LJTI0_1
.set L0_1_set_476, LBB0_476-LJTI0_1
.set L0_1_set_477, LBB0_477-LJTI0_1
.set L0_1_set_478, LBB0_478-LJTI0_1
.set L0_1_set_479, LBB0_479-LJTI0_1
.set L0_1_set_480, LBB0_480-LJTI0_1
.set L0_1_set_481, LBB0_481-LJTI0_1
.set L0_1_set_482, LBB0_482-LJTI0_1
.set L0_1_set_483, LBB0_483-LJTI0_1
.set L0_1_set_484, LBB0_484-LJTI0_1
.set L0_1_set_485, LBB0_485-LJTI0_1
.set L0_1_set_486, LBB0_486-LJTI0_1
.set L0_1_set_487, LBB0_487-LJTI0_1
.set L0_1_set_488, LBB0_488-LJTI0_1
.set L0_1_set_489, LBB0_489-LJTI0_1
.set L0_1_set_490, LBB0_490-LJTI0_1
.set L0_1_set_491, LBB0_491-LJTI0_1
.set L0_1_set_492, LBB0_492-LJTI0_1
LJTI0_1:
	.long	L0_1_set_286
	.long	L0_1_set_427
	.long	L0_1_set_428
	.long	L0_1_set_429
	.long	L0_1_set_430
	.long	L0_1_set_431
	.long	L0_1_set_432
	.long	L0_1_set_433
	.long	L0_1_set_434
	.long	L0_1_set_435
	.long	L0_1_set_436
	.long	L0_1_set_437
	.long	L0_1_set_438
	.long	L0_1_set_439
	.long	L0_1_set_440
	.long	L0_1_set_441
	.long	L0_1_set_442
	.long	L0_1_set_443
	.long	L0_1_set_444
	.long	L0_1_set_445
	.long	L0_1_set_446
	.long	L0_1_set_447
	.long	L0_1_set_449
	.long	L0_1_set_450
	.long	L0_1_set_451
	.long	L0_1_set_452
	.long	L0_1_set_453
	.long	L0_1_set_454
	.long	L0_1_set_455
	.long	L0_1_set_456
	.long	L0_1_set_457
	.long	L0_1_set_458
	.long	L0_1_set_460
	.long	L0_1_set_462
	.long	L0_1_set_464
	.long	L0_1_set_465
	.long	L0_1_set_466
	.long	L0_1_set_467
	.long	L0_1_set_468
	.long	L0_1_set_469
	.long	L0_1_set_470
	.long	L0_1_set_471
	.long	L0_1_set_472
	.long	L0_1_set_473
	.long	L0_1_set_474
	.long	L0_1_set_475
	.long	L0_1_set_476
	.long	L0_1_set_477
	.long	L0_1_set_478
	.long	L0_1_set_479
	.long	L0_1_set_480
	.long	L0_1_set_481
	.long	L0_1_set_482
	.long	L0_1_set_483
	.long	L0_1_set_484
	.long	L0_1_set_485
	.long	L0_1_set_486
	.long	L0_1_set_487
	.long	L0_1_set_488
	.long	L0_1_set_489
	.long	L0_1_set_490
	.long	L0_1_set_491
	.long	L0_1_set_492
.set L0_2_set_288, LBB0_288-LJTI0_2
.set L0_2_set_493, LBB0_493-LJTI0_2
.set L0_2_set_494, LBB0_494-LJTI0_2
.set L0_2_set_495, LBB0_495-LJTI0_2
.set L0_2_set_496, LBB0_496-LJTI0_2
.set L0_2_set_497, LBB0_497-LJTI0_2
.set L0_2_set_498, LBB0_498-LJTI0_2
.set L0_2_set_499, LBB0_499-LJTI0_2
.set L0_2_set_500, LBB0_500-LJTI0_2
.set L0_2_set_501, LBB0_501-LJTI0_2
.set L0_2_set_502, LBB0_502-LJTI0_2
.set L0_2_set_503, LBB0_503-LJTI0_2
.set L0_2_set_504, LBB0_504-LJTI0_2
.set L0_2_set_505, LBB0_505-LJTI0_2
.set L0_2_set_506, LBB0_506-LJTI0_2
.set L0_2_set_507, LBB0_507-LJTI0_2
.set L0_2_set_508, LBB0_508-LJTI0_2
.set L0_2_set_509, LBB0_509-LJTI0_2
.set L0_2_set_510, LBB0_510-LJTI0_2
.set L0_2_set_511, LBB0_511-LJTI0_2
.set L0_2_set_512, LBB0_512-LJTI0_2
.set L0_2_set_513, LBB0_513-LJTI0_2
.set L0_2_set_514, LBB0_514-LJTI0_2
.set L0_2_set_515, LBB0_515-LJTI0_2
.set L0_2_set_516, LBB0_516-LJTI0_2
.set L0_2_set_517, LBB0_517-LJTI0_2
.set L0_2_set_518, LBB0_518-LJTI0_2
.set L0_2_set_519, LBB0_519-LJTI0_2
.set L0_2_set_520, LBB0_520-LJTI0_2
.set L0_2_set_521, LBB0_521-LJTI0_2
.set L0_2_set_522, LBB0_522-LJTI0_2
.set L0_2_set_523, LBB0_523-LJTI0_2
.set L0_2_set_524, LBB0_524-LJTI0_2
.set L0_2_set_525, LBB0_525-LJTI0_2
.set L0_2_set_526, LBB0_526-LJTI0_2
.set L0_2_set_527, LBB0_527-LJTI0_2
.set L0_2_set_528, LBB0_528-LJTI0_2
.set L0_2_set_529, LBB0_529-LJTI0_2
.set L0_2_set_530, LBB0_530-LJTI0_2
.set L0_2_set_531, LBB0_531-LJTI0_2
.set L0_2_set_532, LBB0_532-LJTI0_2
.set L0_2_set_533, LBB0_533-LJTI0_2
.set L0_2_set_534, LBB0_534-LJTI0_2
.set L0_2_set_535, LBB0_535-LJTI0_2
.set L0_2_set_536, LBB0_536-LJTI0_2
.set L0_2_set_537, LBB0_537-LJTI0_2
.set L0_2_set_538, LBB0_538-LJTI0_2
.set L0_2_set_540, LBB0_540-LJTI0_2
.set L0_2_set_541, LBB0_541-LJTI0_2
.set L0_2_set_542, LBB0_542-LJTI0_2
.set L0_2_set_543, LBB0_543-LJTI0_2
.set L0_2_set_544, LBB0_544-LJTI0_2
.set L0_2_set_545, LBB0_545-LJTI0_2
.set L0_2_set_547, LBB0_547-LJTI0_2
.set L0_2_set_548, LBB0_548-LJTI0_2
.set L0_2_set_549, LBB0_549-LJTI0_2
.set L0_2_set_550, LBB0_550-LJTI0_2
.set L0_2_set_551, LBB0_551-LJTI0_2
.set L0_2_set_552, LBB0_552-LJTI0_2
.set L0_2_set_553, LBB0_553-LJTI0_2
.set L0_2_set_554, LBB0_554-LJTI0_2
.set L0_2_set_556, LBB0_556-LJTI0_2
.set L0_2_set_558, LBB0_558-LJTI0_2
LJTI0_2:
	.long	L0_2_set_288
	.long	L0_2_set_493
	.long	L0_2_set_494
	.long	L0_2_set_495
	.long	L0_2_set_496
	.long	L0_2_set_497
	.long	L0_2_set_498
	.long	L0_2_set_499
	.long	L0_2_set_500
	.long	L0_2_set_501
	.long	L0_2_set_502
	.long	L0_2_set_503
	.long	L0_2_set_504
	.long	L0_2_set_505
	.long	L0_2_set_506
	.long	L0_2_set_507
	.long	L0_2_set_508
	.long	L0_2_set_509
	.long	L0_2_set_510
	.long	L0_2_set_511
	.long	L0_2_set_512
	.long	L0_2_set_513
	.long	L0_2_set_514
	.long	L0_2_set_515
	.long	L0_2_set_516
	.long	L0_2_set_517
	.long	L0_2_set_518
	.long	L0_2_set_519
	.long	L0_2_set_520
	.long	L0_2_set_521
	.long	L0_2_set_522
	.long	L0_2_set_523
	.long	L0_2_set_524
	.long	L0_2_set_525
	.long	L0_2_set_526
	.long	L0_2_set_527
	.long	L0_2_set_528
	.long	L0_2_set_529
	.long	L0_2_set_530
	.long	L0_2_set_531
	.long	L0_2_set_532
	.long	L0_2_set_533
	.long	L0_2_set_534
	.long	L0_2_set_535
	.long	L0_2_set_536
	.long	L0_2_set_537
	.long	L0_2_set_538
	.long	L0_2_set_540
	.long	L0_2_set_541
	.long	L0_2_set_542
	.long	L0_2_set_543
	.long	L0_2_set_544
	.long	L0_2_set_545
	.long	L0_2_set_547
	.long	L0_2_set_548
	.long	L0_2_set_549
	.long	L0_2_set_550
	.long	L0_2_set_551
	.long	L0_2_set_552
	.long	L0_2_set_553
	.long	L0_2_set_554
	.long	L0_2_set_556
	.long	L0_2_set_558
.set L0_3_set_290, LBB0_290-LJTI0_3
.set L0_3_set_377, LBB0_377-LJTI0_3
.set L0_3_set_378, LBB0_378-LJTI0_3
.set L0_3_set_379, LBB0_379-LJTI0_3
.set L0_3_set_380, LBB0_380-LJTI0_3
.set L0_3_set_381, LBB0_381-LJTI0_3
.set L0_3_set_382, LBB0_382-LJTI0_3
.set L0_3_set_383, LBB0_383-LJTI0_3
.set L0_3_set_384, LBB0_384-LJTI0_3
.set L0_3_set_385, LBB0_385-LJTI0_3
.set L0_3_set_386, LBB0_386-LJTI0_3
.set L0_3_set_387, LBB0_387-LJTI0_3
.set L0_3_set_388, LBB0_388-LJTI0_3
.set L0_3_set_389, LBB0_389-LJTI0_3
.set L0_3_set_390, LBB0_390-LJTI0_3
.set L0_3_set_391, LBB0_391-LJTI0_3
.set L0_3_set_393, LBB0_393-LJTI0_3
.set L0_3_set_394, LBB0_394-LJTI0_3
.set L0_3_set_395, LBB0_395-LJTI0_3
.set L0_3_set_396, LBB0_396-LJTI0_3
.set L0_3_set_397, LBB0_397-LJTI0_3
.set L0_3_set_398, LBB0_398-LJTI0_3
.set L0_3_set_399, LBB0_399-LJTI0_3
.set L0_3_set_400, LBB0_400-LJTI0_3
.set L0_3_set_401, LBB0_401-LJTI0_3
.set L0_3_set_402, LBB0_402-LJTI0_3
.set L0_3_set_404, LBB0_404-LJTI0_3
.set L0_3_set_405, LBB0_405-LJTI0_3
.set L0_3_set_406, LBB0_406-LJTI0_3
.set L0_3_set_407, LBB0_407-LJTI0_3
.set L0_3_set_408, LBB0_408-LJTI0_3
.set L0_3_set_409, LBB0_409-LJTI0_3
.set L0_3_set_410, LBB0_410-LJTI0_3
.set L0_3_set_411, LBB0_411-LJTI0_3
.set L0_3_set_412, LBB0_412-LJTI0_3
.set L0_3_set_413, LBB0_413-LJTI0_3
.set L0_3_set_414, LBB0_414-LJTI0_3
.set L0_3_set_415, LBB0_415-LJTI0_3
.set L0_3_set_417, LBB0_417-LJTI0_3
.set L0_3_set_418, LBB0_418-LJTI0_3
.set L0_3_set_420, LBB0_420-LJTI0_3
.set L0_3_set_421, LBB0_421-LJTI0_3
.set L0_3_set_422, LBB0_422-LJTI0_3
.set L0_3_set_425, LBB0_425-LJTI0_3
LJTI0_3:
	.long	L0_3_set_290
	.long	L0_3_set_377
	.long	L0_3_set_378
	.long	L0_3_set_379
	.long	L0_3_set_380
	.long	L0_3_set_381
	.long	L0_3_set_382
	.long	L0_3_set_383
	.long	L0_3_set_384
	.long	L0_3_set_385
	.long	L0_3_set_386
	.long	L0_3_set_387
	.long	L0_3_set_388
	.long	L0_3_set_389
	.long	L0_3_set_390
	.long	L0_3_set_391
	.long	L0_3_set_393
	.long	L0_3_set_394
	.long	L0_3_set_395
	.long	L0_3_set_396
	.long	L0_3_set_397
	.long	L0_3_set_398
	.long	L0_3_set_399
	.long	L0_3_set_400
	.long	L0_3_set_401
	.long	L0_3_set_402
	.long	L0_3_set_404
	.long	L0_3_set_405
	.long	L0_3_set_406
	.long	L0_3_set_407
	.long	L0_3_set_408
	.long	L0_3_set_409
	.long	L0_3_set_410
	.long	L0_3_set_411
	.long	L0_3_set_412
	.long	L0_3_set_413
	.long	L0_3_set_414
	.long	L0_3_set_415
	.long	L0_3_set_417
	.long	L0_3_set_418
	.long	L0_3_set_420
	.long	L0_3_set_421
	.long	L0_3_set_422
	.long	L0_3_set_425
.set L0_4_set_292, LBB0_292-LJTI0_4
.set L0_4_set_560, LBB0_560-LJTI0_4
.set L0_4_set_561, LBB0_561-LJTI0_4
.set L0_4_set_562, LBB0_562-LJTI0_4
.set L0_4_set_563, LBB0_563-LJTI0_4
.set L0_4_set_564, LBB0_564-LJTI0_4
.set L0_4_set_565, LBB0_565-LJTI0_4
.set L0_4_set_566, LBB0_566-LJTI0_4
.set L0_4_set_567, LBB0_567-LJTI0_4
.set L0_4_set_568, LBB0_568-LJTI0_4
.set L0_4_set_569, LBB0_569-LJTI0_4
.set L0_4_set_570, LBB0_570-LJTI0_4
.set L0_4_set_571, LBB0_571-LJTI0_4
.set L0_4_set_572, LBB0_572-LJTI0_4
.set L0_4_set_573, LBB0_573-LJTI0_4
.set L0_4_set_574, LBB0_574-LJTI0_4
.set L0_4_set_575, LBB0_575-LJTI0_4
.set L0_4_set_576, LBB0_576-LJTI0_4
.set L0_4_set_578, LBB0_578-LJTI0_4
.set L0_4_set_579, LBB0_579-LJTI0_4
.set L0_4_set_580, LBB0_580-LJTI0_4
.set L0_4_set_581, LBB0_581-LJTI0_4
.set L0_4_set_582, LBB0_582-LJTI0_4
.set L0_4_set_583, LBB0_583-LJTI0_4
.set L0_4_set_584, LBB0_584-LJTI0_4
.set L0_4_set_585, LBB0_585-LJTI0_4
.set L0_4_set_586, LBB0_586-LJTI0_4
.set L0_4_set_587, LBB0_587-LJTI0_4
.set L0_4_set_588, LBB0_588-LJTI0_4
.set L0_4_set_589, LBB0_589-LJTI0_4
.set L0_4_set_590, LBB0_590-LJTI0_4
.set L0_4_set_591, LBB0_591-LJTI0_4
.set L0_4_set_592, LBB0_592-LJTI0_4
.set L0_4_set_593, LBB0_593-LJTI0_4
.set L0_4_set_594, LBB0_594-LJTI0_4
.set L0_4_set_595, LBB0_595-LJTI0_4
.set L0_4_set_596, LBB0_596-LJTI0_4
.set L0_4_set_597, LBB0_597-LJTI0_4
.set L0_4_set_598, LBB0_598-LJTI0_4
.set L0_4_set_599, LBB0_599-LJTI0_4
.set L0_4_set_600, LBB0_600-LJTI0_4
.set L0_4_set_601, LBB0_601-LJTI0_4
.set L0_4_set_602, LBB0_602-LJTI0_4
.set L0_4_set_604, LBB0_604-LJTI0_4
.set L0_4_set_605, LBB0_605-LJTI0_4
.set L0_4_set_606, LBB0_606-LJTI0_4
.set L0_4_set_607, LBB0_607-LJTI0_4
.set L0_4_set_608, LBB0_608-LJTI0_4
.set L0_4_set_609, LBB0_609-LJTI0_4
.set L0_4_set_610, LBB0_610-LJTI0_4
.set L0_4_set_611, LBB0_611-LJTI0_4
.set L0_4_set_612, LBB0_612-LJTI0_4
.set L0_4_set_613, LBB0_613-LJTI0_4
.set L0_4_set_614, LBB0_614-LJTI0_4
.set L0_4_set_615, LBB0_615-LJTI0_4
.set L0_4_set_616, LBB0_616-LJTI0_4
.set L0_4_set_617, LBB0_617-LJTI0_4
.set L0_4_set_618, LBB0_618-LJTI0_4
.set L0_4_set_619, LBB0_619-LJTI0_4
.set L0_4_set_620, LBB0_620-LJTI0_4
LJTI0_4:
	.long	L0_4_set_292
	.long	L0_4_set_560
	.long	L0_4_set_561
	.long	L0_4_set_562
	.long	L0_4_set_563
	.long	L0_4_set_564
	.long	L0_4_set_565
	.long	L0_4_set_566
	.long	L0_4_set_567
	.long	L0_4_set_568
	.long	L0_4_set_569
	.long	L0_4_set_570
	.long	L0_4_set_571
	.long	L0_4_set_572
	.long	L0_4_set_573
	.long	L0_4_set_574
	.long	L0_4_set_575
	.long	L0_4_set_576
	.long	L0_4_set_578
	.long	L0_4_set_579
	.long	L0_4_set_580
	.long	L0_4_set_581
	.long	L0_4_set_582
	.long	L0_4_set_583
	.long	L0_4_set_584
	.long	L0_4_set_585
	.long	L0_4_set_586
	.long	L0_4_set_587
	.long	L0_4_set_588
	.long	L0_4_set_589
	.long	L0_4_set_590
	.long	L0_4_set_591
	.long	L0_4_set_592
	.long	L0_4_set_593
	.long	L0_4_set_594
	.long	L0_4_set_595
	.long	L0_4_set_596
	.long	L0_4_set_597
	.long	L0_4_set_598
	.long	L0_4_set_599
	.long	L0_4_set_600
	.long	L0_4_set_601
	.long	L0_4_set_602
	.long	L0_4_set_604
	.long	L0_4_set_605
	.long	L0_4_set_606
	.long	L0_4_set_607
	.long	L0_4_set_608
	.long	L0_4_set_609
	.long	L0_4_set_610
	.long	L0_4_set_611
	.long	L0_4_set_612
	.long	L0_4_set_613
	.long	L0_4_set_614
	.long	L0_4_set_615
	.long	L0_4_set_616
	.long	L0_4_set_617
	.long	L0_4_set_618
	.long	L0_4_set_619
	.long	L0_4_set_620
	.end_data_region
                                        ## -- End function
	.section	__TEXT,__literal4,4byte_literals
	.p2align	2                               ## -- Begin function train_cost_model.par_for.squashed_head1_filter.s0.s.s.s
LCPI1_0:
	.long	0xbfb8aa3b                      ## float -1.44269502
LCPI1_1:
	.long	0x3f317200                      ## float 0.693145751
LCPI1_2:
	.long	0x35bfbe8e                      ## float 1.42860677E-6
LCPI1_3:
	.long	4294967169                      ## 0xffffff81
LCPI1_4:
	.long	128                             ## 0x80
LCPI1_5:
	.long	0xb9a797f3                      ## float -3.19659332E-4
LCPI1_6:
	.long	0xbc0b192a                      ## float -0.00848988629
LCPI1_7:
	.long	0xbe2aae1f                      ## float -0.166679844
LCPI1_8:
	.long	0xbf800000                      ## float -1
LCPI1_9:
	.long	0x3f800000                      ## float 1
LCPI1_10:
	.long	0x3a9c2e66                      ## float 0.00119156833
LCPI1_11:
	.long	0x3d2a66bc                      ## float 0.0416018814
LCPI1_12:
	.long	0x3effffde                      ## float 0.499998987
	.section	__TEXT,__text,regular,pure_instructions
	.p2align	4, 0x90
_train_cost_model.par_for.squashed_head1_filter.s0.s.s.s: ## @train_cost_model.par_for.squashed_head1_filter.s0.s.s.s
## %bb.0:                               ## %entry
	pushq	%rbx
                                        ## kill: def $esi killed $esi def $rsi
	movslq	(%rdx), %rax
	movslq	4(%rdx), %r11
	movq	8(%rdx), %r8
	movq	24(%rdx), %r9
	cmpl	$11, %esi
	jg	LBB1_5
## %bb.1:                               ## %then_bb
	movl	%esi, %r10d
	sarl	%r10d
	movl	%r10d, %ecx
	andl	$-2, %ecx
	movslq	%ecx, %rcx
	leaq	(%rcx,%rcx,4), %rdx
	shlq	$6, %rdx
	movl	%esi, %ebx
	andl	$3, %ebx
	leaq	(%rbx,%rbx,4), %rdi
	shlq	$4, %rdi
	addq	%rdx, %rdi
	leaq	(%r9,%rdi,4), %rdi
	imulq	%rax, %rbx
	leaq	(%rbx,%rbx,4), %rdx
	imulq	%r11, %rcx
	leaq	(%rcx,%rdx,2), %rcx
	leaq	(%r8,%rcx,4), %rcx
	leaq	(,%rax,4), %rdx
	xorl	%ebx, %ebx
	vbroadcastss	LCPI1_0(%rip), %ymm5    ## ymm5 = [-1.44269502E+0,-1.44269502E+0,-1.44269502E+0,-1.44269502E+0,-1.44269502E+0,-1.44269502E+0,-1.44269502E+0,-1.44269502E+0]
	vbroadcastss	LCPI1_1(%rip), %ymm10   ## ymm10 = [6.93145751E-1,6.93145751E-1,6.93145751E-1,6.93145751E-1,6.93145751E-1,6.93145751E-1,6.93145751E-1,6.93145751E-1]
	vbroadcastss	LCPI1_2(%rip), %ymm0    ## ymm0 = [1.42860677E-6,1.42860677E-6,1.42860677E-6,1.42860677E-6,1.42860677E-6,1.42860677E-6,1.42860677E-6,1.42860677E-6]
	vmovups	%ymm0, -48(%rsp)                ## 32-byte Spill
	vpbroadcastd	LCPI1_3(%rip), %ymm14   ## ymm14 = [4294967169,4294967169,4294967169,4294967169,4294967169,4294967169,4294967169,4294967169]
	vpbroadcastd	LCPI1_4(%rip), %ymm15   ## ymm15 = [128,128,128,128,128,128,128,128]
	vbroadcastss	LCPI1_5(%rip), %ymm0    ## ymm0 = [-3.19659332E-4,-3.19659332E-4,-3.19659332E-4,-3.19659332E-4,-3.19659332E-4,-3.19659332E-4,-3.19659332E-4,-3.19659332E-4]
	vmovups	%ymm0, -80(%rsp)                ## 32-byte Spill
	vbroadcastss	LCPI1_6(%rip), %ymm0    ## ymm0 = [-8.48988629E-3,-8.48988629E-3,-8.48988629E-3,-8.48988629E-3,-8.48988629E-3,-8.48988629E-3,-8.48988629E-3,-8.48988629E-3]
	vmovups	%ymm0, -112(%rsp)               ## 32-byte Spill
	vbroadcastss	LCPI1_7(%rip), %ymm7    ## ymm7 = [-1.66679844E-1,-1.66679844E-1,-1.66679844E-1,-1.66679844E-1,-1.66679844E-1,-1.66679844E-1,-1.66679844E-1,-1.66679844E-1]
	vbroadcastss	LCPI1_8(%rip), %ymm8    ## ymm8 = [-1.0E+0,-1.0E+0,-1.0E+0,-1.0E+0,-1.0E+0,-1.0E+0,-1.0E+0,-1.0E+0]
	vbroadcastss	LCPI1_9(%rip), %ymm9    ## ymm9 = [1.0E+0,1.0E+0,1.0E+0,1.0E+0,1.0E+0,1.0E+0,1.0E+0,1.0E+0]
	vbroadcastss	LCPI1_10(%rip), %ymm2   ## ymm2 = [1.19156833E-3,1.19156833E-3,1.19156833E-3,1.19156833E-3,1.19156833E-3,1.19156833E-3,1.19156833E-3,1.19156833E-3]
	vbroadcastss	LCPI1_11(%rip), %ymm11  ## ymm11 = [4.16018814E-2,4.16018814E-2,4.16018814E-2,4.16018814E-2,4.16018814E-2,4.16018814E-2,4.16018814E-2,4.16018814E-2]
	vbroadcastss	LCPI1_12(%rip), %ymm12  ## ymm12 = [4.99998987E-1,4.99998987E-1,4.99998987E-1,4.99998987E-1,4.99998987E-1,4.99998987E-1,4.99998987E-1,4.99998987E-1]
	vpbroadcastd	LCPI1_9(%rip), %ymm13   ## ymm13 = [1.0E+0,1.0E+0,1.0E+0,1.0E+0,1.0E+0,1.0E+0,1.0E+0,1.0E+0]
	.p2align	4, 0x90
LBB1_2:                                 ## %"for squashed_head1_filter.s0.s.si"
                                        ## =>This Inner Loop Header: Depth=1
	vmovdqa	%ymm14, %ymm6
	vmovups	(%rcx), %ymm14
	vmovdqa	%ymm15, %ymm1
	vmulps	%ymm5, %ymm14, %ymm15
	vroundps	$1, %ymm15, %ymm15
	vfmadd231ps	%ymm10, %ymm15, %ymm14  ## ymm14 = (ymm15 * ymm10) + ymm14
	vfmadd231ps	-48(%rsp), %ymm15, %ymm14 ## 32-byte Folded Reload
                                        ## ymm14 = (ymm15 * mem) + ymm14
	vmulps	%ymm14, %ymm14, %ymm0
	vmovaps	%ymm5, %ymm3
	vmovups	-80(%rsp), %ymm5                ## 32-byte Reload
	vfmadd213ps	-112(%rsp), %ymm0, %ymm5 ## 32-byte Folded Reload
                                        ## ymm5 = (ymm0 * ymm5) + mem
	vfmadd213ps	%ymm7, %ymm0, %ymm5     ## ymm5 = (ymm0 * ymm5) + ymm7
	vfmadd213ps	%ymm8, %ymm0, %ymm5     ## ymm5 = (ymm0 * ymm5) + ymm8
	vmovaps	%ymm10, %ymm4
	vmovaps	%ymm2, %ymm10
	vfmadd213ps	%ymm11, %ymm0, %ymm10   ## ymm10 = (ymm0 * ymm10) + ymm11
	vfmadd213ps	%ymm12, %ymm0, %ymm10   ## ymm10 = (ymm0 * ymm10) + ymm12
	vfmadd213ps	%ymm9, %ymm0, %ymm10    ## ymm10 = (ymm0 * ymm10) + ymm9
	vfmadd231ps	%ymm5, %ymm14, %ymm10   ## ymm10 = (ymm14 * ymm5) + ymm10
	vmovdqa	%ymm6, %ymm14
	vcvttps2dq	%ymm15, %ymm0
	vmovdqa	%ymm1, %ymm15
	vpslld	$23, %ymm0, %ymm5
	vpaddd	%ymm5, %ymm13, %ymm5
	vfmadd213ps	%ymm9, %ymm10, %ymm5    ## ymm5 = (ymm10 * ymm5) + ymm9
	vdivps	%ymm5, %ymm9, %ymm5
	vpcmpgtd	%ymm0, %ymm1, %ymm10
	vpand	%ymm5, %ymm10, %ymm5
	vmovaps	%ymm4, %ymm10
	vpcmpgtd	%ymm6, %ymm0, %ymm0
	vblendvps	%ymm0, %ymm5, %ymm9, %ymm0
	vmovaps	%ymm3, %ymm5
	vmovaps	%ymm0, (%rdi,%rbx)
	addq	$32, %rbx
	addq	%rdx, %rcx
	cmpq	$320, %rbx                      ## imm = 0x140
	jne	LBB1_2
## %bb.3:                               ## %"end for squashed_head1_filter.s0.s.si"
	orl	$1, %r10d
	movslq	%r10d, %rdi
	leaq	(%rdi,%rdi,4), %rcx
	shlq	$6, %rcx
	andl	$3, %esi
	leaq	(%rsi,%rsi,4), %rbx
	shlq	$4, %rbx
	addq	%rcx, %rbx
	leaq	(%r9,%rbx,4), %rcx
	imulq	%rsi, %rax
	leaq	(%rax,%rax,4), %rax
	imulq	%rdi, %r11
	leaq	(%r11,%rax,2), %rax
	leaq	(%r8,%rax,4), %rax
	xorl	%esi, %esi
	.p2align	4, 0x90
LBB1_4:                                 ## %"for squashed_head1_filter.s0.s.si.1"
                                        ## =>This Inner Loop Header: Depth=1
	vmovups	(%rax), %ymm0
	vmulps	%ymm3, %ymm0, %ymm5
	vroundps	$1, %ymm5, %ymm5
	vfmadd231ps	%ymm10, %ymm5, %ymm0    ## ymm0 = (ymm5 * ymm10) + ymm0
	vfmadd231ps	-48(%rsp), %ymm5, %ymm0 ## 32-byte Folded Reload
                                        ## ymm0 = (ymm5 * mem) + ymm0
	vmulps	%ymm0, %ymm0, %ymm10
	vmovups	-80(%rsp), %ymm14               ## 32-byte Reload
	vfmadd213ps	-112(%rsp), %ymm10, %ymm14 ## 32-byte Folded Reload
                                        ## ymm14 = (ymm10 * ymm14) + mem
	vfmadd213ps	%ymm7, %ymm10, %ymm14   ## ymm14 = (ymm10 * ymm14) + ymm7
	vfmadd213ps	%ymm8, %ymm10, %ymm14   ## ymm14 = (ymm10 * ymm14) + ymm8
	vmovaps	%ymm2, %ymm15
	vfmadd213ps	%ymm11, %ymm10, %ymm15  ## ymm15 = (ymm10 * ymm15) + ymm11
	vfmadd213ps	%ymm12, %ymm10, %ymm15  ## ymm15 = (ymm10 * ymm15) + ymm12
	vfmadd213ps	%ymm9, %ymm10, %ymm15   ## ymm15 = (ymm10 * ymm15) + ymm9
	vfmadd231ps	%ymm14, %ymm0, %ymm15   ## ymm15 = (ymm0 * ymm14) + ymm15
	vmovdqa	%ymm6, %ymm14
	vcvttps2dq	%ymm5, %ymm0
	vpslld	$23, %ymm0, %ymm5
	vpaddd	%ymm5, %ymm13, %ymm5
	vfmadd213ps	%ymm9, %ymm15, %ymm5    ## ymm5 = (ymm15 * ymm5) + ymm9
	vmovdqa	%ymm1, %ymm10
	vdivps	%ymm5, %ymm9, %ymm5
	vpcmpgtd	%ymm0, %ymm1, %ymm10
	vpand	%ymm5, %ymm10, %ymm5
	vmovaps	%ymm4, %ymm10
	vpcmpgtd	%ymm6, %ymm0, %ymm0
	vblendvps	%ymm0, %ymm5, %ymm9, %ymm0
	vmovaps	%ymm0, (%rcx,%rsi)
	addq	$32, %rsi
	addq	%rdx, %rax
	cmpq	$320, %rsi                      ## imm = 0x140
	jne	LBB1_4
	jmp	LBB1_7
LBB1_5:                                 ## %next_bb
	addl	%r11d, %r11d
	leal	(%r11,%r11,2), %ecx
	movslq	%ecx, %rdx
	andl	$3, %esi
	leaq	(%rsi,%rsi,4), %rcx
	shlq	$6, %rcx
	addq	%r9, %rcx
	addq	$7680, %rcx                     ## imm = 0x1E00
	imulq	%rax, %rsi
	leaq	(%rsi,%rsi,4), %rsi
	leaq	(%rdx,%rsi,2), %rdx
	leaq	(%r8,%rdx,4), %rdx
	shlq	$2, %rax
	xorl	%esi, %esi
	vbroadcastss	LCPI1_0(%rip), %ymm0    ## ymm0 = [-1.44269502E+0,-1.44269502E+0,-1.44269502E+0,-1.44269502E+0,-1.44269502E+0,-1.44269502E+0,-1.44269502E+0,-1.44269502E+0]
	vmovups	%ymm0, -48(%rsp)                ## 32-byte Spill
	vbroadcastss	LCPI1_1(%rip), %ymm0    ## ymm0 = [6.93145751E-1,6.93145751E-1,6.93145751E-1,6.93145751E-1,6.93145751E-1,6.93145751E-1,6.93145751E-1,6.93145751E-1]
	vmovups	%ymm0, -80(%rsp)                ## 32-byte Spill
	vbroadcastss	LCPI1_2(%rip), %ymm0    ## ymm0 = [1.42860677E-6,1.42860677E-6,1.42860677E-6,1.42860677E-6,1.42860677E-6,1.42860677E-6,1.42860677E-6,1.42860677E-6]
	vmovups	%ymm0, -112(%rsp)               ## 32-byte Spill
	vpbroadcastd	LCPI1_3(%rip), %ymm3    ## ymm3 = [4294967169,4294967169,4294967169,4294967169,4294967169,4294967169,4294967169,4294967169]
	vpbroadcastd	LCPI1_4(%rip), %ymm4    ## ymm4 = [128,128,128,128,128,128,128,128]
	vbroadcastss	LCPI1_5(%rip), %ymm1    ## ymm1 = [-3.19659332E-4,-3.19659332E-4,-3.19659332E-4,-3.19659332E-4,-3.19659332E-4,-3.19659332E-4,-3.19659332E-4,-3.19659332E-4]
	vbroadcastss	LCPI1_6(%rip), %ymm6    ## ymm6 = [-8.48988629E-3,-8.48988629E-3,-8.48988629E-3,-8.48988629E-3,-8.48988629E-3,-8.48988629E-3,-8.48988629E-3,-8.48988629E-3]
	vbroadcastss	LCPI1_7(%rip), %ymm7    ## ymm7 = [-1.66679844E-1,-1.66679844E-1,-1.66679844E-1,-1.66679844E-1,-1.66679844E-1,-1.66679844E-1,-1.66679844E-1,-1.66679844E-1]
	vbroadcastss	LCPI1_8(%rip), %ymm8    ## ymm8 = [-1.0E+0,-1.0E+0,-1.0E+0,-1.0E+0,-1.0E+0,-1.0E+0,-1.0E+0,-1.0E+0]
	vbroadcastss	LCPI1_9(%rip), %ymm9    ## ymm9 = [1.0E+0,1.0E+0,1.0E+0,1.0E+0,1.0E+0,1.0E+0,1.0E+0,1.0E+0]
	vbroadcastss	LCPI1_10(%rip), %ymm2   ## ymm2 = [1.19156833E-3,1.19156833E-3,1.19156833E-3,1.19156833E-3,1.19156833E-3,1.19156833E-3,1.19156833E-3,1.19156833E-3]
	vbroadcastss	LCPI1_11(%rip), %ymm11  ## ymm11 = [4.16018814E-2,4.16018814E-2,4.16018814E-2,4.16018814E-2,4.16018814E-2,4.16018814E-2,4.16018814E-2,4.16018814E-2]
	vbroadcastss	LCPI1_12(%rip), %ymm12  ## ymm12 = [4.99998987E-1,4.99998987E-1,4.99998987E-1,4.99998987E-1,4.99998987E-1,4.99998987E-1,4.99998987E-1,4.99998987E-1]
	vpbroadcastd	LCPI1_9(%rip), %ymm13   ## ymm13 = [1.0E+0,1.0E+0,1.0E+0,1.0E+0,1.0E+0,1.0E+0,1.0E+0,1.0E+0]
	.p2align	4, 0x90
LBB1_6:                                 ## %"for squashed_head1_filter.s0.s.si1"
                                        ## =>This Inner Loop Header: Depth=1
	vmovups	(%rdx), %ymm14
	vmulps	-48(%rsp), %ymm14, %ymm15       ## 32-byte Folded Reload
	vroundps	$1, %ymm15, %ymm15
	vfmadd231ps	-80(%rsp), %ymm15, %ymm14 ## 32-byte Folded Reload
                                        ## ymm14 = (ymm15 * mem) + ymm14
	vfmadd231ps	-112(%rsp), %ymm15, %ymm14 ## 32-byte Folded Reload
                                        ## ymm14 = (ymm15 * mem) + ymm14
	vmulps	%ymm14, %ymm14, %ymm0
	vmovaps	%ymm1, %ymm5
	vfmadd213ps	%ymm6, %ymm0, %ymm5     ## ymm5 = (ymm0 * ymm5) + ymm6
	vfmadd213ps	%ymm7, %ymm0, %ymm5     ## ymm5 = (ymm0 * ymm5) + ymm7
	vfmadd213ps	%ymm8, %ymm0, %ymm5     ## ymm5 = (ymm0 * ymm5) + ymm8
	vmovaps	%ymm2, %ymm10
	vfmadd213ps	%ymm11, %ymm0, %ymm10   ## ymm10 = (ymm0 * ymm10) + ymm11
	vfmadd213ps	%ymm12, %ymm0, %ymm10   ## ymm10 = (ymm0 * ymm10) + ymm12
	vfmadd213ps	%ymm9, %ymm0, %ymm10    ## ymm10 = (ymm0 * ymm10) + ymm9
	vfmadd231ps	%ymm5, %ymm14, %ymm10   ## ymm10 = (ymm14 * ymm5) + ymm10
	vcvttps2dq	%ymm15, %ymm0
	vpslld	$23, %ymm0, %ymm5
	vpaddd	%ymm5, %ymm13, %ymm5
	vfmadd213ps	%ymm9, %ymm10, %ymm5    ## ymm5 = (ymm10 * ymm5) + ymm9
	vdivps	%ymm5, %ymm9, %ymm5
	vpcmpgtd	%ymm0, %ymm4, %ymm10
	vpand	%ymm5, %ymm10, %ymm5
	vpcmpgtd	%ymm3, %ymm0, %ymm0
	vblendvps	%ymm0, %ymm5, %ymm9, %ymm0
	vmovaps	%ymm0, (%rcx,%rsi)
	addq	$32, %rsi
	addq	%rax, %rdx
	cmpq	$320, %rsi                      ## imm = 0x140
	jne	LBB1_6
LBB1_7:                                 ## %destructor_block
	xorl	%eax, %eax
	popq	%rbx
	vzeroupper
	retq
                                        ## -- End function
	.p2align	4, 0x90                         ## -- Begin function train_cost_model.par_for.head1_conv.s0.w
_train_cost_model.par_for.head1_conv.s0.w: ## @train_cost_model.par_for.head1_conv.s0.w
## %bb.0:                               ## %entry
	movq	(%rdx), %rax
	movq	16(%rdx), %rcx
	vmovups	(%rax), %ymm0
	movslq	%esi, %rax
	shlq	$5, %rax
	vmovaps	%ymm0, (%rcx,%rax)
	xorl	%eax, %eax
	vzeroupper
	retq
                                        ## -- End function
	.p2align	4, 0x90                         ## -- Begin function train_cost_model.par_for.head1_conv.s1.w
_train_cost_model.par_for.head1_conv.s1.w: ## @train_cost_model.par_for.head1_conv.s1.w
## %bb.0:                               ## %entry
                                        ## kill: def $esi killed $esi def $rsi
	movslq	(%rdx), %r11
	movq	16(%rdx), %r8
	movq	32(%rdx), %rdi
	leal	(,%rsi,8), %eax
	imull	4(%rdx), %esi
	movq	48(%rdx), %rcx
	subl	8(%rdx), %esi
	movslq	%eax, %r9
	vmovaps	(%r8,%r9,4), %ymm0
	movslq	%esi, %rsi
	leaq	(%rdi,%rsi,4), %rdx
	movl	$16, %eax
	.p2align	4, 0x90
LBB3_1:                                 ## %"for head1_conv.s1.r31$x"
                                        ## =>This Inner Loop Header: Depth=1
	vbroadcastss	-16(%rdx,%rax), %ymm1
	vfmadd132ps	-128(%rcx,%rax,8), %ymm0, %ymm1 ## ymm1 = (ymm1 * mem) + ymm0
	vbroadcastss	-12(%rdx,%rax), %ymm0
	vfmadd132ps	-96(%rcx,%rax,8), %ymm1, %ymm0 ## ymm0 = (ymm0 * mem) + ymm1
	vbroadcastss	-8(%rdx,%rax), %ymm1
	vfmadd132ps	-64(%rcx,%rax,8), %ymm0, %ymm1 ## ymm1 = (ymm1 * mem) + ymm0
	vbroadcastss	-4(%rdx,%rax), %ymm2
	vfmadd132ps	-32(%rcx,%rax,8), %ymm1, %ymm2 ## ymm2 = (ymm2 * mem) + ymm1
	vbroadcastss	(%rdx,%rax), %ymm0
	vfmadd132ps	(%rcx,%rax,8), %ymm2, %ymm0 ## ymm0 = (ymm0 * mem) + ymm2
	addq	$20, %rax
	cmpq	$176, %rax
	jne	LBB3_1
## %bb.2:                               ## %"end for head1_conv.s1.r31$x"
	leaq	(%r11,%rsi), %rax
	leaq	(%rdi,%rax,4), %rdx
	movl	$16, %eax
	.p2align	4, 0x90
LBB3_3:                                 ## %"for head1_conv.s1.r31$x.1"
                                        ## =>This Inner Loop Header: Depth=1
	vbroadcastss	-16(%rdx,%rax), %ymm1
	vfmadd132ps	1152(%rcx,%rax,8), %ymm0, %ymm1 ## ymm1 = (ymm1 * mem) + ymm0
	vbroadcastss	-12(%rdx,%rax), %ymm0
	vfmadd132ps	1184(%rcx,%rax,8), %ymm1, %ymm0 ## ymm0 = (ymm0 * mem) + ymm1
	vbroadcastss	-8(%rdx,%rax), %ymm1
	vfmadd132ps	1216(%rcx,%rax,8), %ymm0, %ymm1 ## ymm1 = (ymm1 * mem) + ymm0
	vbroadcastss	-4(%rdx,%rax), %ymm2
	vfmadd132ps	1248(%rcx,%rax,8), %ymm1, %ymm2 ## ymm2 = (ymm2 * mem) + ymm1
	vbroadcastss	(%rdx,%rax), %ymm0
	vfmadd132ps	1280(%rcx,%rax,8), %ymm2, %ymm0 ## ymm0 = (ymm0 * mem) + ymm2
	addq	$20, %rax
	cmpq	$176, %rax
	jne	LBB3_3
## %bb.4:                               ## %"end for head1_conv.s1.r31$x.1"
	leaq	(%rsi,%r11,2), %rax
	leaq	(%rdi,%rax,4), %rdx
	addq	$16, %rdx
	xorl	%eax, %eax
	.p2align	4, 0x90
LBB3_5:                                 ## %"for head1_conv.s1.r31$x.2"
                                        ## =>This Inner Loop Header: Depth=1
	vbroadcastss	-16(%rdx,%rax), %ymm1
	vfmadd132ps	2560(%rcx,%rax,8), %ymm0, %ymm1 ## ymm1 = (ymm1 * mem) + ymm0
	vbroadcastss	-12(%rdx,%rax), %ymm0
	vfmadd132ps	2592(%rcx,%rax,8), %ymm1, %ymm0 ## ymm0 = (ymm0 * mem) + ymm1
	vbroadcastss	-8(%rdx,%rax), %ymm1
	vfmadd132ps	2624(%rcx,%rax,8), %ymm0, %ymm1 ## ymm1 = (ymm1 * mem) + ymm0
	vbroadcastss	-4(%rdx,%rax), %ymm2
	vfmadd132ps	2656(%rcx,%rax,8), %ymm1, %ymm2 ## ymm2 = (ymm2 * mem) + ymm1
	vbroadcastss	(%rdx,%rax), %ymm0
	vfmadd132ps	2688(%rcx,%rax,8), %ymm2, %ymm0 ## ymm0 = (ymm0 * mem) + ymm2
	addq	$20, %rax
	cmpq	$160, %rax
	jne	LBB3_5
## %bb.6:                               ## %"end for head1_conv.s1.r31$x.2"
	leaq	(%r11,%r11,2), %r10
	leaq	(%r10,%rsi), %rax
	leaq	(%rdi,%rax,4), %rdx
	addq	$16, %rdx
	xorl	%eax, %eax
	.p2align	4, 0x90
LBB3_7:                                 ## %"for head1_conv.s1.r31$x.3"
                                        ## =>This Inner Loop Header: Depth=1
	vbroadcastss	-16(%rdx,%rax), %ymm1
	vfmadd132ps	3840(%rcx,%rax,8), %ymm0, %ymm1 ## ymm1 = (ymm1 * mem) + ymm0
	vbroadcastss	-12(%rdx,%rax), %ymm0
	vfmadd132ps	3872(%rcx,%rax,8), %ymm1, %ymm0 ## ymm0 = (ymm0 * mem) + ymm1
	vbroadcastss	-8(%rdx,%rax), %ymm1
	vfmadd132ps	3904(%rcx,%rax,8), %ymm0, %ymm1 ## ymm1 = (ymm1 * mem) + ymm0
	vbroadcastss	-4(%rdx,%rax), %ymm2
	vfmadd132ps	3936(%rcx,%rax,8), %ymm1, %ymm2 ## ymm2 = (ymm2 * mem) + ymm1
	vbroadcastss	(%rdx,%rax), %ymm0
	vfmadd132ps	3968(%rcx,%rax,8), %ymm2, %ymm0 ## ymm0 = (ymm0 * mem) + ymm2
	addq	$20, %rax
	cmpq	$160, %rax
	jne	LBB3_7
## %bb.8:                               ## %"end for head1_conv.s1.r31$x.3"
	leaq	(%rsi,%r11,4), %rax
	leaq	(%rdi,%rax,4), %rdx
	addq	$16, %rdx
	xorl	%eax, %eax
	.p2align	4, 0x90
LBB3_9:                                 ## %"for head1_conv.s1.r31$x.4"
                                        ## =>This Inner Loop Header: Depth=1
	vbroadcastss	-16(%rdx,%rax), %ymm1
	vfmadd132ps	5120(%rcx,%rax,8), %ymm0, %ymm1 ## ymm1 = (ymm1 * mem) + ymm0
	vbroadcastss	-12(%rdx,%rax), %ymm0
	vfmadd132ps	5152(%rcx,%rax,8), %ymm1, %ymm0 ## ymm0 = (ymm0 * mem) + ymm1
	vbroadcastss	-8(%rdx,%rax), %ymm1
	vfmadd132ps	5184(%rcx,%rax,8), %ymm0, %ymm1 ## ymm1 = (ymm1 * mem) + ymm0
	vbroadcastss	-4(%rdx,%rax), %ymm2
	vfmadd132ps	5216(%rcx,%rax,8), %ymm1, %ymm2 ## ymm2 = (ymm2 * mem) + ymm1
	vbroadcastss	(%rdx,%rax), %ymm0
	vfmadd132ps	5248(%rcx,%rax,8), %ymm2, %ymm0 ## ymm0 = (ymm0 * mem) + ymm2
	addq	$20, %rax
	cmpq	$160, %rax
	jne	LBB3_9
## %bb.10:                              ## %"end for head1_conv.s1.r31$x.4"
	leaq	(%r11,%r11,4), %rax
	addq	%rsi, %rax
	leaq	(%rdi,%rax,4), %rax
	addq	$16, %rax
	xorl	%edx, %edx
	.p2align	4, 0x90
LBB3_11:                                ## %"for head1_conv.s1.r31$x.5"
                                        ## =>This Inner Loop Header: Depth=1
	vbroadcastss	-16(%rax,%rdx), %ymm1
	vfmadd132ps	6400(%rcx,%rdx,8), %ymm0, %ymm1 ## ymm1 = (ymm1 * mem) + ymm0
	vbroadcastss	-12(%rax,%rdx), %ymm0
	vfmadd132ps	6432(%rcx,%rdx,8), %ymm1, %ymm0 ## ymm0 = (ymm0 * mem) + ymm1
	vbroadcastss	-8(%rax,%rdx), %ymm1
	vfmadd132ps	6464(%rcx,%rdx,8), %ymm0, %ymm1 ## ymm1 = (ymm1 * mem) + ymm0
	vbroadcastss	-4(%rax,%rdx), %ymm2
	vfmadd132ps	6496(%rcx,%rdx,8), %ymm1, %ymm2 ## ymm2 = (ymm2 * mem) + ymm1
	vbroadcastss	(%rax,%rdx), %ymm0
	vfmadd132ps	6528(%rcx,%rdx,8), %ymm2, %ymm0 ## ymm0 = (ymm0 * mem) + ymm2
	addq	$20, %rdx
	cmpq	$160, %rdx
	jne	LBB3_11
## %bb.12:                              ## %"end for head1_conv.s1.r31$x.5"
	leaq	(%rsi,%r10,2), %rax
	leaq	(%rdi,%rax,4), %rax
	addq	$16, %rax
	xorl	%edx, %edx
	.p2align	4, 0x90
LBB3_13:                                ## %"for head1_conv.s1.r31$x.6"
                                        ## =>This Inner Loop Header: Depth=1
	vbroadcastss	-16(%rax,%rdx), %ymm1
	vfmadd132ps	7680(%rcx,%rdx,8), %ymm0, %ymm1 ## ymm1 = (ymm1 * mem) + ymm0
	vbroadcastss	-12(%rax,%rdx), %ymm0
	vfmadd132ps	7712(%rcx,%rdx,8), %ymm1, %ymm0 ## ymm0 = (ymm0 * mem) + ymm1
	vbroadcastss	-8(%rax,%rdx), %ymm1
	vfmadd132ps	7744(%rcx,%rdx,8), %ymm0, %ymm1 ## ymm1 = (ymm1 * mem) + ymm0
	vbroadcastss	-4(%rax,%rdx), %ymm2
	vfmadd132ps	7776(%rcx,%rdx,8), %ymm1, %ymm2 ## ymm2 = (ymm2 * mem) + ymm1
	vbroadcastss	(%rax,%rdx), %ymm0
	vfmadd132ps	7808(%rcx,%rdx,8), %ymm2, %ymm0 ## ymm0 = (ymm0 * mem) + ymm2
	addq	$20, %rdx
	cmpq	$160, %rdx
	jne	LBB3_13
## %bb.14:                              ## %"end for head1_conv.s1.r31$x.6"
	vmovaps	%ymm0, (%r8,%r9,4)
	xorl	%eax, %eax
	vzeroupper
	retq
                                        ## -- End function
	.section	__TEXT,__literal4,4byte_literals
	.p2align	2                               ## -- Begin function train_cost_model.par_for.normalized_schedule_features.s0.c.c.c
LCPI4_0:
	.long	0x3f800000                      ## float 1
LCPI4_1:
	.long	2155872255                      ## 0x807fffff
LCPI4_2:
	.long	0xbf800000                      ## float -1
LCPI4_3:
	.long	4294967169                      ## 0xffffff81
LCPI4_4:
	.long	0x3f317218                      ## float 0.693147182
LCPI4_5:
	.long	0x3d9c7946                      ## float 0.0764031857
LCPI4_6:
	.long	0x3e5333c6                      ## float 0.206252187
LCPI4_7:
	.long	0x3eaa99cd                      ## float 0.333204657
LCPI4_8:
	.long	0xbe266e2a                      ## float -0.162529618
LCPI4_9:
	.long	0xbe809085                      ## float -0.251102597
LCPI4_10:
	.long	0xbefffcbe                      ## float -0.499975145
	.section	__TEXT,__text,regular,pure_instructions
	.p2align	4, 0x90
_train_cost_model.par_for.normalized_schedule_features.s0.c.c.c: ## @train_cost_model.par_for.normalized_schedule_features.s0.c.c.c
## %bb.0:                               ## %entry
	pushq	%rbp
	pushq	%r15
	pushq	%r14
	pushq	%r13
	pushq	%r12
	pushq	%rbx
	subq	$360, %rsp                      ## imm = 0x168
	movl	%esi, %ebp
	movl	(%rdx), %edi
	movl	4(%rdx), %ecx
	movl	8(%rdx), %r15d
	movl	20(%rdx), %r14d
	movl	24(%rdx), %ebx
	movslq	32(%rdx), %rax
	movl	36(%rdx), %r9d
	movq	40(%rdx), %rsi
	movq	%rsi, -72(%rsp)                 ## 8-byte Spill
	movq	56(%rdx), %rsi
	movq	%rsi, -80(%rsp)                 ## 8-byte Spill
	cmpl	%ebp, 12(%rdx)
	movl	%ecx, -96(%rsp)                 ## 4-byte Spill
	movl	%r15d, -100(%rsp)               ## 4-byte Spill
	movl	%r9d, -104(%rsp)                ## 4-byte Spill
	movl	%edi, -92(%rsp)                 ## 4-byte Spill
	jle	LBB4_11
## %bb.1:                               ## %then_bb
	movl	16(%rdx), %r10d
	movq	%rbp, %r12
	movl	%r12d, %r11d
	andl	$7, %r11d
	leal	(%r11,%r11,4), %r8d
	movl	%r10d, %edx
	sarl	$3, %edx
	sarl	$3, %r12d
	addl	$7, %ebx
	sarl	$3, %ebx
	movq	%rdx, %rsi
	movq	%rdx, 64(%rsp)                  ## 8-byte Spill
	subl	%edx, %ebx
	cmpl	$34, %r8d
	movl	$34, %esi
	cmoval	%r8d, %esi
	movl	$39, %edx
	subl	%esi, %edx
	movq	%rdx, -64(%rsp)                 ## 8-byte Spill
	subl	%eax, %r9d
	movl	%r10d, -56(%rsp)                ## 4-byte Spill
	andl	$-8, %r10d
	subl	%r10d, %r9d
	movl	%r9d, 32(%rsp)                  ## 4-byte Spill
	imull	$273, %r12d, %edx               ## imm = 0x111
	addl	%r8d, %edx
	imull	%edi, %edx
	movl	%edx, %r9d
	subl	%eax, %r9d
	imull	%r15d, %r12d
	leal	(,%r12,8), %ebp
	subl	%r12d, %ebp
	imull	%ecx, %r11d
	leal	(%r11,%r11,4), %r8d
	leal	(%r8,%rbp), %ecx
	addl	%eax, %ebp
	addl	%r8d, %ebp
	imull	$39, %edi, %esi
	movl	%esi, -28(%rsp)                 ## 4-byte Spill
	subl	%r14d, %ecx
	addl	%r10d, %edx
	addl	%r10d, %ebp
	subl	%r14d, %ebp
	xorl	%esi, %esi
	vbroadcastss	LCPI4_0(%rip), %ymm0    ## ymm0 = [1.0E+0,1.0E+0,1.0E+0,1.0E+0,1.0E+0,1.0E+0,1.0E+0,1.0E+0]
	vbroadcastss	LCPI4_1(%rip), %ymm7    ## ymm7 = [2155872255,2155872255,2155872255,2155872255,2155872255,2155872255,2155872255,2155872255]
	vpbroadcastd	LCPI4_0(%rip), %ymm3    ## ymm3 = [1.0E+0,1.0E+0,1.0E+0,1.0E+0,1.0E+0,1.0E+0,1.0E+0,1.0E+0]
	vbroadcastss	LCPI4_2(%rip), %ymm4    ## ymm4 = [-1.0E+0,-1.0E+0,-1.0E+0,-1.0E+0,-1.0E+0,-1.0E+0,-1.0E+0,-1.0E+0]
	vpbroadcastd	LCPI4_3(%rip), %ymm5    ## ymm5 = [4294967169,4294967169,4294967169,4294967169,4294967169,4294967169,4294967169,4294967169]
	vbroadcastss	LCPI4_4(%rip), %ymm12   ## ymm12 = [6.93147182E-1,6.93147182E-1,6.93147182E-1,6.93147182E-1,6.93147182E-1,6.93147182E-1,6.93147182E-1,6.93147182E-1]
	vbroadcastss	LCPI4_5(%rip), %ymm6    ## ymm6 = [7.64031857E-2,7.64031857E-2,7.64031857E-2,7.64031857E-2,7.64031857E-2,7.64031857E-2,7.64031857E-2,7.64031857E-2]
	vbroadcastss	LCPI4_6(%rip), %ymm2    ## ymm2 = [2.06252187E-1,2.06252187E-1,2.06252187E-1,2.06252187E-1,2.06252187E-1,2.06252187E-1,2.06252187E-1,2.06252187E-1]
	vbroadcastss	LCPI4_7(%rip), %ymm8    ## ymm8 = [3.33204657E-1,3.33204657E-1,3.33204657E-1,3.33204657E-1,3.33204657E-1,3.33204657E-1,3.33204657E-1,3.33204657E-1]
	vbroadcastss	LCPI4_8(%rip), %ymm9    ## ymm9 = [-1.62529618E-1,-1.62529618E-1,-1.62529618E-1,-1.62529618E-1,-1.62529618E-1,-1.62529618E-1,-1.62529618E-1,-1.62529618E-1]
	vbroadcastss	LCPI4_9(%rip), %ymm10   ## ymm10 = [-2.51102597E-1,-2.51102597E-1,-2.51102597E-1,-2.51102597E-1,-2.51102597E-1,-2.51102597E-1,-2.51102597E-1,-2.51102597E-1]
	vbroadcastss	LCPI4_10(%rip), %ymm11  ## ymm11 = [-4.99975145E-1,-4.99975145E-1,-4.99975145E-1,-4.99975145E-1,-4.99975145E-1,-4.99975145E-1,-4.99975145E-1,-4.99975145E-1]
	vmovss	LCPI4_0(%rip), %xmm1            ## xmm1 = mem[0],zero,zero,zero
	vmovups	%ymm7, 320(%rsp)                ## 32-byte Spill
	vmovdqu	%ymm3, 288(%rsp)                ## 32-byte Spill
	vmovups	%ymm4, 256(%rsp)                ## 32-byte Spill
	vmovdqu	%ymm5, 224(%rsp)                ## 32-byte Spill
	vmovups	%ymm0, 192(%rsp)                ## 32-byte Spill
	vmovups	%ymm6, 160(%rsp)                ## 32-byte Spill
	vmovups	%ymm8, 128(%rsp)                ## 32-byte Spill
	vmovups	%ymm2, 96(%rsp)                 ## 32-byte Spill
	vmovups	%ymm11, (%rsp)                  ## 32-byte Spill
	jmp	LBB4_2
	.p2align	4, 0x90
LBB4_34:                                ## %"end for normalized_schedule_features.s0.c.ci"
                                        ##   in Loop: Header=BB4_2 Depth=1
	movq	-16(%rsp), %rsi                 ## 8-byte Reload
	incl	%esi
	movl	-20(%rsp), %r9d                 ## 4-byte Reload
	movl	-28(%rsp), %ebp                 ## 4-byte Reload
	addl	%ebp, %r9d
	movl	-100(%rsp), %r8d                ## 4-byte Reload
	movl	-24(%rsp), %ecx                 ## 4-byte Reload
	addl	%r8d, %ecx
	movl	-112(%rsp), %edx                ## 4-byte Reload
	addl	%ebp, %edx
	movq	-8(%rsp), %rbp                  ## 8-byte Reload
	addl	%r8d, %ebp
	cmpl	$7, %esi
	je	LBB4_35
LBB4_2:                                 ## %"for normalized_schedule_features.s0.s.si"
                                        ## =>This Loop Header: Depth=1
                                        ##     Child Loop BB4_3 Depth 2
                                        ##       Child Loop BB4_5 Depth 3
                                        ##       Child Loop BB4_8 Depth 3
                                        ##         Child Loop BB4_29 Depth 4
                                        ##         Child Loop BB4_31 Depth 4
	movq	%rsi, -16(%rsp)                 ## 8-byte Spill
	movq	%rbp, -8(%rsp)                  ## 8-byte Spill
	movl	%ebp, %esi
	movl	%ebp, -88(%rsp)                 ## 4-byte Spill
	movl	%edx, -112(%rsp)                ## 4-byte Spill
	movl	%edx, -116(%rsp)                ## 4-byte Spill
	movl	%ecx, -24(%rsp)                 ## 4-byte Spill
	movl	%ecx, %r8d
	movl	%r9d, -20(%rsp)                 ## 4-byte Spill
	xorl	%edx, %edx
	jmp	LBB4_3
	.p2align	4, 0x90
LBB4_33:                                ## %"end for normalized_schedule_features.s0.n.n.rebased"
                                        ##   in Loop: Header=BB4_3 Depth=2
	movq	-48(%rsp), %rdx                 ## 8-byte Reload
	incq	%rdx
	movl	-92(%rsp), %edi                 ## 4-byte Reload
	movl	-40(%rsp), %r9d                 ## 4-byte Reload
	addl	%edi, %r9d
	movl	-96(%rsp), %ecx                 ## 4-byte Reload
	movl	-108(%rsp), %r8d                ## 4-byte Reload
	addl	%ecx, %r8d
	addl	%edi, -116(%rsp)                ## 4-byte Folded Spill
	addl	%ecx, -88(%rsp)                 ## 4-byte Folded Spill
	cmpq	-64(%rsp), %rdx                 ## 8-byte Folded Reload
	vmovups	320(%rsp), %ymm7                ## 32-byte Reload
	vmovdqu	288(%rsp), %ymm3                ## 32-byte Reload
	vmovups	256(%rsp), %ymm4                ## 32-byte Reload
	vmovdqu	224(%rsp), %ymm5                ## 32-byte Reload
	vmovups	192(%rsp), %ymm0                ## 32-byte Reload
	vmovups	160(%rsp), %ymm6                ## 32-byte Reload
	vmovups	128(%rsp), %ymm8                ## 32-byte Reload
	vmovups	96(%rsp), %ymm2                 ## 32-byte Reload
	je	LBB4_34
LBB4_3:                                 ## %"for normalized_schedule_features.s0.c.ci"
                                        ##   Parent Loop BB4_2 Depth=1
                                        ## =>  This Loop Header: Depth=2
                                        ##       Child Loop BB4_5 Depth 3
                                        ##       Child Loop BB4_8 Depth 3
                                        ##         Child Loop BB4_29 Depth 4
                                        ##         Child Loop BB4_31 Depth 4
	movq	%rdx, -48(%rsp)                 ## 8-byte Spill
	cmpl	$8, -56(%rsp)                   ## 4-byte Folded Reload
	vmovaps	%ymm9, %ymm11
	jl	LBB4_6
## %bb.4:                               ## %"for normalized_schedule_features.s0.n.n.preheader"
                                        ##   in Loop: Header=BB4_3 Depth=2
	vmovaps	%ymm7, %ymm9
	movslq	%r9d, %rcx
	addq	%rax, %rcx
	movslq	%r8d, %rsi
	addq	%rax, %rsi
	movq	-72(%rsp), %rdx                 ## 8-byte Reload
	leaq	(%rdx,%rcx,4), %rdx
	movq	-80(%rsp), %rcx                 ## 8-byte Reload
	leaq	(%rcx,%rsi,4), %rsi
	xorl	%edi, %edi
	movq	64(%rsp), %rbp                  ## 8-byte Reload
	.p2align	4, 0x90
LBB4_5:                                 ## %"for normalized_schedule_features.s0.n.n"
                                        ##   Parent Loop BB4_2 Depth=1
                                        ##     Parent Loop BB4_3 Depth=2
                                        ## =>    This Inner Loop Header: Depth=3
	vmovaps	%ymm12, %ymm7
	vaddps	(%rsi,%rdi), %ymm0, %ymm14
	vandps	%ymm9, %ymm14, %ymm13
	vpsrad	$22, %ymm13, %ymm15
	vpslld	$23, %ymm15, %ymm12
	vpsubd	%ymm12, %ymm13, %ymm12
	vpaddd	%ymm3, %ymm12, %ymm12
	vaddps	%ymm4, %ymm12, %ymm12
	vmulps	%ymm12, %ymm12, %ymm13
	vpsrad	$23, %ymm14, %ymm14
	vpaddd	%ymm5, %ymm14, %ymm14
	vpaddd	%ymm14, %ymm15, %ymm14
	vcvtdq2ps	%ymm14, %ymm14
	vmovaps	%ymm6, %ymm15
	vfmadd213ps	%ymm2, %ymm13, %ymm15   ## ymm15 = (ymm13 * ymm15) + ymm2
	vfmadd213ps	%ymm8, %ymm13, %ymm15   ## ymm15 = (ymm13 * ymm15) + ymm8
	vfmadd213ps	%ymm0, %ymm13, %ymm15   ## ymm15 = (ymm13 * ymm15) + ymm0
	vmulps	%ymm15, %ymm12, %ymm12
	vmovaps	%ymm11, %ymm15
	vfmadd213ps	%ymm10, %ymm13, %ymm15  ## ymm15 = (ymm13 * ymm15) + ymm10
	vfmadd213ps	(%rsp), %ymm13, %ymm15  ## 32-byte Folded Reload
                                        ## ymm15 = (ymm13 * ymm15) + mem
	vfmadd213ps	%ymm12, %ymm13, %ymm15  ## ymm15 = (ymm13 * ymm15) + ymm12
	vfmadd231ps	%ymm14, %ymm7, %ymm15   ## ymm15 = (ymm7 * ymm14) + ymm15
	vmovaps	%ymm7, %ymm12
	vmovups	%ymm15, (%rdx,%rdi)
	addq	$32, %rdi
	decq	%rbp
	jne	LBB4_5
LBB4_6:                                 ## %"end for normalized_schedule_features.s0.n.n"
                                        ##   in Loop: Header=BB4_3 Depth=2
	movl	%r9d, -40(%rsp)                 ## 4-byte Spill
	movl	%r8d, -108(%rsp)                ## 4-byte Spill
	testl	%ebx, %ebx
	vmovss	LCPI4_2(%rip), %xmm7            ## xmm7 = mem[0],zero,zero,zero
	vmovss	LCPI4_5(%rip), %xmm14           ## xmm14 = mem[0],zero,zero,zero
	vmovss	LCPI4_6(%rip), %xmm0            ## xmm0 = mem[0],zero,zero,zero
	vmovss	LCPI4_7(%rip), %xmm6            ## xmm6 = mem[0],zero,zero,zero
	vmovss	LCPI4_8(%rip), %xmm15           ## xmm15 = mem[0],zero,zero,zero
	vmovaps	%ymm11, %ymm9
	vmovups	(%rsp), %ymm11                  ## 32-byte Reload
	jle	LBB4_33
## %bb.7:                               ## %"for normalized_schedule_features.s0.n.n.rebased.preheader"
                                        ##   in Loop: Header=BB4_3 Depth=2
	movl	32(%rsp), %esi                  ## 4-byte Reload
	movl	%esi, %edx
	movl	-88(%rsp), %ebp                 ## 4-byte Reload
	movl	-116(%rsp), %r8d                ## 4-byte Reload
	xorl	%r12d, %r12d
	jmp	LBB4_8
	.p2align	4, 0x90
LBB4_32:                                ## %"end for normalized_schedule_features.s0.n.ni"
                                        ##   in Loop: Header=BB4_8 Depth=3
	incq	%r12
	addl	$-8, %esi
	addl	$8, %r8d
	addl	$8, %ebp
	addl	$-8, %edx
	movq	%r9, %rbx
	cmpq	%r9, %r12
	je	LBB4_33
LBB4_8:                                 ## %"for normalized_schedule_features.s0.n.n.rebased"
                                        ##   Parent Loop BB4_2 Depth=1
                                        ##     Parent Loop BB4_3 Depth=2
                                        ## =>    This Loop Header: Depth=3
                                        ##         Child Loop BB4_29 Depth 4
                                        ##         Child Loop BB4_31 Depth 4
	movq	%rbx, %r9
	cmpl	$8, %edx
	movl	$8, %r14d
	cmovll	%edx, %r14d
	cmpl	$8, %esi
	movl	$8, %r11d
	cmovll	%esi, %r11d
	leal	(,%r12,8), %ecx
	movl	32(%rsp), %edi                  ## 4-byte Reload
                                        ## kill: def $edi killed $edi def $rdi
	subl	%ecx, %edi
	cmpl	$8, %edi
	movl	$8, %ecx
	cmovgel	%ecx, %edi
	movq	64(%rsp), %rcx                  ## 8-byte Reload
	addl	%r12d, %ecx
	leal	(%rax,%rcx,8), %ecx
	cmpl	%ecx, -104(%rsp)                ## 4-byte Folded Reload
	jle	LBB4_32
## %bb.9:                               ## %"for normalized_schedule_features.s0.n.ni.preheader"
                                        ##   in Loop: Header=BB4_8 Depth=3
	movslq	%r8d, %rcx
	movq	-72(%rsp), %rbx                 ## 8-byte Reload
	leaq	(%rbx,%rcx,4), %r15
	movslq	%ebp, %rcx
	movq	-80(%rsp), %rbx                 ## 8-byte Reload
	leaq	(%rbx,%rcx,4), %r13
	cmpl	$8, %edi
	jae	LBB4_28
## %bb.10:                              ##   in Loop: Header=BB4_8 Depth=3
	xorl	%r10d, %r10d
	vmovss	LCPI4_9(%rip), %xmm13           ## xmm13 = mem[0],zero,zero,zero
	vmovss	LCPI4_10(%rip), %xmm8           ## xmm8 = mem[0],zero,zero,zero
	jmp	LBB4_31
	.p2align	4, 0x90
LBB4_28:                                ## %vector.ph63
                                        ##   in Loop: Header=BB4_8 Depth=3
	andl	$-8, %r11d
	movl	%edi, %r10d
	andl	$-8, %r10d
	xorl	%ebx, %ebx
	vmovups	320(%rsp), %ymm2                ## 32-byte Reload
	vmovdqu	288(%rsp), %ymm3                ## 32-byte Reload
	vmovups	256(%rsp), %ymm4                ## 32-byte Reload
	vmovdqu	224(%rsp), %ymm5                ## 32-byte Reload
	vmovaps	%ymm9, %ymm1
	vmovaps	%ymm11, %ymm9
	vmovaps	%ymm10, %ymm11
	vmovups	160(%rsp), %ymm10               ## 32-byte Reload
	vmovups	128(%rsp), %ymm6                ## 32-byte Reload
	vmovups	96(%rsp), %ymm8                 ## 32-byte Reload
	vmovaps	%ymm12, %ymm7
	vmovups	192(%rsp), %ymm0                ## 32-byte Reload
	.p2align	4, 0x90
LBB4_29:                                ## %vector.body60
                                        ##   Parent Loop BB4_2 Depth=1
                                        ##     Parent Loop BB4_3 Depth=2
                                        ##       Parent Loop BB4_8 Depth=3
                                        ## =>      This Inner Loop Header: Depth=4
	vaddps	(%r13,%rbx,4), %ymm0, %ymm12
	vandps	%ymm2, %ymm12, %ymm13
	vpsrad	$22, %ymm13, %ymm14
	vpslld	$23, %ymm14, %ymm15
	vpsubd	%ymm15, %ymm13, %ymm13
	vpaddd	%ymm3, %ymm13, %ymm13
	vaddps	%ymm4, %ymm13, %ymm13
	vmulps	%ymm13, %ymm13, %ymm15
	vpsrad	$23, %ymm12, %ymm12
	vpaddd	%ymm5, %ymm12, %ymm12
	vpaddd	%ymm12, %ymm14, %ymm12
	vcvtdq2ps	%ymm12, %ymm12
	vmovaps	%ymm10, %ymm14
	vfmadd213ps	%ymm8, %ymm15, %ymm14   ## ymm14 = (ymm15 * ymm14) + ymm8
	vfmadd213ps	%ymm6, %ymm15, %ymm14   ## ymm14 = (ymm15 * ymm14) + ymm6
	vfmadd213ps	%ymm0, %ymm15, %ymm14   ## ymm14 = (ymm15 * ymm14) + ymm0
	vmulps	%ymm14, %ymm13, %ymm13
	vmovaps	%ymm1, %ymm14
	vfmadd213ps	%ymm11, %ymm15, %ymm14  ## ymm14 = (ymm15 * ymm14) + ymm11
	vfmadd213ps	%ymm9, %ymm15, %ymm14   ## ymm14 = (ymm15 * ymm14) + ymm9
	vfmadd213ps	%ymm13, %ymm15, %ymm14  ## ymm14 = (ymm15 * ymm14) + ymm13
	vfmadd231ps	%ymm12, %ymm7, %ymm14   ## ymm14 = (ymm7 * ymm12) + ymm14
	vmovups	%ymm14, (%r15,%rbx,4)
	addq	$8, %rbx
	cmpq	%rbx, %r11
	jne	LBB4_29
## %bb.30:                              ## %middle.block58
                                        ##   in Loop: Header=BB4_8 Depth=3
	cmpq	%rdi, %r10
	vmovaps	%ymm7, %ymm12
	vmovaps	%ymm11, %ymm10
	vmovaps	%ymm9, %ymm11
	vmovaps	%ymm1, %ymm9
	vmovss	LCPI4_0(%rip), %xmm1            ## xmm1 = mem[0],zero,zero,zero
	vmovss	LCPI4_2(%rip), %xmm7            ## xmm7 = mem[0],zero,zero,zero
	vmovss	LCPI4_5(%rip), %xmm14           ## xmm14 = mem[0],zero,zero,zero
	vmovss	LCPI4_6(%rip), %xmm0            ## xmm0 = mem[0],zero,zero,zero
	vmovss	LCPI4_7(%rip), %xmm6            ## xmm6 = mem[0],zero,zero,zero
	vmovss	LCPI4_8(%rip), %xmm15           ## xmm15 = mem[0],zero,zero,zero
	vmovss	LCPI4_9(%rip), %xmm13           ## xmm13 = mem[0],zero,zero,zero
	vmovss	LCPI4_10(%rip), %xmm8           ## xmm8 = mem[0],zero,zero,zero
	je	LBB4_32
	.p2align	4, 0x90
LBB4_31:                                ## %"for normalized_schedule_features.s0.n.ni"
                                        ##   Parent Loop BB4_2 Depth=1
                                        ##     Parent Loop BB4_3 Depth=2
                                        ##       Parent Loop BB4_8 Depth=3
                                        ## =>      This Inner Loop Header: Depth=4
	vaddss	(%r13,%r10,4), %xmm1, %xmm2
	vmovd	%xmm2, %ecx
	movl	%ecx, %ebx
	andl	$-2139095041, %ebx              ## imm = 0x807FFFFF
	movl	%ebx, %edi
	sarl	$22, %edi
	sarl	$23, %ecx
	addl	%edi, %ecx
	addl	$-127, %ecx
                                        ## kill: def $edi killed $edi killed $rdi def $rdi
	shll	$23, %edi
	negl	%edi
	addl	%ebx, %edi
	addl	$1065353216, %edi               ## imm = 0x3F800000
	vmovd	%edi, %xmm2
	vaddss	%xmm7, %xmm2, %xmm2
	vmulss	%xmm2, %xmm2, %xmm3
	vxorps	%xmm4, %xmm4, %xmm4
	vcvtsi2ss	%ecx, %xmm4, %xmm4
	vmovaps	%xmm14, %xmm5
	vfmadd213ss	%xmm0, %xmm3, %xmm5     ## xmm5 = (xmm3 * xmm5) + xmm0
	vfmadd213ss	%xmm6, %xmm3, %xmm5     ## xmm5 = (xmm3 * xmm5) + xmm6
	vfmadd213ss	%xmm1, %xmm3, %xmm5     ## xmm5 = (xmm3 * xmm5) + xmm1
	vmulss	%xmm5, %xmm2, %xmm2
	vmovaps	%xmm15, %xmm5
	vfmadd213ss	%xmm13, %xmm3, %xmm5    ## xmm5 = (xmm3 * xmm5) + xmm13
	vfmadd213ss	%xmm8, %xmm3, %xmm5     ## xmm5 = (xmm3 * xmm5) + xmm8
	vfmadd213ss	%xmm2, %xmm3, %xmm5     ## xmm5 = (xmm3 * xmm5) + xmm2
	vmovss	LCPI4_4(%rip), %xmm2            ## xmm2 = mem[0],zero,zero,zero
	vfmadd231ss	%xmm2, %xmm4, %xmm5     ## xmm5 = (xmm4 * xmm2) + xmm5
	vmovss	%xmm5, (%r15,%r10,4)
	incq	%r10
	cmpq	%r10, %r14
	jne	LBB4_31
	jmp	LBB4_32
LBB4_11:                                ## %next_bb
	movl	28(%rdx), %r9d
	movl	%ebp, %r8d
	sarl	$3, %r8d
	movl	%ebp, %esi
	andl	$-8, %esi
	subl	%r8d, %esi
	movq	%rsi, %rdx
	movq	%rsi, -56(%rsp)                 ## 8-byte Spill
	subl	%esi, %r9d
	cmpl	$7, %r9d
	movl	$7, %edx
	cmovll	%r9d, %edx
	movl	%edx, -64(%rsp)                 ## 4-byte Spill
	testl	%r9d, %r9d
	jle	LBB4_35
## %bb.12:                              ## %next_bb
	testl	%ebx, %ebx
	jle	LBB4_35
## %bb.13:                              ## %"for normalized_schedule_features.s0.s.si2.us.preheader"
	andl	$7, %ebp
	leal	(%rbp,%rbp,4), %r9d
	imull	$39, %edi, %r12d
	cmpl	$34, %r9d
	movl	$34, %r10d
	cmoval	%r9d, %r10d
	movl	$39, %edx
	subl	%r10d, %edx
	movq	%rdx, -88(%rsp)                 ## 8-byte Spill
	addl	$7, %ebx
	sarl	$3, %ebx
	movl	-104(%rsp), %r10d               ## 4-byte Reload
	subl	%eax, %r10d
	imull	$273, %r8d, %r13d               ## imm = 0x111
	movq	%r9, %rdx
	movq	%r9, 96(%rsp)                   ## 8-byte Spill
	addl	%r9d, %r13d
	imull	%edi, %r13d
	imull	%r15d, %r8d
	leal	(,%r8,8), %edx
	subl	%r8d, %edx
	addl	%eax, %edx
	imull	%ecx, %ebp
	leal	(%rbp,%rbp,4), %esi
	addl	%edx, %esi
	subl	%r14d, %esi
	xorl	%ebp, %ebp
	vmovss	LCPI4_0(%rip), %xmm10           ## xmm10 = mem[0],zero,zero,zero
	vmovss	LCPI4_5(%rip), %xmm7            ## xmm7 = mem[0],zero,zero,zero
	vmovss	LCPI4_6(%rip), %xmm5            ## xmm5 = mem[0],zero,zero,zero
	vmovss	LCPI4_7(%rip), %xmm0            ## xmm0 = mem[0],zero,zero,zero
	vmovss	LCPI4_8(%rip), %xmm15           ## xmm15 = mem[0],zero,zero,zero
	vmovss	LCPI4_9(%rip), %xmm1            ## xmm1 = mem[0],zero,zero,zero
	vmovss	LCPI4_10(%rip), %xmm8           ## xmm8 = mem[0],zero,zero,zero
	vmovss	LCPI4_4(%rip), %xmm12           ## xmm12 = mem[0],zero,zero,zero
	vbroadcastss	LCPI4_0(%rip), %ymm9    ## ymm9 = [1.0E+0,1.0E+0,1.0E+0,1.0E+0,1.0E+0,1.0E+0,1.0E+0,1.0E+0]
	vbroadcastss	LCPI4_1(%rip), %ymm3    ## ymm3 = [2155872255,2155872255,2155872255,2155872255,2155872255,2155872255,2155872255,2155872255]
	vmovups	%ymm3, (%rsp)                   ## 32-byte Spill
	vpbroadcastd	LCPI4_0(%rip), %ymm3    ## ymm3 = [1.0E+0,1.0E+0,1.0E+0,1.0E+0,1.0E+0,1.0E+0,1.0E+0,1.0E+0]
	vbroadcastss	LCPI4_2(%rip), %ymm13   ## ymm13 = [-1.0E+0,-1.0E+0,-1.0E+0,-1.0E+0,-1.0E+0,-1.0E+0,-1.0E+0,-1.0E+0]
	vpbroadcastd	LCPI4_3(%rip), %ymm14   ## ymm14 = [4294967169,4294967169,4294967169,4294967169,4294967169,4294967169,4294967169,4294967169]
	vbroadcastss	LCPI4_4(%rip), %ymm4    ## ymm4 = [6.93147182E-1,6.93147182E-1,6.93147182E-1,6.93147182E-1,6.93147182E-1,6.93147182E-1,6.93147182E-1,6.93147182E-1]
	movq	%rbx, %r11
	vmovdqu	%ymm3, 64(%rsp)                 ## 32-byte Spill
	vmovups	%ymm4, 32(%rsp)                 ## 32-byte Spill
	movl	%r14d, -48(%rsp)                ## 4-byte Spill
	movl	%r12d, -112(%rsp)               ## 4-byte Spill
	jmp	LBB4_14
	.p2align	4, 0x90
LBB4_21:                                ## %"end for normalized_schedule_features.s0.c.ci6.split.us.us"
                                        ##   in Loop: Header=BB4_14 Depth=1
	movq	-40(%rsp), %rbp                 ## 8-byte Reload
	incl	%ebp
	movl	-112(%rsp), %r12d               ## 4-byte Reload
	movl	-116(%rsp), %r13d               ## 4-byte Reload
	addl	%r12d, %r13d
	movl	-100(%rsp), %r15d               ## 4-byte Reload
	movl	-108(%rsp), %esi                ## 4-byte Reload
	addl	%r15d, %esi
	cmpl	-64(%rsp), %ebp                 ## 4-byte Folded Reload
	movl	-48(%rsp), %r14d                ## 4-byte Reload
	je	LBB4_35
LBB4_14:                                ## %"for normalized_schedule_features.s0.s.si2.us"
                                        ## =>This Loop Header: Depth=1
                                        ##     Child Loop BB4_15 Depth 2
                                        ##       Child Loop BB4_16 Depth 3
                                        ##         Child Loop BB4_26 Depth 4
                                        ##         Child Loop BB4_18 Depth 4
	movq	-56(%rsp), %rdx                 ## 8-byte Reload
	movq	%rbp, -40(%rsp)                 ## 8-byte Spill
	addl	%ebp, %edx
	imull	%edx, %r12d
	subl	%eax, %r12d
	movl	%r12d, 128(%rsp)                ## 4-byte Spill
	imull	%r15d, %edx
	subl	%r14d, %edx
	movl	%edx, 160(%rsp)                 ## 4-byte Spill
	movl	%esi, -108(%rsp)                ## 4-byte Spill
	movl	%r13d, -116(%rsp)               ## 4-byte Spill
	movl	%r13d, %r8d
	xorl	%ebp, %ebp
	jmp	LBB4_15
	.p2align	4, 0x90
LBB4_20:                                ## %"end for normalized_schedule_features.s0.n.n9.loopexit.us.us"
                                        ##   in Loop: Header=BB4_15 Depth=2
	movq	192(%rsp), %rbp                 ## 8-byte Reload
	incq	%rbp
	movl	-92(%rsp), %edi                 ## 4-byte Reload
	movl	224(%rsp), %r8d                 ## 4-byte Reload
	addl	%edi, %r8d
	movl	-96(%rsp), %ecx                 ## 4-byte Reload
	movl	256(%rsp), %esi                 ## 4-byte Reload
	addl	%ecx, %esi
	cmpq	-88(%rsp), %rbp                 ## 8-byte Folded Reload
	je	LBB4_21
LBB4_15:                                ## %"for normalized_schedule_features.s0.c.ci5.us.us"
                                        ##   Parent Loop BB4_14 Depth=1
                                        ## =>  This Loop Header: Depth=2
                                        ##       Child Loop BB4_16 Depth 3
                                        ##         Child Loop BB4_26 Depth 4
                                        ##         Child Loop BB4_18 Depth 4
	movq	96(%rsp), %rdx                  ## 8-byte Reload
	movq	%rbp, 192(%rsp)                 ## 8-byte Spill
	addl	%ebp, %edx
	movl	%edx, %ebp
	imull	%ecx, %ebp
	addl	160(%rsp), %ebp                 ## 4-byte Folded Reload
	imull	%edi, %edx
	addl	128(%rsp), %edx                 ## 4-byte Folded Reload
	movslq	%ebp, %rcx
	movq	%rcx, 320(%rsp)                 ## 8-byte Spill
	movslq	%edx, %rcx
	movq	%rcx, 288(%rsp)                 ## 8-byte Spill
	movl	%esi, 256(%rsp)                 ## 4-byte Spill
	movl	%esi, %r13d
	movl	%r8d, 224(%rsp)                 ## 4-byte Spill
	movl	%r10d, %edx
	xorl	%r15d, %r15d
	jmp	LBB4_16
	.p2align	4, 0x90
LBB4_17:                                ## %then_bb13.us.us
                                        ##   in Loop: Header=BB4_16 Depth=3
	movslq	%esi, %rsi
	movq	320(%rsp), %rcx                 ## 8-byte Reload
	leaq	(%rsi,%rcx), %rdi
	movq	-80(%rsp), %rcx                 ## 8-byte Reload
	vmovaps	%xmm15, %xmm6
	vaddps	(%rcx,%rdi,4), %ymm9, %ymm15
	vandps	(%rsp), %ymm15, %ymm8           ## 32-byte Folded Reload
	vmovdqa	%ymm3, %ymm2
	vpsrad	$22, %ymm8, %ymm3
	vmovaps	%ymm4, %ymm11
	vpslld	$23, %ymm3, %ymm4
	vpsubd	%ymm4, %ymm8, %ymm4
	vpaddd	%ymm2, %ymm4, %ymm4
	vaddps	%ymm4, %ymm13, %ymm4
	vmulps	%ymm4, %ymm4, %ymm8
	vpsrad	$23, %ymm15, %ymm15
	vpaddd	%ymm14, %ymm15, %ymm15
	vpaddd	%ymm3, %ymm15, %ymm3
	vcvtdq2ps	%ymm3, %ymm3
	vbroadcastss	LCPI4_5(%rip), %ymm15   ## ymm15 = [7.64031857E-2,7.64031857E-2,7.64031857E-2,7.64031857E-2,7.64031857E-2,7.64031857E-2,7.64031857E-2,7.64031857E-2]
	vbroadcastss	LCPI4_6(%rip), %ymm2    ## ymm2 = [2.06252187E-1,2.06252187E-1,2.06252187E-1,2.06252187E-1,2.06252187E-1,2.06252187E-1,2.06252187E-1,2.06252187E-1]
	vfmadd231ps	%ymm15, %ymm8, %ymm2    ## ymm2 = (ymm8 * ymm15) + ymm2
	vbroadcastss	LCPI4_7(%rip), %ymm15   ## ymm15 = [3.33204657E-1,3.33204657E-1,3.33204657E-1,3.33204657E-1,3.33204657E-1,3.33204657E-1,3.33204657E-1,3.33204657E-1]
	vfmadd231ps	%ymm2, %ymm8, %ymm15    ## ymm15 = (ymm8 * ymm2) + ymm15
	vfmadd213ps	%ymm9, %ymm8, %ymm15    ## ymm15 = (ymm8 * ymm15) + ymm9
	vmulps	%ymm4, %ymm15, %ymm2
	vbroadcastss	LCPI4_8(%rip), %ymm4    ## ymm4 = [-1.62529618E-1,-1.62529618E-1,-1.62529618E-1,-1.62529618E-1,-1.62529618E-1,-1.62529618E-1,-1.62529618E-1,-1.62529618E-1]
	vbroadcastss	LCPI4_9(%rip), %ymm15   ## ymm15 = [-2.51102597E-1,-2.51102597E-1,-2.51102597E-1,-2.51102597E-1,-2.51102597E-1,-2.51102597E-1,-2.51102597E-1,-2.51102597E-1]
	vfmadd231ps	%ymm4, %ymm8, %ymm15    ## ymm15 = (ymm8 * ymm4) + ymm15
	vbroadcastss	LCPI4_10(%rip), %ymm4   ## ymm4 = [-4.99975145E-1,-4.99975145E-1,-4.99975145E-1,-4.99975145E-1,-4.99975145E-1,-4.99975145E-1,-4.99975145E-1,-4.99975145E-1]
	vfmadd231ps	%ymm15, %ymm8, %ymm4    ## ymm4 = (ymm8 * ymm15) + ymm4
	vmovaps	%xmm6, %xmm15
	vfmadd213ps	%ymm2, %ymm8, %ymm4     ## ymm4 = (ymm8 * ymm4) + ymm2
	vmovss	LCPI4_10(%rip), %xmm8           ## xmm8 = mem[0],zero,zero,zero
	vmovss	LCPI4_9(%rip), %xmm1            ## xmm1 = mem[0],zero,zero,zero
	vfmadd231ps	%ymm3, %ymm11, %ymm4    ## ymm4 = (ymm11 * ymm3) + ymm4
	addq	288(%rsp), %rsi                 ## 8-byte Folded Reload
	movq	-72(%rsp), %rcx                 ## 8-byte Reload
	vmovups	%ymm4, (%rcx,%rsi,4)
LBB4_19:                                ## %after_bb12.us.us
                                        ##   in Loop: Header=BB4_16 Depth=3
	incq	%r15
	addl	$-8, %edx
	addl	$8, %r8d
	addl	$8, %r13d
	movq	%r11, %rcx
	cmpq	%r11, %r15
	vmovdqu	64(%rsp), %ymm3                 ## 32-byte Reload
	vmovups	32(%rsp), %ymm4                 ## 32-byte Reload
	je	LBB4_20
LBB4_16:                                ## %"for normalized_schedule_features.s0.n.n8.us.us"
                                        ##   Parent Loop BB4_14 Depth=1
                                        ##     Parent Loop BB4_15 Depth=2
                                        ## =>    This Loop Header: Depth=3
                                        ##         Child Loop BB4_26 Depth 4
                                        ##         Child Loop BB4_18 Depth 4
	cmpl	$8, %edx
	movl	$8, %r9d
	cmovll	%edx, %r9d
	leal	(,%r15,8), %esi
	movl	%r10d, %r12d
	subl	%esi, %r12d
	cmpl	$8, %r12d
	movl	$8, %ecx
	cmovgel	%ecx, %r12d
	leal	(%rax,%r15,8), %esi
	leal	(%rax,%r15,8), %edi
	addl	$8, %edi
	movl	-104(%rsp), %ecx                ## 4-byte Reload
	cmpl	%ecx, %edi
	jle	LBB4_17
## %bb.22:                              ## %next_bb14.us.us
                                        ##   in Loop: Header=BB4_16 Depth=3
	cmpl	%esi, %ecx
	jle	LBB4_19
## %bb.23:                              ## %"for normalized_schedule_features.s0.n.ni15.preheader.us.us"
                                        ##   in Loop: Header=BB4_16 Depth=3
	movslq	%r8d, %rcx
	movq	-72(%rsp), %rsi                 ## 8-byte Reload
	leaq	(%rsi,%rcx,4), %rbp
	movslq	%r13d, %rcx
	movq	-80(%rsp), %rsi                 ## 8-byte Reload
	leaq	(%rsi,%rcx,4), %r14
	cmpl	$8, %r12d
	jae	LBB4_25
## %bb.24:                              ##   in Loop: Header=BB4_16 Depth=3
	xorl	%esi, %esi
	jmp	LBB4_18
	.p2align	4, 0x90
LBB4_25:                                ## %vector.ph
                                        ##   in Loop: Header=BB4_16 Depth=3
	movl	%r9d, %edi
	andl	$-8, %edi
	movl	%r12d, %esi
	andl	$-8, %esi
	xorl	%ebx, %ebx
	vmovups	(%rsp), %ymm1                   ## 32-byte Reload
	vmovdqa	%ymm14, %ymm10
	vmovaps	%ymm13, %ymm14
	vmovdqu	64(%rsp), %ymm13                ## 32-byte Reload
	vmovups	32(%rsp), %ymm11                ## 32-byte Reload
	.p2align	4, 0x90
LBB4_26:                                ## %vector.body
                                        ##   Parent Loop BB4_14 Depth=1
                                        ##     Parent Loop BB4_15 Depth=2
                                        ##       Parent Loop BB4_16 Depth=3
                                        ## =>      This Inner Loop Header: Depth=4
	vaddps	(%r14,%rbx,4), %ymm9, %ymm2
	vandps	%ymm1, %ymm2, %ymm3
	vpsrad	$22, %ymm3, %ymm4
	vpslld	$23, %ymm4, %ymm6
	vpsubd	%ymm6, %ymm3, %ymm3
	vpaddd	%ymm3, %ymm13, %ymm3
	vaddps	%ymm3, %ymm14, %ymm3
	vmulps	%ymm3, %ymm3, %ymm6
	vpsrad	$23, %ymm2, %ymm2
	vpaddd	%ymm2, %ymm10, %ymm2
	vpaddd	%ymm2, %ymm4, %ymm2
	vcvtdq2ps	%ymm2, %ymm2
	vbroadcastss	LCPI4_5(%rip), %ymm4    ## ymm4 = [7.64031857E-2,7.64031857E-2,7.64031857E-2,7.64031857E-2,7.64031857E-2,7.64031857E-2,7.64031857E-2,7.64031857E-2]
	vbroadcastss	LCPI4_6(%rip), %ymm8    ## ymm8 = [2.06252187E-1,2.06252187E-1,2.06252187E-1,2.06252187E-1,2.06252187E-1,2.06252187E-1,2.06252187E-1,2.06252187E-1]
	vfmadd231ps	%ymm4, %ymm6, %ymm8     ## ymm8 = (ymm6 * ymm4) + ymm8
	vbroadcastss	LCPI4_7(%rip), %ymm4    ## ymm4 = [3.33204657E-1,3.33204657E-1,3.33204657E-1,3.33204657E-1,3.33204657E-1,3.33204657E-1,3.33204657E-1,3.33204657E-1]
	vfmadd231ps	%ymm8, %ymm6, %ymm4     ## ymm4 = (ymm6 * ymm8) + ymm4
	vfmadd213ps	%ymm9, %ymm6, %ymm4     ## ymm4 = (ymm6 * ymm4) + ymm9
	vmulps	%ymm4, %ymm3, %ymm3
	vbroadcastss	LCPI4_8(%rip), %ymm4    ## ymm4 = [-1.62529618E-1,-1.62529618E-1,-1.62529618E-1,-1.62529618E-1,-1.62529618E-1,-1.62529618E-1,-1.62529618E-1,-1.62529618E-1]
	vbroadcastss	LCPI4_9(%rip), %ymm8    ## ymm8 = [-2.51102597E-1,-2.51102597E-1,-2.51102597E-1,-2.51102597E-1,-2.51102597E-1,-2.51102597E-1,-2.51102597E-1,-2.51102597E-1]
	vfmadd231ps	%ymm4, %ymm6, %ymm8     ## ymm8 = (ymm6 * ymm4) + ymm8
	vbroadcastss	LCPI4_10(%rip), %ymm4   ## ymm4 = [-4.99975145E-1,-4.99975145E-1,-4.99975145E-1,-4.99975145E-1,-4.99975145E-1,-4.99975145E-1,-4.99975145E-1,-4.99975145E-1]
	vfmadd231ps	%ymm8, %ymm6, %ymm4     ## ymm4 = (ymm6 * ymm8) + ymm4
	vfmadd213ps	%ymm3, %ymm6, %ymm4     ## ymm4 = (ymm6 * ymm4) + ymm3
	vfmadd231ps	%ymm2, %ymm11, %ymm4    ## ymm4 = (ymm11 * ymm2) + ymm4
	vmovups	%ymm4, (%rbp,%rbx,4)
	addq	$8, %rbx
	cmpq	%rbx, %rdi
	jne	LBB4_26
## %bb.27:                              ## %middle.block
                                        ##   in Loop: Header=BB4_16 Depth=3
	cmpq	%r12, %rsi
	vmovss	LCPI4_9(%rip), %xmm1            ## xmm1 = mem[0],zero,zero,zero
	vmovss	LCPI4_10(%rip), %xmm8           ## xmm8 = mem[0],zero,zero,zero
	vmovaps	%ymm14, %ymm13
	vmovdqa	%ymm10, %ymm14
	vmovss	LCPI4_0(%rip), %xmm10           ## xmm10 = mem[0],zero,zero,zero
	je	LBB4_19
	.p2align	4, 0x90
LBB4_18:                                ## %"for normalized_schedule_features.s0.n.ni15.us.us"
                                        ##   Parent Loop BB4_14 Depth=1
                                        ##     Parent Loop BB4_15 Depth=2
                                        ##       Parent Loop BB4_16 Depth=3
                                        ## =>      This Inner Loop Header: Depth=4
	vaddss	(%r14,%rsi,4), %xmm10, %xmm2
	vmovd	%xmm2, %edi
	movl	%edi, %ecx
	andl	$-2139095041, %ecx              ## imm = 0x807FFFFF
	movl	%ecx, %ebx
	sarl	$22, %ebx
	sarl	$23, %edi
	addl	%ebx, %edi
	addl	$-127, %edi
                                        ## kill: def $ebx killed $ebx killed $rbx def $rbx
	shll	$23, %ebx
	negl	%ebx
	addl	%ebx, %ecx
	addl	$1065353216, %ecx               ## imm = 0x3F800000
	vmovd	%ecx, %xmm2
	vaddss	LCPI4_2(%rip), %xmm2, %xmm2
	vmulss	%xmm2, %xmm2, %xmm3
	vcvtsi2ss	%edi, %xmm7, %xmm4
	vmovaps	%xmm7, %xmm6
	vfmadd213ss	%xmm5, %xmm3, %xmm6     ## xmm6 = (xmm3 * xmm6) + xmm5
	vfmadd213ss	%xmm0, %xmm3, %xmm6     ## xmm6 = (xmm3 * xmm6) + xmm0
	vfmadd213ss	%xmm10, %xmm3, %xmm6    ## xmm6 = (xmm3 * xmm6) + xmm10
	vmulss	%xmm6, %xmm2, %xmm2
	vmovaps	%xmm15, %xmm6
	vfmadd213ss	%xmm1, %xmm3, %xmm6     ## xmm6 = (xmm3 * xmm6) + xmm1
	vfmadd213ss	%xmm8, %xmm3, %xmm6     ## xmm6 = (xmm3 * xmm6) + xmm8
	vfmadd213ss	%xmm2, %xmm3, %xmm6     ## xmm6 = (xmm3 * xmm6) + xmm2
	vfmadd231ss	%xmm12, %xmm4, %xmm6    ## xmm6 = (xmm4 * xmm12) + xmm6
	vmovss	%xmm6, (%rbp,%rsi,4)
	incq	%rsi
	cmpq	%rsi, %r9
	jne	LBB4_18
	jmp	LBB4_19
LBB4_35:                                ## %destructor_block
	xorl	%eax, %eax
	addq	$360, %rsp                      ## imm = 0x168
	popq	%rbx
	popq	%r12
	popq	%r13
	popq	%r14
	popq	%r15
	popq	%rbp
	vzeroupper
	retq
                                        ## -- End function
	.p2align	4, 0x90                         ## -- Begin function train_cost_model.par_for.head2_conv.s0.n.n.n
_train_cost_model.par_for.head2_conv.s0.n.n.n: ## @train_cost_model.par_for.head2_conv.s0.n.n.n
## %bb.0:                               ## %entry
	pushq	%rbp
	pushq	%r15
	pushq	%r14
	pushq	%r13
	pushq	%r12
	pushq	%rbx
	subq	$72, %rsp
	movq	%rdx, %r14
	movl	%esi, %r10d
	movl	4(%rdx), %r9d
	movl	8(%rdx), %ecx
	movl	12(%rdx), %r8d
	movl	16(%rdx), %ebx
	movl	%esi, %r12d
	sarl	$31, %r12d
	xorl	%ebp, %ebp
	testl	%ecx, %ecx
	sete	%bpl
	movl	%ebp, %edi
	negl	%edi
	movl	%ecx, %esi
	subl	%r12d, %r10d
	orl	%ecx, %edi
	movl	%r10d, %eax
	cltd
	idivl	%edi
	sarl	$31, %esi
	movl	%esi, %r11d
	xorl	%ecx, %r11d
	movl	%esi, %r15d
	notl	%r15d
	addl	%r15d, %r11d
	andl	%r12d, %r11d
	addl	%edx, %r11d
	leal	-1(%rbp), %edi
	andl	%edi, %r11d
	addl	%ebp, %ecx
	movl	%r10d, %eax
	cltd
	idivl	%ecx
	leal	(,%r11,8), %r10d
	leal	(%r10,%r10,4), %edx
	subl	%esi, %r15d
	andl	%r12d, %r15d
	addl	%eax, %r15d
	andl	%edi, %r15d
	movl	%r15d, %r12d
	andl	$-2, %r12d
	subl	%r12d, %r8d
	subl	%r9d, %ebx
	movl	%ebx, %ebp
	subl	%edx, %ebp
	movl	%ebp, %r13d
	sarl	$3, %r13d
	cmpl	$5, %r13d
	movl	$5, %eax
	cmovgel	%eax, %r13d
	xorl	%edx, %edx
	cmpl	$7, %ebp
	cmovlel	%edx, %r13d
	cmpl	$2, %r8d
	movl	$2, %r9d
	cmovll	%r8d, %r9d
	leal	7(%rbp), %ecx
	sarl	$3, %ecx
	cmpl	$5, %ecx
	cmovgel	%eax, %ecx
	testl	%r8d, %r8d
	cmovlel	%edx, %r9d
	jle	LBB5_135
## %bb.1:                               ## %"for head2_conv.s0.w.wi.preheader"
	movslq	(%r14), %rax
	movq	%rax, -80(%rsp)                 ## 8-byte Spill
	movl	20(%r14), %r8d
	movq	24(%r14), %rdx
	movq	%rdx, -56(%rsp)                 ## 8-byte Spill
	movq	40(%r14), %rdx
	movq	%rdx, -128(%rsp)                ## 8-byte Spill
	subl	%r13d, %ecx
	movl	%r15d, %edx
	andl	$1, %edx
	shll	$2, %edx
	leal	(%rdx,%rdx,2), %esi
	testl	%ecx, %ecx
	jle	LBB5_48
## %bb.2:                               ## %"for head2_conv.s0.w.wi.preheader.split.us"
	movl	%ecx, %edx
	sarl	$31, %edx
	andnl	%ecx, %edx, %ecx
	cmpl	$7, %ebp
	movq	%r13, -24(%rsp)                 ## 8-byte Spill
	movq	%rbp, -32(%rsp)                 ## 8-byte Spill
	jle	LBB5_3
## %bb.9:                               ## %"for head2_conv.s0.w.wi.us.us.preheader"
	movl	%r13d, %edi
	movl	%ecx, %eax
	movq	%rax, -40(%rsp)                 ## 8-byte Spill
	movl	%esi, %ecx
	movq	%rcx, -48(%rsp)                 ## 8-byte Spill
	movl	%r9d, %ecx
	movq	%rcx, (%rsp)                    ## 8-byte Spill
	leal	(,%r13,8), %ecx
	subl	%ecx, %ebx
	leal	(%r10,%r10,4), %r9d
	subl	%r9d, %ebx
	leaq	-1(%rdi), %rcx
	movq	%rcx, 56(%rsp)                  ## 8-byte Spill
	movl	%edi, %r11d
	andl	$7, %r11d
	andl	$-8, %edi
	movl	%esi, %ecx
	movl	%r15d, %esi
	andl	$1, %esi
	movq	-80(%rsp), %r14                 ## 8-byte Reload
	imulq	%r14, %rsi
	leaq	(%rsi,%rsi,2), %rax
	shlq	$4, %rax
	movq	-128(%rsp), %r10                ## 8-byte Reload
	leaq	(%r10,%rax), %rsi
	addq	$224, %rsi
	movq	%rsi, -16(%rsp)                 ## 8-byte Spill
	subl	%r8d, %r12d
	incl	%r12d
	imull	%r14d, %r12d
	leal	(%r12,%r12,2), %esi
	leal	(%r9,%rsi,8), %r12d
	leal	(,%r14,8), %esi
	leal	(%rsi,%rsi,2), %esi
	movl	%esi, 68(%rsp)                  ## 4-byte Spill
	leaq	(,%r14,4), %rsi
	movq	%rsi, 40(%rsp)                  ## 8-byte Spill
	addq	%r10, %rax
	movq	%rax, -8(%rsp)                  ## 8-byte Spill
	shrl	%r15d
	leal	(%r15,%r15,2), %esi
	shll	$4, %esi
	addl	%ecx, %esi
	movq	%r11, 48(%rsp)                  ## 8-byte Spill
	shlq	$5, %r11
	shll	$3, %r8d
	leal	(%r8,%r8,2), %eax
	subl	%eax, %esi
	movl	%r12d, %eax
	leaq	992(%r10), %rcx
	movq	%rcx, -96(%rsp)                 ## 8-byte Spill
	addl	$24, %esi
	imull	%r14d, %esi
	addl	%r9d, %esi
	leal	(%rsi,%r13,8), %ecx
	leaq	96(%r10), %rsi
	movq	%rsi, -88(%rsp)                 ## 8-byte Spill
	xorl	%esi, %esi
	movl	$8, %edx
	movq	%rdi, -64(%rsp)                 ## 8-byte Spill
	movq	%r11, 32(%rsp)                  ## 8-byte Spill
	jmp	LBB5_10
	.p2align	4, 0x90
LBB5_25:                                ## %"end for head2_conv.s0.c.ci.split.us.us.split.us.us"
                                        ##   in Loop: Header=BB5_10 Depth=1
	movl	24(%rsp), %eax                  ## 4-byte Reload
	movl	68(%rsp), %ecx                  ## 4-byte Reload
	addl	%ecx, %eax
	movl	16(%rsp), %esi                  ## 4-byte Reload
	addl	%ecx, %esi
	movl	%esi, %ecx
	movq	8(%rsp), %rsi                   ## 8-byte Reload
	cmpq	(%rsp), %rsi                    ## 8-byte Folded Reload
	je	LBB5_135
LBB5_10:                                ## %"for head2_conv.s0.w.wi.us.us"
                                        ## =>This Loop Header: Depth=1
                                        ##     Child Loop BB5_11 Depth 2
                                        ##       Child Loop BB5_14 Depth 3
                                        ##       Child Loop BB5_17 Depth 3
                                        ##       Child Loop BB5_19 Depth 3
                                        ##         Child Loop BB5_29 Depth 4
                                        ##         Child Loop BB5_32 Depth 4
                                        ##         Child Loop BB5_22 Depth 4
	movl	%eax, 24(%rsp)                  ## 4-byte Spill
	movslq	%eax, %r9
	movq	%rsi, %rax
	movl	%ecx, %r8d
	movq	-8(%rsp), %rcx                  ## 8-byte Reload
	leaq	(%rcx,%r9,4), %r10
	movq	-16(%rsp), %rcx                 ## 8-byte Reload
	leaq	(%rcx,%r9,4), %rsi
	incq	%rax
	movq	%rax, 8(%rsp)                   ## 8-byte Spill
	movl	%r8d, 16(%rsp)                  ## 4-byte Spill
	xorl	%ecx, %ecx
	jmp	LBB5_11
	.p2align	4, 0x90
LBB5_24:                                ## %"end for head2_conv.s0.n.ni.ni.rebased.loopexit.us.us.us.us"
                                        ##   in Loop: Header=BB5_11 Depth=2
	movq	-104(%rsp), %rcx                ## 8-byte Reload
	incq	%rcx
	movq	40(%rsp), %rax                  ## 8-byte Reload
	movq	-120(%rsp), %rsi                ## 8-byte Reload
	addq	%rax, %rsi
	movq	-112(%rsp), %r10                ## 8-byte Reload
	addq	%rax, %r10
	movl	-72(%rsp), %r8d                 ## 4-byte Reload
	addl	-80(%rsp), %r8d                 ## 4-byte Folded Reload
	cmpq	$12, %rcx
	movq	-64(%rsp), %rdi                 ## 8-byte Reload
	movq	32(%rsp), %r11                  ## 8-byte Reload
	je	LBB5_25
LBB5_11:                                ## %"for head2_conv.s0.c.ci.us.us.us.us"
                                        ##   Parent Loop BB5_10 Depth=1
                                        ## =>  This Loop Header: Depth=2
                                        ##       Child Loop BB5_14 Depth 3
                                        ##       Child Loop BB5_17 Depth 3
                                        ##       Child Loop BB5_19 Depth 3
                                        ##         Child Loop BB5_29 Depth 4
                                        ##         Child Loop BB5_32 Depth 4
                                        ##         Child Loop BB5_22 Depth 4
	movq	-48(%rsp), %rax                 ## 8-byte Reload
	movq	%rcx, -104(%rsp)                ## 8-byte Spill
	addq	%rcx, %rax
	movq	-56(%rsp), %rcx                 ## 8-byte Reload
	vbroadcastss	(%rcx,%rax,4), %ymm0
	cmpq	$7, 56(%rsp)                    ## 8-byte Folded Reload
	jae	LBB5_13
## %bb.12:                              ##   in Loop: Header=BB5_11 Depth=2
	xorl	%eax, %eax
	jmp	LBB5_15
	.p2align	4, 0x90
LBB5_13:                                ## %"for head2_conv.s0.n.ni.ni.us.us.us.us.preheader"
                                        ##   in Loop: Header=BB5_11 Depth=2
	movq	%rsi, %rcx
	xorl	%eax, %eax
	.p2align	4, 0x90
LBB5_14:                                ## %"for head2_conv.s0.n.ni.ni.us.us.us.us"
                                        ##   Parent Loop BB5_10 Depth=1
                                        ##     Parent Loop BB5_11 Depth=2
                                        ## =>    This Inner Loop Header: Depth=3
	vmovups	%ymm0, -224(%rcx)
	vmovups	%ymm0, -192(%rcx)
	vmovups	%ymm0, -160(%rcx)
	vmovups	%ymm0, -128(%rcx)
	vmovups	%ymm0, -96(%rcx)
	vmovups	%ymm0, -64(%rcx)
	vmovups	%ymm0, -32(%rcx)
	vmovups	%ymm0, (%rcx)
	addq	$8, %rax
	addq	$256, %rcx                      ## imm = 0x100
	cmpq	%rax, %rdi
	jne	LBB5_14
LBB5_15:                                ## %"end for head2_conv.s0.n.ni.ni.loopexit.us.us.us.us.unr-lcssa"
                                        ##   in Loop: Header=BB5_11 Depth=2
	movq	%rsi, -120(%rsp)                ## 8-byte Spill
	cmpq	$0, 48(%rsp)                    ## 8-byte Folded Reload
	je	LBB5_18
## %bb.16:                              ## %"for head2_conv.s0.n.ni.ni.us.us.us.us.epil.preheader"
                                        ##   in Loop: Header=BB5_11 Depth=2
	shlq	$5, %rax
	addq	%r10, %rax
	xorl	%ecx, %ecx
	.p2align	4, 0x90
LBB5_17:                                ## %"for head2_conv.s0.n.ni.ni.us.us.us.us.epil"
                                        ##   Parent Loop BB5_10 Depth=1
                                        ##     Parent Loop BB5_11 Depth=2
                                        ## =>    This Inner Loop Header: Depth=3
	vmovups	%ymm0, (%rax,%rcx)
	addq	$32, %rcx
	cmpq	%rcx, %r11
	jne	LBB5_17
LBB5_18:                                ## %"end for head2_conv.s0.n.ni.ni.loopexit.us.us.us.us"
                                        ##   in Loop: Header=BB5_11 Depth=2
	movq	%r10, -112(%rsp)                ## 8-byte Spill
	vbroadcastss	%xmm0, %ymm1
	movl	%ebx, %edi
	movl	%r8d, -72(%rsp)                 ## 4-byte Spill
	movl	%r8d, %r10d
	xorl	%r9d, %r9d
	jmp	LBB5_19
	.p2align	4, 0x90
LBB5_23:                                ## %"end for head2_conv.s0.n.ni.nii.us.us.us.us"
                                        ##   in Loop: Header=BB5_19 Depth=3
	incq	%r9
	addl	$8, %r10d
	addl	$-8, %edi
	cmpq	-40(%rsp), %r9                  ## 8-byte Folded Reload
	je	LBB5_24
LBB5_19:                                ## %"for head2_conv.s0.n.ni.ni.rebased.us.us.us.us"
                                        ##   Parent Loop BB5_10 Depth=1
                                        ##     Parent Loop BB5_11 Depth=2
                                        ## =>    This Loop Header: Depth=3
                                        ##         Child Loop BB5_29 Depth 4
                                        ##         Child Loop BB5_32 Depth 4
                                        ##         Child Loop BB5_22 Depth 4
	cmpl	$8, %edi
	movl	$8, %r11d
	cmovll	%edi, %r11d
	leal	(,%r9,8), %eax
	movl	%ebx, %r14d
	subl	%eax, %r14d
	cmpl	$8, %r14d
	cmovgel	%edx, %r14d
	leal	(%r9,%r13), %eax
	shll	$3, %eax
	cmpl	%eax, %ebp
	jle	LBB5_23
## %bb.20:                              ## %"for head2_conv.s0.n.ni.nii.preheader.us.us.us.us"
                                        ##   in Loop: Header=BB5_19 Depth=3
	movslq	%r10d, %r8
	cmpl	$32, %r14d
	jae	LBB5_26
## %bb.21:                              ##   in Loop: Header=BB5_19 Depth=3
	xorl	%eax, %eax
	jmp	LBB5_34
	.p2align	4, 0x90
LBB5_26:                                ## %vector.ph64
                                        ##   in Loop: Header=BB5_19 Depth=3
	movl	%ebx, %r15d
	movl	%r11d, %r12d
	andl	$-32, %r12d
	addq	$-32, %r12
	shrq	$5, %r12
	movl	%r14d, %eax
	andl	$-32, %eax
	addq	$-32, %rax
	movq	%rax, %r13
	shrq	$5, %r13
	incq	%r13
	cmpq	$224, %rax
	jae	LBB5_28
## %bb.27:                              ##   in Loop: Header=BB5_19 Depth=3
	xorl	%ecx, %ecx
	jmp	LBB5_30
	.p2align	4, 0x90
LBB5_28:                                ## %vector.ph64.new
                                        ##   in Loop: Header=BB5_19 Depth=3
	movq	-96(%rsp), %rax                 ## 8-byte Reload
	leaq	(%rax,%r8,4), %rax
	leaq	1(%r12), %rbx
	andq	$-8, %rbx
	negq	%rbx
	xorl	%ecx, %ecx
	.p2align	4, 0x90
LBB5_29:                                ## %vector.body61
                                        ##   Parent Loop BB5_10 Depth=1
                                        ##     Parent Loop BB5_11 Depth=2
                                        ##       Parent Loop BB5_19 Depth=3
                                        ## =>      This Inner Loop Header: Depth=4
	vmovups	%ymm1, -992(%rax,%rcx,4)
	vmovups	%ymm1, -960(%rax,%rcx,4)
	vmovups	%ymm1, -928(%rax,%rcx,4)
	vmovups	%ymm1, -896(%rax,%rcx,4)
	vmovups	%ymm1, -864(%rax,%rcx,4)
	vmovups	%ymm1, -832(%rax,%rcx,4)
	vmovups	%ymm1, -800(%rax,%rcx,4)
	vmovups	%ymm1, -768(%rax,%rcx,4)
	vmovups	%ymm1, -736(%rax,%rcx,4)
	vmovups	%ymm1, -704(%rax,%rcx,4)
	vmovups	%ymm1, -672(%rax,%rcx,4)
	vmovups	%ymm1, -640(%rax,%rcx,4)
	vmovups	%ymm1, -608(%rax,%rcx,4)
	vmovups	%ymm1, -576(%rax,%rcx,4)
	vmovups	%ymm1, -544(%rax,%rcx,4)
	vmovups	%ymm1, -512(%rax,%rcx,4)
	vmovups	%ymm1, -480(%rax,%rcx,4)
	vmovups	%ymm1, -448(%rax,%rcx,4)
	vmovups	%ymm1, -416(%rax,%rcx,4)
	vmovups	%ymm1, -384(%rax,%rcx,4)
	vmovups	%ymm1, -352(%rax,%rcx,4)
	vmovups	%ymm1, -320(%rax,%rcx,4)
	vmovups	%ymm1, -288(%rax,%rcx,4)
	vmovups	%ymm1, -256(%rax,%rcx,4)
	vmovups	%ymm1, -224(%rax,%rcx,4)
	vmovups	%ymm1, -192(%rax,%rcx,4)
	vmovups	%ymm1, -160(%rax,%rcx,4)
	vmovups	%ymm1, -128(%rax,%rcx,4)
	vmovups	%ymm1, -96(%rax,%rcx,4)
	vmovups	%ymm1, -64(%rax,%rcx,4)
	vmovups	%ymm1, -32(%rax,%rcx,4)
	vmovups	%ymm1, (%rax,%rcx,4)
	addq	$256, %rcx                      ## imm = 0x100
	addq	$8, %rbx
	jne	LBB5_29
LBB5_30:                                ## %middle.block59.unr-lcssa
                                        ##   in Loop: Header=BB5_19 Depth=3
	movl	%r14d, %eax
	andl	$-32, %eax
	testb	$7, %r13b
	je	LBB5_33
## %bb.31:                              ## %vector.body61.epil.preheader
                                        ##   in Loop: Header=BB5_19 Depth=3
	incb	%r12b
	movzbl	%r12b, %ebx
	andl	$7, %ebx
	shlq	$7, %rbx
	addq	%r8, %rcx
	movq	-88(%rsp), %rsi                 ## 8-byte Reload
	leaq	(%rsi,%rcx,4), %rcx
	xorl	%esi, %esi
	.p2align	4, 0x90
LBB5_32:                                ## %vector.body61.epil
                                        ##   Parent Loop BB5_10 Depth=1
                                        ##     Parent Loop BB5_11 Depth=2
                                        ##       Parent Loop BB5_19 Depth=3
                                        ## =>      This Inner Loop Header: Depth=4
	vmovups	%ymm1, -96(%rcx,%rsi)
	vmovups	%ymm1, -64(%rcx,%rsi)
	vmovups	%ymm1, -32(%rcx,%rsi)
	vmovups	%ymm1, (%rcx,%rsi)
	subq	$-128, %rsi
	cmpq	%rsi, %rbx
	jne	LBB5_32
LBB5_33:                                ## %middle.block59
                                        ##   in Loop: Header=BB5_19 Depth=3
	cmpq	%r14, %rax
	movl	%r15d, %ebx
	movq	-24(%rsp), %r13                 ## 8-byte Reload
	movq	-32(%rsp), %rbp                 ## 8-byte Reload
	movl	$8, %edx
	je	LBB5_23
LBB5_34:                                ## %"for head2_conv.s0.n.ni.nii.us.us.us.us.preheader"
                                        ##   in Loop: Header=BB5_19 Depth=3
	movq	-128(%rsp), %rcx                ## 8-byte Reload
	leaq	(%rcx,%r8,4), %rcx
	.p2align	4, 0x90
LBB5_22:                                ## %"for head2_conv.s0.n.ni.nii.us.us.us.us"
                                        ##   Parent Loop BB5_10 Depth=1
                                        ##     Parent Loop BB5_11 Depth=2
                                        ##       Parent Loop BB5_19 Depth=3
                                        ## =>      This Inner Loop Header: Depth=4
	vmovss	%xmm0, (%rcx,%rax,4)
	incq	%rax
	cmpq	%rax, %r11
	jne	LBB5_22
	jmp	LBB5_23
LBB5_48:                                ## %"for head2_conv.s0.w.wi.preheader.split"
	cmpl	$8, %ebp
	jl	LBB5_135
## %bb.49:                              ## %"for head2_conv.s0.w.wi.us17.preheader"
	movl	%r13d, %r10d
	movl	%esi, %r14d
	movl	%r9d, %ecx
	movq	%rcx, -40(%rsp)                 ## 8-byte Spill
	movq	-56(%rsp), %rcx                 ## 8-byte Reload
	vbroadcastss	(%rcx,%r14,4), %ymm0
	movq	%r14, %rdi
	orq	$1, %rdi
	vbroadcastss	(%rcx,%rdi,4), %ymm1
	movq	%r14, %rsi
	orq	$2, %rsi
	vbroadcastss	(%rcx,%rsi,4), %ymm2
	movq	%r14, %rax
	orq	$3, %rax
	vbroadcastss	(%rcx,%rax,4), %ymm3
	movq	%rax, %r9
	leaq	4(%r14), %rax
	movq	%rax, -112(%rsp)                ## 8-byte Spill
	vbroadcastss	16(%rcx,%r14,4), %ymm4
	leaq	5(%r14), %rax
	movq	%rax, -120(%rsp)                ## 8-byte Spill
	vbroadcastss	20(%rcx,%r14,4), %ymm5
	leaq	6(%r14), %rax
	movq	%rax, -72(%rsp)                 ## 8-byte Spill
	vbroadcastss	24(%rcx,%r14,4), %ymm6
	vbroadcastss	28(%rcx,%r14,4), %ymm7
	vbroadcastss	32(%rcx,%r14,4), %ymm8
	vbroadcastss	36(%rcx,%r14,4), %ymm9
	vbroadcastss	40(%rcx,%r14,4), %ymm10
	vbroadcastss	44(%rcx,%r14,4), %ymm11
	leaq	7(%r14), %rax
	movq	%rax, -88(%rsp)                 ## 8-byte Spill
	subl	%r8d, %r12d
	leaq	8(%r14), %rax
	movq	%rax, -96(%rsp)                 ## 8-byte Spill
	incl	%r12d
	movq	-80(%rsp), %rdx                 ## 8-byte Reload
	imull	%edx, %r12d
	leal	(%r12,%r12,2), %eax
	leal	(%r11,%r11,4), %ecx
	shll	$3, %ecx
	leal	(%rcx,%rax,8), %ebx
	leaq	9(%r14), %rbp
	leaq	10(%r14), %r12
	addq	$11, %r14
	movl	%r10d, %r13d
	andl	$7, %r13d
	movl	%r10d, %ecx
	andl	$-8, %ecx
	andl	$1, %r15d
	imulq	%rdx, %r15
	shlq	$2, %r15
	leaq	(%r15,%r15,2), %rax
	movq	%rax, -56(%rsp)                 ## 8-byte Spill
	imulq	%rdx, %rdi
	imulq	%rdx, %rsi
	imulq	%rdx, %r9
	movq	%r9, -104(%rsp)                 ## 8-byte Spill
	movq	-112(%rsp), %r11                ## 8-byte Reload
	imulq	%rdx, %r11
	movq	-120(%rsp), %r8                 ## 8-byte Reload
	imulq	%rdx, %r8
	movq	-72(%rsp), %r9                  ## 8-byte Reload
	imulq	%rdx, %r9
	movq	-88(%rsp), %rax                 ## 8-byte Reload
	imulq	%rdx, %rax
	movq	%rax, -88(%rsp)                 ## 8-byte Spill
	movq	-96(%rsp), %rax                 ## 8-byte Reload
	imulq	%rdx, %rax
	movq	%rax, -96(%rsp)                 ## 8-byte Spill
	imulq	%rdx, %rbp
	imulq	%rdx, %r12
	imulq	%rdx, %r14
	leal	(,%rdx,8), %eax
	leal	(%rax,%rax,2), %eax
	movl	%eax, -24(%rsp)                 ## 4-byte Spill
	movq	%r13, -32(%rsp)                 ## 8-byte Spill
	movq	%r13, %r15
	shlq	$5, %r15
	leaq	-1(%r10), %r13
	movq	-128(%rsp), %rax                ## 8-byte Reload
	movq	-56(%rsp), %rdx                 ## 8-byte Reload
	leaq	(%rax,%rdx,4), %rdx
	addq	$224, %rdx
	movq	%rdx, -16(%rsp)                 ## 8-byte Spill
	movq	%rdi, -80(%rsp)                 ## 8-byte Spill
	leaq	224(%rax,%rdi,4), %rdi
	movq	%rdi, -48(%rsp)                 ## 8-byte Spill
	movq	%rsi, -64(%rsp)                 ## 8-byte Spill
	leaq	224(%rax,%rsi,4), %rsi
	movq	%rsi, 56(%rsp)                  ## 8-byte Spill
	movq	-104(%rsp), %rdx                ## 8-byte Reload
	leaq	224(%rax,%rdx,4), %rsi
	movq	%rsi, 48(%rsp)                  ## 8-byte Spill
	movq	%r11, -112(%rsp)                ## 8-byte Spill
	leaq	224(%rax,%r11,4), %rsi
	movq	%rsi, 40(%rsp)                  ## 8-byte Spill
	movq	%r8, -120(%rsp)                 ## 8-byte Spill
	leaq	224(%rax,%r8,4), %rsi
	movq	%rsi, 32(%rsp)                  ## 8-byte Spill
	movq	%r9, -72(%rsp)                  ## 8-byte Spill
	leaq	224(%rax,%r9,4), %rsi
	movq	%rsi, 24(%rsp)                  ## 8-byte Spill
	movq	-88(%rsp), %rdx                 ## 8-byte Reload
	leaq	224(%rax,%rdx,4), %rsi
	movq	%rsi, 16(%rsp)                  ## 8-byte Spill
	movq	-96(%rsp), %rdx                 ## 8-byte Reload
	leaq	224(%rax,%rdx,4), %rsi
	movq	%rsi, 8(%rsp)                   ## 8-byte Spill
	movq	%rbp, %r9
	leaq	224(%rax,%rbp,4), %rsi
	movq	%rsi, (%rsp)                    ## 8-byte Spill
	movq	%r12, %r8
	leaq	224(%rax,%r12,4), %r11
	movq	%r14, %r12
	leaq	(%rax,%r14,4), %rax
	addq	$224, %rax
	movq	%rax, -8(%rsp)                  ## 8-byte Spill
	xorl	%ebp, %ebp
	jmp	LBB5_50
LBB5_134:                               ## %"end for head2_conv.s0.n.ni.ni.loopexit.us16.us.11"
                                        ##   in Loop: Header=BB5_50 Depth=1
	addl	-24(%rsp), %ebx                 ## 4-byte Folded Reload
	cmpq	-40(%rsp), %rbp                 ## 8-byte Folded Reload
	je	LBB5_135
LBB5_50:                                ## %"for head2_conv.s0.w.wi.us17"
                                        ## =>This Loop Header: Depth=1
                                        ##     Child Loop BB5_53 Depth 2
                                        ##     Child Loop BB5_56 Depth 2
                                        ##     Child Loop BB5_60 Depth 2
                                        ##     Child Loop BB5_63 Depth 2
                                        ##     Child Loop BB5_67 Depth 2
                                        ##     Child Loop BB5_70 Depth 2
                                        ##     Child Loop BB5_74 Depth 2
                                        ##     Child Loop BB5_77 Depth 2
                                        ##     Child Loop BB5_81 Depth 2
                                        ##     Child Loop BB5_84 Depth 2
                                        ##     Child Loop BB5_88 Depth 2
                                        ##     Child Loop BB5_91 Depth 2
                                        ##     Child Loop BB5_95 Depth 2
                                        ##     Child Loop BB5_98 Depth 2
                                        ##     Child Loop BB5_102 Depth 2
                                        ##     Child Loop BB5_105 Depth 2
                                        ##     Child Loop BB5_109 Depth 2
                                        ##     Child Loop BB5_112 Depth 2
                                        ##     Child Loop BB5_116 Depth 2
                                        ##     Child Loop BB5_119 Depth 2
                                        ##     Child Loop BB5_123 Depth 2
                                        ##     Child Loop BB5_126 Depth 2
                                        ##     Child Loop BB5_130 Depth 2
                                        ##     Child Loop BB5_133 Depth 2
	movslq	%ebx, %rbx
	cmpq	$7, %r13
	jae	LBB5_52
## %bb.51:                              ##   in Loop: Header=BB5_50 Depth=1
	xorl	%eax, %eax
	jmp	LBB5_54
LBB5_52:                                ## %"for head2_conv.s0.n.ni.ni.us10.us.preheader"
                                        ##   in Loop: Header=BB5_50 Depth=1
	movq	-16(%rsp), %rax                 ## 8-byte Reload
	leaq	(%rax,%rbx,4), %rdi
	xorl	%eax, %eax
LBB5_53:                                ## %"for head2_conv.s0.n.ni.ni.us10.us"
                                        ##   Parent Loop BB5_50 Depth=1
                                        ## =>  This Inner Loop Header: Depth=2
	vmovups	%ymm0, -224(%rdi)
	vmovups	%ymm0, -192(%rdi)
	vmovups	%ymm0, -160(%rdi)
	vmovups	%ymm0, -128(%rdi)
	vmovups	%ymm0, -96(%rdi)
	vmovups	%ymm0, -64(%rdi)
	vmovups	%ymm0, -32(%rdi)
	vmovups	%ymm0, (%rdi)
	addq	$8, %rax
	addq	$256, %rdi                      ## imm = 0x100
	cmpq	%rax, %rcx
	jne	LBB5_53
LBB5_54:                                ## %"end for head2_conv.s0.n.ni.ni.loopexit.us16.us.unr-lcssa"
                                        ##   in Loop: Header=BB5_50 Depth=1
	cmpq	$0, -32(%rsp)                   ## 8-byte Folded Reload
	je	LBB5_57
## %bb.55:                              ## %"for head2_conv.s0.n.ni.ni.us10.us.epil.preheader"
                                        ##   in Loop: Header=BB5_50 Depth=1
	movq	-56(%rsp), %rsi                 ## 8-byte Reload
	leaq	(%rsi,%rax,8), %rax
	addq	%rbx, %rax
	movq	-128(%rsp), %rdi                ## 8-byte Reload
	leaq	(%rdi,%rax,4), %rax
	xorl	%edi, %edi
LBB5_56:                                ## %"for head2_conv.s0.n.ni.ni.us10.us.epil"
                                        ##   Parent Loop BB5_50 Depth=1
                                        ## =>  This Inner Loop Header: Depth=2
	vmovups	%ymm0, (%rax,%rdi)
	addq	$32, %rdi
	cmpq	%rdi, %r15
	jne	LBB5_56
LBB5_57:                                ## %"end for head2_conv.s0.n.ni.ni.loopexit.us16.us"
                                        ##   in Loop: Header=BB5_50 Depth=1
	cmpq	$7, %r13
	jae	LBB5_59
## %bb.58:                              ##   in Loop: Header=BB5_50 Depth=1
	xorl	%eax, %eax
	jmp	LBB5_61
LBB5_59:                                ## %"for head2_conv.s0.n.ni.ni.us10.us.1.preheader"
                                        ##   in Loop: Header=BB5_50 Depth=1
	movq	-48(%rsp), %rax                 ## 8-byte Reload
	leaq	(%rax,%rbx,4), %rdi
	xorl	%eax, %eax
LBB5_60:                                ## %"for head2_conv.s0.n.ni.ni.us10.us.1"
                                        ##   Parent Loop BB5_50 Depth=1
                                        ## =>  This Inner Loop Header: Depth=2
	vmovups	%ymm1, -224(%rdi)
	vmovups	%ymm1, -192(%rdi)
	vmovups	%ymm1, -160(%rdi)
	vmovups	%ymm1, -128(%rdi)
	vmovups	%ymm1, -96(%rdi)
	vmovups	%ymm1, -64(%rdi)
	vmovups	%ymm1, -32(%rdi)
	vmovups	%ymm1, (%rdi)
	addq	$8, %rax
	addq	$256, %rdi                      ## imm = 0x100
	cmpq	%rax, %rcx
	jne	LBB5_60
LBB5_61:                                ## %"end for head2_conv.s0.n.ni.ni.loopexit.us16.us.1.unr-lcssa"
                                        ##   in Loop: Header=BB5_50 Depth=1
	testb	$7, %r10b
	je	LBB5_64
## %bb.62:                              ## %"for head2_conv.s0.n.ni.ni.us10.us.1.epil.preheader"
                                        ##   in Loop: Header=BB5_50 Depth=1
	movq	-80(%rsp), %rdi                 ## 8-byte Reload
	leaq	(%rdi,%rax,8), %rax
	addq	%rbx, %rax
	movq	-128(%rsp), %rdi                ## 8-byte Reload
	leaq	(%rdi,%rax,4), %rax
	xorl	%edi, %edi
LBB5_63:                                ## %"for head2_conv.s0.n.ni.ni.us10.us.1.epil"
                                        ##   Parent Loop BB5_50 Depth=1
                                        ## =>  This Inner Loop Header: Depth=2
	vmovups	%ymm1, (%rax,%rdi)
	addq	$32, %rdi
	cmpq	%rdi, %r15
	jne	LBB5_63
LBB5_64:                                ## %"end for head2_conv.s0.n.ni.ni.loopexit.us16.us.1"
                                        ##   in Loop: Header=BB5_50 Depth=1
	cmpq	$7, %r13
	jae	LBB5_66
## %bb.65:                              ##   in Loop: Header=BB5_50 Depth=1
	xorl	%eax, %eax
	jmp	LBB5_68
LBB5_66:                                ## %"for head2_conv.s0.n.ni.ni.us10.us.2.preheader"
                                        ##   in Loop: Header=BB5_50 Depth=1
	movq	56(%rsp), %rax                  ## 8-byte Reload
	leaq	(%rax,%rbx,4), %rdi
	xorl	%eax, %eax
LBB5_67:                                ## %"for head2_conv.s0.n.ni.ni.us10.us.2"
                                        ##   Parent Loop BB5_50 Depth=1
                                        ## =>  This Inner Loop Header: Depth=2
	vmovups	%ymm2, -224(%rdi)
	vmovups	%ymm2, -192(%rdi)
	vmovups	%ymm2, -160(%rdi)
	vmovups	%ymm2, -128(%rdi)
	vmovups	%ymm2, -96(%rdi)
	vmovups	%ymm2, -64(%rdi)
	vmovups	%ymm2, -32(%rdi)
	vmovups	%ymm2, (%rdi)
	addq	$8, %rax
	addq	$256, %rdi                      ## imm = 0x100
	cmpq	%rax, %rcx
	jne	LBB5_67
LBB5_68:                                ## %"end for head2_conv.s0.n.ni.ni.loopexit.us16.us.2.unr-lcssa"
                                        ##   in Loop: Header=BB5_50 Depth=1
	testb	$7, %r10b
	je	LBB5_71
## %bb.69:                              ## %"for head2_conv.s0.n.ni.ni.us10.us.2.epil.preheader"
                                        ##   in Loop: Header=BB5_50 Depth=1
	movq	-64(%rsp), %rsi                 ## 8-byte Reload
	leaq	(%rsi,%rax,8), %rax
	addq	%rbx, %rax
	movq	-128(%rsp), %rdi                ## 8-byte Reload
	leaq	(%rdi,%rax,4), %rax
	xorl	%edi, %edi
LBB5_70:                                ## %"for head2_conv.s0.n.ni.ni.us10.us.2.epil"
                                        ##   Parent Loop BB5_50 Depth=1
                                        ## =>  This Inner Loop Header: Depth=2
	vmovups	%ymm2, (%rax,%rdi)
	addq	$32, %rdi
	cmpq	%rdi, %r15
	jne	LBB5_70
LBB5_71:                                ## %"end for head2_conv.s0.n.ni.ni.loopexit.us16.us.2"
                                        ##   in Loop: Header=BB5_50 Depth=1
	cmpq	$7, %r13
	jae	LBB5_73
## %bb.72:                              ##   in Loop: Header=BB5_50 Depth=1
	xorl	%eax, %eax
	jmp	LBB5_75
LBB5_73:                                ## %"for head2_conv.s0.n.ni.ni.us10.us.3.preheader"
                                        ##   in Loop: Header=BB5_50 Depth=1
	movq	48(%rsp), %rax                  ## 8-byte Reload
	leaq	(%rax,%rbx,4), %rdi
	xorl	%eax, %eax
LBB5_74:                                ## %"for head2_conv.s0.n.ni.ni.us10.us.3"
                                        ##   Parent Loop BB5_50 Depth=1
                                        ## =>  This Inner Loop Header: Depth=2
	vmovups	%ymm3, -224(%rdi)
	vmovups	%ymm3, -192(%rdi)
	vmovups	%ymm3, -160(%rdi)
	vmovups	%ymm3, -128(%rdi)
	vmovups	%ymm3, -96(%rdi)
	vmovups	%ymm3, -64(%rdi)
	vmovups	%ymm3, -32(%rdi)
	vmovups	%ymm3, (%rdi)
	addq	$8, %rax
	addq	$256, %rdi                      ## imm = 0x100
	cmpq	%rax, %rcx
	jne	LBB5_74
LBB5_75:                                ## %"end for head2_conv.s0.n.ni.ni.loopexit.us16.us.3.unr-lcssa"
                                        ##   in Loop: Header=BB5_50 Depth=1
	testb	$7, %r10b
	je	LBB5_78
## %bb.76:                              ## %"for head2_conv.s0.n.ni.ni.us10.us.3.epil.preheader"
                                        ##   in Loop: Header=BB5_50 Depth=1
	movq	-104(%rsp), %rsi                ## 8-byte Reload
	leaq	(%rsi,%rax,8), %rax
	addq	%rbx, %rax
	movq	-128(%rsp), %rdi                ## 8-byte Reload
	leaq	(%rdi,%rax,4), %rax
	xorl	%edi, %edi
LBB5_77:                                ## %"for head2_conv.s0.n.ni.ni.us10.us.3.epil"
                                        ##   Parent Loop BB5_50 Depth=1
                                        ## =>  This Inner Loop Header: Depth=2
	vmovups	%ymm3, (%rax,%rdi)
	addq	$32, %rdi
	cmpq	%rdi, %r15
	jne	LBB5_77
LBB5_78:                                ## %"end for head2_conv.s0.n.ni.ni.loopexit.us16.us.3"
                                        ##   in Loop: Header=BB5_50 Depth=1
	cmpq	$7, %r13
	jae	LBB5_80
## %bb.79:                              ##   in Loop: Header=BB5_50 Depth=1
	xorl	%eax, %eax
	jmp	LBB5_82
LBB5_80:                                ## %"for head2_conv.s0.n.ni.ni.us10.us.4.preheader"
                                        ##   in Loop: Header=BB5_50 Depth=1
	movq	40(%rsp), %rax                  ## 8-byte Reload
	leaq	(%rax,%rbx,4), %rdi
	xorl	%eax, %eax
LBB5_81:                                ## %"for head2_conv.s0.n.ni.ni.us10.us.4"
                                        ##   Parent Loop BB5_50 Depth=1
                                        ## =>  This Inner Loop Header: Depth=2
	vmovups	%ymm4, -224(%rdi)
	vmovups	%ymm4, -192(%rdi)
	vmovups	%ymm4, -160(%rdi)
	vmovups	%ymm4, -128(%rdi)
	vmovups	%ymm4, -96(%rdi)
	vmovups	%ymm4, -64(%rdi)
	vmovups	%ymm4, -32(%rdi)
	vmovups	%ymm4, (%rdi)
	addq	$8, %rax
	addq	$256, %rdi                      ## imm = 0x100
	cmpq	%rax, %rcx
	jne	LBB5_81
LBB5_82:                                ## %"end for head2_conv.s0.n.ni.ni.loopexit.us16.us.4.unr-lcssa"
                                        ##   in Loop: Header=BB5_50 Depth=1
	testb	$7, %r10b
	je	LBB5_85
## %bb.83:                              ## %"for head2_conv.s0.n.ni.ni.us10.us.4.epil.preheader"
                                        ##   in Loop: Header=BB5_50 Depth=1
	movq	-112(%rsp), %rdi                ## 8-byte Reload
	leaq	(%rdi,%rax,8), %rax
	addq	%rbx, %rax
	movq	-128(%rsp), %rdi                ## 8-byte Reload
	leaq	(%rdi,%rax,4), %rax
	xorl	%edi, %edi
LBB5_84:                                ## %"for head2_conv.s0.n.ni.ni.us10.us.4.epil"
                                        ##   Parent Loop BB5_50 Depth=1
                                        ## =>  This Inner Loop Header: Depth=2
	vmovups	%ymm4, (%rax,%rdi)
	addq	$32, %rdi
	cmpq	%rdi, %r15
	jne	LBB5_84
LBB5_85:                                ## %"end for head2_conv.s0.n.ni.ni.loopexit.us16.us.4"
                                        ##   in Loop: Header=BB5_50 Depth=1
	cmpq	$7, %r13
	jae	LBB5_87
## %bb.86:                              ##   in Loop: Header=BB5_50 Depth=1
	xorl	%eax, %eax
	jmp	LBB5_89
LBB5_87:                                ## %"for head2_conv.s0.n.ni.ni.us10.us.5.preheader"
                                        ##   in Loop: Header=BB5_50 Depth=1
	movq	32(%rsp), %rax                  ## 8-byte Reload
	leaq	(%rax,%rbx,4), %rdi
	xorl	%eax, %eax
LBB5_88:                                ## %"for head2_conv.s0.n.ni.ni.us10.us.5"
                                        ##   Parent Loop BB5_50 Depth=1
                                        ## =>  This Inner Loop Header: Depth=2
	vmovups	%ymm5, -224(%rdi)
	vmovups	%ymm5, -192(%rdi)
	vmovups	%ymm5, -160(%rdi)
	vmovups	%ymm5, -128(%rdi)
	vmovups	%ymm5, -96(%rdi)
	vmovups	%ymm5, -64(%rdi)
	vmovups	%ymm5, -32(%rdi)
	vmovups	%ymm5, (%rdi)
	addq	$8, %rax
	addq	$256, %rdi                      ## imm = 0x100
	cmpq	%rax, %rcx
	jne	LBB5_88
LBB5_89:                                ## %"end for head2_conv.s0.n.ni.ni.loopexit.us16.us.5.unr-lcssa"
                                        ##   in Loop: Header=BB5_50 Depth=1
	testb	$7, %r10b
	je	LBB5_92
## %bb.90:                              ## %"for head2_conv.s0.n.ni.ni.us10.us.5.epil.preheader"
                                        ##   in Loop: Header=BB5_50 Depth=1
	movq	-120(%rsp), %rsi                ## 8-byte Reload
	leaq	(%rsi,%rax,8), %rax
	addq	%rbx, %rax
	movq	-128(%rsp), %rdi                ## 8-byte Reload
	leaq	(%rdi,%rax,4), %rax
	xorl	%edi, %edi
LBB5_91:                                ## %"for head2_conv.s0.n.ni.ni.us10.us.5.epil"
                                        ##   Parent Loop BB5_50 Depth=1
                                        ## =>  This Inner Loop Header: Depth=2
	vmovups	%ymm5, (%rax,%rdi)
	addq	$32, %rdi
	cmpq	%rdi, %r15
	jne	LBB5_91
LBB5_92:                                ## %"end for head2_conv.s0.n.ni.ni.loopexit.us16.us.5"
                                        ##   in Loop: Header=BB5_50 Depth=1
	cmpq	$7, %r13
	jae	LBB5_94
## %bb.93:                              ##   in Loop: Header=BB5_50 Depth=1
	xorl	%eax, %eax
	jmp	LBB5_96
LBB5_94:                                ## %"for head2_conv.s0.n.ni.ni.us10.us.6.preheader"
                                        ##   in Loop: Header=BB5_50 Depth=1
	movq	24(%rsp), %rax                  ## 8-byte Reload
	leaq	(%rax,%rbx,4), %rdi
	xorl	%eax, %eax
LBB5_95:                                ## %"for head2_conv.s0.n.ni.ni.us10.us.6"
                                        ##   Parent Loop BB5_50 Depth=1
                                        ## =>  This Inner Loop Header: Depth=2
	vmovups	%ymm6, -224(%rdi)
	vmovups	%ymm6, -192(%rdi)
	vmovups	%ymm6, -160(%rdi)
	vmovups	%ymm6, -128(%rdi)
	vmovups	%ymm6, -96(%rdi)
	vmovups	%ymm6, -64(%rdi)
	vmovups	%ymm6, -32(%rdi)
	vmovups	%ymm6, (%rdi)
	addq	$8, %rax
	addq	$256, %rdi                      ## imm = 0x100
	cmpq	%rax, %rcx
	jne	LBB5_95
LBB5_96:                                ## %"end for head2_conv.s0.n.ni.ni.loopexit.us16.us.6.unr-lcssa"
                                        ##   in Loop: Header=BB5_50 Depth=1
	testb	$7, %r10b
	je	LBB5_99
## %bb.97:                              ## %"for head2_conv.s0.n.ni.ni.us10.us.6.epil.preheader"
                                        ##   in Loop: Header=BB5_50 Depth=1
	movq	-72(%rsp), %rsi                 ## 8-byte Reload
	leaq	(%rsi,%rax,8), %rax
	addq	%rbx, %rax
	movq	-128(%rsp), %rdi                ## 8-byte Reload
	leaq	(%rdi,%rax,4), %rax
	xorl	%edi, %edi
LBB5_98:                                ## %"for head2_conv.s0.n.ni.ni.us10.us.6.epil"
                                        ##   Parent Loop BB5_50 Depth=1
                                        ## =>  This Inner Loop Header: Depth=2
	vmovups	%ymm6, (%rax,%rdi)
	addq	$32, %rdi
	cmpq	%rdi, %r15
	jne	LBB5_98
LBB5_99:                                ## %"end for head2_conv.s0.n.ni.ni.loopexit.us16.us.6"
                                        ##   in Loop: Header=BB5_50 Depth=1
	cmpq	$7, %r13
	jae	LBB5_101
## %bb.100:                             ##   in Loop: Header=BB5_50 Depth=1
	xorl	%eax, %eax
	jmp	LBB5_103
LBB5_101:                               ## %"for head2_conv.s0.n.ni.ni.us10.us.7.preheader"
                                        ##   in Loop: Header=BB5_50 Depth=1
	movq	16(%rsp), %rax                  ## 8-byte Reload
	leaq	(%rax,%rbx,4), %rdi
	xorl	%eax, %eax
LBB5_102:                               ## %"for head2_conv.s0.n.ni.ni.us10.us.7"
                                        ##   Parent Loop BB5_50 Depth=1
                                        ## =>  This Inner Loop Header: Depth=2
	vmovups	%ymm7, -224(%rdi)
	vmovups	%ymm7, -192(%rdi)
	vmovups	%ymm7, -160(%rdi)
	vmovups	%ymm7, -128(%rdi)
	vmovups	%ymm7, -96(%rdi)
	vmovups	%ymm7, -64(%rdi)
	vmovups	%ymm7, -32(%rdi)
	vmovups	%ymm7, (%rdi)
	addq	$8, %rax
	addq	$256, %rdi                      ## imm = 0x100
	cmpq	%rax, %rcx
	jne	LBB5_102
LBB5_103:                               ## %"end for head2_conv.s0.n.ni.ni.loopexit.us16.us.7.unr-lcssa"
                                        ##   in Loop: Header=BB5_50 Depth=1
	testb	$7, %r10b
	je	LBB5_106
## %bb.104:                             ## %"for head2_conv.s0.n.ni.ni.us10.us.7.epil.preheader"
                                        ##   in Loop: Header=BB5_50 Depth=1
	movq	-88(%rsp), %rsi                 ## 8-byte Reload
	leaq	(%rsi,%rax,8), %rax
	addq	%rbx, %rax
	movq	-128(%rsp), %rdi                ## 8-byte Reload
	leaq	(%rdi,%rax,4), %rax
	xorl	%edi, %edi
LBB5_105:                               ## %"for head2_conv.s0.n.ni.ni.us10.us.7.epil"
                                        ##   Parent Loop BB5_50 Depth=1
                                        ## =>  This Inner Loop Header: Depth=2
	vmovups	%ymm7, (%rax,%rdi)
	addq	$32, %rdi
	cmpq	%rdi, %r15
	jne	LBB5_105
LBB5_106:                               ## %"end for head2_conv.s0.n.ni.ni.loopexit.us16.us.7"
                                        ##   in Loop: Header=BB5_50 Depth=1
	cmpq	$7, %r13
	jae	LBB5_108
## %bb.107:                             ##   in Loop: Header=BB5_50 Depth=1
	xorl	%eax, %eax
	jmp	LBB5_110
LBB5_108:                               ## %"for head2_conv.s0.n.ni.ni.us10.us.8.preheader"
                                        ##   in Loop: Header=BB5_50 Depth=1
	movq	8(%rsp), %rax                   ## 8-byte Reload
	leaq	(%rax,%rbx,4), %rdi
	xorl	%eax, %eax
LBB5_109:                               ## %"for head2_conv.s0.n.ni.ni.us10.us.8"
                                        ##   Parent Loop BB5_50 Depth=1
                                        ## =>  This Inner Loop Header: Depth=2
	vmovups	%ymm8, -224(%rdi)
	vmovups	%ymm8, -192(%rdi)
	vmovups	%ymm8, -160(%rdi)
	vmovups	%ymm8, -128(%rdi)
	vmovups	%ymm8, -96(%rdi)
	vmovups	%ymm8, -64(%rdi)
	vmovups	%ymm8, -32(%rdi)
	vmovups	%ymm8, (%rdi)
	addq	$8, %rax
	addq	$256, %rdi                      ## imm = 0x100
	cmpq	%rax, %rcx
	jne	LBB5_109
LBB5_110:                               ## %"end for head2_conv.s0.n.ni.ni.loopexit.us16.us.8.unr-lcssa"
                                        ##   in Loop: Header=BB5_50 Depth=1
	testb	$7, %r10b
	je	LBB5_113
## %bb.111:                             ## %"for head2_conv.s0.n.ni.ni.us10.us.8.epil.preheader"
                                        ##   in Loop: Header=BB5_50 Depth=1
	movq	-96(%rsp), %rsi                 ## 8-byte Reload
	leaq	(%rsi,%rax,8), %rax
	addq	%rbx, %rax
	movq	-128(%rsp), %rdi                ## 8-byte Reload
	leaq	(%rdi,%rax,4), %rax
	xorl	%edi, %edi
LBB5_112:                               ## %"for head2_conv.s0.n.ni.ni.us10.us.8.epil"
                                        ##   Parent Loop BB5_50 Depth=1
                                        ## =>  This Inner Loop Header: Depth=2
	vmovups	%ymm8, (%rax,%rdi)
	addq	$32, %rdi
	cmpq	%rdi, %r15
	jne	LBB5_112
LBB5_113:                               ## %"end for head2_conv.s0.n.ni.ni.loopexit.us16.us.8"
                                        ##   in Loop: Header=BB5_50 Depth=1
	cmpq	$7, %r13
	jae	LBB5_115
## %bb.114:                             ##   in Loop: Header=BB5_50 Depth=1
	xorl	%eax, %eax
	jmp	LBB5_117
LBB5_115:                               ## %"for head2_conv.s0.n.ni.ni.us10.us.9.preheader"
                                        ##   in Loop: Header=BB5_50 Depth=1
	movq	(%rsp), %rax                    ## 8-byte Reload
	leaq	(%rax,%rbx,4), %rdi
	xorl	%eax, %eax
LBB5_116:                               ## %"for head2_conv.s0.n.ni.ni.us10.us.9"
                                        ##   Parent Loop BB5_50 Depth=1
                                        ## =>  This Inner Loop Header: Depth=2
	vmovups	%ymm9, -224(%rdi)
	vmovups	%ymm9, -192(%rdi)
	vmovups	%ymm9, -160(%rdi)
	vmovups	%ymm9, -128(%rdi)
	vmovups	%ymm9, -96(%rdi)
	vmovups	%ymm9, -64(%rdi)
	vmovups	%ymm9, -32(%rdi)
	vmovups	%ymm9, (%rdi)
	addq	$8, %rax
	addq	$256, %rdi                      ## imm = 0x100
	cmpq	%rax, %rcx
	jne	LBB5_116
LBB5_117:                               ## %"end for head2_conv.s0.n.ni.ni.loopexit.us16.us.9.unr-lcssa"
                                        ##   in Loop: Header=BB5_50 Depth=1
	testb	$7, %r10b
	je	LBB5_120
## %bb.118:                             ## %"for head2_conv.s0.n.ni.ni.us10.us.9.epil.preheader"
                                        ##   in Loop: Header=BB5_50 Depth=1
	leaq	(%r9,%rax,8), %rax
	addq	%rbx, %rax
	movq	-128(%rsp), %rdi                ## 8-byte Reload
	leaq	(%rdi,%rax,4), %rax
	xorl	%edi, %edi
LBB5_119:                               ## %"for head2_conv.s0.n.ni.ni.us10.us.9.epil"
                                        ##   Parent Loop BB5_50 Depth=1
                                        ## =>  This Inner Loop Header: Depth=2
	vmovups	%ymm9, (%rax,%rdi)
	addq	$32, %rdi
	cmpq	%rdi, %r15
	jne	LBB5_119
LBB5_120:                               ## %"end for head2_conv.s0.n.ni.ni.loopexit.us16.us.9"
                                        ##   in Loop: Header=BB5_50 Depth=1
	cmpq	$7, %r13
	jae	LBB5_122
## %bb.121:                             ##   in Loop: Header=BB5_50 Depth=1
	xorl	%eax, %eax
	jmp	LBB5_124
LBB5_122:                               ## %"for head2_conv.s0.n.ni.ni.us10.us.10.preheader"
                                        ##   in Loop: Header=BB5_50 Depth=1
	leaq	(%r11,%rbx,4), %rdi
	xorl	%eax, %eax
LBB5_123:                               ## %"for head2_conv.s0.n.ni.ni.us10.us.10"
                                        ##   Parent Loop BB5_50 Depth=1
                                        ## =>  This Inner Loop Header: Depth=2
	vmovups	%ymm10, -224(%rdi)
	vmovups	%ymm10, -192(%rdi)
	vmovups	%ymm10, -160(%rdi)
	vmovups	%ymm10, -128(%rdi)
	vmovups	%ymm10, -96(%rdi)
	vmovups	%ymm10, -64(%rdi)
	vmovups	%ymm10, -32(%rdi)
	vmovups	%ymm10, (%rdi)
	addq	$8, %rax
	addq	$256, %rdi                      ## imm = 0x100
	cmpq	%rax, %rcx
	jne	LBB5_123
LBB5_124:                               ## %"end for head2_conv.s0.n.ni.ni.loopexit.us16.us.10.unr-lcssa"
                                        ##   in Loop: Header=BB5_50 Depth=1
	testb	$7, %r10b
	je	LBB5_127
## %bb.125:                             ## %"for head2_conv.s0.n.ni.ni.us10.us.10.epil.preheader"
                                        ##   in Loop: Header=BB5_50 Depth=1
	leaq	(%r8,%rax,8), %rax
	addq	%rbx, %rax
	movq	-128(%rsp), %rdi                ## 8-byte Reload
	leaq	(%rdi,%rax,4), %rax
	xorl	%edi, %edi
LBB5_126:                               ## %"for head2_conv.s0.n.ni.ni.us10.us.10.epil"
                                        ##   Parent Loop BB5_50 Depth=1
                                        ## =>  This Inner Loop Header: Depth=2
	vmovups	%ymm10, (%rax,%rdi)
	addq	$32, %rdi
	cmpq	%rdi, %r15
	jne	LBB5_126
LBB5_127:                               ## %"end for head2_conv.s0.n.ni.ni.loopexit.us16.us.10"
                                        ##   in Loop: Header=BB5_50 Depth=1
	cmpq	$7, %r13
	jae	LBB5_129
## %bb.128:                             ##   in Loop: Header=BB5_50 Depth=1
	xorl	%eax, %eax
	jmp	LBB5_131
LBB5_129:                               ## %"for head2_conv.s0.n.ni.ni.us10.us.11.preheader"
                                        ##   in Loop: Header=BB5_50 Depth=1
	movq	-8(%rsp), %rax                  ## 8-byte Reload
	leaq	(%rax,%rbx,4), %rdi
	xorl	%eax, %eax
LBB5_130:                               ## %"for head2_conv.s0.n.ni.ni.us10.us.11"
                                        ##   Parent Loop BB5_50 Depth=1
                                        ## =>  This Inner Loop Header: Depth=2
	vmovups	%ymm11, -224(%rdi)
	vmovups	%ymm11, -192(%rdi)
	vmovups	%ymm11, -160(%rdi)
	vmovups	%ymm11, -128(%rdi)
	vmovups	%ymm11, -96(%rdi)
	vmovups	%ymm11, -64(%rdi)
	vmovups	%ymm11, -32(%rdi)
	vmovups	%ymm11, (%rdi)
	addq	$8, %rax
	addq	$256, %rdi                      ## imm = 0x100
	cmpq	%rax, %rcx
	jne	LBB5_130
LBB5_131:                               ## %"end for head2_conv.s0.n.ni.ni.loopexit.us16.us.11.unr-lcssa"
                                        ##   in Loop: Header=BB5_50 Depth=1
	incq	%rbp
	testb	$7, %r10b
	je	LBB5_134
## %bb.132:                             ## %"for head2_conv.s0.n.ni.ni.us10.us.11.epil.preheader"
                                        ##   in Loop: Header=BB5_50 Depth=1
	leaq	(%r12,%rax,8), %rax
	addq	%rbx, %rax
	movq	-128(%rsp), %rdi                ## 8-byte Reload
	leaq	(%rdi,%rax,4), %rax
	xorl	%edi, %edi
LBB5_133:                               ## %"for head2_conv.s0.n.ni.ni.us10.us.11.epil"
                                        ##   Parent Loop BB5_50 Depth=1
                                        ## =>  This Inner Loop Header: Depth=2
	vmovups	%ymm11, (%rax,%rdi)
	addq	$32, %rdi
	cmpq	%rdi, %r15
	jne	LBB5_133
	jmp	LBB5_134
LBB5_3:                                 ## %"for head2_conv.s0.w.wi.us.preheader"
	movl	%ecx, %eax
	movq	%rax, -40(%rsp)                 ## 8-byte Spill
	movl	%esi, %eax
	movq	%rax, -72(%rsp)                 ## 8-byte Spill
	leal	(,%r13,8), %ecx
	subl	%ecx, %ebx
	leal	(%r10,%r10,4), %ecx
	subl	%ecx, %ebx
	movq	-128(%rsp), %rdx                ## 8-byte Reload
	leaq	992(%rdx), %rax
	movq	%rax, -96(%rsp)                 ## 8-byte Spill
	shrl	%r15d
	leal	(%r15,%r15,2), %ecx
	shll	$4, %ecx
	addl	%esi, %ecx
	shll	$3, %r8d
	leal	(%r8,%r8,2), %eax
	subl	%eax, %ecx
	addl	$24, %ecx
	movq	-80(%rsp), %rsi                 ## 8-byte Reload
	imull	%esi, %ecx
	leal	(%r11,%r11,4), %eax
	leal	(%rcx,%rax,8), %eax
	leal	(%rax,%r13,8), %eax
	movl	%eax, -64(%rsp)                 ## 4-byte Spill
	leal	(,%rsi,8), %eax
	leal	(%rax,%rax,2), %eax
	movl	%eax, -48(%rsp)                 ## 4-byte Spill
	leaq	96(%rdx), %rax
	movq	%rax, -88(%rsp)                 ## 8-byte Spill
	xorl	%eax, %eax
	movq	%rax, -104(%rsp)                ## 8-byte Spill
	jmp	LBB5_4
LBB5_38:                                ## %"end for head2_conv.s0.c.ci.split.us.us.split"
                                        ##   in Loop: Header=BB5_4 Depth=1
	movl	-48(%rsp), %eax                 ## 4-byte Reload
	addl	%eax, -64(%rsp)                 ## 4-byte Folded Spill
	cmpl	%r9d, -104(%rsp)                ## 4-byte Folded Reload
	movl	%r14d, %ebx
	je	LBB5_135
LBB5_4:                                 ## %"for head2_conv.s0.w.wi.us"
                                        ## =>This Loop Header: Depth=1
                                        ##     Child Loop BB5_5 Depth 2
                                        ##       Child Loop BB5_6 Depth 3
                                        ##         Child Loop BB5_42 Depth 4
                                        ##         Child Loop BB5_45 Depth 4
                                        ##         Child Loop BB5_35 Depth 4
	movl	%ebx, %r14d
	movq	-104(%rsp), %rax                ## 8-byte Reload
	incl	%eax
	movq	%rax, -104(%rsp)                ## 8-byte Spill
	movl	-64(%rsp), %eax                 ## 4-byte Reload
	movl	%eax, -112(%rsp)                ## 4-byte Spill
	xorl	%eax, %eax
	movq	%rax, -120(%rsp)                ## 8-byte Spill
	jmp	LBB5_5
LBB5_37:                                ## %"end for head2_conv.s0.n.ni.ni.rebased.loopexit.us.us"
                                        ##   in Loop: Header=BB5_5 Depth=2
	movq	-120(%rsp), %rcx                ## 8-byte Reload
	incq	%rcx
	movl	-112(%rsp), %eax                ## 4-byte Reload
	addl	-80(%rsp), %eax                 ## 4-byte Folded Reload
	movl	%eax, -112(%rsp)                ## 4-byte Spill
	movq	%rcx, %rax
	movq	%rcx, -120(%rsp)                ## 8-byte Spill
	cmpq	$12, %rcx
	je	LBB5_38
LBB5_5:                                 ## %"for head2_conv.s0.c.ci.us.us"
                                        ##   Parent Loop BB5_4 Depth=1
                                        ## =>  This Loop Header: Depth=2
                                        ##       Child Loop BB5_6 Depth 3
                                        ##         Child Loop BB5_42 Depth 4
                                        ##         Child Loop BB5_45 Depth 4
                                        ##         Child Loop BB5_35 Depth 4
	movq	-72(%rsp), %rax                 ## 8-byte Reload
	movq	-120(%rsp), %rcx                ## 8-byte Reload
	leaq	(%rcx,%rax), %r10
	movl	%r14d, %edi
	movl	-112(%rsp), %r12d               ## 4-byte Reload
	xorl	%r8d, %r8d
	jmp	LBB5_6
LBB5_36:                                ## %"end for head2_conv.s0.n.ni.nii.us.us"
                                        ##   in Loop: Header=BB5_6 Depth=3
	incq	%r8
	addl	$8, %r12d
	addl	$-8, %edi
	cmpq	-40(%rsp), %r8                  ## 8-byte Folded Reload
	je	LBB5_37
LBB5_6:                                 ## %"for head2_conv.s0.n.ni.ni.rebased.us.us"
                                        ##   Parent Loop BB5_4 Depth=1
                                        ##     Parent Loop BB5_5 Depth=2
                                        ## =>    This Loop Header: Depth=3
                                        ##         Child Loop BB5_42 Depth 4
                                        ##         Child Loop BB5_45 Depth 4
                                        ##         Child Loop BB5_35 Depth 4
	cmpl	$8, %edi
	movl	$8, %eax
	cmovll	%edi, %eax
	leal	(,%r8,8), %ecx
	movl	%r14d, %r15d
	subl	%ecx, %r15d
	cmpl	$8, %r15d
	movl	$8, %ecx
	cmovgel	%ecx, %r15d
	movq	-24(%rsp), %rcx                 ## 8-byte Reload
	addl	%r8d, %ecx
	shll	$3, %ecx
	cmpl	%ecx, -32(%rsp)                 ## 4-byte Folded Reload
	jle	LBB5_36
## %bb.7:                               ## %"for head2_conv.s0.n.ni.nii.preheader.us.us"
                                        ##   in Loop: Header=BB5_6 Depth=3
	movslq	%r12d, %r11
	movq	-56(%rsp), %rcx                 ## 8-byte Reload
	vmovss	(%rcx,%r10,4), %xmm0            ## xmm0 = mem[0],zero,zero,zero
	cmpl	$32, %r15d
	jae	LBB5_39
## %bb.8:                               ##   in Loop: Header=BB5_6 Depth=3
	xorl	%ecx, %ecx
	jmp	LBB5_47
LBB5_39:                                ## %vector.ph
                                        ##   in Loop: Header=BB5_6 Depth=3
	movl	%eax, %r13d
	andl	$-32, %r13d
	addq	$-32, %r13
	shrq	$5, %r13
	movl	%r15d, %ecx
	andl	$-32, %ecx
	addq	$-32, %rcx
	movq	%rcx, %rbp
	shrq	$5, %rbp
	incq	%rbp
	vbroadcastss	%xmm0, %ymm1
	cmpq	$224, %rcx
	jae	LBB5_41
## %bb.40:                              ##   in Loop: Header=BB5_6 Depth=3
	xorl	%edx, %edx
	jmp	LBB5_43
LBB5_41:                                ## %vector.ph.new
                                        ##   in Loop: Header=BB5_6 Depth=3
	movq	-96(%rsp), %rcx                 ## 8-byte Reload
	leaq	(%rcx,%r11,4), %rcx
	leaq	1(%r13), %rbx
	andq	$-8, %rbx
	negq	%rbx
	xorl	%edx, %edx
LBB5_42:                                ## %vector.body
                                        ##   Parent Loop BB5_4 Depth=1
                                        ##     Parent Loop BB5_5 Depth=2
                                        ##       Parent Loop BB5_6 Depth=3
                                        ## =>      This Inner Loop Header: Depth=4
	vmovups	%ymm1, -992(%rcx,%rdx,4)
	vmovups	%ymm1, -960(%rcx,%rdx,4)
	vmovups	%ymm1, -928(%rcx,%rdx,4)
	vmovups	%ymm1, -896(%rcx,%rdx,4)
	vmovups	%ymm1, -864(%rcx,%rdx,4)
	vmovups	%ymm1, -832(%rcx,%rdx,4)
	vmovups	%ymm1, -800(%rcx,%rdx,4)
	vmovups	%ymm1, -768(%rcx,%rdx,4)
	vmovups	%ymm1, -736(%rcx,%rdx,4)
	vmovups	%ymm1, -704(%rcx,%rdx,4)
	vmovups	%ymm1, -672(%rcx,%rdx,4)
	vmovups	%ymm1, -640(%rcx,%rdx,4)
	vmovups	%ymm1, -608(%rcx,%rdx,4)
	vmovups	%ymm1, -576(%rcx,%rdx,4)
	vmovups	%ymm1, -544(%rcx,%rdx,4)
	vmovups	%ymm1, -512(%rcx,%rdx,4)
	vmovups	%ymm1, -480(%rcx,%rdx,4)
	vmovups	%ymm1, -448(%rcx,%rdx,4)
	vmovups	%ymm1, -416(%rcx,%rdx,4)
	vmovups	%ymm1, -384(%rcx,%rdx,4)
	vmovups	%ymm1, -352(%rcx,%rdx,4)
	vmovups	%ymm1, -320(%rcx,%rdx,4)
	vmovups	%ymm1, -288(%rcx,%rdx,4)
	vmovups	%ymm1, -256(%rcx,%rdx,4)
	vmovups	%ymm1, -224(%rcx,%rdx,4)
	vmovups	%ymm1, -192(%rcx,%rdx,4)
	vmovups	%ymm1, -160(%rcx,%rdx,4)
	vmovups	%ymm1, -128(%rcx,%rdx,4)
	vmovups	%ymm1, -96(%rcx,%rdx,4)
	vmovups	%ymm1, -64(%rcx,%rdx,4)
	vmovups	%ymm1, -32(%rcx,%rdx,4)
	vmovups	%ymm1, (%rcx,%rdx,4)
	addq	$256, %rdx                      ## imm = 0x100
	addq	$8, %rbx
	jne	LBB5_42
LBB5_43:                                ## %middle.block.unr-lcssa
                                        ##   in Loop: Header=BB5_6 Depth=3
	movl	%r15d, %ecx
	andl	$-32, %ecx
	testb	$7, %bpl
	je	LBB5_46
## %bb.44:                              ## %vector.body.epil.preheader
                                        ##   in Loop: Header=BB5_6 Depth=3
	incb	%r13b
	movzbl	%r13b, %ebx
	andl	$7, %ebx
	shlq	$7, %rbx
	addq	%r11, %rdx
	movq	-88(%rsp), %rsi                 ## 8-byte Reload
	leaq	(%rsi,%rdx,4), %rdx
	xorl	%esi, %esi
LBB5_45:                                ## %vector.body.epil
                                        ##   Parent Loop BB5_4 Depth=1
                                        ##     Parent Loop BB5_5 Depth=2
                                        ##       Parent Loop BB5_6 Depth=3
                                        ## =>      This Inner Loop Header: Depth=4
	vmovups	%ymm1, -96(%rdx,%rsi)
	vmovups	%ymm1, -64(%rdx,%rsi)
	vmovups	%ymm1, -32(%rdx,%rsi)
	vmovups	%ymm1, (%rdx,%rsi)
	subq	$-128, %rsi
	cmpq	%rsi, %rbx
	jne	LBB5_45
LBB5_46:                                ## %middle.block
                                        ##   in Loop: Header=BB5_6 Depth=3
	cmpq	%r15, %rcx
	je	LBB5_36
LBB5_47:                                ## %"for head2_conv.s0.n.ni.nii.us.us.preheader"
                                        ##   in Loop: Header=BB5_6 Depth=3
	movq	-128(%rsp), %rdx                ## 8-byte Reload
	leaq	(%rdx,%r11,4), %rdx
LBB5_35:                                ## %"for head2_conv.s0.n.ni.nii.us.us"
                                        ##   Parent Loop BB5_4 Depth=1
                                        ##     Parent Loop BB5_5 Depth=2
                                        ##       Parent Loop BB5_6 Depth=3
                                        ## =>      This Inner Loop Header: Depth=4
	vmovss	%xmm0, (%rdx,%rcx,4)
	incq	%rcx
	cmpq	%rcx, %rax
	jne	LBB5_35
	jmp	LBB5_36
LBB5_135:                               ## %destructor_block
	xorl	%eax, %eax
	addq	$72, %rsp
	popq	%rbx
	popq	%r12
	popq	%r13
	popq	%r14
	popq	%r15
	popq	%rbp
	vzeroupper
	retq
                                        ## -- End function
	.p2align	4, 0x90                         ## -- Begin function train_cost_model.par_for.head2_conv.s1.n.n.n
_train_cost_model.par_for.head2_conv.s1.n.n.n: ## @train_cost_model.par_for.head2_conv.s1.n.n.n
## %bb.0:                               ## %entry
	pushq	%rbp
	pushq	%r15
	pushq	%r14
	pushq	%r13
	pushq	%r12
	pushq	%rbx
	subq	$224, %rsp
	movq	%rdx, %rcx
	movl	%esi, %r14d
	movl	12(%rdx), %r8d
	movl	16(%rdx), %ebp
	movl	20(%rdx), %r9d
	movl	24(%rdx), %r13d
	movl	%esi, %r10d
	sarl	$31, %r10d
	xorl	%edi, %edi
	testl	%ebp, %ebp
	sete	%dil
	movl	%edi, %ebx
	negl	%ebx
	movl	%ebp, %esi
	subl	%r10d, %r14d
	orl	%ebp, %ebx
	movl	%r14d, %eax
	cltd
	idivl	%ebx
	sarl	$31, %esi
	movl	%esi, %r11d
	xorl	%ebp, %r11d
	movl	%esi, %r12d
	notl	%r12d
	addl	%r12d, %r11d
	andl	%r10d, %r11d
	addl	%edx, %r11d
	leal	-1(%rdi), %r15d
	andl	%r15d, %r11d
	addl	%edi, %ebp
	movl	%r14d, %eax
	cltd
	idivl	%ebp
	leal	(,%r11,8), %ebx
	leal	(%rbx,%rbx,4), %edx
	subl	%esi, %r12d
	andl	%r10d, %r12d
	addl	%eax, %r12d
	andl	%r15d, %r12d
	movl	%r12d, %eax
	andl	$-2, %eax
	movl	%eax, -68(%rsp)                 ## 4-byte Spill
	subl	%eax, %r9d
	subl	%r8d, %r13d
	movl	%r13d, %edi
	movl	%edx, -116(%rsp)                ## 4-byte Spill
	subl	%edx, %edi
	movl	%edi, %eax
	sarl	$3, %eax
	cmpl	$5, %eax
	movl	$5, %esi
	cmovgel	%esi, %eax
	xorl	%ebp, %ebp
	cmpl	$7, %edi
	cmovlel	%ebp, %eax
	movq	%rax, -24(%rsp)                 ## 8-byte Spill
	cmpl	$2, %r9d
	movl	$2, %r15d
	cmovll	%r9d, %r15d
	movq	%rdi, -104(%rsp)                ## 8-byte Spill
	leal	7(%rdi), %edx
	sarl	$3, %edx
	cmpl	$5, %edx
	cmovgel	%esi, %edx
	testl	%r9d, %r9d
	cmovlel	%ebp, %r15d
	jle	LBB6_35
## %bb.1:                               ## %"for head2_conv.s1.w.wi.preheader"
	movl	%r13d, %r10d
	movl	(%rcx), %ebp
	movslq	8(%rcx), %r13
	movl	28(%rcx), %r8d
	movq	32(%rcx), %rax
	movq	%rax, -96(%rsp)                 ## 8-byte Spill
	movq	48(%rcx), %r14
	movq	64(%rcx), %rax
	movq	%rax, -48(%rsp)                 ## 8-byte Spill
	movq	-24(%rsp), %rax                 ## 8-byte Reload
	subl	%eax, %edx
	movl	%r12d, %esi
	andl	$1, %esi
	shll	$2, %esi
	leal	(%rsi,%rsi,2), %r9d
	movslq	4(%rcx), %rcx
	testl	%edx, %edx
	movq	%r14, -88(%rsp)                 ## 8-byte Spill
	movq	%rbp, 48(%rsp)                  ## 8-byte Spill
	movq	%r8, 40(%rsp)                   ## 8-byte Spill
	jle	LBB6_26
## %bb.2:                               ## %"for head2_conv.s1.w.wi.us.preheader"
	movl	%edx, %edi
	sarl	$31, %edi
	andnl	%edx, %edi, %edx
	imull	$39, %r13d, %edi
	movl	%edi, 24(%rsp)                  ## 4-byte Spill
	movslq	-116(%rsp), %rsi                ## 4-byte Folded Reload
	movq	%rsi, 56(%rsp)                  ## 8-byte Spill
	movl	%eax, %edi
	movq	%rdi, 192(%rsp)                 ## 8-byte Spill
	movl	%edx, %edx
	movq	%rdx, 184(%rsp)                 ## 8-byte Spill
	movl	%r9d, %edi
	movl	%r15d, %edx
	movq	%rdx, 96(%rsp)                  ## 8-byte Spill
	leal	(,%rax,8), %eax
	subl	%eax, %r10d
	leal	(%rbx,%rbx,4), %edx
	subl	%edx, %r10d
	movl	%r10d, -124(%rsp)               ## 4-byte Spill
	movq	%rdi, 112(%rsp)                 ## 8-byte Spill
	leaq	(%r14,%rdi,4), %rdx
	movq	%rdx, 88(%rsp)                  ## 8-byte Spill
	leaq	(,%rcx,4), %rdx
	leaq	(%rdx,%rdx,2), %rdx
	movq	%rdx, 176(%rsp)                 ## 8-byte Spill
	leaq	(,%rsi,4), %rdx
	leaq	(%rdx,%r13,8), %rdi
	movq	%rdi, 80(%rsp)                  ## 8-byte Spill
	shrl	%r12d
	imull	$39, %r13d, %edi
	movl	%edi, 20(%rsp)                  ## 4-byte Spill
	leal	(%r11,%r11,4), %edi
	leal	(%rax,%rdi,8), %eax
	movl	%eax, 36(%rsp)                  ## 4-byte Spill
	movl	%r13d, %eax
	imull	%r12d, %eax
	imull	$78, %eax, %edi
	leaq	(%rdx,%r13,4), %rax
	movq	%rax, 72(%rsp)                  ## 8-byte Spill
	leal	(%r12,%r12,2), %ebx
	shll	$4, %ebx
	addl	%r9d, %ebx
	leal	(,%r8,8), %eax
	leal	(%rax,%rax,2), %eax
	subl	%eax, %ebx
	addl	$24, %ebx
	imull	%ebp, %ebx
	leal	(,%rbp,8), %eax
	leal	(%rax,%rax,2), %eax
	movl	%eax, 16(%rsp)                  ## 4-byte Spill
	leaq	(,%r13,4), %r15
	leaq	(%r15,%r15,2), %rax
	movq	%rax, 168(%rsp)                 ## 8-byte Spill
	movq	-48(%rsp), %r14                 ## 8-byte Reload
	leaq	96(%r14), %rax
	movq	%rax, 64(%rsp)                  ## 8-byte Spill
	movq	-96(%rsp), %r11                 ## 8-byte Reload
	leaq	96(%r11), %rax
	movq	%rax, 160(%rsp)                 ## 8-byte Spill
	xorl	%eax, %eax
	jmp	LBB6_3
	.p2align	4, 0x90
LBB6_16:                                ## %"end for head2_conv.s1.c.ci.split.us.us"
                                        ##   in Loop: Header=BB6_3 Depth=1
	movl	32(%rsp), %edi                  ## 4-byte Reload
	addl	20(%rsp), %edi                  ## 4-byte Folded Reload
	movl	28(%rsp), %ebx                  ## 4-byte Reload
	addl	16(%rsp), %ebx                  ## 4-byte Folded Reload
	movq	104(%rsp), %rax                 ## 8-byte Reload
	cmpq	96(%rsp), %rax                  ## 8-byte Folded Reload
	movq	40(%rsp), %r8                   ## 8-byte Reload
	je	LBB6_35
LBB6_3:                                 ## %"for head2_conv.s1.w.wi.us"
                                        ## =>This Loop Header: Depth=1
                                        ##     Child Loop BB6_4 Depth 2
                                        ##       Child Loop BB6_6 Depth 3
                                        ##         Child Loop BB6_7 Depth 4
                                        ##       Child Loop BB6_10 Depth 3
                                        ##         Child Loop BB6_12 Depth 4
                                        ##           Child Loop BB6_20 Depth 5
                                        ##           Child Loop BB6_24 Depth 5
	movl	%edi, 32(%rsp)                  ## 4-byte Spill
	movslq	%edi, %rdx
	movq	80(%rsp), %rsi                  ## 8-byte Reload
	leaq	(%rsi,%rdx,4), %rsi
	movq	%rsi, 144(%rsp)                 ## 8-byte Spill
	movq	72(%rsp), %rsi                  ## 8-byte Reload
	leaq	(%rsi,%rdx,4), %rsi
	movq	%rsi, 136(%rsp)                 ## 8-byte Spill
	movq	56(%rsp), %rsi                  ## 8-byte Reload
	addq	%rdx, %rsi
	shlq	$2, %rsi
	movq	%rsi, 128(%rsp)                 ## 8-byte Spill
	leaq	1(%rax), %rsi
	movq	%rsi, 104(%rsp)                 ## 8-byte Spill
                                        ## kill: def $esi killed $esi killed $rsi def $rsi
	subl	%r8d, %esi
	movl	-68(%rsp), %edi                 ## 4-byte Reload
	addl	%edi, %esi
	shll	$3, %esi
	leal	(%rsi,%rsi,2), %esi
	movq	%rsi, 120(%rsp)                 ## 8-byte Spill
	addl	%edi, %eax
	imull	24(%rsp), %eax                  ## 4-byte Folded Reload
	cltq
	movq	%rax, -8(%rsp)                  ## 8-byte Spill
	movq	64(%rsp), %rax                  ## 8-byte Reload
	leaq	(%rax,%rdx,4), %rax
	movq	%rax, 208(%rsp)                 ## 8-byte Spill
	leaq	(%r14,%rdx,4), %rax
	movq	%rax, 200(%rsp)                 ## 8-byte Spill
	movl	%ebx, 28(%rsp)                  ## 4-byte Spill
	movq	88(%rsp), %rax                  ## 8-byte Reload
	movq	%rax, -64(%rsp)                 ## 8-byte Spill
	xorl	%edx, %edx
	jmp	LBB6_4
	.p2align	4, 0x90
LBB6_15:                                ## %"end for head2_conv.s1.n.ni.ni.rebased.loopexit.us.us"
                                        ##   in Loop: Header=BB6_4 Depth=2
	movq	152(%rsp), %rdx                 ## 8-byte Reload
	incq	%rdx
	addq	$4, -64(%rsp)                   ## 8-byte Folded Spill
	movq	48(%rsp), %rbp                  ## 8-byte Reload
	movq	-56(%rsp), %rax                 ## 8-byte Reload
	leal	(%rax,%rbp), %ebx
	cmpq	$12, %rdx
	movq	-96(%rsp), %r11                 ## 8-byte Reload
	movq	-48(%rsp), %r14                 ## 8-byte Reload
	je	LBB6_16
LBB6_4:                                 ## %"for head2_conv.s1.c.ci.us.us"
                                        ##   Parent Loop BB6_3 Depth=1
                                        ## =>  This Loop Header: Depth=2
                                        ##       Child Loop BB6_6 Depth 3
                                        ##         Child Loop BB6_7 Depth 4
                                        ##       Child Loop BB6_10 Depth 3
                                        ##         Child Loop BB6_12 Depth 4
                                        ##           Child Loop BB6_20 Depth 5
                                        ##           Child Loop BB6_24 Depth 5
	movl	%ebx, -80(%rsp)                 ## 4-byte Spill
	movq	112(%rsp), %rax                 ## 8-byte Reload
	movq	%rdx, 152(%rsp)                 ## 8-byte Spill
	addq	%rax, %rdx
	movq	120(%rsp), %rax                 ## 8-byte Reload
	movq	%rdx, -32(%rsp)                 ## 8-byte Spill
	addl	%edx, %eax
	imull	%ebp, %eax
	cltq
	movq	%rax, -40(%rsp)                 ## 8-byte Spill
	cmpl	$8, -104(%rsp)                  ## 4-byte Folded Reload
	jl	LBB6_9
## %bb.5:                               ## %"for head2_conv.s1.n.ni.ni.us.us.preheader"
                                        ##   in Loop: Header=BB6_4 Depth=2
	movq	128(%rsp), %rdx                 ## 8-byte Reload
	movq	136(%rsp), %rsi                 ## 8-byte Reload
	movq	144(%rsp), %rdi                 ## 8-byte Reload
	xorl	%r10d, %r10d
	.p2align	4, 0x90
LBB6_6:                                 ## %"for head2_conv.s1.n.ni.ni.us.us"
                                        ##   Parent Loop BB6_3 Depth=1
                                        ##     Parent Loop BB6_4 Depth=2
                                        ## =>    This Loop Header: Depth=3
                                        ##         Child Loop BB6_7 Depth 4
	movq	56(%rsp), %rax                  ## 8-byte Reload
	leaq	(%rax,%r10,8), %r9
	addq	-40(%rsp), %r9                  ## 8-byte Folded Reload
	vmovups	(%r11,%r9,4), %ymm0
	movl	$39, %eax
	movq	%r14, %rbp
	movq	-64(%rsp), %rbx                 ## 8-byte Reload
	movq	176(%rsp), %r8                  ## 8-byte Reload
	movq	168(%rsp), %r12                 ## 8-byte Reload
	.p2align	4, 0x90
LBB6_7:                                 ## %"for head2_conv.s1.r40$x.us.us"
                                        ##   Parent Loop BB6_3 Depth=1
                                        ##     Parent Loop BB6_4 Depth=2
                                        ##       Parent Loop BB6_6 Depth=3
                                        ## =>      This Inner Loop Header: Depth=4
	vbroadcastss	(%rbx), %ymm1
	vfmadd132ps	(%rbp,%rdx), %ymm0, %ymm1 ## ymm1 = (ymm1 * mem) + ymm0
	vbroadcastss	(%rbx,%rcx,4), %ymm2
	vfmadd132ps	(%rbp,%rsi), %ymm1, %ymm2 ## ymm2 = (ymm2 * mem) + ymm1
	vbroadcastss	(%rbx,%rcx,8), %ymm0
	vfmadd132ps	(%rbp,%rdi), %ymm2, %ymm0 ## ymm0 = (ymm0 * mem) + ymm2
	addq	%r8, %rbx
	addq	%r12, %rbp
	addq	$-3, %rax
	jne	LBB6_7
## %bb.8:                               ## %"end for head2_conv.s1.r40$x.us.us"
                                        ##   in Loop: Header=BB6_6 Depth=3
	vmovups	%ymm0, (%r11,%r9,4)
	incq	%r10
	addq	$32, %rdi
	addq	$32, %rsi
	addq	$32, %rdx
	cmpq	192(%rsp), %r10                 ## 8-byte Folded Reload
	jne	LBB6_6
LBB6_9:                                 ## %"end for head2_conv.s1.n.ni.ni.us.us"
                                        ##   in Loop: Header=BB6_4 Depth=2
	movslq	-80(%rsp), %rax                 ## 4-byte Folded Reload
	movq	%rax, -56(%rsp)                 ## 8-byte Spill
	leaq	(%r11,%rax,4), %rax
	movq	%rax, 216(%rsp)                 ## 8-byte Spill
	movl	-124(%rsp), %eax                ## 4-byte Reload
	movl	36(%rsp), %edx                  ## 4-byte Reload
	movl	%edx, -112(%rsp)                ## 4-byte Spill
	xorl	%esi, %esi
	movq	-88(%rsp), %rdi                 ## 8-byte Reload
	jmp	LBB6_10
	.p2align	4, 0x90
LBB6_14:                                ## %"end for head2_conv.s1.r40$x2.us.us"
                                        ##   in Loop: Header=BB6_10 Depth=3
	movq	-16(%rsp), %rsi                 ## 8-byte Reload
	incq	%rsi
	addl	$8, -112(%rsp)                  ## 4-byte Folded Spill
	movl	-120(%rsp), %eax                ## 4-byte Reload
	addl	$-8, %eax
	cmpq	184(%rsp), %rsi                 ## 8-byte Folded Reload
	je	LBB6_15
LBB6_10:                                ## %"for head2_conv.s1.n.ni.ni.rebased.us.us"
                                        ##   Parent Loop BB6_3 Depth=1
                                        ##     Parent Loop BB6_4 Depth=2
                                        ## =>    This Loop Header: Depth=3
                                        ##         Child Loop BB6_12 Depth 4
                                        ##           Child Loop BB6_20 Depth 5
                                        ##           Child Loop BB6_24 Depth 5
	cmpl	$8, %eax
	movl	$8, %r9d
	movl	%eax, -120(%rsp)                ## 4-byte Spill
	cmovll	%eax, %r9d
	leal	(,%rsi,8), %eax
	movl	-124(%rsp), %edx                ## 4-byte Reload
                                        ## kill: def $edx killed $edx def $rdx
	subl	%eax, %edx
	cmpl	$8, %edx
	movl	$8, %eax
	cmovgel	%eax, %edx
	movq	-24(%rsp), %rax                 ## 8-byte Reload
	movq	%rsi, -16(%rsp)                 ## 8-byte Spill
	leal	(%rax,%rsi), %ebp
	shll	$3, %ebp
	cmpl	%ebp, -104(%rsp)                ## 4-byte Folded Reload
	jle	LBB6_14
## %bb.11:                              ## %"for head2_conv.s1.r40$x1.us.us.us.preheader"
                                        ##   in Loop: Header=BB6_10 Depth=3
	movslq	-112(%rsp), %rax                ## 4-byte Folded Reload
	movq	200(%rsp), %rsi                 ## 8-byte Reload
	leaq	(%rsi,%rax,4), %r14
	movq	216(%rsp), %rsi                 ## 8-byte Reload
	leaq	(%rsi,%rax,4), %rsi
	movq	208(%rsp), %rbx                 ## 8-byte Reload
	leaq	(%rbx,%rax,4), %r8
	addq	-56(%rsp), %rax                 ## 8-byte Folded Reload
	movq	160(%rsp), %rbx                 ## 8-byte Reload
	leaq	(%rbx,%rax,4), %rax
	movl	%r9d, %ebx
	andl	$-16, %ebx
	addq	$-16, %rbx
	shrq	$4, %rbx
	incq	%rbx
	andq	$-2, %rbx
	negq	%rbx
	movq	%rbx, 8(%rsp)                   ## 8-byte Spill
	movl	%edx, %r12d
	andl	$-16, %r12d
	addq	$-16, %r12
	movq	%r12, %rbx
	shrq	$4, %rbx
	incq	%rbx
	movq	%rbx, -80(%rsp)                 ## 8-byte Spill
	addl	-116(%rsp), %ebp                ## 4-byte Folded Reload
	movslq	%ebp, %rbp
	movq	%rbp, (%rsp)                    ## 8-byte Spill
	movl	%edx, %ebx
	andl	$-16, %ebx
	xorl	%r10d, %r10d
	jmp	LBB6_12
	.p2align	4, 0x90
LBB6_25:                                ## %"end for head2_conv.s1.n.ni.nii.loopexit.us.us.us"
                                        ##   in Loop: Header=BB6_12 Depth=4
	incq	%r10
	addq	%r15, %r8
	addq	%r15, %r14
	cmpq	$39, %r10
	je	LBB6_14
LBB6_12:                                ## %"for head2_conv.s1.r40$x1.us.us.us"
                                        ##   Parent Loop BB6_3 Depth=1
                                        ##     Parent Loop BB6_4 Depth=2
                                        ##       Parent Loop BB6_10 Depth=3
                                        ## =>      This Loop Header: Depth=4
                                        ##           Child Loop BB6_20 Depth 5
                                        ##           Child Loop BB6_24 Depth 5
	movq	%r10, %rbp
	imulq	%rcx, %rbp
	addq	-32(%rsp), %rbp                 ## 8-byte Folded Reload
	vmovss	(%rdi,%rbp,4), %xmm0            ## xmm0 = mem[0],zero,zero,zero
	cmpl	$16, %edx
	jae	LBB6_17
## %bb.13:                              ##   in Loop: Header=BB6_12 Depth=4
	xorl	%ebp, %ebp
	jmp	LBB6_24
	.p2align	4, 0x90
LBB6_17:                                ## %vector.ph
                                        ##   in Loop: Header=BB6_12 Depth=4
	movq	%r13, %rdi
	vbroadcastss	%xmm0, %ymm1
	testq	%r12, %r12
	je	LBB6_18
## %bb.19:                              ## %vector.body.preheader
                                        ##   in Loop: Header=BB6_12 Depth=4
	movq	8(%rsp), %r13                   ## 8-byte Reload
	xorl	%r11d, %r11d
	.p2align	4, 0x90
LBB6_20:                                ## %vector.body
                                        ##   Parent Loop BB6_3 Depth=1
                                        ##     Parent Loop BB6_4 Depth=2
                                        ##       Parent Loop BB6_10 Depth=3
                                        ##         Parent Loop BB6_12 Depth=4
                                        ## =>        This Inner Loop Header: Depth=5
	vmovups	-96(%r8,%r11,4), %ymm2
	vmovups	-64(%r8,%r11,4), %ymm3
	vfmadd213ps	-96(%rax,%r11,4), %ymm1, %ymm2 ## ymm2 = (ymm1 * ymm2) + mem
	vfmadd213ps	-64(%rax,%r11,4), %ymm1, %ymm3 ## ymm3 = (ymm1 * ymm3) + mem
	vmovups	%ymm2, -96(%rax,%r11,4)
	vmovups	%ymm3, -64(%rax,%r11,4)
	vmovups	-32(%r8,%r11,4), %ymm2
	vmovups	(%r8,%r11,4), %ymm3
	vfmadd213ps	-32(%rax,%r11,4), %ymm1, %ymm2 ## ymm2 = (ymm1 * ymm2) + mem
	vfmadd213ps	(%rax,%r11,4), %ymm1, %ymm3 ## ymm3 = (ymm1 * ymm3) + mem
	vmovups	%ymm2, -32(%rax,%r11,4)
	vmovups	%ymm3, (%rax,%r11,4)
	addq	$32, %r11
	addq	$2, %r13
	jne	LBB6_20
## %bb.21:                              ## %middle.block.unr-lcssa
                                        ##   in Loop: Header=BB6_12 Depth=4
	testb	$1, -80(%rsp)                   ## 1-byte Folded Reload
	movq	%rdi, %r13
	je	LBB6_23
LBB6_22:                                ## %vector.body.epil
                                        ##   in Loop: Header=BB6_12 Depth=4
	movq	%r10, %rbp
	imulq	%r13, %rbp
	addq	-8(%rsp), %rbp                  ## 8-byte Folded Reload
	addq	(%rsp), %r11                    ## 8-byte Folded Reload
	movq	-40(%rsp), %rdi                 ## 8-byte Reload
	addq	%r11, %rdi
	addq	%rbp, %r11
	movq	-48(%rsp), %rbp                 ## 8-byte Reload
	vmovups	(%rbp,%r11,4), %ymm2
	vmovups	32(%rbp,%r11,4), %ymm3
	movq	-96(%rsp), %rbp                 ## 8-byte Reload
	vfmadd213ps	(%rbp,%rdi,4), %ymm1, %ymm2 ## ymm2 = (ymm1 * ymm2) + mem
	vfmadd213ps	32(%rbp,%rdi,4), %ymm1, %ymm3 ## ymm3 = (ymm1 * ymm3) + mem
	vmovups	%ymm2, (%rbp,%rdi,4)
	vmovups	%ymm3, 32(%rbp,%rdi,4)
LBB6_23:                                ## %middle.block
                                        ##   in Loop: Header=BB6_12 Depth=4
	movq	%rbx, %rbp
	cmpq	%rdx, %rbx
	movq	-88(%rsp), %rdi                 ## 8-byte Reload
	je	LBB6_25
	.p2align	4, 0x90
LBB6_24:                                ## %"for head2_conv.s1.n.ni.nii.us.us.us"
                                        ##   Parent Loop BB6_3 Depth=1
                                        ##     Parent Loop BB6_4 Depth=2
                                        ##       Parent Loop BB6_10 Depth=3
                                        ##         Parent Loop BB6_12 Depth=4
                                        ## =>        This Inner Loop Header: Depth=5
	vmovss	(%r14,%rbp,4), %xmm1            ## xmm1 = mem[0],zero,zero,zero
	vfmadd213ss	(%rsi,%rbp,4), %xmm0, %xmm1 ## xmm1 = (xmm0 * xmm1) + mem
	vmovss	%xmm1, (%rsi,%rbp,4)
	incq	%rbp
	cmpq	%rbp, %r9
	jne	LBB6_24
	jmp	LBB6_25
LBB6_18:                                ##   in Loop: Header=BB6_12 Depth=4
	xorl	%r11d, %r11d
	testb	$1, -80(%rsp)                   ## 1-byte Folded Reload
	movq	%rdi, %r13
	jne	LBB6_22
	jmp	LBB6_23
LBB6_26:                                ## %"for head2_conv.s1.w.wi.preheader.split"
	cmpl	$8, -104(%rsp)                  ## 4-byte Folded Reload
	jl	LBB6_35
## %bb.27:                              ## %"for head2_conv.s1.w.wi.us39.preheader"
	movslq	-116(%rsp), %rdx                ## 4-byte Folded Reload
	movl	-24(%rsp), %eax                 ## 4-byte Reload
	movq	%rax, -80(%rsp)                 ## 8-byte Spill
	movl	%r9d, %esi
	movl	%r15d, %eax
	movq	%rax, -16(%rsp)                 ## 8-byte Spill
	movq	-88(%rsp), %rax                 ## 8-byte Reload
	movq	%rsi, -8(%rsp)                  ## 8-byte Spill
	leaq	(%rax,%rsi,4), %rax
	movq	%rax, -56(%rsp)                 ## 8-byte Spill
	leaq	(,%rcx,4), %rax
	movq	%r13, %rdi
	leaq	(%rax,%rax,2), %r13
	movq	%rdx, -32(%rsp)                 ## 8-byte Spill
	leaq	(,%rdx,4), %rax
	leaq	(%rax,%rdi,8), %rdx
	movq	%rdx, -64(%rsp)                 ## 8-byte Spill
	shrl	%r12d
	imull	%edi, %r12d
	imull	$78, %r12d, %edx
	movl	%edx, -120(%rsp)                ## 4-byte Spill
	imull	$39, %edi, %edx
	movl	%edx, -124(%rsp)                ## 4-byte Spill
	leaq	(,%rdi,4), %rsi
	leaq	(%rsi,%rsi,2), %rdx
	leaq	(%rax,%rdi,4), %rax
	movq	%rax, -104(%rsp)                ## 8-byte Spill
	xorl	%eax, %eax
	movq	%rax, -112(%rsp)                ## 8-byte Spill
LBB6_28:                                ## %"for head2_conv.s1.w.wi.us39"
                                        ## =>This Loop Header: Depth=1
                                        ##     Child Loop BB6_29 Depth 2
                                        ##       Child Loop BB6_30 Depth 3
                                        ##         Child Loop BB6_31 Depth 4
	movslq	-120(%rsp), %rsi                ## 4-byte Folded Reload
	movq	-64(%rsp), %rax                 ## 8-byte Reload
	leaq	(%rax,%rsi,4), %rax
	movq	%rax, -88(%rsp)                 ## 8-byte Spill
	movq	-104(%rsp), %rax                ## 8-byte Reload
	leaq	(%rax,%rsi,4), %rax
	movq	%rax, 8(%rsp)                   ## 8-byte Spill
	addq	-32(%rsp), %rsi                 ## 8-byte Folded Reload
	shlq	$2, %rsi
	movq	%rsi, (%rsp)                    ## 8-byte Spill
	movq	-112(%rsp), %rsi                ## 8-byte Reload
	incq	%rsi
	movq	%rsi, %rax
	movq	%rsi, -112(%rsp)                ## 8-byte Spill
	subl	%r8d, %eax
	addl	-68(%rsp), %eax                 ## 4-byte Folded Reload
	shll	$3, %eax
	leal	(%rax,%rax,2), %eax
	movl	%eax, -40(%rsp)                 ## 4-byte Spill
	movq	-56(%rsp), %r11                 ## 8-byte Reload
	xorl	%ebp, %ebp
LBB6_29:                                ## %"for head2_conv.s1.c.ci.us19.us"
                                        ##   Parent Loop BB6_28 Depth=1
                                        ## =>  This Loop Header: Depth=2
                                        ##       Child Loop BB6_30 Depth 3
                                        ##         Child Loop BB6_31 Depth 4
	movq	-8(%rsp), %rax                  ## 8-byte Reload
	addl	%ebp, %eax
	addl	-40(%rsp), %eax                 ## 4-byte Folded Reload
	imull	48(%rsp), %eax                  ## 4-byte Folded Reload
	movslq	%eax, %r14
	movq	(%rsp), %rdi                    ## 8-byte Reload
	movq	8(%rsp), %r8                    ## 8-byte Reload
	movq	-88(%rsp), %rsi                 ## 8-byte Reload
	xorl	%eax, %eax
LBB6_30:                                ## %"for head2_conv.s1.n.ni.ni.us24.us"
                                        ##   Parent Loop BB6_28 Depth=1
                                        ##     Parent Loop BB6_29 Depth=2
                                        ## =>    This Loop Header: Depth=3
                                        ##         Child Loop BB6_31 Depth 4
	movq	-32(%rsp), %rbx                 ## 8-byte Reload
	leaq	(%rbx,%rax,8), %r12
	addq	%r14, %r12
	movq	-96(%rsp), %rbx                 ## 8-byte Reload
	vmovups	(%rbx,%r12,4), %ymm0
	movl	$39, %r9d
	movq	-48(%rsp), %r15                 ## 8-byte Reload
	movq	%r11, %r10
LBB6_31:                                ## %"for head2_conv.s1.r40$x.us29.us"
                                        ##   Parent Loop BB6_28 Depth=1
                                        ##     Parent Loop BB6_29 Depth=2
                                        ##       Parent Loop BB6_30 Depth=3
                                        ## =>      This Inner Loop Header: Depth=4
	vbroadcastss	(%r10), %ymm1
	vfmadd132ps	(%r15,%rdi), %ymm0, %ymm1 ## ymm1 = (ymm1 * mem) + ymm0
	vbroadcastss	(%r10,%rcx,4), %ymm2
	vfmadd132ps	(%r15,%r8), %ymm1, %ymm2 ## ymm2 = (ymm2 * mem) + ymm1
	vbroadcastss	(%r10,%rcx,8), %ymm0
	vfmadd132ps	(%r15,%rsi), %ymm2, %ymm0 ## ymm0 = (ymm0 * mem) + ymm2
	addq	%r13, %r10
	addq	%rdx, %r15
	addq	$-3, %r9
	jne	LBB6_31
## %bb.32:                              ## %"end for head2_conv.s1.r40$x.us32.us"
                                        ##   in Loop: Header=BB6_30 Depth=3
	movq	-96(%rsp), %rbx                 ## 8-byte Reload
	vmovups	%ymm0, (%rbx,%r12,4)
	incq	%rax
	addq	$32, %rsi
	addq	$32, %r8
	addq	$32, %rdi
	cmpq	-80(%rsp), %rax                 ## 8-byte Folded Reload
	jne	LBB6_30
## %bb.33:                              ## %"end for head2_conv.s1.n.ni.ni.loopexit.us38.us"
                                        ##   in Loop: Header=BB6_29 Depth=2
	movq	%rbp, %rax
	incq	%rax
	addq	$4, %r11
	movq	%rax, %rbp
	cmpq	$12, %rax
	jne	LBB6_29
## %bb.34:                              ## %"end for head2_conv.s1.c.ci.split.split.us.us"
                                        ##   in Loop: Header=BB6_28 Depth=1
	movl	-124(%rsp), %eax                ## 4-byte Reload
	addl	%eax, -120(%rsp)                ## 4-byte Folded Spill
	movq	-112(%rsp), %rax                ## 8-byte Reload
	cmpq	-16(%rsp), %rax                 ## 8-byte Folded Reload
	movq	40(%rsp), %r8                   ## 8-byte Reload
	jne	LBB6_28
LBB6_35:                                ## %destructor_block
	xorl	%eax, %eax
	addq	$224, %rsp
	popq	%rbx
	popq	%r12
	popq	%r13
	popq	%r14
	popq	%r15
	popq	%rbp
	vzeroupper
	retq
                                        ## -- End function
	.p2align	4, 0x90                         ## -- Begin function train_cost_model.par_for.head2_relu.s0.c.c.c
_train_cost_model.par_for.head2_relu.s0.c.c.c: ## @train_cost_model.par_for.head2_relu.s0.c.c.c
## %bb.0:                               ## %entry
	pushq	%rbp
	pushq	%r15
	pushq	%r14
	pushq	%r13
	pushq	%r12
	pushq	%rbx
	subq	$208, %rsp
                                        ## kill: def $esi killed $esi def $rsi
	movslq	(%rdx), %rax
	movq	%rax, (%rsp)                    ## 8-byte Spill
	movslq	4(%rdx), %r14
	movl	16(%rdx), %eax
	movq	%rax, -16(%rsp)                 ## 8-byte Spill
	movl	20(%rdx), %ecx
	movl	28(%rdx), %r8d
	movl	32(%rdx), %r12d
	movq	40(%rdx), %rbx
	movq	56(%rdx), %r11
	cmpl	%esi, 8(%rdx)
	movq	%rbx, -120(%rsp)                ## 8-byte Spill
	movq	%r11, -88(%rsp)                 ## 8-byte Spill
	jle	LBB7_59
## %bb.1:                               ## %then_bb
	movl	12(%rdx), %ebp
	movl	%esi, %r9d
	sarl	$3, %r9d
	movl	%esi, %eax
	andl	$-8, %eax
	subl	%r9d, %eax
	movl	%ebp, %edi
	sarl	$3, %edi
	movl	%esi, %edx
	andl	$7, %edx
	leal	(%rdx,%rdx,2), %r10d
	addl	$7, %ecx
	sarl	$3, %ecx
	movq	%rdi, %rdx
	movq	%rdi, -112(%rsp)                ## 8-byte Spill
	subl	%edi, %ecx
	jle	LBB7_48
## %bb.2:                               ## %then_bb.split.us
	subl	%r8d, %r12d
	movl	%ecx, %ecx
	movq	%rcx, -48(%rsp)                 ## 8-byte Spill
	cmpl	$7, %ebp
	movl	%r12d, -56(%rsp)                ## 4-byte Spill
	jle	LBB7_3
## %bb.8:                               ## %"for head2_relu.s0.w.wi.us.us.preheader"
	movq	%r14, %r13
	movl	%r13d, %edi
	movl	-112(%rsp), %edx                ## 4-byte Reload
	movl	%r10d, %ecx
	leaq	1(%rcx), %rbx
	movq	%rbx, 112(%rsp)                 ## 8-byte Spill
	addq	$2, %rcx
	movq	%rcx, 120(%rsp)                 ## 8-byte Spill
	andl	$-8, %ebp
	movl	%r12d, %ecx
	subl	%ebp, %ecx
	movl	%ecx, -128(%rsp)                ## 4-byte Spill
	movl	%edx, %ebp
	movq	%rdx, %r8
	movq	%rdx, 40(%rsp)                  ## 8-byte Spill
	andl	$3, %ebp
	imull	$168, %r9d, %ecx
	movl	%r13d, %ebx
	leal	(%rcx,%r10), %r15d
	addl	$2, %r15d
	imull	%r13d, %r15d
	imull	%r9d, %edi
	movq	-16(%rsp), %rdx                 ## 8-byte Reload
	subl	%edx, %eax
	leal	(%rcx,%r10), %r14d
	imull	%r14d, %ebx
	movl	%ebx, -104(%rsp)                ## 4-byte Spill
	shll	$3, %edx
	leal	(%rdx,%rdx,2), %edx
	subl	%edx, %r14d
	movl	%r8d, %edx
	andl	$-4, %edx
	movq	%rdx, -8(%rsp)                  ## 8-byte Spill
	andl	$7, %esi
	leal	(%rcx,%r10), %r8d
	incl	%r8d
	movq	%r13, %rcx
	imulq	%rsi, %rcx
	leaq	(%rcx,%rcx,2), %rcx
	leaq	(%r11,%rcx,4), %rcx
	movq	%rcx, 64(%rsp)                  ## 8-byte Spill
	imull	$168, %edi, %edi
	leal	(,%r13,8), %ecx
	leal	(%rcx,%rcx,2), %ecx
	movq	%rcx, 144(%rsp)                 ## 8-byte Spill
	movq	(%rsp), %rdx                    ## 8-byte Reload
	imulq	%rdx, %rsi
	movq	%r11, %rbx
	leaq	(%rsi,%rsi,2), %r11
	incl	%eax
	imull	%edx, %eax
	shll	$3, %eax
	leal	(%rax,%rax,2), %ecx
	leal	24(%r14), %r10d
	imull	%edx, %r10d
	movq	112(%rsp), %rax                 ## 8-byte Reload
	movq	%rax, %rsi
	imulq	%r13, %rsi
	imulq	%rdx, %rax
	movq	%rax, 112(%rsp)                 ## 8-byte Spill
	imull	%r13d, %r8d
	leal	25(%r14), %r9d
	imull	%edx, %r9d
	movq	120(%rsp), %rax                 ## 8-byte Reload
	imulq	%rax, %r13
	imulq	%rdx, %rax
	movq	%rax, 120(%rsp)                 ## 8-byte Spill
	addl	$26, %r14d
	imull	%edx, %r14d
	leal	(,%rdx,8), %eax
	leal	(%rax,%rax,2), %eax
	movq	%rax, 136(%rsp)                 ## 8-byte Spill
	movq	%rbp, 152(%rsp)                 ## 8-byte Spill
	shlq	$5, %rbp
	movq	%rbp, -40(%rsp)                 ## 8-byte Spill
	movq	-112(%rsp), %rax                ## 8-byte Reload
	leal	(,%rax,8), %eax
	movl	%eax, (%rsp)                    ## 4-byte Spill
	subl	%eax, %r12d
	movl	%r12d, -16(%rsp)                ## 4-byte Spill
	vxorps	%xmm0, %xmm0, %xmm0
	vxorps	%xmm1, %xmm1, %xmm1
	movq	40(%rsp), %rax                  ## 8-byte Reload
	decq	%rax
	movq	%rax, 48(%rsp)                  ## 8-byte Spill
	movq	%r11, 176(%rsp)                 ## 8-byte Spill
	movq	-120(%rsp), %rdx                ## 8-byte Reload
	leaq	(%rdx,%r11,4), %rax
	movq	%rax, 160(%rsp)                 ## 8-byte Spill
	leaq	224(%rbx), %rax
	movq	%rax, 16(%rsp)                  ## 8-byte Spill
	leaq	224(%rdx), %rax
	movq	%rax, -32(%rsp)                 ## 8-byte Spill
	leaq	32(%rbx), %rax
	movq	%rax, -24(%rsp)                 ## 8-byte Spill
	leaq	32(%rdx), %rax
	movq	%rax, 24(%rsp)                  ## 8-byte Spill
	movq	%rsi, 168(%rsp)                 ## 8-byte Spill
	leaq	(%rbx,%rsi,4), %rax
	movq	%rax, 128(%rsp)                 ## 8-byte Spill
	movq	112(%rsp), %rax                 ## 8-byte Reload
	leaq	(%rdx,%rax,4), %rax
	movq	%rax, 200(%rsp)                 ## 8-byte Spill
	movq	%r13, 72(%rsp)                  ## 8-byte Spill
	leaq	(%rbx,%r13,4), %rax
	movq	%rax, 192(%rsp)                 ## 8-byte Spill
	movq	120(%rsp), %rax                 ## 8-byte Reload
	leaq	(%rdx,%rax,4), %rax
	movq	%rax, 184(%rsp)                 ## 8-byte Spill
	xorl	%esi, %esi
	jmp	LBB7_9
	.p2align	4, 0x90
LBB7_197:                               ## %"end for head2_relu.s0.n.n.rebased.loopexit.us.us.us.us.2"
                                        ##   in Loop: Header=BB7_9 Depth=1
	movq	144(%rsp), %rax                 ## 8-byte Reload
	movq	56(%rsp), %rcx                  ## 8-byte Reload
	leal	(%rcx,%rax), %edi
	movq	136(%rsp), %rdx                 ## 8-byte Reload
	movq	8(%rsp), %rcx                   ## 8-byte Reload
	addl	%edx, %ecx
	movq	-104(%rsp), %rsi                ## 8-byte Reload
	leal	(%rsi,%rax), %esi
	movl	%esi, -104(%rsp)                ## 4-byte Spill
	movq	32(%rsp), %rsi                  ## 8-byte Reload
	leal	(%rsi,%rdx), %r10d
	movq	-80(%rsp), %rsi                 ## 8-byte Reload
	leal	(%rsi,%rax), %r8d
	movq	-96(%rsp), %rsi                 ## 8-byte Reload
	leal	(%rsi,%rdx), %r9d
	movq	-72(%rsp), %rsi                 ## 8-byte Reload
	leal	(%rsi,%rax), %r15d
	movq	-64(%rsp), %rax                 ## 8-byte Reload
	leal	(%rax,%rdx), %r14d
	movq	104(%rsp), %rsi                 ## 8-byte Reload
	cmpq	$7, %rsi
	je	LBB7_114
LBB7_9:                                 ## %"for head2_relu.s0.w.wi.us.us"
                                        ## =>This Loop Header: Depth=1
                                        ##     Child Loop BB7_12 Depth 2
                                        ##     Child Loop BB7_15 Depth 2
                                        ##     Child Loop BB7_17 Depth 2
                                        ##       Child Loop BB7_23 Depth 3
                                        ##       Child Loop BB7_26 Depth 3
                                        ##       Child Loop BB7_29 Depth 3
                                        ##     Child Loop BB7_156 Depth 2
                                        ##     Child Loop BB7_159 Depth 2
                                        ##     Child Loop BB7_161 Depth 2
                                        ##       Child Loop BB7_167 Depth 3
                                        ##       Child Loop BB7_170 Depth 3
                                        ##       Child Loop BB7_173 Depth 3
                                        ##     Child Loop BB7_178 Depth 2
                                        ##     Child Loop BB7_181 Depth 2
                                        ##     Child Loop BB7_183 Depth 2
                                        ##       Child Loop BB7_189 Depth 3
                                        ##       Child Loop BB7_192 Depth 3
                                        ##       Child Loop BB7_195 Depth 3
	movslq	%edi, %rbx
	movslq	%ecx, %r12
	cmpq	$3, 48(%rsp)                    ## 8-byte Folded Reload
	jae	LBB7_11
## %bb.10:                              ##   in Loop: Header=BB7_9 Depth=1
	xorl	%ecx, %ecx
	movq	-120(%rsp), %r13                ## 8-byte Reload
	movq	-8(%rsp), %r11                  ## 8-byte Reload
	movq	-40(%rsp), %rbp                 ## 8-byte Reload
	jmp	LBB7_13
	.p2align	4, 0x90
LBB7_11:                                ## %"for head2_relu.s0.n.n.us.us.us.us.preheader"
                                        ##   in Loop: Header=BB7_9 Depth=1
	movq	64(%rsp), %rax                  ## 8-byte Reload
	leaq	(%rax,%rbx,4), %rdi
	movq	160(%rsp), %rax                 ## 8-byte Reload
	leaq	(%rax,%r12,4), %rax
	movl	$96, %edx
	xorl	%ecx, %ecx
	movq	-120(%rsp), %r13                ## 8-byte Reload
	movq	-8(%rsp), %r11                  ## 8-byte Reload
	movq	-40(%rsp), %rbp                 ## 8-byte Reload
	.p2align	4, 0x90
LBB7_12:                                ## %"for head2_relu.s0.n.n.us.us.us.us"
                                        ##   Parent Loop BB7_9 Depth=1
                                        ## =>  This Inner Loop Header: Depth=2
	vmovups	-96(%rax,%rdx), %ymm2
	vmaxps	%ymm1, %ymm2, %ymm2
	vmovups	%ymm2, -96(%rdi,%rdx)
	vmovups	-64(%rax,%rdx), %ymm2
	vmaxps	%ymm1, %ymm2, %ymm2
	vmovups	%ymm2, -64(%rdi,%rdx)
	vmovups	-32(%rax,%rdx), %ymm2
	vmaxps	%ymm1, %ymm2, %ymm2
	vmovups	%ymm2, -32(%rdi,%rdx)
	vmovups	(%rax,%rdx), %ymm2
	vmaxps	%ymm1, %ymm2, %ymm2
	vmovups	%ymm2, (%rdi,%rdx)
	addq	$4, %rcx
	subq	$-128, %rdx
	cmpq	%rcx, %r11
	jne	LBB7_12
LBB7_13:                                ## %"end for head2_relu.s0.n.n.loopexit.us.us.us.us.unr-lcssa"
                                        ##   in Loop: Header=BB7_9 Depth=1
	cmpq	$0, 152(%rsp)                   ## 8-byte Folded Reload
	je	LBB7_16
## %bb.14:                              ## %"for head2_relu.s0.n.n.us.us.us.us.epil.preheader"
                                        ##   in Loop: Header=BB7_9 Depth=1
	leaq	(%rbx,%rcx,8), %rax
	movq	64(%rsp), %rdx                  ## 8-byte Reload
	leaq	(%rdx,%rax,4), %rax
	movq	176(%rsp), %rdx                 ## 8-byte Reload
	leaq	(%rdx,%rcx,8), %rcx
	addq	%r12, %rcx
	leaq	(,%rcx,4), %rcx
	addq	%r13, %rcx
	xorl	%edx, %edx
	.p2align	4, 0x90
LBB7_15:                                ## %"for head2_relu.s0.n.n.us.us.us.us.epil"
                                        ##   Parent Loop BB7_9 Depth=1
                                        ## =>  This Inner Loop Header: Depth=2
	vmovups	(%rcx,%rdx), %ymm2
	vmaxps	%ymm1, %ymm2, %ymm2
	vmovups	%ymm2, (%rax,%rdx)
	addq	$32, %rdx
	cmpq	%rdx, %rbp
	jne	LBB7_15
LBB7_16:                                ## %"end for head2_relu.s0.n.n.loopexit.us.us.us.us"
                                        ##   in Loop: Header=BB7_9 Depth=1
	incq	%rsi
	movq	%rsi, 104(%rsp)                 ## 8-byte Spill
	movslq	%r15d, %rax
	movq	%rax, -72(%rsp)                 ## 8-byte Spill
	movslq	%r14d, %rax
	movq	%rax, -64(%rsp)                 ## 8-byte Spill
	movq	192(%rsp), %rax                 ## 8-byte Reload
	leaq	(%rax,%rbx,4), %rax
	movq	%rax, 96(%rsp)                  ## 8-byte Spill
	movq	184(%rsp), %rax                 ## 8-byte Reload
	leaq	(%rax,%r12,4), %rax
	movq	%rax, 88(%rsp)                  ## 8-byte Spill
	movslq	%r8d, %rax
	movq	%rax, -80(%rsp)                 ## 8-byte Spill
	movslq	%r9d, %rax
	movq	%rax, -96(%rsp)                 ## 8-byte Spill
	movq	128(%rsp), %rax                 ## 8-byte Reload
	movq	%rbx, 56(%rsp)                  ## 8-byte Spill
	leaq	(%rax,%rbx,4), %rax
	movq	%rax, 80(%rsp)                  ## 8-byte Spill
	movq	200(%rsp), %rax                 ## 8-byte Reload
	movq	%r12, 8(%rsp)                   ## 8-byte Spill
	leaq	(%rax,%r12,4), %r8
	movslq	-104(%rsp), %rax                ## 4-byte Folded Reload
	movq	%rax, -104(%rsp)                ## 8-byte Spill
	movslq	%r10d, %r10
	movl	-16(%rsp), %r15d                ## 4-byte Reload
	movl	-128(%rsp), %r9d                ## 4-byte Reload
	movl	(%rsp), %eax                    ## 4-byte Reload
	movl	%eax, %r14d
	xorl	%ecx, %ecx
	movq	%r10, 32(%rsp)                  ## 8-byte Spill
	jmp	LBB7_17
	.p2align	4, 0x90
LBB7_30:                                ## %"end for head2_relu.s0.n.ni.us.us.us.us"
                                        ##   in Loop: Header=BB7_17 Depth=2
	incq	%rcx
	addl	$8, %r14d
	addl	$-8, %r9d
	addl	$-8, %r15d
	cmpq	-48(%rsp), %rcx                 ## 8-byte Folded Reload
	je	LBB7_31
LBB7_17:                                ## %"for head2_relu.s0.n.n.rebased.us.us.us.us"
                                        ##   Parent Loop BB7_9 Depth=1
                                        ## =>  This Loop Header: Depth=2
                                        ##       Child Loop BB7_23 Depth 3
                                        ##       Child Loop BB7_26 Depth 3
                                        ##       Child Loop BB7_29 Depth 3
	movslq	%r14d, %r14
	cmpl	$8, %r15d
	movl	$8, %edx
	cmovll	%r15d, %edx
	cmpl	$8, %r9d
	movl	$8, %ebx
	cmovll	%r9d, %ebx
	leal	(,%rcx,8), %eax
	movl	-128(%rsp), %esi                ## 4-byte Reload
	movl	%esi, %r12d
	subl	%eax, %r12d
	cmpl	$8, %r12d
	movl	$8, %eax
	cmovgel	%eax, %r12d
	movq	-112(%rsp), %rax                ## 8-byte Reload
	addl	%ecx, %eax
	shll	$3, %eax
	movl	-56(%rsp), %esi                 ## 4-byte Reload
	subl	%eax, %esi
	testl	%esi, %esi
	jle	LBB7_30
## %bb.18:                              ## %"for head2_relu.s0.n.ni.us.us.us.us.preheader"
                                        ##   in Loop: Header=BB7_17 Depth=2
	cmpl	$16, %r12d
	jae	LBB7_20
## %bb.19:                              ##   in Loop: Header=BB7_17 Depth=2
	xorl	%esi, %esi
	jmp	LBB7_28
	.p2align	4, 0x90
LBB7_20:                                ## %vector.ph205
                                        ##   in Loop: Header=BB7_17 Depth=2
	andl	$-16, %ebx
	addq	$-16, %rbx
	shrq	$4, %rbx
	movl	%r12d, %eax
	andl	$-16, %eax
	addq	$-16, %rax
	movq	%rax, %r13
	shrq	$4, %r13
	incq	%r13
	cmpq	$48, %rax
	jae	LBB7_22
## %bb.21:                              ##   in Loop: Header=BB7_17 Depth=2
	xorl	%edi, %edi
	jmp	LBB7_24
	.p2align	4, 0x90
LBB7_22:                                ## %vector.ph205.new
                                        ##   in Loop: Header=BB7_17 Depth=2
	movq	-104(%rsp), %rax                ## 8-byte Reload
	addq	%r14, %rax
	movq	16(%rsp), %rsi                  ## 8-byte Reload
	leaq	(%rsi,%rax,4), %rsi
	leaq	(%r10,%r14), %rax
	movq	-32(%rsp), %rdi                 ## 8-byte Reload
	leaq	(%rdi,%rax,4), %r10
	leaq	1(%rbx), %r11
	andq	$-4, %r11
	negq	%r11
	xorl	%edi, %edi
	.p2align	4, 0x90
LBB7_23:                                ## %vector.body202
                                        ##   Parent Loop BB7_9 Depth=1
                                        ##     Parent Loop BB7_17 Depth=2
                                        ## =>    This Inner Loop Header: Depth=3
	vmovups	-224(%r10,%rdi,4), %ymm2
	vmovups	-192(%r10,%rdi,4), %ymm3
	vmaxps	%ymm1, %ymm2, %ymm2
	vmaxps	%ymm1, %ymm3, %ymm3
	vmovups	%ymm2, -224(%rsi,%rdi,4)
	vmovups	%ymm3, -192(%rsi,%rdi,4)
	vmovups	-160(%r10,%rdi,4), %ymm2
	vmovups	-128(%r10,%rdi,4), %ymm3
	vmaxps	%ymm1, %ymm2, %ymm2
	vmaxps	%ymm1, %ymm3, %ymm3
	vmovups	%ymm2, -160(%rsi,%rdi,4)
	vmovups	%ymm3, -128(%rsi,%rdi,4)
	vmovups	-96(%r10,%rdi,4), %ymm2
	vmovups	-64(%r10,%rdi,4), %ymm3
	vmaxps	%ymm1, %ymm2, %ymm2
	vmaxps	%ymm1, %ymm3, %ymm3
	vmovups	%ymm2, -96(%rsi,%rdi,4)
	vmovups	%ymm3, -64(%rsi,%rdi,4)
	vmovups	-32(%r10,%rdi,4), %ymm2
	vmovups	(%r10,%rdi,4), %ymm3
	vmaxps	%ymm1, %ymm2, %ymm2
	vmaxps	%ymm1, %ymm3, %ymm3
	vmovups	%ymm2, -32(%rsi,%rdi,4)
	vmovups	%ymm3, (%rsi,%rdi,4)
	addq	$64, %rdi
	addq	$4, %r11
	jne	LBB7_23
LBB7_24:                                ## %middle.block200.unr-lcssa
                                        ##   in Loop: Header=BB7_17 Depth=2
	movl	%r12d, %esi
	andl	$-16, %esi
	testb	$3, %r13b
	movq	32(%rsp), %r10                  ## 8-byte Reload
	je	LBB7_27
## %bb.25:                              ## %vector.body202.epil.preheader
                                        ##   in Loop: Header=BB7_17 Depth=2
	incb	%bl
	movzbl	%bl, %ebx
	andl	$3, %ebx
	shlq	$6, %rbx
	movq	-104(%rsp), %rax                ## 8-byte Reload
	addq	%rdi, %rax
	addq	%r14, %rax
	movq	-24(%rsp), %rbp                 ## 8-byte Reload
	leaq	(,%rax,4), %rax
	addq	%rbp, %rax
	addq	%r10, %rdi
	addq	%r14, %rdi
	movq	24(%rsp), %rbp                  ## 8-byte Reload
	leaq	(,%rdi,4), %rdi
	addq	%rbp, %rdi
	xorl	%ebp, %ebp
	.p2align	4, 0x90
LBB7_26:                                ## %vector.body202.epil
                                        ##   Parent Loop BB7_9 Depth=1
                                        ##     Parent Loop BB7_17 Depth=2
                                        ## =>    This Inner Loop Header: Depth=3
	vmovups	-32(%rdi,%rbp), %ymm2
	vmovups	(%rdi,%rbp), %ymm3
	vmaxps	%ymm1, %ymm2, %ymm2
	vmaxps	%ymm1, %ymm3, %ymm3
	vmovups	%ymm2, -32(%rax,%rbp)
	vmovups	%ymm3, (%rax,%rbp)
	addq	$64, %rbp
	cmpq	%rbp, %rbx
	jne	LBB7_26
LBB7_27:                                ## %middle.block200
                                        ##   in Loop: Header=BB7_17 Depth=2
	cmpq	%r12, %rsi
	movq	-120(%rsp), %r13                ## 8-byte Reload
	movq	-8(%rsp), %r11                  ## 8-byte Reload
	movq	-40(%rsp), %rbp                 ## 8-byte Reload
	je	LBB7_30
LBB7_28:                                ## %"for head2_relu.s0.n.ni.us.us.us.us.preheader214"
                                        ##   in Loop: Header=BB7_17 Depth=2
	subq	%rsi, %rdx
	movq	-104(%rsp), %rax                ## 8-byte Reload
	addq	%rsi, %rax
	addq	%r14, %rax
	movq	-88(%rsp), %rdi                 ## 8-byte Reload
	leaq	(%rdi,%rax,4), %rax
	addq	%r10, %rsi
	addq	%r14, %rsi
	leaq	(,%rsi,4), %rsi
	addq	%r13, %rsi
	xorl	%edi, %edi
	.p2align	4, 0x90
LBB7_29:                                ## %"for head2_relu.s0.n.ni.us.us.us.us"
                                        ##   Parent Loop BB7_9 Depth=1
                                        ##     Parent Loop BB7_17 Depth=2
                                        ## =>    This Inner Loop Header: Depth=3
	vmovss	(%rsi,%rdi,4), %xmm2            ## xmm2 = mem[0],zero,zero,zero
	vmaxss	%xmm0, %xmm2, %xmm2
	vmovss	%xmm2, (%rax,%rdi,4)
	incq	%rdi
	cmpq	%rdi, %rdx
	jne	LBB7_29
	jmp	LBB7_30
	.p2align	4, 0x90
LBB7_31:                                ## %"end for head2_relu.s0.n.n.rebased.loopexit.us.us.us.us"
                                        ##   in Loop: Header=BB7_9 Depth=1
	cmpq	$3, 48(%rsp)                    ## 8-byte Folded Reload
	jae	LBB7_155
## %bb.32:                              ##   in Loop: Header=BB7_9 Depth=1
	xorl	%eax, %eax
	movq	-88(%rsp), %r15                 ## 8-byte Reload
	movl	-56(%rsp), %r12d                ## 4-byte Reload
	jmp	LBB7_157
	.p2align	4, 0x90
LBB7_155:                               ## %"for head2_relu.s0.n.n.us.us.us.us.1.preheader"
                                        ##   in Loop: Header=BB7_9 Depth=1
	movl	$96, %ecx
	xorl	%eax, %eax
	movq	-88(%rsp), %r15                 ## 8-byte Reload
	movl	-56(%rsp), %r12d                ## 4-byte Reload
	movq	80(%rsp), %rdx                  ## 8-byte Reload
	.p2align	4, 0x90
LBB7_156:                               ## %"for head2_relu.s0.n.n.us.us.us.us.1"
                                        ##   Parent Loop BB7_9 Depth=1
                                        ## =>  This Inner Loop Header: Depth=2
	vmovups	-96(%r8,%rcx), %ymm2
	vmaxps	%ymm1, %ymm2, %ymm2
	vmovups	%ymm2, -96(%rdx,%rcx)
	vmovups	-64(%r8,%rcx), %ymm2
	vmaxps	%ymm1, %ymm2, %ymm2
	vmovups	%ymm2, -64(%rdx,%rcx)
	vmovups	-32(%r8,%rcx), %ymm2
	vmaxps	%ymm1, %ymm2, %ymm2
	vmovups	%ymm2, -32(%rdx,%rcx)
	vmovups	(%r8,%rcx), %ymm2
	vmaxps	%ymm1, %ymm2, %ymm2
	vmovups	%ymm2, (%rdx,%rcx)
	addq	$4, %rax
	subq	$-128, %rcx
	cmpq	%rax, %r11
	jne	LBB7_156
LBB7_157:                               ## %"end for head2_relu.s0.n.n.loopexit.us.us.us.us.1.unr-lcssa"
                                        ##   in Loop: Header=BB7_9 Depth=1
	testb	$3, 40(%rsp)                    ## 1-byte Folded Reload
	je	LBB7_160
## %bb.158:                             ## %"for head2_relu.s0.n.n.us.us.us.us.1.epil.preheader"
                                        ##   in Loop: Header=BB7_9 Depth=1
	movq	168(%rsp), %rcx                 ## 8-byte Reload
	leaq	(%rcx,%rax,8), %rcx
	addq	56(%rsp), %rcx                  ## 8-byte Folded Reload
	leaq	(%r15,%rcx,4), %rcx
	movq	112(%rsp), %rdx                 ## 8-byte Reload
	leaq	(%rdx,%rax,8), %rax
	addq	8(%rsp), %rax                   ## 8-byte Folded Reload
	leaq	(,%rax,4), %rax
	addq	%r13, %rax
	xorl	%edx, %edx
	.p2align	4, 0x90
LBB7_159:                               ## %"for head2_relu.s0.n.n.us.us.us.us.1.epil"
                                        ##   Parent Loop BB7_9 Depth=1
                                        ## =>  This Inner Loop Header: Depth=2
	vmovups	(%rax,%rdx), %ymm2
	vmaxps	%ymm1, %ymm2, %ymm2
	vmovups	%ymm2, (%rcx,%rdx)
	addq	$32, %rdx
	cmpq	%rdx, %rbp
	jne	LBB7_159
LBB7_160:                               ## %"end for head2_relu.s0.n.n.loopexit.us.us.us.us.1"
                                        ##   in Loop: Header=BB7_9 Depth=1
	movl	-16(%rsp), %r8d                 ## 4-byte Reload
	movl	-128(%rsp), %r14d               ## 4-byte Reload
	movl	(%rsp), %eax                    ## 4-byte Reload
	movl	%eax, %r11d
	xorl	%r10d, %r10d
	jmp	LBB7_161
	.p2align	4, 0x90
LBB7_174:                               ## %"end for head2_relu.s0.n.ni.us.us.us.us.1"
                                        ##   in Loop: Header=BB7_161 Depth=2
	incq	%r10
	addl	$8, %r11d
	addl	$-8, %r14d
	addl	$-8, %r8d
	cmpq	-48(%rsp), %r10                 ## 8-byte Folded Reload
	je	LBB7_175
LBB7_161:                               ## %"for head2_relu.s0.n.n.rebased.us.us.us.us.1"
                                        ##   Parent Loop BB7_9 Depth=1
                                        ## =>  This Loop Header: Depth=2
                                        ##       Child Loop BB7_167 Depth 3
                                        ##       Child Loop BB7_170 Depth 3
                                        ##       Child Loop BB7_173 Depth 3
	movslq	%r11d, %r11
	cmpl	$8, %r8d
	movl	$8, %esi
	cmovll	%r8d, %esi
	cmpl	$8, %r14d
	movl	$8, %edi
	cmovll	%r14d, %edi
	leal	(,%r10,8), %eax
	movl	-128(%rsp), %ecx                ## 4-byte Reload
                                        ## kill: def $ecx killed $ecx def $rcx
	subl	%eax, %ecx
	cmpl	$8, %ecx
	movl	$8, %eax
	cmovgel	%eax, %ecx
	movq	-112(%rsp), %rax                ## 8-byte Reload
	addl	%r10d, %eax
	shll	$3, %eax
	movl	%r12d, %edx
	subl	%eax, %edx
	testl	%edx, %edx
	jle	LBB7_174
## %bb.162:                             ## %"for head2_relu.s0.n.ni.us.us.us.us.preheader.1"
                                        ##   in Loop: Header=BB7_161 Depth=2
	cmpl	$16, %ecx
	jae	LBB7_164
## %bb.163:                             ##   in Loop: Header=BB7_161 Depth=2
	xorl	%ebx, %ebx
	jmp	LBB7_172
	.p2align	4, 0x90
LBB7_164:                               ## %vector.ph191
                                        ##   in Loop: Header=BB7_161 Depth=2
	andl	$-16, %edi
	addq	$-16, %rdi
	shrq	$4, %rdi
	movl	%ecx, %eax
	andl	$-16, %eax
	addq	$-16, %rax
	movq	%rax, %r9
	shrq	$4, %r9
	incq	%r9
	cmpq	$48, %rax
	jae	LBB7_166
## %bb.165:                             ##   in Loop: Header=BB7_161 Depth=2
	xorl	%eax, %eax
	jmp	LBB7_168
	.p2align	4, 0x90
LBB7_166:                               ## %vector.ph191.new
                                        ##   in Loop: Header=BB7_161 Depth=2
	movq	-80(%rsp), %rax                 ## 8-byte Reload
	addq	%r11, %rax
	movq	16(%rsp), %rdx                  ## 8-byte Reload
	leaq	(%rdx,%rax,4), %rbx
	movq	-96(%rsp), %rax                 ## 8-byte Reload
	addq	%r11, %rax
	movq	-32(%rsp), %rdx                 ## 8-byte Reload
	leaq	(%rdx,%rax,4), %rdx
	leaq	1(%rdi), %rbp
	andq	$-4, %rbp
	negq	%rbp
	xorl	%eax, %eax
	.p2align	4, 0x90
LBB7_167:                               ## %vector.body188
                                        ##   Parent Loop BB7_9 Depth=1
                                        ##     Parent Loop BB7_161 Depth=2
                                        ## =>    This Inner Loop Header: Depth=3
	vmovups	-224(%rdx,%rax,4), %ymm2
	vmovups	-192(%rdx,%rax,4), %ymm3
	vmaxps	%ymm1, %ymm2, %ymm2
	vmaxps	%ymm1, %ymm3, %ymm3
	vmovups	%ymm2, -224(%rbx,%rax,4)
	vmovups	%ymm3, -192(%rbx,%rax,4)
	vmovups	-160(%rdx,%rax,4), %ymm2
	vmovups	-128(%rdx,%rax,4), %ymm3
	vmaxps	%ymm1, %ymm2, %ymm2
	vmaxps	%ymm1, %ymm3, %ymm3
	vmovups	%ymm2, -160(%rbx,%rax,4)
	vmovups	%ymm3, -128(%rbx,%rax,4)
	vmovups	-96(%rdx,%rax,4), %ymm2
	vmovups	-64(%rdx,%rax,4), %ymm3
	vmaxps	%ymm1, %ymm2, %ymm2
	vmaxps	%ymm1, %ymm3, %ymm3
	vmovups	%ymm2, -96(%rbx,%rax,4)
	vmovups	%ymm3, -64(%rbx,%rax,4)
	vmovups	-32(%rdx,%rax,4), %ymm2
	vmovups	(%rdx,%rax,4), %ymm3
	vmaxps	%ymm1, %ymm2, %ymm2
	vmaxps	%ymm1, %ymm3, %ymm3
	vmovups	%ymm2, -32(%rbx,%rax,4)
	vmovups	%ymm3, (%rbx,%rax,4)
	addq	$64, %rax
	addq	$4, %rbp
	jne	LBB7_167
LBB7_168:                               ## %middle.block186.unr-lcssa
                                        ##   in Loop: Header=BB7_161 Depth=2
	movl	%ecx, %ebx
	andl	$-16, %ebx
	testb	$3, %r9b
	je	LBB7_171
## %bb.169:                             ## %vector.body188.epil.preheader
                                        ##   in Loop: Header=BB7_161 Depth=2
	incb	%dil
	movzbl	%dil, %edx
	andl	$3, %edx
	shlq	$6, %rdx
	movq	-80(%rsp), %rdi                 ## 8-byte Reload
	addq	%rax, %rdi
	addq	%r11, %rdi
	movq	-24(%rsp), %rbp                 ## 8-byte Reload
	leaq	(,%rdi,4), %rdi
	addq	%rbp, %rdi
	addq	-96(%rsp), %rax                 ## 8-byte Folded Reload
	addq	%r11, %rax
	movq	24(%rsp), %rbp                  ## 8-byte Reload
	leaq	(,%rax,4), %rax
	addq	%rbp, %rax
	xorl	%ebp, %ebp
	.p2align	4, 0x90
LBB7_170:                               ## %vector.body188.epil
                                        ##   Parent Loop BB7_9 Depth=1
                                        ##     Parent Loop BB7_161 Depth=2
                                        ## =>    This Inner Loop Header: Depth=3
	vmovups	-32(%rax,%rbp), %ymm2
	vmovups	(%rax,%rbp), %ymm3
	vmaxps	%ymm1, %ymm2, %ymm2
	vmaxps	%ymm1, %ymm3, %ymm3
	vmovups	%ymm2, -32(%rdi,%rbp)
	vmovups	%ymm3, (%rdi,%rbp)
	addq	$64, %rbp
	cmpq	%rbp, %rdx
	jne	LBB7_170
LBB7_171:                               ## %middle.block186
                                        ##   in Loop: Header=BB7_161 Depth=2
	cmpq	%rcx, %rbx
	je	LBB7_174
LBB7_172:                               ## %"for head2_relu.s0.n.ni.us.us.us.us.1.preheader"
                                        ##   in Loop: Header=BB7_161 Depth=2
	subq	%rbx, %rsi
	movq	-80(%rsp), %rax                 ## 8-byte Reload
	addq	%rbx, %rax
	addq	%r11, %rax
	leaq	(%r15,%rax,4), %rax
	addq	-96(%rsp), %rbx                 ## 8-byte Folded Reload
	addq	%r11, %rbx
	leaq	(,%rbx,4), %rcx
	addq	%r13, %rcx
	xorl	%edx, %edx
	.p2align	4, 0x90
LBB7_173:                               ## %"for head2_relu.s0.n.ni.us.us.us.us.1"
                                        ##   Parent Loop BB7_9 Depth=1
                                        ##     Parent Loop BB7_161 Depth=2
                                        ## =>    This Inner Loop Header: Depth=3
	vmovss	(%rcx,%rdx,4), %xmm2            ## xmm2 = mem[0],zero,zero,zero
	vmaxss	%xmm0, %xmm2, %xmm2
	vmovss	%xmm2, (%rax,%rdx,4)
	incq	%rdx
	cmpq	%rdx, %rsi
	jne	LBB7_173
	jmp	LBB7_174
	.p2align	4, 0x90
LBB7_175:                               ## %"end for head2_relu.s0.n.n.rebased.loopexit.us.us.us.us.1"
                                        ##   in Loop: Header=BB7_9 Depth=1
	cmpq	$3, 48(%rsp)                    ## 8-byte Folded Reload
	movq	-8(%rsp), %rdx                  ## 8-byte Reload
	jae	LBB7_177
## %bb.176:                             ##   in Loop: Header=BB7_9 Depth=1
	xorl	%eax, %eax
	movq	-40(%rsp), %rsi                 ## 8-byte Reload
	jmp	LBB7_179
	.p2align	4, 0x90
LBB7_177:                               ## %"for head2_relu.s0.n.n.us.us.us.us.2.preheader"
                                        ##   in Loop: Header=BB7_9 Depth=1
	movl	$96, %ecx
	xorl	%eax, %eax
	movq	-40(%rsp), %rsi                 ## 8-byte Reload
	movq	96(%rsp), %rdi                  ## 8-byte Reload
	movq	88(%rsp), %rbp                  ## 8-byte Reload
	.p2align	4, 0x90
LBB7_178:                               ## %"for head2_relu.s0.n.n.us.us.us.us.2"
                                        ##   Parent Loop BB7_9 Depth=1
                                        ## =>  This Inner Loop Header: Depth=2
	vmovups	-96(%rbp,%rcx), %ymm2
	vmaxps	%ymm1, %ymm2, %ymm2
	vmovups	%ymm2, -96(%rdi,%rcx)
	vmovups	-64(%rbp,%rcx), %ymm2
	vmaxps	%ymm1, %ymm2, %ymm2
	vmovups	%ymm2, -64(%rdi,%rcx)
	vmovups	-32(%rbp,%rcx), %ymm2
	vmaxps	%ymm1, %ymm2, %ymm2
	vmovups	%ymm2, -32(%rdi,%rcx)
	vmovups	(%rbp,%rcx), %ymm2
	vmaxps	%ymm1, %ymm2, %ymm2
	vmovups	%ymm2, (%rdi,%rcx)
	addq	$4, %rax
	subq	$-128, %rcx
	cmpq	%rax, %rdx
	jne	LBB7_178
LBB7_179:                               ## %"end for head2_relu.s0.n.n.loopexit.us.us.us.us.2.unr-lcssa"
                                        ##   in Loop: Header=BB7_9 Depth=1
	testb	$3, 40(%rsp)                    ## 1-byte Folded Reload
	je	LBB7_182
## %bb.180:                             ## %"for head2_relu.s0.n.n.us.us.us.us.2.epil.preheader"
                                        ##   in Loop: Header=BB7_9 Depth=1
	movq	72(%rsp), %rcx                  ## 8-byte Reload
	leaq	(%rcx,%rax,8), %rcx
	addq	56(%rsp), %rcx                  ## 8-byte Folded Reload
	leaq	(%r15,%rcx,4), %rcx
	movq	120(%rsp), %rdx                 ## 8-byte Reload
	leaq	(%rdx,%rax,8), %rax
	addq	8(%rsp), %rax                   ## 8-byte Folded Reload
	leaq	(,%rax,4), %rax
	addq	%r13, %rax
	xorl	%edx, %edx
	.p2align	4, 0x90
LBB7_181:                               ## %"for head2_relu.s0.n.n.us.us.us.us.2.epil"
                                        ##   Parent Loop BB7_9 Depth=1
                                        ## =>  This Inner Loop Header: Depth=2
	vmovups	(%rax,%rdx), %ymm2
	vmaxps	%ymm1, %ymm2, %ymm2
	vmovups	%ymm2, (%rcx,%rdx)
	addq	$32, %rdx
	cmpq	%rdx, %rsi
	jne	LBB7_181
LBB7_182:                               ## %"end for head2_relu.s0.n.n.loopexit.us.us.us.us.2"
                                        ##   in Loop: Header=BB7_9 Depth=1
	movl	-16(%rsp), %r8d                 ## 4-byte Reload
	movl	-128(%rsp), %r11d               ## 4-byte Reload
	movl	(%rsp), %eax                    ## 4-byte Reload
	movl	%eax, %r14d
	xorl	%r10d, %r10d
	jmp	LBB7_183
	.p2align	4, 0x90
LBB7_196:                               ## %"end for head2_relu.s0.n.ni.us.us.us.us.2"
                                        ##   in Loop: Header=BB7_183 Depth=2
	incq	%r10
	addl	$8, %r14d
	addl	$-8, %r11d
	addl	$-8, %r8d
	cmpq	-48(%rsp), %r10                 ## 8-byte Folded Reload
	je	LBB7_197
LBB7_183:                               ## %"for head2_relu.s0.n.n.rebased.us.us.us.us.2"
                                        ##   Parent Loop BB7_9 Depth=1
                                        ## =>  This Loop Header: Depth=2
                                        ##       Child Loop BB7_189 Depth 3
                                        ##       Child Loop BB7_192 Depth 3
                                        ##       Child Loop BB7_195 Depth 3
	movslq	%r14d, %r14
	cmpl	$8, %r8d
	movl	$8, %esi
	cmovll	%r8d, %esi
	cmpl	$8, %r11d
	movl	$8, %ebx
	cmovll	%r11d, %ebx
	leal	(,%r10,8), %eax
	movl	-128(%rsp), %ecx                ## 4-byte Reload
                                        ## kill: def $ecx killed $ecx def $rcx
	subl	%eax, %ecx
	cmpl	$8, %ecx
	movl	$8, %eax
	cmovgel	%eax, %ecx
	movq	-112(%rsp), %rax                ## 8-byte Reload
	addl	%r10d, %eax
	shll	$3, %eax
	movl	%r12d, %edx
	subl	%eax, %edx
	testl	%edx, %edx
	jle	LBB7_196
## %bb.184:                             ## %"for head2_relu.s0.n.ni.us.us.us.us.preheader.2"
                                        ##   in Loop: Header=BB7_183 Depth=2
	cmpl	$16, %ecx
	jae	LBB7_186
## %bb.185:                             ##   in Loop: Header=BB7_183 Depth=2
	xorl	%ebp, %ebp
	jmp	LBB7_194
	.p2align	4, 0x90
LBB7_186:                               ## %vector.ph177
                                        ##   in Loop: Header=BB7_183 Depth=2
	andl	$-16, %ebx
	addq	$-16, %rbx
	shrq	$4, %rbx
	movl	%ecx, %eax
	andl	$-16, %eax
	addq	$-16, %rax
	movq	%rax, %r9
	shrq	$4, %r9
	incq	%r9
	cmpq	$48, %rax
	jae	LBB7_188
## %bb.187:                             ##   in Loop: Header=BB7_183 Depth=2
	xorl	%eax, %eax
	jmp	LBB7_190
	.p2align	4, 0x90
LBB7_188:                               ## %vector.ph177.new
                                        ##   in Loop: Header=BB7_183 Depth=2
	movq	-72(%rsp), %rax                 ## 8-byte Reload
	addq	%r14, %rax
	movq	16(%rsp), %rdx                  ## 8-byte Reload
	leaq	(%rdx,%rax,4), %rbp
	movq	-64(%rsp), %rax                 ## 8-byte Reload
	addq	%r14, %rax
	movq	-32(%rsp), %rdx                 ## 8-byte Reload
	leaq	(%rdx,%rax,4), %rdx
	leaq	1(%rbx), %rdi
	andq	$-4, %rdi
	negq	%rdi
	xorl	%eax, %eax
	.p2align	4, 0x90
LBB7_189:                               ## %vector.body174
                                        ##   Parent Loop BB7_9 Depth=1
                                        ##     Parent Loop BB7_183 Depth=2
                                        ## =>    This Inner Loop Header: Depth=3
	vmovups	-224(%rdx,%rax,4), %ymm2
	vmovups	-192(%rdx,%rax,4), %ymm3
	vmaxps	%ymm1, %ymm2, %ymm2
	vmaxps	%ymm1, %ymm3, %ymm3
	vmovups	%ymm2, -224(%rbp,%rax,4)
	vmovups	%ymm3, -192(%rbp,%rax,4)
	vmovups	-160(%rdx,%rax,4), %ymm2
	vmovups	-128(%rdx,%rax,4), %ymm3
	vmaxps	%ymm1, %ymm2, %ymm2
	vmaxps	%ymm1, %ymm3, %ymm3
	vmovups	%ymm2, -160(%rbp,%rax,4)
	vmovups	%ymm3, -128(%rbp,%rax,4)
	vmovups	-96(%rdx,%rax,4), %ymm2
	vmovups	-64(%rdx,%rax,4), %ymm3
	vmaxps	%ymm1, %ymm2, %ymm2
	vmaxps	%ymm1, %ymm3, %ymm3
	vmovups	%ymm2, -96(%rbp,%rax,4)
	vmovups	%ymm3, -64(%rbp,%rax,4)
	vmovups	-32(%rdx,%rax,4), %ymm2
	vmovups	(%rdx,%rax,4), %ymm3
	vmaxps	%ymm1, %ymm2, %ymm2
	vmaxps	%ymm1, %ymm3, %ymm3
	vmovups	%ymm2, -32(%rbp,%rax,4)
	vmovups	%ymm3, (%rbp,%rax,4)
	addq	$64, %rax
	addq	$4, %rdi
	jne	LBB7_189
LBB7_190:                               ## %middle.block172.unr-lcssa
                                        ##   in Loop: Header=BB7_183 Depth=2
	movl	%ecx, %ebp
	andl	$-16, %ebp
	testb	$3, %r9b
	je	LBB7_193
## %bb.191:                             ## %vector.body174.epil.preheader
                                        ##   in Loop: Header=BB7_183 Depth=2
	incb	%bl
	movzbl	%bl, %edx
	andl	$3, %edx
	shlq	$6, %rdx
	movq	-72(%rsp), %rdi                 ## 8-byte Reload
	addq	%rax, %rdi
	addq	%r14, %rdi
	movq	-24(%rsp), %rbx                 ## 8-byte Reload
	leaq	(%rbx,%rdi,4), %rdi
	addq	-64(%rsp), %rax                 ## 8-byte Folded Reload
	addq	%r14, %rax
	movq	24(%rsp), %rbx                  ## 8-byte Reload
	leaq	(%rbx,%rax,4), %rax
	xorl	%ebx, %ebx
	.p2align	4, 0x90
LBB7_192:                               ## %vector.body174.epil
                                        ##   Parent Loop BB7_9 Depth=1
                                        ##     Parent Loop BB7_183 Depth=2
                                        ## =>    This Inner Loop Header: Depth=3
	vmovups	-32(%rax,%rbx), %ymm2
	vmovups	(%rax,%rbx), %ymm3
	vmaxps	%ymm1, %ymm2, %ymm2
	vmaxps	%ymm1, %ymm3, %ymm3
	vmovups	%ymm2, -32(%rdi,%rbx)
	vmovups	%ymm3, (%rdi,%rbx)
	addq	$64, %rbx
	cmpq	%rbx, %rdx
	jne	LBB7_192
LBB7_193:                               ## %middle.block172
                                        ##   in Loop: Header=BB7_183 Depth=2
	cmpq	%rcx, %rbp
	je	LBB7_196
LBB7_194:                               ## %"for head2_relu.s0.n.ni.us.us.us.us.2.preheader"
                                        ##   in Loop: Header=BB7_183 Depth=2
	subq	%rbp, %rsi
	movq	-72(%rsp), %rax                 ## 8-byte Reload
	addq	%rbp, %rax
	addq	%r14, %rax
	leaq	(%r15,%rax,4), %rax
	addq	-64(%rsp), %rbp                 ## 8-byte Folded Reload
	addq	%r14, %rbp
	leaq	(,%rbp,4), %rcx
	addq	%r13, %rcx
	xorl	%edx, %edx
	.p2align	4, 0x90
LBB7_195:                               ## %"for head2_relu.s0.n.ni.us.us.us.us.2"
                                        ##   Parent Loop BB7_9 Depth=1
                                        ##     Parent Loop BB7_183 Depth=2
                                        ## =>    This Inner Loop Header: Depth=3
	vmovss	(%rcx,%rdx,4), %xmm2            ## xmm2 = mem[0],zero,zero,zero
	vmaxss	%xmm0, %xmm2, %xmm2
	vmovss	%xmm2, (%rax,%rdx,4)
	incq	%rdx
	cmpq	%rdx, %rsi
	jne	LBB7_195
	jmp	LBB7_196
LBB7_59:                                ## %next_bb
	movl	24(%rdx), %edx
	movl	%esi, %eax
	sarl	$3, %eax
	movl	%esi, %edi
	andl	$-8, %edi
	subl	%eax, %edi
	movq	%rdi, 48(%rsp)                  ## 8-byte Spill
	subl	%edi, %edx
	cmpl	$7, %edx
	movl	$7, %edi
	cmovll	%edx, %edi
	testl	%edx, %edx
	jle	LBB7_114
## %bb.60:                              ## %"for head2_relu.s0.w.wi2.preheader"
	testl	%ecx, %ecx
	jle	LBB7_114
## %bb.61:                              ## %"for head2_relu.s0.w.wi2.us.preheader"
	addl	$7, %ecx
	sarl	$3, %ecx
	andl	$7, %esi
	leal	(%rsi,%rsi,2), %ebp
	movq	(%rsp), %r10                    ## 8-byte Reload
	movl	%r10d, %r9d
	leal	(,%r9,8), %ebx
	leal	(%rbx,%rbx,2), %ebx
	movl	%ebx, 40(%rsp)                  ## 4-byte Spill
	leal	(,%r14,8), %ebx
	leal	(%rbx,%rbx,2), %ebx
	movl	%ebx, 104(%rsp)                 ## 4-byte Spill
	movl	%ecx, %ecx
	movq	%rcx, -128(%rsp)                ## 8-byte Spill
	movl	%ebp, %r11d
	movl	%edi, %ecx
	movq	%rcx, 88(%rsp)                  ## 8-byte Spill
	movq	%r11, %rcx
	imulq	%r10, %rcx
	movq	%rcx, 80(%rsp)                  ## 8-byte Spill
	movq	%r11, %rcx
	imulq	%r14, %rcx
	movq	%rcx, 64(%rsp)                  ## 8-byte Spill
	imull	$168, %eax, %ebp
	leal	(%rsi,%rsi,2), %ebx
	leal	(%rbx,%rbp), %esi
	movl	%r14d, %r15d
	imull	%esi, %r15d
	movq	-16(%rsp), %rcx                 ## 8-byte Reload
	leal	(,%rcx,8), %ecx
	leal	(%rcx,%rcx,2), %ecx
	subl	%ecx, %esi
	leal	24(%rsi), %ecx
	imull	%r10d, %ecx
	leal	(%rbx,%rbp), %edi
	incl	%edi
	addl	%ebp, %ebx
	addl	$2, %ebx
	leal	25(%rsi), %ebp
	addl	$26, %esi
	imull	%r9d, %esi
	leaq	1(%r11), %rdx
	movq	%rdx, %rax
	imulq	%r10, %rax
	movq	%rax, 144(%rsp)                 ## 8-byte Spill
	imulq	%r14, %rdx
	movq	%rdx, 152(%rsp)                 ## 8-byte Spill
	addq	$2, %r11
	imulq	%r11, %r10
	movq	%r10, (%rsp)                    ## 8-byte Spill
	imulq	%r14, %r11
	movq	%r11, 96(%rsp)                  ## 8-byte Spill
	movl	%r12d, %eax
	subl	%r8d, %eax
	movl	%eax, -112(%rsp)                ## 4-byte Spill
	imull	%r14d, %edi
	imull	%r14d, %ebx
	leal	(,%r14,8), %edx
	leal	(%rdx,%rdx,2), %eax
	movq	%rax, 136(%rsp)                 ## 8-byte Spill
	imull	%r9d, %ebp
	leal	(,%r9,8), %edx
	leal	(%rdx,%rdx,2), %eax
	movq	%rax, 128(%rsp)                 ## 8-byte Spill
	vxorps	%xmm0, %xmm0, %xmm0
	vxorps	%xmm1, %xmm1, %xmm1
	movq	-88(%rsp), %r9                  ## 8-byte Reload
	leaq	224(%r9), %rax
	movq	%rax, 24(%rsp)                  ## 8-byte Spill
	movq	-120(%rsp), %rdx                ## 8-byte Reload
	leaq	224(%rdx), %rax
	movq	%rax, 16(%rsp)                  ## 8-byte Spill
	leaq	32(%r9), %rax
	movq	%rax, -104(%rsp)                ## 8-byte Spill
	leaq	32(%rdx), %rax
	movq	%rax, -24(%rsp)                 ## 8-byte Spill
	xorl	%edx, %edx
	movl	%r12d, -56(%rsp)                ## 4-byte Spill
	jmp	LBB7_62
	.p2align	4, 0x90
LBB7_113:                               ## %"end for head2_relu.s0.n.n9.loopexit.us.us.2"
                                        ##   in Loop: Header=BB7_62 Depth=1
	movq	136(%rsp), %rax                 ## 8-byte Reload
	movq	-40(%rsp), %rcx                 ## 8-byte Reload
	leal	(%rcx,%rax), %r15d
	movq	128(%rsp), %rdx                 ## 8-byte Reload
	movq	-8(%rsp), %rcx                  ## 8-byte Reload
	addl	%edx, %ecx
	movq	-96(%rsp), %rsi                 ## 8-byte Reload
	leal	(%rsi,%rax), %edi
	movq	-72(%rsp), %rsi                 ## 8-byte Reload
	leal	(%rsi,%rdx), %ebp
	movq	-64(%rsp), %rsi                 ## 8-byte Reload
	leal	(%rsi,%rax), %ebx
	movq	-48(%rsp), %rax                 ## 8-byte Reload
	leal	(%rax,%rdx), %esi
	movq	56(%rsp), %rdx                  ## 8-byte Reload
	cmpq	88(%rsp), %rdx                  ## 8-byte Folded Reload
	movl	-56(%rsp), %r12d                ## 4-byte Reload
	je	LBB7_114
LBB7_62:                                ## %"for head2_relu.s0.w.wi2.us"
                                        ## =>This Loop Header: Depth=1
                                        ##     Child Loop BB7_63 Depth 2
                                        ##       Child Loop BB7_71 Depth 3
                                        ##       Child Loop BB7_74 Depth 3
                                        ##       Child Loop BB7_77 Depth 3
                                        ##     Child Loop BB7_80 Depth 2
                                        ##       Child Loop BB7_88 Depth 3
                                        ##       Child Loop BB7_91 Depth 3
                                        ##       Child Loop BB7_94 Depth 3
                                        ##     Child Loop BB7_97 Depth 2
                                        ##       Child Loop BB7_105 Depth 3
                                        ##       Child Loop BB7_108 Depth 3
                                        ##       Child Loop BB7_111 Depth 3
	movslq	%ebx, %rax
	movq	%rax, -64(%rsp)                 ## 8-byte Spill
	movslq	%esi, %rax
	movq	%rax, -48(%rsp)                 ## 8-byte Spill
	movslq	%edi, %rax
	movq	%rax, -96(%rsp)                 ## 8-byte Spill
	movslq	%ebp, %rax
	movq	%rax, -72(%rsp)                 ## 8-byte Spill
	movslq	%r15d, %r15
	movslq	%ecx, %r11
	leaq	1(%rdx), %rax
	movq	48(%rsp), %rcx                  ## 8-byte Reload
	movq	%rax, 56(%rsp)                  ## 8-byte Spill
	addl	%ecx, %eax
	subl	-16(%rsp), %eax                 ## 4-byte Folded Reload
	imull	40(%rsp), %eax                  ## 4-byte Folded Reload
	addl	%ecx, %edx
	imull	104(%rsp), %edx                 ## 4-byte Folded Reload
	movslq	%eax, %rcx
	movslq	%edx, %rdx
	movq	80(%rsp), %rax                  ## 8-byte Reload
	movq	%rcx, 8(%rsp)                   ## 8-byte Spill
	leaq	(%rax,%rcx), %rdi
	movq	64(%rsp), %rax                  ## 8-byte Reload
	movq	%rdx, -32(%rsp)                 ## 8-byte Spill
	leaq	(%rax,%rdx), %rbx
	movl	-112(%rsp), %ebp                ## 4-byte Reload
	movl	%ebp, %r14d
	xorl	%r13d, %r13d
	xorl	%r9d, %r9d
	movq	-88(%rsp), %r10                 ## 8-byte Reload
	movq	%r11, -8(%rsp)                  ## 8-byte Spill
	movq	%r15, -40(%rsp)                 ## 8-byte Spill
	movq	%rdi, -80(%rsp)                 ## 8-byte Spill
	movq	%rbx, 32(%rsp)                  ## 8-byte Spill
	jmp	LBB7_63
	.p2align	4, 0x90
LBB7_64:                                ## %then_bb13.us.us
                                        ##   in Loop: Header=BB7_63 Depth=2
	movl	%eax, %eax
	leaq	(%rdi,%rax), %rcx
	movq	-120(%rsp), %rdx                ## 8-byte Reload
	vmovups	(%rdx,%rcx,4), %ymm2
	vmaxps	%ymm1, %ymm2, %ymm2
	addq	%rbx, %rax
	vmovups	%ymm2, (%r10,%rax,4)
LBB7_78:                                ## %after_bb12.us.us
                                        ##   in Loop: Header=BB7_63 Depth=2
	incq	%r9
	addl	$8, %r13d
	addl	$-8, %r14d
	cmpq	-128(%rsp), %r9                 ## 8-byte Folded Reload
	movl	-112(%rsp), %ebp                ## 4-byte Reload
	je	LBB7_79
LBB7_63:                                ## %"for head2_relu.s0.n.n8.us.us"
                                        ##   Parent Loop BB7_62 Depth=1
                                        ## =>  This Loop Header: Depth=2
                                        ##       Child Loop BB7_71 Depth 3
                                        ##       Child Loop BB7_74 Depth 3
                                        ##       Child Loop BB7_77 Depth 3
	cmpl	$8, %r14d
	movl	$8, %edx
	cmovll	%r14d, %edx
	movl	%r13d, %r13d
	leal	(,%r9,8), %eax
                                        ## kill: def $ebp killed $ebp def $rbp
	subl	%eax, %ebp
	cmpl	$8, %ebp
	movl	$8, %ecx
	cmovgel	%ecx, %ebp
	leal	(%r8,%r9,8), %ecx
	addl	$8, %ecx
	cmpl	%r12d, %ecx
	jle	LBB7_64
## %bb.65:                              ## %next_bb14.us.us
                                        ##   in Loop: Header=BB7_63 Depth=2
	leal	(%r8,%r9,8), %eax
	cmpl	%eax, %r12d
	jle	LBB7_78
## %bb.66:                              ## %"for head2_relu.s0.n.ni15.us.us.preheader"
                                        ##   in Loop: Header=BB7_63 Depth=2
	cmpl	$16, %ebp
	jae	LBB7_68
## %bb.67:                              ##   in Loop: Header=BB7_63 Depth=2
	xorl	%ecx, %ecx
	jmp	LBB7_76
	.p2align	4, 0x90
LBB7_68:                                ## %vector.ph121
                                        ##   in Loop: Header=BB7_63 Depth=2
	movl	%edx, %r10d
	andl	$-16, %r10d
	addq	$-16, %r10
	shrq	$4, %r10
	movl	%ebp, %eax
	andl	$-16, %eax
	addq	$-16, %rax
	movq	%rax, %r12
	shrq	$4, %r12
	incq	%r12
	cmpq	$48, %rax
	jae	LBB7_70
## %bb.69:                              ##   in Loop: Header=BB7_63 Depth=2
	xorl	%ebx, %ebx
	jmp	LBB7_72
LBB7_70:                                ## %vector.ph121.new
                                        ##   in Loop: Header=BB7_63 Depth=2
	leaq	(%r15,%r13), %rax
	movq	24(%rsp), %rcx                  ## 8-byte Reload
	leaq	(%rcx,%rax,4), %rcx
	leaq	(%r11,%r13), %rax
	movq	16(%rsp), %rsi                  ## 8-byte Reload
	leaq	(%rsi,%rax,4), %r15
	leaq	1(%r10), %r11
	andq	$-4, %r11
	negq	%r11
	xorl	%ebx, %ebx
	.p2align	4, 0x90
LBB7_71:                                ## %vector.body118
                                        ##   Parent Loop BB7_62 Depth=1
                                        ##     Parent Loop BB7_63 Depth=2
                                        ## =>    This Inner Loop Header: Depth=3
	vmovups	-224(%r15,%rbx,4), %ymm2
	vmovups	-192(%r15,%rbx,4), %ymm3
	vmaxps	%ymm1, %ymm2, %ymm2
	vmaxps	%ymm1, %ymm3, %ymm3
	vmovups	%ymm2, -224(%rcx,%rbx,4)
	vmovups	%ymm3, -192(%rcx,%rbx,4)
	vmovups	-160(%r15,%rbx,4), %ymm2
	vmovups	-128(%r15,%rbx,4), %ymm3
	vmaxps	%ymm1, %ymm2, %ymm2
	vmaxps	%ymm1, %ymm3, %ymm3
	vmovups	%ymm2, -160(%rcx,%rbx,4)
	vmovups	%ymm3, -128(%rcx,%rbx,4)
	vmovups	-96(%r15,%rbx,4), %ymm2
	vmovups	-64(%r15,%rbx,4), %ymm3
	vmaxps	%ymm1, %ymm2, %ymm2
	vmaxps	%ymm1, %ymm3, %ymm3
	vmovups	%ymm2, -96(%rcx,%rbx,4)
	vmovups	%ymm3, -64(%rcx,%rbx,4)
	vmovups	-32(%r15,%rbx,4), %ymm2
	vmovups	(%r15,%rbx,4), %ymm3
	vmaxps	%ymm1, %ymm2, %ymm2
	vmaxps	%ymm1, %ymm3, %ymm3
	vmovups	%ymm2, -32(%rcx,%rbx,4)
	vmovups	%ymm3, (%rcx,%rbx,4)
	addq	$64, %rbx
	addq	$4, %r11
	jne	LBB7_71
LBB7_72:                                ## %middle.block116.unr-lcssa
                                        ##   in Loop: Header=BB7_63 Depth=2
	movl	%ebp, %ecx
	andl	$-16, %ecx
	testb	$3, %r12b
	movq	-8(%rsp), %r11                  ## 8-byte Reload
	movq	-40(%rsp), %r15                 ## 8-byte Reload
	je	LBB7_75
## %bb.73:                              ## %vector.body118.epil.preheader
                                        ##   in Loop: Header=BB7_63 Depth=2
	incb	%r10b
	movzbl	%r10b, %edi
	andl	$3, %edi
	shlq	$6, %rdi
	leaq	(%r15,%rbx), %rax
	addq	%r13, %rax
	movq	-104(%rsp), %rsi                ## 8-byte Reload
	leaq	(%rsi,%rax,4), %rax
	addq	%r11, %rbx
	addq	%r13, %rbx
	movq	-24(%rsp), %rsi                 ## 8-byte Reload
	leaq	(%rsi,%rbx,4), %rbx
	xorl	%esi, %esi
	.p2align	4, 0x90
LBB7_74:                                ## %vector.body118.epil
                                        ##   Parent Loop BB7_62 Depth=1
                                        ##     Parent Loop BB7_63 Depth=2
                                        ## =>    This Inner Loop Header: Depth=3
	vmovups	-32(%rbx,%rsi), %ymm2
	vmovups	(%rbx,%rsi), %ymm3
	vmaxps	%ymm1, %ymm2, %ymm2
	vmaxps	%ymm1, %ymm3, %ymm3
	vmovups	%ymm2, -32(%rax,%rsi)
	vmovups	%ymm3, (%rax,%rsi)
	addq	$64, %rsi
	cmpq	%rsi, %rdi
	jne	LBB7_74
LBB7_75:                                ## %middle.block116
                                        ##   in Loop: Header=BB7_63 Depth=2
	cmpq	%rbp, %rcx
	movq	-88(%rsp), %r10                 ## 8-byte Reload
	movl	-56(%rsp), %r12d                ## 4-byte Reload
	movq	-80(%rsp), %rdi                 ## 8-byte Reload
	movq	32(%rsp), %rbx                  ## 8-byte Reload
	je	LBB7_78
LBB7_76:                                ## %"for head2_relu.s0.n.ni15.us.us.preheader218"
                                        ##   in Loop: Header=BB7_63 Depth=2
	subq	%rcx, %rdx
	leaq	(%r15,%rcx), %rax
	addq	%r13, %rax
	leaq	(%r10,%rax,4), %rax
	addq	%r11, %rcx
	addq	%r13, %rcx
	movq	-120(%rsp), %rsi                ## 8-byte Reload
	leaq	(%rsi,%rcx,4), %rcx
	xorl	%esi, %esi
	.p2align	4, 0x90
LBB7_77:                                ## %"for head2_relu.s0.n.ni15.us.us"
                                        ##   Parent Loop BB7_62 Depth=1
                                        ##     Parent Loop BB7_63 Depth=2
                                        ## =>    This Inner Loop Header: Depth=3
	vmovss	(%rcx,%rsi,4), %xmm2            ## xmm2 = mem[0],zero,zero,zero
	vmaxss	%xmm0, %xmm2, %xmm2
	vmovss	%xmm2, (%rax,%rsi,4)
	incq	%rsi
	cmpq	%rsi, %rdx
	jne	LBB7_77
	jmp	LBB7_78
	.p2align	4, 0x90
LBB7_79:                                ## %"end for head2_relu.s0.n.n9.loopexit.us.us"
                                        ##   in Loop: Header=BB7_62 Depth=1
	movq	144(%rsp), %rax                 ## 8-byte Reload
	movq	8(%rsp), %rcx                   ## 8-byte Reload
	addq	%rcx, %rax
	movq	%rax, -80(%rsp)                 ## 8-byte Spill
	movq	152(%rsp), %rax                 ## 8-byte Reload
	movq	-32(%rsp), %rcx                 ## 8-byte Reload
	leaq	(%rax,%rcx), %r11
	xorl	%r14d, %r14d
	movl	%ebp, %r15d
	xorl	%r9d, %r9d
	movq	-120(%rsp), %r13                ## 8-byte Reload
	jmp	LBB7_80
	.p2align	4, 0x90
LBB7_81:                                ## %then_bb13.us.us.1
                                        ##   in Loop: Header=BB7_80 Depth=2
	movl	%eax, %eax
	movq	-80(%rsp), %rcx                 ## 8-byte Reload
	addq	%rax, %rcx
	vmovups	(%r13,%rcx,4), %ymm2
	vmaxps	%ymm1, %ymm2, %ymm2
	addq	%r11, %rax
	vmovups	%ymm2, (%r10,%rax,4)
LBB7_95:                                ## %after_bb12.us.us.1
                                        ##   in Loop: Header=BB7_80 Depth=2
	incq	%r9
	addl	$8, %r14d
	addl	$-8, %r15d
	cmpq	-128(%rsp), %r9                 ## 8-byte Folded Reload
	je	LBB7_96
LBB7_80:                                ## %"for head2_relu.s0.n.n8.us.us.1"
                                        ##   Parent Loop BB7_62 Depth=1
                                        ## =>  This Loop Header: Depth=2
                                        ##       Child Loop BB7_88 Depth 3
                                        ##       Child Loop BB7_91 Depth 3
                                        ##       Child Loop BB7_94 Depth 3
	cmpl	$8, %r15d
	movl	$8, %esi
	cmovll	%r15d, %esi
	movl	%r14d, %r14d
	leal	(,%r9,8), %eax
	movl	%ebp, %ebx
	subl	%eax, %ebx
	cmpl	$8, %ebx
	movl	$8, %ecx
	cmovgel	%ecx, %ebx
	leal	(%r8,%r9,8), %ecx
	addl	$8, %ecx
	cmpl	%r12d, %ecx
	jle	LBB7_81
## %bb.82:                              ## %next_bb14.us.us.1
                                        ##   in Loop: Header=BB7_80 Depth=2
	leal	(%r8,%r9,8), %eax
	cmpl	%eax, %r12d
	jle	LBB7_95
## %bb.83:                              ## %"for head2_relu.s0.n.ni15.us.us.preheader.1"
                                        ##   in Loop: Header=BB7_80 Depth=2
	cmpl	$16, %ebx
	jae	LBB7_85
## %bb.84:                              ##   in Loop: Header=BB7_80 Depth=2
	xorl	%edi, %edi
	jmp	LBB7_93
	.p2align	4, 0x90
LBB7_85:                                ## %vector.ph107
                                        ##   in Loop: Header=BB7_80 Depth=2
	movl	%esi, %r12d
	andl	$-16, %r12d
	addq	$-16, %r12
	shrq	$4, %r12
	movl	%ebx, %eax
	andl	$-16, %eax
	addq	$-16, %rax
	movq	%rax, %r10
	shrq	$4, %r10
	incq	%r10
	cmpq	$48, %rax
	jae	LBB7_87
## %bb.86:                              ##   in Loop: Header=BB7_80 Depth=2
	xorl	%ebp, %ebp
	jmp	LBB7_89
LBB7_87:                                ## %vector.ph107.new
                                        ##   in Loop: Header=BB7_80 Depth=2
	movq	-96(%rsp), %rax                 ## 8-byte Reload
	addq	%r14, %rax
	movq	24(%rsp), %rcx                  ## 8-byte Reload
	leaq	(%rcx,%rax,4), %rdi
	movq	-72(%rsp), %rax                 ## 8-byte Reload
	addq	%r14, %rax
	movq	16(%rsp), %rcx                  ## 8-byte Reload
	leaq	(%rcx,%rax,4), %rcx
	leaq	1(%r12), %r13
	andq	$-4, %r13
	negq	%r13
	xorl	%ebp, %ebp
	.p2align	4, 0x90
LBB7_88:                                ## %vector.body104
                                        ##   Parent Loop BB7_62 Depth=1
                                        ##     Parent Loop BB7_80 Depth=2
                                        ## =>    This Inner Loop Header: Depth=3
	vmovups	-224(%rcx,%rbp,4), %ymm2
	vmovups	-192(%rcx,%rbp,4), %ymm3
	vmaxps	%ymm1, %ymm2, %ymm2
	vmaxps	%ymm1, %ymm3, %ymm3
	vmovups	%ymm2, -224(%rdi,%rbp,4)
	vmovups	%ymm3, -192(%rdi,%rbp,4)
	vmovups	-160(%rcx,%rbp,4), %ymm2
	vmovups	-128(%rcx,%rbp,4), %ymm3
	vmaxps	%ymm1, %ymm2, %ymm2
	vmaxps	%ymm1, %ymm3, %ymm3
	vmovups	%ymm2, -160(%rdi,%rbp,4)
	vmovups	%ymm3, -128(%rdi,%rbp,4)
	vmovups	-96(%rcx,%rbp,4), %ymm2
	vmovups	-64(%rcx,%rbp,4), %ymm3
	vmaxps	%ymm1, %ymm2, %ymm2
	vmaxps	%ymm1, %ymm3, %ymm3
	vmovups	%ymm2, -96(%rdi,%rbp,4)
	vmovups	%ymm3, -64(%rdi,%rbp,4)
	vmovups	-32(%rcx,%rbp,4), %ymm2
	vmovups	(%rcx,%rbp,4), %ymm3
	vmaxps	%ymm1, %ymm2, %ymm2
	vmaxps	%ymm1, %ymm3, %ymm3
	vmovups	%ymm2, -32(%rdi,%rbp,4)
	vmovups	%ymm3, (%rdi,%rbp,4)
	addq	$64, %rbp
	addq	$4, %r13
	jne	LBB7_88
LBB7_89:                                ## %middle.block102.unr-lcssa
                                        ##   in Loop: Header=BB7_80 Depth=2
	movl	%ebx, %edi
	andl	$-16, %edi
	testb	$3, %r10b
	je	LBB7_92
## %bb.90:                              ## %vector.body104.epil.preheader
                                        ##   in Loop: Header=BB7_80 Depth=2
	incb	%r12b
	movzbl	%r12b, %ecx
	andl	$3, %ecx
	shlq	$6, %rcx
	movq	-96(%rsp), %rax                 ## 8-byte Reload
	addq	%rbp, %rax
	addq	%r14, %rax
	movq	-104(%rsp), %rdx                ## 8-byte Reload
	leaq	(%rdx,%rax,4), %rdx
	addq	-72(%rsp), %rbp                 ## 8-byte Folded Reload
	addq	%r14, %rbp
	movq	-24(%rsp), %rax                 ## 8-byte Reload
	leaq	(%rax,%rbp,4), %rbp
	xorl	%eax, %eax
	.p2align	4, 0x90
LBB7_91:                                ## %vector.body104.epil
                                        ##   Parent Loop BB7_62 Depth=1
                                        ##     Parent Loop BB7_80 Depth=2
                                        ## =>    This Inner Loop Header: Depth=3
	vmovups	-32(%rbp,%rax), %ymm2
	vmovups	(%rbp,%rax), %ymm3
	vmaxps	%ymm1, %ymm2, %ymm2
	vmaxps	%ymm1, %ymm3, %ymm3
	vmovups	%ymm2, -32(%rdx,%rax)
	vmovups	%ymm3, (%rdx,%rax)
	addq	$64, %rax
	cmpq	%rax, %rcx
	jne	LBB7_91
LBB7_92:                                ## %middle.block102
                                        ##   in Loop: Header=BB7_80 Depth=2
	cmpq	%rbx, %rdi
	movq	-120(%rsp), %r13                ## 8-byte Reload
	movq	-88(%rsp), %r10                 ## 8-byte Reload
	movl	-56(%rsp), %r12d                ## 4-byte Reload
	movl	-112(%rsp), %ebp                ## 4-byte Reload
	je	LBB7_95
LBB7_93:                                ## %"for head2_relu.s0.n.ni15.us.us.1.preheader"
                                        ##   in Loop: Header=BB7_80 Depth=2
	subq	%rdi, %rsi
	movq	-96(%rsp), %rax                 ## 8-byte Reload
	addq	%rdi, %rax
	addq	%r14, %rax
	leaq	(%r10,%rax,4), %rcx
	addq	-72(%rsp), %rdi                 ## 8-byte Folded Reload
	addq	%r14, %rdi
	leaq	(,%rdi,4), %rax
	addq	%r13, %rax
	xorl	%edx, %edx
	.p2align	4, 0x90
LBB7_94:                                ## %"for head2_relu.s0.n.ni15.us.us.1"
                                        ##   Parent Loop BB7_62 Depth=1
                                        ##     Parent Loop BB7_80 Depth=2
                                        ## =>    This Inner Loop Header: Depth=3
	vmovss	(%rax,%rdx,4), %xmm2            ## xmm2 = mem[0],zero,zero,zero
	vmaxss	%xmm0, %xmm2, %xmm2
	vmovss	%xmm2, (%rcx,%rdx,4)
	incq	%rdx
	cmpq	%rdx, %rsi
	jne	LBB7_94
	jmp	LBB7_95
	.p2align	4, 0x90
LBB7_96:                                ## %"end for head2_relu.s0.n.n9.loopexit.us.us.1"
                                        ##   in Loop: Header=BB7_62 Depth=1
	movq	8(%rsp), %r14                   ## 8-byte Reload
	addq	(%rsp), %r14                    ## 8-byte Folded Reload
	movq	-32(%rsp), %r10                 ## 8-byte Reload
	addq	96(%rsp), %r10                  ## 8-byte Folded Reload
	xorl	%r12d, %r12d
	movl	%ebp, %r15d
	xorl	%r9d, %r9d
	movq	%r10, -32(%rsp)                 ## 8-byte Spill
	jmp	LBB7_97
	.p2align	4, 0x90
LBB7_98:                                ## %then_bb13.us.us.2
                                        ##   in Loop: Header=BB7_97 Depth=2
	movl	%eax, %eax
	leaq	(%r14,%rax), %rcx
	vmovups	(%r13,%rcx,4), %ymm2
	vmaxps	%ymm1, %ymm2, %ymm2
	addq	%r10, %rax
	movq	-88(%rsp), %rcx                 ## 8-byte Reload
	vmovups	%ymm2, (%rcx,%rax,4)
LBB7_112:                               ## %after_bb12.us.us.2
                                        ##   in Loop: Header=BB7_97 Depth=2
	incq	%r9
	addl	$8, %r12d
	addl	$-8, %r15d
	cmpq	-128(%rsp), %r9                 ## 8-byte Folded Reload
	je	LBB7_113
LBB7_97:                                ## %"for head2_relu.s0.n.n8.us.us.2"
                                        ##   Parent Loop BB7_62 Depth=1
                                        ## =>  This Loop Header: Depth=2
                                        ##       Child Loop BB7_105 Depth 3
                                        ##       Child Loop BB7_108 Depth 3
                                        ##       Child Loop BB7_111 Depth 3
	cmpl	$8, %r15d
	movl	$8, %esi
	cmovll	%r15d, %esi
	movl	%r12d, %r12d
	leal	(,%r9,8), %eax
	movl	%ebp, %ebx
	subl	%eax, %ebx
	cmpl	$8, %ebx
	movl	$8, %ecx
	cmovgel	%ecx, %ebx
	leal	(%r8,%r9,8), %ecx
	addl	$8, %ecx
	movl	-56(%rsp), %edx                 ## 4-byte Reload
	cmpl	%edx, %ecx
	jle	LBB7_98
## %bb.99:                              ## %next_bb14.us.us.2
                                        ##   in Loop: Header=BB7_97 Depth=2
	leal	(%r8,%r9,8), %eax
	cmpl	%eax, %edx
	jle	LBB7_112
## %bb.100:                             ## %"for head2_relu.s0.n.ni15.us.us.preheader.2"
                                        ##   in Loop: Header=BB7_97 Depth=2
	cmpl	$16, %ebx
	jae	LBB7_102
## %bb.101:                             ##   in Loop: Header=BB7_97 Depth=2
	xorl	%edi, %edi
	jmp	LBB7_110
	.p2align	4, 0x90
LBB7_102:                               ## %vector.ph
                                        ##   in Loop: Header=BB7_97 Depth=2
	movl	%esi, %r11d
	andl	$-16, %r11d
	addq	$-16, %r11
	shrq	$4, %r11
	movl	%ebx, %eax
	andl	$-16, %eax
	addq	$-16, %rax
	movq	%rax, %r10
	shrq	$4, %r10
	incq	%r10
	cmpq	$48, %rax
	jae	LBB7_104
## %bb.103:                             ##   in Loop: Header=BB7_97 Depth=2
	xorl	%ebp, %ebp
	jmp	LBB7_106
LBB7_104:                               ## %vector.ph.new
                                        ##   in Loop: Header=BB7_97 Depth=2
	movq	-64(%rsp), %rax                 ## 8-byte Reload
	addq	%r12, %rax
	movq	24(%rsp), %rcx                  ## 8-byte Reload
	leaq	(%rcx,%rax,4), %rdi
	movq	-48(%rsp), %rax                 ## 8-byte Reload
	addq	%r12, %rax
	movq	16(%rsp), %rcx                  ## 8-byte Reload
	leaq	(%rcx,%rax,4), %rdx
	leaq	1(%r11), %rcx
	andq	$-4, %rcx
	negq	%rcx
	xorl	%ebp, %ebp
	.p2align	4, 0x90
LBB7_105:                               ## %vector.body
                                        ##   Parent Loop BB7_62 Depth=1
                                        ##     Parent Loop BB7_97 Depth=2
                                        ## =>    This Inner Loop Header: Depth=3
	vmovups	-224(%rdx,%rbp,4), %ymm2
	vmovups	-192(%rdx,%rbp,4), %ymm3
	vmaxps	%ymm1, %ymm2, %ymm2
	vmaxps	%ymm1, %ymm3, %ymm3
	vmovups	%ymm2, -224(%rdi,%rbp,4)
	vmovups	%ymm3, -192(%rdi,%rbp,4)
	vmovups	-160(%rdx,%rbp,4), %ymm2
	vmovups	-128(%rdx,%rbp,4), %ymm3
	vmaxps	%ymm1, %ymm2, %ymm2
	vmaxps	%ymm1, %ymm3, %ymm3
	vmovups	%ymm2, -160(%rdi,%rbp,4)
	vmovups	%ymm3, -128(%rdi,%rbp,4)
	vmovups	-96(%rdx,%rbp,4), %ymm2
	vmovups	-64(%rdx,%rbp,4), %ymm3
	vmaxps	%ymm1, %ymm2, %ymm2
	vmaxps	%ymm1, %ymm3, %ymm3
	vmovups	%ymm2, -96(%rdi,%rbp,4)
	vmovups	%ymm3, -64(%rdi,%rbp,4)
	vmovups	-32(%rdx,%rbp,4), %ymm2
	vmovups	(%rdx,%rbp,4), %ymm3
	vmaxps	%ymm1, %ymm2, %ymm2
	vmaxps	%ymm1, %ymm3, %ymm3
	vmovups	%ymm2, -32(%rdi,%rbp,4)
	vmovups	%ymm3, (%rdi,%rbp,4)
	addq	$64, %rbp
	addq	$4, %rcx
	jne	LBB7_105
LBB7_106:                               ## %middle.block.unr-lcssa
                                        ##   in Loop: Header=BB7_97 Depth=2
	movl	%ebx, %edi
	andl	$-16, %edi
	testb	$3, %r10b
	je	LBB7_109
## %bb.107:                             ## %vector.body.epil.preheader
                                        ##   in Loop: Header=BB7_97 Depth=2
	incb	%r11b
	movzbl	%r11b, %ecx
	andl	$3, %ecx
	shlq	$6, %rcx
	movq	-64(%rsp), %rax                 ## 8-byte Reload
	addq	%rbp, %rax
	addq	%r12, %rax
	movq	-104(%rsp), %rdx                ## 8-byte Reload
	leaq	(%rdx,%rax,4), %rdx
	addq	-48(%rsp), %rbp                 ## 8-byte Folded Reload
	addq	%r12, %rbp
	movq	-24(%rsp), %rax                 ## 8-byte Reload
	leaq	(%rax,%rbp,4), %rbp
	xorl	%eax, %eax
	.p2align	4, 0x90
LBB7_108:                               ## %vector.body.epil
                                        ##   Parent Loop BB7_62 Depth=1
                                        ##     Parent Loop BB7_97 Depth=2
                                        ## =>    This Inner Loop Header: Depth=3
	vmovups	-32(%rbp,%rax), %ymm2
	vmovups	(%rbp,%rax), %ymm3
	vmaxps	%ymm1, %ymm2, %ymm2
	vmaxps	%ymm1, %ymm3, %ymm3
	vmovups	%ymm2, -32(%rdx,%rax)
	vmovups	%ymm3, (%rdx,%rax)
	addq	$64, %rax
	cmpq	%rax, %rcx
	jne	LBB7_108
LBB7_109:                               ## %middle.block
                                        ##   in Loop: Header=BB7_97 Depth=2
	cmpq	%rbx, %rdi
	movl	-112(%rsp), %ebp                ## 4-byte Reload
	movq	-32(%rsp), %r10                 ## 8-byte Reload
	je	LBB7_112
LBB7_110:                               ## %"for head2_relu.s0.n.ni15.us.us.2.preheader"
                                        ##   in Loop: Header=BB7_97 Depth=2
	subq	%rdi, %rsi
	movq	-64(%rsp), %rax                 ## 8-byte Reload
	addq	%rdi, %rax
	addq	%r12, %rax
	movq	-88(%rsp), %rcx                 ## 8-byte Reload
	leaq	(%rcx,%rax,4), %rcx
	addq	-48(%rsp), %rdi                 ## 8-byte Folded Reload
	addq	%r12, %rdi
	leaq	(,%rdi,4), %rax
	addq	%r13, %rax
	xorl	%edx, %edx
	.p2align	4, 0x90
LBB7_111:                               ## %"for head2_relu.s0.n.ni15.us.us.2"
                                        ##   Parent Loop BB7_62 Depth=1
                                        ##     Parent Loop BB7_97 Depth=2
                                        ## =>    This Inner Loop Header: Depth=3
	vmovss	(%rax,%rdx,4), %xmm2            ## xmm2 = mem[0],zero,zero,zero
	vmaxss	%xmm0, %xmm2, %xmm2
	vmovss	%xmm2, (%rcx,%rdx,4)
	incq	%rdx
	cmpq	%rdx, %rsi
	jne	LBB7_111
	jmp	LBB7_112
LBB7_48:                                ## %then_bb.split
	cmpl	$8, %ebp
	jl	LBB7_114
## %bb.49:                              ## %"for head2_relu.s0.w.wi.us44.preheader"
	movl	%r9d, %r15d
	movl	-112(%rsp), %r13d               ## 4-byte Reload
	movl	%r10d, %ebx
	leaq	1(%rbx), %rdi
	addq	$2, %rbx
	leaq	-1(%r13), %r8
	movl	%r13d, %r12d
	andl	$3, %r12d
	movl	%r13d, %edx
	andl	$-4, %edx
	andl	$7, %esi
	movq	%r14, %rbp
	movq	%r14, %rcx
	imulq	%rsi, %rcx
	leaq	(%rcx,%rcx,2), %rcx
	movq	-88(%rsp), %r9                  ## 8-byte Reload
	leaq	(%r9,%rcx,4), %r11
	imull	%ebp, %r15d
	imull	$168, %r15d, %r14d
	leal	(,%rbp,8), %ecx
	leal	(%rcx,%rcx,2), %ecx
	movl	%ecx, -128(%rsp)                ## 4-byte Spill
	movq	(%rsp), %rcx                    ## 8-byte Reload
	imulq	%rcx, %rsi
	leaq	(%rsi,%rsi,2), %rsi
	movq	-120(%rsp), %r15                ## 8-byte Reload
	movq	%rsi, -56(%rsp)                 ## 8-byte Spill
	leaq	(%r15,%rsi,4), %rsi
	movq	%rsi, -72(%rsp)                 ## 8-byte Spill
	subl	-16(%rsp), %eax                 ## 4-byte Folded Reload
	incl	%eax
	imull	%ecx, %eax
	shll	$3, %eax
	leal	(%rax,%rax,2), %esi
	leal	(,%rcx,8), %eax
	leal	(%rax,%rax,2), %r10d
	movq	%rdi, %rax
	imulq	%rbp, %rax
	imulq	%rcx, %rdi
	imulq	%rbx, %rbp
	imulq	%rcx, %rbx
	movq	%r12, %rcx
	shlq	$5, %rcx
	vxorps	%xmm0, %xmm0, %xmm0
	movq	%rax, -64(%rsp)                 ## 8-byte Spill
	leaq	(%r9,%rax,4), %rax
	movq	%rax, -96(%rsp)                 ## 8-byte Spill
	movq	%rdi, -48(%rsp)                 ## 8-byte Spill
	leaq	(%r15,%rdi,4), %rax
	movq	%rax, -80(%rsp)                 ## 8-byte Spill
	movq	%rbp, 72(%rsp)                  ## 8-byte Spill
	leaq	(%r9,%rbp,4), %rax
	movq	%rax, -104(%rsp)                ## 8-byte Spill
	movq	%rbx, -112(%rsp)                ## 8-byte Spill
	leaq	(%r15,%rbx,4), %rax
	movq	%rax, -24(%rsp)                 ## 8-byte Spill
	xorl	%r15d, %r15d
	jmp	LBB7_50
LBB7_127:                               ## %"end for head2_relu.s0.n.n.loopexit.us43.us.2"
                                        ##   in Loop: Header=BB7_50 Depth=1
	addl	-128(%rsp), %r14d               ## 4-byte Folded Reload
	addl	%r10d, %esi
	cmpq	$7, %r15
	je	LBB7_114
LBB7_50:                                ## %"for head2_relu.s0.w.wi.us44"
                                        ## =>This Loop Header: Depth=1
                                        ##     Child Loop BB7_53 Depth 2
                                        ##     Child Loop BB7_56 Depth 2
                                        ##     Child Loop BB7_116 Depth 2
                                        ##     Child Loop BB7_119 Depth 2
                                        ##     Child Loop BB7_123 Depth 2
                                        ##     Child Loop BB7_126 Depth 2
	movslq	%r14d, %r14
	movslq	%esi, %rsi
	cmpq	$3, %r8
	jae	LBB7_52
## %bb.51:                              ##   in Loop: Header=BB7_50 Depth=1
	xorl	%eax, %eax
	jmp	LBB7_54
LBB7_52:                                ## %"for head2_relu.s0.n.n.us36.us.preheader"
                                        ##   in Loop: Header=BB7_50 Depth=1
	leaq	(%r11,%r14,4), %rdi
	movq	-72(%rsp), %rax                 ## 8-byte Reload
	leaq	(%rax,%rsi,4), %rbx
	movl	$96, %ebp
	xorl	%eax, %eax
LBB7_53:                                ## %"for head2_relu.s0.n.n.us36.us"
                                        ##   Parent Loop BB7_50 Depth=1
                                        ## =>  This Inner Loop Header: Depth=2
	vmovups	-96(%rbx,%rbp), %ymm1
	vmaxps	%ymm0, %ymm1, %ymm1
	vmovups	%ymm1, -96(%rdi,%rbp)
	vmovups	-64(%rbx,%rbp), %ymm1
	vmaxps	%ymm0, %ymm1, %ymm1
	vmovups	%ymm1, -64(%rdi,%rbp)
	vmovups	-32(%rbx,%rbp), %ymm1
	vmaxps	%ymm0, %ymm1, %ymm1
	vmovups	%ymm1, -32(%rdi,%rbp)
	vmovups	(%rbx,%rbp), %ymm1
	vmaxps	%ymm0, %ymm1, %ymm1
	vmovups	%ymm1, (%rdi,%rbp)
	addq	$4, %rax
	subq	$-128, %rbp
	cmpq	%rax, %rdx
	jne	LBB7_53
LBB7_54:                                ## %"end for head2_relu.s0.n.n.loopexit.us43.us.unr-lcssa"
                                        ##   in Loop: Header=BB7_50 Depth=1
	testq	%r12, %r12
	je	LBB7_57
## %bb.55:                              ## %"for head2_relu.s0.n.n.us36.us.epil.preheader"
                                        ##   in Loop: Header=BB7_50 Depth=1
	leaq	(%r14,%rax,8), %rdi
	leaq	(%r11,%rdi,4), %rdi
	movq	-56(%rsp), %rbp                 ## 8-byte Reload
	leaq	(,%rax,8), %rax
	addq	%rbp, %rax
	addq	%rsi, %rax
	movq	-120(%rsp), %rbp                ## 8-byte Reload
	leaq	(,%rax,4), %rax
	addq	%rbp, %rax
	xorl	%ebp, %ebp
LBB7_56:                                ## %"for head2_relu.s0.n.n.us36.us.epil"
                                        ##   Parent Loop BB7_50 Depth=1
                                        ## =>  This Inner Loop Header: Depth=2
	vmovups	(%rax,%rbp), %ymm1
	vmaxps	%ymm0, %ymm1, %ymm1
	vmovups	%ymm1, (%rdi,%rbp)
	addq	$32, %rbp
	cmpq	%rbp, %rcx
	jne	LBB7_56
LBB7_57:                                ## %"end for head2_relu.s0.n.n.loopexit.us43.us"
                                        ##   in Loop: Header=BB7_50 Depth=1
	cmpq	$3, %r8
	jae	LBB7_115
## %bb.58:                              ##   in Loop: Header=BB7_50 Depth=1
	xorl	%eax, %eax
	jmp	LBB7_117
LBB7_115:                               ## %"for head2_relu.s0.n.n.us36.us.1.preheader"
                                        ##   in Loop: Header=BB7_50 Depth=1
	movq	-96(%rsp), %rax                 ## 8-byte Reload
	leaq	(%rax,%r14,4), %rdi
	movq	-80(%rsp), %rax                 ## 8-byte Reload
	leaq	(%rax,%rsi,4), %r9
	movl	$96, %ebx
	xorl	%eax, %eax
LBB7_116:                               ## %"for head2_relu.s0.n.n.us36.us.1"
                                        ##   Parent Loop BB7_50 Depth=1
                                        ## =>  This Inner Loop Header: Depth=2
	vmovups	-96(%r9,%rbx), %ymm1
	vmaxps	%ymm0, %ymm1, %ymm1
	vmovups	%ymm1, -96(%rdi,%rbx)
	vmovups	-64(%r9,%rbx), %ymm1
	vmaxps	%ymm0, %ymm1, %ymm1
	vmovups	%ymm1, -64(%rdi,%rbx)
	vmovups	-32(%r9,%rbx), %ymm1
	vmaxps	%ymm0, %ymm1, %ymm1
	vmovups	%ymm1, -32(%rdi,%rbx)
	vmovups	(%r9,%rbx), %ymm1
	vmaxps	%ymm0, %ymm1, %ymm1
	vmovups	%ymm1, (%rdi,%rbx)
	addq	$4, %rax
	subq	$-128, %rbx
	cmpq	%rax, %rdx
	jne	LBB7_116
LBB7_117:                               ## %"end for head2_relu.s0.n.n.loopexit.us43.us.1.unr-lcssa"
                                        ##   in Loop: Header=BB7_50 Depth=1
	testb	$3, %r13b
	je	LBB7_120
## %bb.118:                             ## %"for head2_relu.s0.n.n.us36.us.1.epil.preheader"
                                        ##   in Loop: Header=BB7_50 Depth=1
	movq	-64(%rsp), %rdi                 ## 8-byte Reload
	leaq	(%rdi,%rax,8), %rdi
	addq	%r14, %rdi
	movq	-88(%rsp), %rbp                 ## 8-byte Reload
	leaq	(,%rdi,4), %rdi
	addq	%rbp, %rdi
	movq	-48(%rsp), %rbp                 ## 8-byte Reload
	leaq	(,%rax,8), %rax
	addq	%rbp, %rax
	addq	%rsi, %rax
	movq	-120(%rsp), %rbp                ## 8-byte Reload
	leaq	(,%rax,4), %rax
	addq	%rbp, %rax
	xorl	%ebp, %ebp
LBB7_119:                               ## %"for head2_relu.s0.n.n.us36.us.1.epil"
                                        ##   Parent Loop BB7_50 Depth=1
                                        ## =>  This Inner Loop Header: Depth=2
	vmovups	(%rax,%rbp), %ymm1
	vmaxps	%ymm0, %ymm1, %ymm1
	vmovups	%ymm1, (%rdi,%rbp)
	addq	$32, %rbp
	cmpq	%rbp, %rcx
	jne	LBB7_119
LBB7_120:                               ## %"end for head2_relu.s0.n.n.loopexit.us43.us.1"
                                        ##   in Loop: Header=BB7_50 Depth=1
	cmpq	$3, %r8
	jae	LBB7_122
## %bb.121:                             ##   in Loop: Header=BB7_50 Depth=1
	xorl	%eax, %eax
	jmp	LBB7_124
LBB7_122:                               ## %"for head2_relu.s0.n.n.us36.us.2.preheader"
                                        ##   in Loop: Header=BB7_50 Depth=1
	movq	-104(%rsp), %rax                ## 8-byte Reload
	leaq	(%rax,%r14,4), %rdi
	movq	-24(%rsp), %rax                 ## 8-byte Reload
	leaq	(%rax,%rsi,4), %rbx
	movl	$96, %ebp
	xorl	%eax, %eax
LBB7_123:                               ## %"for head2_relu.s0.n.n.us36.us.2"
                                        ##   Parent Loop BB7_50 Depth=1
                                        ## =>  This Inner Loop Header: Depth=2
	vmovups	-96(%rbx,%rbp), %ymm1
	vmaxps	%ymm0, %ymm1, %ymm1
	vmovups	%ymm1, -96(%rdi,%rbp)
	vmovups	-64(%rbx,%rbp), %ymm1
	vmaxps	%ymm0, %ymm1, %ymm1
	vmovups	%ymm1, -64(%rdi,%rbp)
	vmovups	-32(%rbx,%rbp), %ymm1
	vmaxps	%ymm0, %ymm1, %ymm1
	vmovups	%ymm1, -32(%rdi,%rbp)
	vmovups	(%rbx,%rbp), %ymm1
	vmaxps	%ymm0, %ymm1, %ymm1
	vmovups	%ymm1, (%rdi,%rbp)
	addq	$4, %rax
	subq	$-128, %rbp
	cmpq	%rax, %rdx
	jne	LBB7_123
LBB7_124:                               ## %"end for head2_relu.s0.n.n.loopexit.us43.us.2.unr-lcssa"
                                        ##   in Loop: Header=BB7_50 Depth=1
	incq	%r15
	testb	$3, %r13b
	je	LBB7_127
## %bb.125:                             ## %"for head2_relu.s0.n.n.us36.us.2.epil.preheader"
                                        ##   in Loop: Header=BB7_50 Depth=1
	movq	72(%rsp), %rdi                  ## 8-byte Reload
	leaq	(%rdi,%rax,8), %rdi
	addq	%r14, %rdi
	movq	-88(%rsp), %rbp                 ## 8-byte Reload
	leaq	(,%rdi,4), %rdi
	addq	%rbp, %rdi
	movq	-112(%rsp), %rbp                ## 8-byte Reload
	leaq	(,%rax,8), %rax
	addq	%rbp, %rax
	addq	%rsi, %rax
	movq	-120(%rsp), %rbp                ## 8-byte Reload
	leaq	(,%rax,4), %rax
	addq	%rbp, %rax
	xorl	%ebp, %ebp
LBB7_126:                               ## %"for head2_relu.s0.n.n.us36.us.2.epil"
                                        ##   Parent Loop BB7_50 Depth=1
                                        ## =>  This Inner Loop Header: Depth=2
	vmovups	(%rax,%rbp), %ymm1
	vmaxps	%ymm0, %ymm1, %ymm1
	vmovups	%ymm1, (%rdi,%rbp)
	addq	$32, %rbp
	cmpq	%rbp, %rcx
	jne	LBB7_126
	jmp	LBB7_127
LBB7_3:                                 ## %"for head2_relu.s0.w.wi.us.preheader"
	movl	%r10d, %ebx
	leaq	1(%rbx), %r8
	addq	$2, %rbx
	andl	$-8, %ebp
	movl	%r12d, %ecx
	subl	%ebp, %ecx
	movl	%ecx, -128(%rsp)                ## 4-byte Spill
	andl	$7, %esi
	movq	%r14, %rdi
	movq	%r14, %rcx
	imulq	%rsi, %rcx
	leaq	(%rcx,%rcx,2), %rcx
	movq	%rcx, 48(%rsp)                  ## 8-byte Spill
	leaq	(%r11,%rcx,4), %rcx
	movq	%rcx, 40(%rsp)                  ## 8-byte Spill
	imull	%edi, %r9d
	imull	$168, %r9d, %ecx
	leal	(,%r14,8), %edx
	leal	(%rdx,%rdx,2), %edx
	movq	%rdx, 104(%rsp)                 ## 8-byte Spill
	movq	-112(%rsp), %rdx                ## 8-byte Reload
	leal	(,%rdx,8), %r9d
	movq	(%rsp), %rbp                    ## 8-byte Reload
	imulq	%rbp, %rsi
	leaq	(%rsi,%rsi,2), %rdx
	movq	%rdx, 96(%rsp)                  ## 8-byte Spill
	movq	-120(%rsp), %rsi                ## 8-byte Reload
	leaq	(%rsi,%rdx,4), %rdx
	movq	%rdx, 88(%rsp)                  ## 8-byte Spill
	subl	-16(%rsp), %eax                 ## 4-byte Folded Reload
	incl	%eax
	imull	%ebp, %eax
	shll	$3, %eax
	leal	(%rax,%rax,2), %eax
	leal	(,%rbp,8), %edx
	leal	(%rdx,%rdx,2), %edx
	movq	%rdx, 80(%rsp)                  ## 8-byte Spill
	leaq	32(%r11), %rdx
	movq	%rdx, -96(%rsp)                 ## 8-byte Spill
	movq	%r8, %rdx
	imulq	%r14, %rdx
	movq	%rdx, 64(%rsp)                  ## 8-byte Spill
	imulq	%rbp, %r8
	movq	%r8, -16(%rsp)                  ## 8-byte Spill
	imulq	%rbx, %rdi
	movq	%rdi, 72(%rsp)                  ## 8-byte Spill
	imulq	%rbp, %rbx
	movq	%rbx, 56(%rsp)                  ## 8-byte Spill
	movl	%r9d, -40(%rsp)                 ## 4-byte Spill
	subl	%r9d, %r12d
	movl	%r12d, 32(%rsp)                 ## 4-byte Spill
	vxorps	%xmm0, %xmm0, %xmm0
	vxorps	%xmm1, %xmm1, %xmm1
	leaq	32(%rsi), %rdx
	movq	%rdx, -80(%rsp)                 ## 8-byte Spill
	leaq	224(%r11), %rdx
	movq	%rdx, -104(%rsp)                ## 8-byte Spill
	leaq	224(%rsi), %rdx
	movq	%rdx, -24(%rsp)                 ## 8-byte Spill
	xorl	%edx, %edx
	movq	%rdx, 8(%rsp)                   ## 8-byte Spill
	jmp	LBB7_4
LBB7_154:                               ## %"end for head2_relu.s0.n.n.rebased.loopexit.us.us.2"
                                        ##   in Loop: Header=BB7_4 Depth=1
	movq	104(%rsp), %rax                 ## 8-byte Reload
	movq	-8(%rsp), %rcx                  ## 8-byte Reload
	addl	%eax, %ecx
	movq	80(%rsp), %rax                  ## 8-byte Reload
	movq	-32(%rsp), %rdx                 ## 8-byte Reload
	addl	%edx, %eax
	cmpq	$7, 8(%rsp)                     ## 8-byte Folded Reload
	je	LBB7_114
LBB7_4:                                 ## %"for head2_relu.s0.w.wi.us"
                                        ## =>This Loop Header: Depth=1
                                        ##     Child Loop BB7_5 Depth 2
                                        ##       Child Loop BB7_36 Depth 3
                                        ##       Child Loop BB7_39 Depth 3
                                        ##       Child Loop BB7_42 Depth 3
                                        ##     Child Loop BB7_45 Depth 2
                                        ##       Child Loop BB7_131 Depth 3
                                        ##       Child Loop BB7_134 Depth 3
                                        ##       Child Loop BB7_137 Depth 3
                                        ##     Child Loop BB7_140 Depth 2
                                        ##       Child Loop BB7_146 Depth 3
                                        ##       Child Loop BB7_149 Depth 3
                                        ##       Child Loop BB7_152 Depth 3
	movslq	%ecx, %rdx
	movslq	%eax, %rcx
	incq	8(%rsp)                         ## 8-byte Folded Spill
	movq	40(%rsp), %rax                  ## 8-byte Reload
	leaq	(%rax,%rdx,4), %rax
	movq	%rax, 24(%rsp)                  ## 8-byte Spill
	movq	88(%rsp), %rax                  ## 8-byte Reload
	leaq	(%rax,%rcx,4), %rax
	movq	%rax, 16(%rsp)                  ## 8-byte Spill
	movq	48(%rsp), %rax                  ## 8-byte Reload
	movq	%rdx, -8(%rsp)                  ## 8-byte Spill
	addq	%rdx, %rax
	movq	%rax, -64(%rsp)                 ## 8-byte Spill
	movq	96(%rsp), %rax                  ## 8-byte Reload
	movq	%rcx, -32(%rsp)                 ## 8-byte Spill
	addq	%rcx, %rax
	movq	%rax, -72(%rsp)                 ## 8-byte Spill
	movl	32(%rsp), %r9d                  ## 4-byte Reload
	movl	-128(%rsp), %r12d               ## 4-byte Reload
	movl	-40(%rsp), %eax                 ## 4-byte Reload
	movl	%eax, %ebx
	xorl	%r13d, %r13d
	jmp	LBB7_5
LBB7_43:                                ## %"end for head2_relu.s0.n.ni.us.us"
                                        ##   in Loop: Header=BB7_5 Depth=2
	incq	%r13
	addl	$8, %ebx
	addl	$-8, %r12d
	addl	$-8, %r9d
	cmpq	-48(%rsp), %r13                 ## 8-byte Folded Reload
	je	LBB7_44
LBB7_5:                                 ## %"for head2_relu.s0.n.n.rebased.us.us"
                                        ##   Parent Loop BB7_4 Depth=1
                                        ## =>  This Loop Header: Depth=2
                                        ##       Child Loop BB7_36 Depth 3
                                        ##       Child Loop BB7_39 Depth 3
                                        ##       Child Loop BB7_42 Depth 3
	movslq	%ebx, %rbx
	cmpl	$8, %r9d
	movl	$8, %ebp
	cmovll	%r9d, %ebp
	cmpl	$8, %r12d
	movl	$8, %r10d
	cmovll	%r12d, %r10d
	leal	(,%r13,8), %eax
	movl	-128(%rsp), %ecx                ## 4-byte Reload
	movl	%ecx, %r14d
	subl	%eax, %r14d
	cmpl	$8, %r14d
	movl	$8, %eax
	cmovgel	%eax, %r14d
	movq	-112(%rsp), %rax                ## 8-byte Reload
	addl	%r13d, %eax
	shll	$3, %eax
	movl	-56(%rsp), %ecx                 ## 4-byte Reload
	subl	%eax, %ecx
	testl	%ecx, %ecx
	jle	LBB7_43
## %bb.6:                               ## %"for head2_relu.s0.n.ni.us.us.preheader"
                                        ##   in Loop: Header=BB7_5 Depth=2
	cmpl	$16, %r14d
	jae	LBB7_33
## %bb.7:                               ##   in Loop: Header=BB7_5 Depth=2
	xorl	%esi, %esi
	jmp	LBB7_41
LBB7_33:                                ## %vector.ph163
                                        ##   in Loop: Header=BB7_5 Depth=2
	andl	$-16, %r10d
	addq	$-16, %r10
	shrq	$4, %r10
	movl	%r14d, %ecx
	andl	$-16, %ecx
	addq	$-16, %rcx
	movq	%rcx, %rax
	shrq	$4, %rax
	incq	%rax
	cmpq	$48, %rcx
	jae	LBB7_35
## %bb.34:                              ##   in Loop: Header=BB7_5 Depth=2
	xorl	%r11d, %r11d
	jmp	LBB7_37
LBB7_35:                                ## %vector.ph163.new
                                        ##   in Loop: Header=BB7_5 Depth=2
	movq	24(%rsp), %rcx                  ## 8-byte Reload
	leaq	(%rcx,%rbx,4), %rsi
	movq	16(%rsp), %rcx                  ## 8-byte Reload
	leaq	(%rcx,%rbx,4), %r15
	leaq	1(%r10), %r8
	andq	$-4, %r8
	negq	%r8
	xorl	%r11d, %r11d
LBB7_36:                                ## %vector.body160
                                        ##   Parent Loop BB7_4 Depth=1
                                        ##     Parent Loop BB7_5 Depth=2
                                        ## =>    This Inner Loop Header: Depth=3
	vmovups	(%r15,%r11,4), %ymm2
	vmovups	32(%r15,%r11,4), %ymm3
	vmaxps	%ymm1, %ymm2, %ymm2
	vmaxps	%ymm1, %ymm3, %ymm3
	vmovups	%ymm2, (%rsi,%r11,4)
	vmovups	%ymm3, 32(%rsi,%r11,4)
	vmovups	64(%r15,%r11,4), %ymm2
	vmovups	96(%r15,%r11,4), %ymm3
	vmaxps	%ymm1, %ymm2, %ymm2
	vmaxps	%ymm1, %ymm3, %ymm3
	vmovups	%ymm2, 64(%rsi,%r11,4)
	vmovups	%ymm3, 96(%rsi,%r11,4)
	vmovups	128(%r15,%r11,4), %ymm2
	vmovups	160(%r15,%r11,4), %ymm3
	vmaxps	%ymm1, %ymm2, %ymm2
	vmaxps	%ymm1, %ymm3, %ymm3
	vmovups	%ymm2, 128(%rsi,%r11,4)
	vmovups	%ymm3, 160(%rsi,%r11,4)
	vmovups	192(%r15,%r11,4), %ymm2
	vmovups	224(%r15,%r11,4), %ymm3
	vmaxps	%ymm1, %ymm2, %ymm2
	vmaxps	%ymm1, %ymm3, %ymm3
	vmovups	%ymm2, 192(%rsi,%r11,4)
	vmovups	%ymm3, 224(%rsi,%r11,4)
	addq	$64, %r11
	addq	$4, %r8
	jne	LBB7_36
LBB7_37:                                ## %middle.block158.unr-lcssa
                                        ##   in Loop: Header=BB7_5 Depth=2
	movl	%r14d, %esi
	andl	$-16, %esi
	testb	$3, %al
	je	LBB7_40
## %bb.38:                              ## %vector.body160.epil.preheader
                                        ##   in Loop: Header=BB7_5 Depth=2
	incb	%r10b
	movzbl	%r10b, %eax
	andl	$3, %eax
	shlq	$6, %rax
	movq	-64(%rsp), %rcx                 ## 8-byte Reload
	addq	%r11, %rcx
	addq	%rbx, %rcx
	movq	-96(%rsp), %rdx                 ## 8-byte Reload
	leaq	(%rdx,%rcx,4), %rcx
	addq	-72(%rsp), %r11                 ## 8-byte Folded Reload
	addq	%rbx, %r11
	movq	-80(%rsp), %rdx                 ## 8-byte Reload
	leaq	(%rdx,%r11,4), %rdx
	xorl	%edi, %edi
LBB7_39:                                ## %vector.body160.epil
                                        ##   Parent Loop BB7_4 Depth=1
                                        ##     Parent Loop BB7_5 Depth=2
                                        ## =>    This Inner Loop Header: Depth=3
	vmovups	-32(%rdx,%rdi), %ymm2
	vmovups	(%rdx,%rdi), %ymm3
	vmaxps	%ymm1, %ymm2, %ymm2
	vmaxps	%ymm1, %ymm3, %ymm3
	vmovups	%ymm2, -32(%rcx,%rdi)
	vmovups	%ymm3, (%rcx,%rdi)
	addq	$64, %rdi
	cmpq	%rdi, %rax
	jne	LBB7_39
LBB7_40:                                ## %middle.block158
                                        ##   in Loop: Header=BB7_5 Depth=2
	cmpq	%r14, %rsi
	je	LBB7_43
LBB7_41:                                ## %"for head2_relu.s0.n.ni.us.us.preheader215"
                                        ##   in Loop: Header=BB7_5 Depth=2
	subq	%rsi, %rbp
	movq	-64(%rsp), %rax                 ## 8-byte Reload
	addq	%rsi, %rax
	addq	%rbx, %rax
	movq	-88(%rsp), %rcx                 ## 8-byte Reload
	leaq	(%rcx,%rax,4), %rax
	addq	-72(%rsp), %rsi                 ## 8-byte Folded Reload
	addq	%rbx, %rsi
	movq	-120(%rsp), %rcx                ## 8-byte Reload
	leaq	(%rcx,%rsi,4), %rcx
	xorl	%edx, %edx
LBB7_42:                                ## %"for head2_relu.s0.n.ni.us.us"
                                        ##   Parent Loop BB7_4 Depth=1
                                        ##     Parent Loop BB7_5 Depth=2
                                        ## =>    This Inner Loop Header: Depth=3
	vmovss	(%rcx,%rdx,4), %xmm2            ## xmm2 = mem[0],zero,zero,zero
	vmaxss	%xmm0, %xmm2, %xmm2
	vmovss	%xmm2, (%rax,%rdx,4)
	incq	%rdx
	cmpq	%rdx, %rbp
	jne	LBB7_42
	jmp	LBB7_43
LBB7_44:                                ## %"end for head2_relu.s0.n.n.rebased.loopexit.us.us"
                                        ##   in Loop: Header=BB7_4 Depth=1
	movq	64(%rsp), %rax                  ## 8-byte Reload
	movq	-8(%rsp), %rcx                  ## 8-byte Reload
	leaq	(%rax,%rcx), %r10
	movq	-16(%rsp), %rax                 ## 8-byte Reload
	movq	-32(%rsp), %rcx                 ## 8-byte Reload
	leaq	(%rax,%rcx), %r11
	movl	32(%rsp), %r14d                 ## 4-byte Reload
	movl	-128(%rsp), %r12d               ## 4-byte Reload
	movl	-40(%rsp), %eax                 ## 4-byte Reload
	movl	%eax, %r8d
	xorl	%r15d, %r15d
	jmp	LBB7_45
LBB7_138:                               ## %"end for head2_relu.s0.n.ni.us.us.1"
                                        ##   in Loop: Header=BB7_45 Depth=2
	incq	%r15
	addl	$8, %r8d
	addl	$-8, %r12d
	addl	$-8, %r14d
	cmpq	-48(%rsp), %r15                 ## 8-byte Folded Reload
	je	LBB7_139
LBB7_45:                                ## %"for head2_relu.s0.n.n.rebased.us.us.1"
                                        ##   Parent Loop BB7_4 Depth=1
                                        ## =>  This Loop Header: Depth=2
                                        ##       Child Loop BB7_131 Depth 3
                                        ##       Child Loop BB7_134 Depth 3
                                        ##       Child Loop BB7_137 Depth 3
	cmpl	$8, %r14d
	movl	$8, %edx
	cmovll	%r14d, %edx
	cmpl	$8, %r12d
	movl	$8, %edi
	cmovll	%r12d, %edi
	leal	(,%r15,8), %eax
	movl	-128(%rsp), %ecx                ## 4-byte Reload
	movl	%ecx, %ebx
	subl	%eax, %ebx
	cmpl	$8, %ebx
	movl	$8, %eax
	cmovll	%ebx, %eax
	movslq	%r8d, %r8
	cmpl	$8, %ebx
	movl	$8, %ecx
	cmovgel	%ecx, %ebx
	movq	-112(%rsp), %rcx                ## 8-byte Reload
	addl	%r15d, %ecx
	shll	$3, %ecx
	movl	-56(%rsp), %esi                 ## 4-byte Reload
	subl	%ecx, %esi
	testl	%esi, %esi
	jle	LBB7_138
## %bb.46:                              ## %"for head2_relu.s0.n.ni.us.us.preheader.1"
                                        ##   in Loop: Header=BB7_45 Depth=2
	cmpl	$16, %ebx
	jae	LBB7_128
## %bb.47:                              ##   in Loop: Header=BB7_45 Depth=2
	xorl	%esi, %esi
	jmp	LBB7_136
LBB7_128:                               ## %vector.ph149
                                        ##   in Loop: Header=BB7_45 Depth=2
	andl	$-16, %edi
	addq	$-16, %rdi
	shrq	$4, %rdi
	andl	$-16, %eax
	addq	$-16, %rax
	movq	%rax, %r13
	shrq	$4, %r13
	incq	%r13
	cmpq	$48, %rax
	jae	LBB7_130
## %bb.129:                             ##   in Loop: Header=BB7_45 Depth=2
	xorl	%ebp, %ebp
	jmp	LBB7_132
LBB7_130:                               ## %vector.ph149.new
                                        ##   in Loop: Header=BB7_45 Depth=2
	leaq	(%r10,%r8), %rax
	movq	-104(%rsp), %rcx                ## 8-byte Reload
	leaq	(%rcx,%rax,4), %rsi
	leaq	(%r11,%r8), %rax
	movq	-24(%rsp), %rcx                 ## 8-byte Reload
	leaq	(%rcx,%rax,4), %rax
	leaq	1(%rdi), %rcx
	andq	$-4, %rcx
	negq	%rcx
	xorl	%ebp, %ebp
LBB7_131:                               ## %vector.body146
                                        ##   Parent Loop BB7_4 Depth=1
                                        ##     Parent Loop BB7_45 Depth=2
                                        ## =>    This Inner Loop Header: Depth=3
	vmovups	-224(%rax,%rbp,4), %ymm2
	vmovups	-192(%rax,%rbp,4), %ymm3
	vmaxps	%ymm1, %ymm2, %ymm2
	vmaxps	%ymm1, %ymm3, %ymm3
	vmovups	%ymm2, -224(%rsi,%rbp,4)
	vmovups	%ymm3, -192(%rsi,%rbp,4)
	vmovups	-160(%rax,%rbp,4), %ymm2
	vmovups	-128(%rax,%rbp,4), %ymm3
	vmaxps	%ymm1, %ymm2, %ymm2
	vmaxps	%ymm1, %ymm3, %ymm3
	vmovups	%ymm2, -160(%rsi,%rbp,4)
	vmovups	%ymm3, -128(%rsi,%rbp,4)
	vmovups	-96(%rax,%rbp,4), %ymm2
	vmovups	-64(%rax,%rbp,4), %ymm3
	vmaxps	%ymm1, %ymm2, %ymm2
	vmaxps	%ymm1, %ymm3, %ymm3
	vmovups	%ymm2, -96(%rsi,%rbp,4)
	vmovups	%ymm3, -64(%rsi,%rbp,4)
	vmovups	-32(%rax,%rbp,4), %ymm2
	vmovups	(%rax,%rbp,4), %ymm3
	vmaxps	%ymm1, %ymm2, %ymm2
	vmaxps	%ymm1, %ymm3, %ymm3
	vmovups	%ymm2, -32(%rsi,%rbp,4)
	vmovups	%ymm3, (%rsi,%rbp,4)
	addq	$64, %rbp
	addq	$4, %rcx
	jne	LBB7_131
LBB7_132:                               ## %middle.block144.unr-lcssa
                                        ##   in Loop: Header=BB7_45 Depth=2
	movl	%ebx, %esi
	andl	$-16, %esi
	testb	$3, %r13b
	je	LBB7_135
## %bb.133:                             ## %vector.body146.epil.preheader
                                        ##   in Loop: Header=BB7_45 Depth=2
	incb	%dil
	movzbl	%dil, %eax
	andl	$3, %eax
	shlq	$6, %rax
	leaq	(%r10,%rbp), %rcx
	addq	%r8, %rcx
	movq	-96(%rsp), %rdi                 ## 8-byte Reload
	leaq	(%rdi,%rcx,4), %rcx
	addq	%r11, %rbp
	addq	%r8, %rbp
	movq	-80(%rsp), %rdi                 ## 8-byte Reload
	leaq	(%rdi,%rbp,4), %rdi
	xorl	%ebp, %ebp
LBB7_134:                               ## %vector.body146.epil
                                        ##   Parent Loop BB7_4 Depth=1
                                        ##     Parent Loop BB7_45 Depth=2
                                        ## =>    This Inner Loop Header: Depth=3
	vmovups	-32(%rdi,%rbp), %ymm2
	vmovups	(%rdi,%rbp), %ymm3
	vmaxps	%ymm1, %ymm2, %ymm2
	vmaxps	%ymm1, %ymm3, %ymm3
	vmovups	%ymm2, -32(%rcx,%rbp)
	vmovups	%ymm3, (%rcx,%rbp)
	addq	$64, %rbp
	cmpq	%rbp, %rax
	jne	LBB7_134
LBB7_135:                               ## %middle.block144
                                        ##   in Loop: Header=BB7_45 Depth=2
	cmpq	%rbx, %rsi
	je	LBB7_138
LBB7_136:                               ## %"for head2_relu.s0.n.ni.us.us.1.preheader"
                                        ##   in Loop: Header=BB7_45 Depth=2
	subq	%rsi, %rdx
	leaq	(%r10,%rsi), %rax
	addq	%r8, %rax
	movq	-88(%rsp), %rcx                 ## 8-byte Reload
	leaq	(%rcx,%rax,4), %rax
	addq	%r11, %rsi
	addq	%r8, %rsi
	movq	-120(%rsp), %rcx                ## 8-byte Reload
	leaq	(%rcx,%rsi,4), %rcx
	xorl	%esi, %esi
LBB7_137:                               ## %"for head2_relu.s0.n.ni.us.us.1"
                                        ##   Parent Loop BB7_4 Depth=1
                                        ##     Parent Loop BB7_45 Depth=2
                                        ## =>    This Inner Loop Header: Depth=3
	vmovss	(%rcx,%rsi,4), %xmm2            ## xmm2 = mem[0],zero,zero,zero
	vmaxss	%xmm0, %xmm2, %xmm2
	vmovss	%xmm2, (%rax,%rsi,4)
	incq	%rsi
	cmpq	%rsi, %rdx
	jne	LBB7_137
	jmp	LBB7_138
LBB7_139:                               ## %"end for head2_relu.s0.n.n.rebased.loopexit.us.us.1"
                                        ##   in Loop: Header=BB7_4 Depth=1
	movq	-8(%rsp), %r10                  ## 8-byte Reload
	addq	72(%rsp), %r10                  ## 8-byte Folded Reload
	movq	-32(%rsp), %r11                 ## 8-byte Reload
	addq	56(%rsp), %r11                  ## 8-byte Folded Reload
	movl	32(%rsp), %r14d                 ## 4-byte Reload
	movl	-128(%rsp), %r12d               ## 4-byte Reload
	movl	-40(%rsp), %eax                 ## 4-byte Reload
	movl	%eax, %r8d
	xorl	%r15d, %r15d
	jmp	LBB7_140
LBB7_153:                               ## %"end for head2_relu.s0.n.ni.us.us.2"
                                        ##   in Loop: Header=BB7_140 Depth=2
	incq	%r15
	addl	$8, %r8d
	addl	$-8, %r12d
	addl	$-8, %r14d
	cmpq	-48(%rsp), %r15                 ## 8-byte Folded Reload
	je	LBB7_154
LBB7_140:                               ## %"for head2_relu.s0.n.n.rebased.us.us.2"
                                        ##   Parent Loop BB7_4 Depth=1
                                        ## =>  This Loop Header: Depth=2
                                        ##       Child Loop BB7_146 Depth 3
                                        ##       Child Loop BB7_149 Depth 3
                                        ##       Child Loop BB7_152 Depth 3
	cmpl	$8, %r14d
	movl	$8, %edx
	cmovll	%r14d, %edx
	cmpl	$8, %r12d
	movl	$8, %ebp
	cmovll	%r12d, %ebp
	leal	(,%r15,8), %eax
	movl	-128(%rsp), %ecx                ## 4-byte Reload
	movl	%ecx, %esi
	subl	%eax, %esi
	cmpl	$8, %esi
	movl	$8, %eax
	cmovll	%esi, %eax
	movslq	%r8d, %r8
	cmpl	$8, %esi
	movl	$8, %ecx
	cmovgel	%ecx, %esi
	movq	-112(%rsp), %rcx                ## 8-byte Reload
	addl	%r15d, %ecx
	shll	$3, %ecx
	movl	-56(%rsp), %edi                 ## 4-byte Reload
	subl	%ecx, %edi
	testl	%edi, %edi
	jle	LBB7_153
## %bb.141:                             ## %"for head2_relu.s0.n.ni.us.us.preheader.2"
                                        ##   in Loop: Header=BB7_140 Depth=2
	cmpl	$16, %esi
	jae	LBB7_143
## %bb.142:                             ##   in Loop: Header=BB7_140 Depth=2
	xorl	%edi, %edi
	jmp	LBB7_151
LBB7_143:                               ## %vector.ph135
                                        ##   in Loop: Header=BB7_140 Depth=2
	andl	$-16, %ebp
	addq	$-16, %rbp
	shrq	$4, %rbp
	andl	$-16, %eax
	addq	$-16, %rax
	movq	%rax, %r13
	shrq	$4, %r13
	incq	%r13
	cmpq	$48, %rax
	jae	LBB7_145
## %bb.144:                             ##   in Loop: Header=BB7_140 Depth=2
	xorl	%ebx, %ebx
	jmp	LBB7_147
LBB7_145:                               ## %vector.ph135.new
                                        ##   in Loop: Header=BB7_140 Depth=2
	leaq	(%r10,%r8), %rax
	movq	-104(%rsp), %rcx                ## 8-byte Reload
	leaq	(%rcx,%rax,4), %rdi
	leaq	(%r11,%r8), %rax
	movq	-24(%rsp), %rcx                 ## 8-byte Reload
	leaq	(%rcx,%rax,4), %rcx
	leaq	1(%rbp), %rax
	andq	$-4, %rax
	negq	%rax
	xorl	%ebx, %ebx
LBB7_146:                               ## %vector.body132
                                        ##   Parent Loop BB7_4 Depth=1
                                        ##     Parent Loop BB7_140 Depth=2
                                        ## =>    This Inner Loop Header: Depth=3
	vmovups	-224(%rcx,%rbx,4), %ymm2
	vmovups	-192(%rcx,%rbx,4), %ymm3
	vmaxps	%ymm1, %ymm2, %ymm2
	vmaxps	%ymm1, %ymm3, %ymm3
	vmovups	%ymm2, -224(%rdi,%rbx,4)
	vmovups	%ymm3, -192(%rdi,%rbx,4)
	vmovups	-160(%rcx,%rbx,4), %ymm2
	vmovups	-128(%rcx,%rbx,4), %ymm3
	vmaxps	%ymm1, %ymm2, %ymm2
	vmaxps	%ymm1, %ymm3, %ymm3
	vmovups	%ymm2, -160(%rdi,%rbx,4)
	vmovups	%ymm3, -128(%rdi,%rbx,4)
	vmovups	-96(%rcx,%rbx,4), %ymm2
	vmovups	-64(%rcx,%rbx,4), %ymm3
	vmaxps	%ymm1, %ymm2, %ymm2
	vmaxps	%ymm1, %ymm3, %ymm3
	vmovups	%ymm2, -96(%rdi,%rbx,4)
	vmovups	%ymm3, -64(%rdi,%rbx,4)
	vmovups	-32(%rcx,%rbx,4), %ymm2
	vmovups	(%rcx,%rbx,4), %ymm3
	vmaxps	%ymm1, %ymm2, %ymm2
	vmaxps	%ymm1, %ymm3, %ymm3
	vmovups	%ymm2, -32(%rdi,%rbx,4)
	vmovups	%ymm3, (%rdi,%rbx,4)
	addq	$64, %rbx
	addq	$4, %rax
	jne	LBB7_146
LBB7_147:                               ## %middle.block130.unr-lcssa
                                        ##   in Loop: Header=BB7_140 Depth=2
	movl	%esi, %edi
	andl	$-16, %edi
	testb	$3, %r13b
	je	LBB7_150
## %bb.148:                             ## %vector.body132.epil.preheader
                                        ##   in Loop: Header=BB7_140 Depth=2
	incb	%bpl
	movzbl	%bpl, %eax
	andl	$3, %eax
	shlq	$6, %rax
	leaq	(%r10,%rbx), %rcx
	addq	%r8, %rcx
	movq	-96(%rsp), %rbp                 ## 8-byte Reload
	leaq	(,%rcx,4), %rcx
	addq	%rbp, %rcx
	addq	%r11, %rbx
	addq	%r8, %rbx
	movq	-80(%rsp), %rbp                 ## 8-byte Reload
	leaq	(%rbp,%rbx,4), %rbp
	xorl	%ebx, %ebx
LBB7_149:                               ## %vector.body132.epil
                                        ##   Parent Loop BB7_4 Depth=1
                                        ##     Parent Loop BB7_140 Depth=2
                                        ## =>    This Inner Loop Header: Depth=3
	vmovups	-32(%rbp,%rbx), %ymm2
	vmovups	(%rbp,%rbx), %ymm3
	vmaxps	%ymm1, %ymm2, %ymm2
	vmaxps	%ymm1, %ymm3, %ymm3
	vmovups	%ymm2, -32(%rcx,%rbx)
	vmovups	%ymm3, (%rcx,%rbx)
	addq	$64, %rbx
	cmpq	%rbx, %rax
	jne	LBB7_149
LBB7_150:                               ## %middle.block130
                                        ##   in Loop: Header=BB7_140 Depth=2
	cmpq	%rsi, %rdi
	je	LBB7_153
LBB7_151:                               ## %"for head2_relu.s0.n.ni.us.us.2.preheader"
                                        ##   in Loop: Header=BB7_140 Depth=2
	subq	%rdi, %rdx
	leaq	(%r10,%rdi), %rax
	addq	%r8, %rax
	movq	-88(%rsp), %rcx                 ## 8-byte Reload
	leaq	(%rcx,%rax,4), %rax
	addq	%r11, %rdi
	addq	%r8, %rdi
	movq	-120(%rsp), %rcx                ## 8-byte Reload
	leaq	(%rcx,%rdi,4), %rcx
	xorl	%esi, %esi
LBB7_152:                               ## %"for head2_relu.s0.n.ni.us.us.2"
                                        ##   Parent Loop BB7_4 Depth=1
                                        ##     Parent Loop BB7_140 Depth=2
                                        ## =>    This Inner Loop Header: Depth=3
	vmovss	(%rcx,%rsi,4), %xmm2            ## xmm2 = mem[0],zero,zero,zero
	vmaxss	%xmm0, %xmm2, %xmm2
	vmovss	%xmm2, (%rax,%rsi,4)
	incq	%rsi
	cmpq	%rsi, %rdx
	jne	LBB7_152
	jmp	LBB7_153
LBB7_114:                               ## %destructor_block
	xorl	%eax, %eax
	addq	$208, %rsp
	popq	%rbx
	popq	%r12
	popq	%r13
	popq	%r14
	popq	%r15
	popq	%rbp
	vzeroupper
	retq
                                        ## -- End function
	.p2align	4, 0x90                         ## -- Begin function train_cost_model.par_for.conv1_stage2.s0.c.c.c
_train_cost_model.par_for.conv1_stage2.s0.c.c.c: ## @train_cost_model.par_for.conv1_stage2.s0.c.c.c
## %bb.0:                               ## %entry
	pushq	%rbp
	movq	%rsp, %rbp
	pushq	%r15
	pushq	%r14
	pushq	%r13
	pushq	%r12
	pushq	%rbx
	andq	$-32, %rsp
	subq	$384, %rsp                      ## imm = 0x180
                                        ## kill: def $esi killed $esi def $rsi
	movl	(%rdx), %eax
	movl	%eax, 88(%rsp)                  ## 4-byte Spill
	movslq	4(%rdx), %r13
	movl	16(%rdx), %eax
	movl	%eax, 144(%rsp)                 ## 4-byte Spill
	movl	20(%rdx), %eax
	movq	%rax, 32(%rsp)                  ## 8-byte Spill
	movl	28(%rdx), %ecx
	movslq	32(%rdx), %r9
	movl	36(%rdx), %r11d
	movl	%esi, %eax
	andl	$-4, %eax
	movl	%ecx, 344(%rsp)                 ## 4-byte Spill
	movl	%eax, 152(%rsp)                 ## 4-byte Spill
	subl	%eax, %ecx
	cmpl	$4, %ecx
	movl	$4, %eax
	cmovll	%ecx, %eax
	leal	(,%rsi,8), %edi
	andl	$24, %edi
	movq	%rdi, 56(%rsp)                  ## 8-byte Spill
	movq	40(%rdx), %rdi
	movq	56(%rdx), %r10
	movq	72(%rdx), %r15
	movq	88(%rdx), %r14
	movq	%rsi, 120(%rsp)                 ## 8-byte Spill
	cmpl	%esi, 8(%rdx)
	movq	%r9, (%rsp)                     ## 8-byte Spill
	movq	%r10, 24(%rsp)                  ## 8-byte Spill
	movl	%r11d, 20(%rsp)                 ## 4-byte Spill
	jle	LBB8_4
## %bb.1:                               ## %then_bb
	movq	%r15, 40(%rsp)                  ## 8-byte Spill
	movq	%r14, 8(%rsp)                   ## 8-byte Spill
	movl	12(%rdx), %edx
	movl	%edx, 304(%rsp)                 ## 4-byte Spill
	testl	%ecx, %ecx
	jle	LBB8_25
## %bb.2:                               ## %"for conv1_stage1.s0.w.rebased.preheader"
	movq	%r13, %r15
	movl	56(%rsp), %r8d                  ## 4-byte Reload
	vmovups	(%rdi,%r8,4), %ymm0
	movl	%eax, %r14d
	leaq	-1(%r14), %r12
	movl	%r14d, %eax
	andl	$7, %eax
	cmpq	$7, %r12
	jae	LBB8_7
## %bb.3:
	xorl	%esi, %esi
	jmp	LBB8_9
LBB8_4:                                 ## %next_bb
	movl	24(%rdx), %r8d
	testl	%ecx, %ecx
	jle	LBB8_104
## %bb.5:                               ## %"for conv1_stage1.s0.w.rebased3.preheader"
	movl	56(%rsp), %r9d                  ## 4-byte Reload
	vmovups	(%rdi,%r9,4), %ymm0
	movl	%eax, %ebx
	leaq	-1(%rbx), %r12
	movl	%ebx, %eax
	andl	$7, %eax
	cmpq	$7, %r12
	jae	LBB8_86
## %bb.6:
	xorl	%edx, %edx
	jmp	LBB8_88
LBB8_7:                                 ## %"for conv1_stage1.s0.w.rebased.preheader.new"
	movl	%r14d, %edi
	andl	$-8, %edi
	leaq	384(%rsp), %rdx
	xorl	%esi, %esi
	.p2align	4, 0x90
LBB8_8:                                 ## %"for conv1_stage1.s0.w.rebased"
                                        ## =>This Inner Loop Header: Depth=1
	vmovaps	%ymm0, -224(%rdx)
	vmovaps	%ymm0, -192(%rdx)
	vmovaps	%ymm0, -160(%rdx)
	vmovaps	%ymm0, -128(%rdx)
	vmovaps	%ymm0, -96(%rdx)
	vmovaps	%ymm0, -64(%rdx)
	vmovaps	%ymm0, -32(%rdx)
	vmovaps	%ymm0, (%rdx)
	addq	$8, %rsi
	addq	$256, %rdx                      ## imm = 0x100
	cmpq	%rsi, %rdi
	jne	LBB8_8
LBB8_9:                                 ## %"end for conv1_stage1.s0.w.rebased.unr-lcssa"
	testq	%rax, %rax
	movq	(%rsp), %r9                     ## 8-byte Reload
	je	LBB8_12
## %bb.10:                              ## %"for conv1_stage1.s0.w.rebased.epil.preheader"
	shlq	$5, %rsi
	addq	%rsp, %rsi
	addq	$160, %rsi
	shlq	$5, %rax
	xorl	%edi, %edi
	.p2align	4, 0x90
LBB8_11:                                ## %"for conv1_stage1.s0.w.rebased.epil"
                                        ## =>This Inner Loop Header: Depth=1
	vmovaps	%ymm0, (%rsi,%rdi)
	addq	$32, %rdi
	cmpq	%rdi, %rax
	jne	LBB8_11
LBB8_12:                                ## %"end for conv1_stage1.s0.w.rebased"
	testl	%ecx, %ecx
	jle	LBB8_25
## %bb.13:                              ## %"for conv1_stage1.s1.r54$x.us.preheader"
	movslq	152(%rsp), %r13                 ## 4-byte Folded Reload
	movq	40(%rsp), %rax                  ## 8-byte Reload
	vmovups	(%rax,%r8,4), %ymm0
	movl	%r14d, %eax
	andl	$3, %eax
	cmpq	$3, %r12
	jae	LBB8_15
## %bb.14:
	xorl	%ecx, %ecx
	jmp	LBB8_17
LBB8_86:                                ## %"for conv1_stage1.s0.w.rebased3.preheader.new"
	movl	%ebx, %esi
	andl	$-8, %esi
	leaq	384(%rsp), %rdi
	xorl	%edx, %edx
	.p2align	4, 0x90
LBB8_87:                                ## %"for conv1_stage1.s0.w.rebased3"
                                        ## =>This Inner Loop Header: Depth=1
	vmovaps	%ymm0, -224(%rdi)
	vmovaps	%ymm0, -192(%rdi)
	vmovaps	%ymm0, -160(%rdi)
	vmovaps	%ymm0, -128(%rdi)
	vmovaps	%ymm0, -96(%rdi)
	vmovaps	%ymm0, -64(%rdi)
	vmovaps	%ymm0, -32(%rdi)
	vmovaps	%ymm0, (%rdi)
	addq	$8, %rdx
	addq	$256, %rdi                      ## imm = 0x100
	cmpq	%rdx, %rsi
	jne	LBB8_87
LBB8_88:                                ## %"end for conv1_stage1.s0.w.rebased4.unr-lcssa"
	testq	%rax, %rax
	je	LBB8_91
## %bb.89:                              ## %"for conv1_stage1.s0.w.rebased3.epil.preheader"
	shlq	$5, %rdx
	addq	%rsp, %rdx
	addq	$160, %rdx
	shlq	$5, %rax
	xorl	%esi, %esi
	.p2align	4, 0x90
LBB8_90:                                ## %"for conv1_stage1.s0.w.rebased3.epil"
                                        ## =>This Inner Loop Header: Depth=1
	vmovaps	%ymm0, (%rdx,%rsi)
	addq	$32, %rsi
	cmpq	%rsi, %rax
	jne	LBB8_90
LBB8_91:                                ## %"end for conv1_stage1.s0.w.rebased4"
	testl	%ecx, %ecx
	jle	LBB8_104
## %bb.92:                              ## %"for conv1_stage1.s1.r54$x6.us.preheader"
	movslq	152(%rsp), %r11                 ## 4-byte Folded Reload
	vmovups	(%r15,%r9,4), %ymm0
	movl	%ebx, %eax
	andl	$3, %eax
	cmpq	$3, %r12
	jae	LBB8_94
## %bb.93:
	xorl	%ecx, %ecx
	jmp	LBB8_96
LBB8_15:                                ## %"for conv1_stage1.s1.r54$x.us.preheader.new"
	movl	%r14d, %esi
	andl	$-4, %esi
	movq	%r13, %rbx
	shlq	$5, %rbx
	addq	8(%rsp), %rbx                   ## 8-byte Folded Reload
	movl	$96, %edi
	xorl	%ecx, %ecx
	.p2align	4, 0x90
LBB8_16:                                ## %"for conv1_stage1.s1.w.rebased.us"
                                        ## =>This Inner Loop Header: Depth=1
	vbroadcastss	-96(%rbx,%rdi), %ymm1
	vfmadd213ps	64(%rsp,%rdi), %ymm0, %ymm1 ## ymm1 = (ymm0 * ymm1) + mem
	vmovaps	%ymm1, 64(%rsp,%rdi)
	vbroadcastss	-64(%rbx,%rdi), %ymm1
	vfmadd213ps	96(%rsp,%rdi), %ymm0, %ymm1 ## ymm1 = (ymm0 * ymm1) + mem
	vmovaps	%ymm1, 96(%rsp,%rdi)
	vbroadcastss	-32(%rbx,%rdi), %ymm1
	vfmadd213ps	128(%rsp,%rdi), %ymm0, %ymm1 ## ymm1 = (ymm0 * ymm1) + mem
	vmovaps	%ymm1, 128(%rsp,%rdi)
	vbroadcastss	(%rbx,%rdi), %ymm1
	vfmadd213ps	160(%rsp,%rdi), %ymm0, %ymm1 ## ymm1 = (ymm0 * ymm1) + mem
	vmovaps	%ymm1, 160(%rsp,%rdi)
	addq	$4, %rcx
	subq	$-128, %rdi
	cmpq	%rcx, %rsi
	jne	LBB8_16
LBB8_17:                                ## %"end for conv1_stage1.s1.w.rebased.loopexit.us.unr-lcssa"
	testq	%rax, %rax
	je	LBB8_20
## %bb.18:                              ## %"for conv1_stage1.s1.w.rebased.us.epil.preheader"
	leaq	(%rcx,%r13), %rsi
	shlq	$5, %rsi
	addq	8(%rsp), %rsi                   ## 8-byte Folded Reload
	shlq	$5, %rcx
	addq	%rsp, %rcx
	addq	$160, %rcx
	shlq	$5, %rax
	xorl	%edi, %edi
	.p2align	4, 0x90
LBB8_19:                                ## %"for conv1_stage1.s1.w.rebased.us.epil"
                                        ## =>This Inner Loop Header: Depth=1
	vbroadcastss	(%rsi,%rdi), %ymm1
	vfmadd213ps	(%rcx,%rdi), %ymm0, %ymm1 ## ymm1 = (ymm0 * ymm1) + mem
	vmovaps	%ymm1, (%rcx,%rdi)
	addq	$32, %rdi
	cmpq	%rdi, %rax
	jne	LBB8_19
LBB8_20:                                ## %"end for conv1_stage1.s1.w.rebased.loopexit.us"
	leaq	(%r15,%r8), %rax
	movq	40(%rsp), %rcx                  ## 8-byte Reload
	vmovups	(%rcx,%rax,4), %ymm0
	movl	%r14d, %eax
	andl	$3, %eax
	cmpq	$3, %r12
	jae	LBB8_175
## %bb.21:
	xorl	%ecx, %ecx
	jmp	LBB8_177
LBB8_94:                                ## %"for conv1_stage1.s1.r54$x6.us.preheader.new"
	movl	%ebx, %edi
	andl	$-4, %edi
	movq	%r11, %rdx
	shlq	$5, %rdx
	addq	%r14, %rdx
	movl	$96, %esi
	xorl	%ecx, %ecx
	.p2align	4, 0x90
LBB8_95:                                ## %"for conv1_stage1.s1.w.rebased9.us"
                                        ## =>This Inner Loop Header: Depth=1
	vbroadcastss	-96(%rdx,%rsi), %ymm1
	vfmadd213ps	64(%rsp,%rsi), %ymm0, %ymm1 ## ymm1 = (ymm0 * ymm1) + mem
	vmovaps	%ymm1, 64(%rsp,%rsi)
	vbroadcastss	-64(%rdx,%rsi), %ymm1
	vfmadd213ps	96(%rsp,%rsi), %ymm0, %ymm1 ## ymm1 = (ymm0 * ymm1) + mem
	vmovaps	%ymm1, 96(%rsp,%rsi)
	vbroadcastss	-32(%rdx,%rsi), %ymm1
	vfmadd213ps	128(%rsp,%rsi), %ymm0, %ymm1 ## ymm1 = (ymm0 * ymm1) + mem
	vmovaps	%ymm1, 128(%rsp,%rsi)
	vbroadcastss	(%rdx,%rsi), %ymm1
	vfmadd213ps	160(%rsp,%rsi), %ymm0, %ymm1 ## ymm1 = (ymm0 * ymm1) + mem
	vmovaps	%ymm1, 160(%rsp,%rsi)
	addq	$4, %rcx
	subq	$-128, %rsi
	cmpq	%rcx, %rdi
	jne	LBB8_95
LBB8_96:                                ## %"end for conv1_stage1.s1.w.rebased10.loopexit.us.unr-lcssa"
	testq	%rax, %rax
	je	LBB8_99
## %bb.97:                              ## %"for conv1_stage1.s1.w.rebased9.us.epil.preheader"
	leaq	(%rcx,%r11), %rdx
	shlq	$5, %rdx
	addq	%r14, %rdx
	shlq	$5, %rcx
	addq	%rsp, %rcx
	addq	$160, %rcx
	shlq	$5, %rax
	xorl	%esi, %esi
	.p2align	4, 0x90
LBB8_98:                                ## %"for conv1_stage1.s1.w.rebased9.us.epil"
                                        ## =>This Inner Loop Header: Depth=1
	vbroadcastss	(%rdx,%rsi), %ymm1
	vfmadd213ps	(%rcx,%rsi), %ymm0, %ymm1 ## ymm1 = (ymm0 * ymm1) + mem
	vmovaps	%ymm1, (%rcx,%rsi)
	addq	$32, %rsi
	cmpq	%rsi, %rax
	jne	LBB8_98
LBB8_99:                                ## %"end for conv1_stage1.s1.w.rebased10.loopexit.us"
	leaq	(%r9,%r13), %rax
	vmovups	(%r15,%rax,4), %ymm0
	movl	%ebx, %eax
	andl	$3, %eax
	cmpq	$3, %r12
	jae	LBB8_131
## %bb.100:
	xorl	%ecx, %ecx
	jmp	LBB8_133
LBB8_175:                               ## %"end for conv1_stage1.s1.w.rebased.loopexit.us.new"
	movl	%r14d, %esi
	andl	$-4, %esi
	movq	%r13, %rcx
	shlq	$5, %rcx
	movq	8(%rsp), %rdx                   ## 8-byte Reload
	leaq	(%rcx,%rdx), %rbx
	addq	$4, %rbx
	movl	$96, %edi
	xorl	%ecx, %ecx
	.p2align	4, 0x90
LBB8_176:                               ## %"for conv1_stage1.s1.w.rebased.us.1"
                                        ## =>This Inner Loop Header: Depth=1
	vbroadcastss	-96(%rbx,%rdi), %ymm1
	vfmadd213ps	64(%rsp,%rdi), %ymm0, %ymm1 ## ymm1 = (ymm0 * ymm1) + mem
	vmovaps	%ymm1, 64(%rsp,%rdi)
	vbroadcastss	-64(%rbx,%rdi), %ymm1
	vfmadd213ps	96(%rsp,%rdi), %ymm0, %ymm1 ## ymm1 = (ymm0 * ymm1) + mem
	vmovaps	%ymm1, 96(%rsp,%rdi)
	vbroadcastss	-32(%rbx,%rdi), %ymm1
	vfmadd213ps	128(%rsp,%rdi), %ymm0, %ymm1 ## ymm1 = (ymm0 * ymm1) + mem
	vmovaps	%ymm1, 128(%rsp,%rdi)
	vbroadcastss	(%rbx,%rdi), %ymm1
	vfmadd213ps	160(%rsp,%rdi), %ymm0, %ymm1 ## ymm1 = (ymm0 * ymm1) + mem
	vmovaps	%ymm1, 160(%rsp,%rdi)
	addq	$4, %rcx
	subq	$-128, %rdi
	cmpq	%rcx, %rsi
	jne	LBB8_176
LBB8_177:                               ## %"end for conv1_stage1.s1.w.rebased.loopexit.us.1.unr-lcssa"
	testq	%rax, %rax
	je	LBB8_180
## %bb.178:                             ## %"for conv1_stage1.s1.w.rebased.us.1.epil.preheader"
	leaq	(%rcx,%r13), %rsi
	shlq	$5, %rsi
	movq	8(%rsp), %rdx                   ## 8-byte Reload
	addq	%rdx, %rsi
	addq	$4, %rsi
	shlq	$5, %rcx
	addq	%rsp, %rcx
	addq	$160, %rcx
	shlq	$5, %rax
	xorl	%edi, %edi
	.p2align	4, 0x90
LBB8_179:                               ## %"for conv1_stage1.s1.w.rebased.us.1.epil"
                                        ## =>This Inner Loop Header: Depth=1
	vbroadcastss	(%rsi,%rdi), %ymm1
	vfmadd213ps	(%rcx,%rdi), %ymm0, %ymm1 ## ymm1 = (ymm0 * ymm1) + mem
	vmovaps	%ymm1, (%rcx,%rdi)
	addq	$32, %rdi
	cmpq	%rdi, %rax
	jne	LBB8_179
LBB8_180:                               ## %"end for conv1_stage1.s1.w.rebased.loopexit.us.1"
	leaq	(%r8,%r15,2), %rax
	movq	40(%rsp), %rcx                  ## 8-byte Reload
	vmovups	(%rcx,%rax,4), %ymm0
	movl	%r14d, %eax
	andl	$3, %eax
	cmpq	$3, %r12
	jae	LBB8_182
## %bb.181:
	xorl	%ecx, %ecx
	jmp	LBB8_184
LBB8_131:                               ## %"end for conv1_stage1.s1.w.rebased10.loopexit.us.new"
	movl	%ebx, %edi
	andl	$-4, %edi
	movq	%r11, %rcx
	shlq	$5, %rcx
	leaq	(%rcx,%r14), %rdx
	addq	$4, %rdx
	movl	$96, %esi
	xorl	%ecx, %ecx
	.p2align	4, 0x90
LBB8_132:                               ## %"for conv1_stage1.s1.w.rebased9.us.1"
                                        ## =>This Inner Loop Header: Depth=1
	vbroadcastss	-96(%rdx,%rsi), %ymm1
	vfmadd213ps	64(%rsp,%rsi), %ymm0, %ymm1 ## ymm1 = (ymm0 * ymm1) + mem
	vmovaps	%ymm1, 64(%rsp,%rsi)
	vbroadcastss	-64(%rdx,%rsi), %ymm1
	vfmadd213ps	96(%rsp,%rsi), %ymm0, %ymm1 ## ymm1 = (ymm0 * ymm1) + mem
	vmovaps	%ymm1, 96(%rsp,%rsi)
	vbroadcastss	-32(%rdx,%rsi), %ymm1
	vfmadd213ps	128(%rsp,%rsi), %ymm0, %ymm1 ## ymm1 = (ymm0 * ymm1) + mem
	vmovaps	%ymm1, 128(%rsp,%rsi)
	vbroadcastss	(%rdx,%rsi), %ymm1
	vfmadd213ps	160(%rsp,%rsi), %ymm0, %ymm1 ## ymm1 = (ymm0 * ymm1) + mem
	vmovaps	%ymm1, 160(%rsp,%rsi)
	addq	$4, %rcx
	subq	$-128, %rsi
	cmpq	%rcx, %rdi
	jne	LBB8_132
LBB8_133:                               ## %"end for conv1_stage1.s1.w.rebased10.loopexit.us.1.unr-lcssa"
	testq	%rax, %rax
	je	LBB8_136
## %bb.134:                             ## %"for conv1_stage1.s1.w.rebased9.us.1.epil.preheader"
	leaq	(%rcx,%r11), %rdx
	shlq	$5, %rdx
	addq	%r14, %rdx
	addq	$4, %rdx
	shlq	$5, %rcx
	addq	%rsp, %rcx
	addq	$160, %rcx
	shlq	$5, %rax
	xorl	%esi, %esi
	.p2align	4, 0x90
LBB8_135:                               ## %"for conv1_stage1.s1.w.rebased9.us.1.epil"
                                        ## =>This Inner Loop Header: Depth=1
	vbroadcastss	(%rdx,%rsi), %ymm1
	vfmadd213ps	(%rcx,%rsi), %ymm0, %ymm1 ## ymm1 = (ymm0 * ymm1) + mem
	vmovaps	%ymm1, (%rcx,%rsi)
	addq	$32, %rsi
	cmpq	%rsi, %rax
	jne	LBB8_135
LBB8_136:                               ## %"end for conv1_stage1.s1.w.rebased10.loopexit.us.1"
	leaq	(%r9,%r13,2), %rax
	vmovups	(%r15,%rax,4), %ymm0
	movl	%ebx, %eax
	andl	$3, %eax
	cmpq	$3, %r12
	jae	LBB8_138
## %bb.137:
	xorl	%ecx, %ecx
	jmp	LBB8_140
LBB8_182:                               ## %"end for conv1_stage1.s1.w.rebased.loopexit.us.1.new"
	movl	%r14d, %esi
	andl	$-4, %esi
	movq	%r13, %rcx
	shlq	$5, %rcx
	movq	8(%rsp), %rdx                   ## 8-byte Reload
	leaq	(%rcx,%rdx), %rbx
	addq	$8, %rbx
	movl	$96, %edi
	xorl	%ecx, %ecx
	.p2align	4, 0x90
LBB8_183:                               ## %"for conv1_stage1.s1.w.rebased.us.2"
                                        ## =>This Inner Loop Header: Depth=1
	vbroadcastss	-96(%rbx,%rdi), %ymm1
	vfmadd213ps	64(%rsp,%rdi), %ymm0, %ymm1 ## ymm1 = (ymm0 * ymm1) + mem
	vmovaps	%ymm1, 64(%rsp,%rdi)
	vbroadcastss	-64(%rbx,%rdi), %ymm1
	vfmadd213ps	96(%rsp,%rdi), %ymm0, %ymm1 ## ymm1 = (ymm0 * ymm1) + mem
	vmovaps	%ymm1, 96(%rsp,%rdi)
	vbroadcastss	-32(%rbx,%rdi), %ymm1
	vfmadd213ps	128(%rsp,%rdi), %ymm0, %ymm1 ## ymm1 = (ymm0 * ymm1) + mem
	vmovaps	%ymm1, 128(%rsp,%rdi)
	vbroadcastss	(%rbx,%rdi), %ymm1
	vfmadd213ps	160(%rsp,%rdi), %ymm0, %ymm1 ## ymm1 = (ymm0 * ymm1) + mem
	vmovaps	%ymm1, 160(%rsp,%rdi)
	addq	$4, %rcx
	subq	$-128, %rdi
	cmpq	%rcx, %rsi
	jne	LBB8_183
LBB8_184:                               ## %"end for conv1_stage1.s1.w.rebased.loopexit.us.2.unr-lcssa"
	testq	%rax, %rax
	je	LBB8_187
## %bb.185:                             ## %"for conv1_stage1.s1.w.rebased.us.2.epil.preheader"
	leaq	(%rcx,%r13), %rsi
	shlq	$5, %rsi
	movq	8(%rsp), %rdx                   ## 8-byte Reload
	addq	%rdx, %rsi
	addq	$8, %rsi
	shlq	$5, %rcx
	addq	%rsp, %rcx
	addq	$160, %rcx
	shlq	$5, %rax
	xorl	%edi, %edi
	.p2align	4, 0x90
LBB8_186:                               ## %"for conv1_stage1.s1.w.rebased.us.2.epil"
                                        ## =>This Inner Loop Header: Depth=1
	vbroadcastss	(%rsi,%rdi), %ymm1
	vfmadd213ps	(%rcx,%rdi), %ymm0, %ymm1 ## ymm1 = (ymm0 * ymm1) + mem
	vmovaps	%ymm1, (%rcx,%rdi)
	addq	$32, %rdi
	cmpq	%rdi, %rax
	jne	LBB8_186
LBB8_187:                               ## %"end for conv1_stage1.s1.w.rebased.loopexit.us.2"
	leaq	(%r15,%r15,2), %r10
	leaq	(%r10,%r8), %rax
	movq	40(%rsp), %rcx                  ## 8-byte Reload
	vmovups	(%rcx,%rax,4), %ymm0
	movl	%r14d, %eax
	andl	$3, %eax
	cmpq	$3, %r12
	jae	LBB8_189
## %bb.188:
	xorl	%ecx, %ecx
	jmp	LBB8_191
LBB8_138:                               ## %"end for conv1_stage1.s1.w.rebased10.loopexit.us.1.new"
	movl	%ebx, %edi
	andl	$-4, %edi
	movq	%r11, %rcx
	shlq	$5, %rcx
	leaq	(%rcx,%r14), %rdx
	addq	$8, %rdx
	movl	$96, %esi
	xorl	%ecx, %ecx
	.p2align	4, 0x90
LBB8_139:                               ## %"for conv1_stage1.s1.w.rebased9.us.2"
                                        ## =>This Inner Loop Header: Depth=1
	vbroadcastss	-96(%rdx,%rsi), %ymm1
	vfmadd213ps	64(%rsp,%rsi), %ymm0, %ymm1 ## ymm1 = (ymm0 * ymm1) + mem
	vmovaps	%ymm1, 64(%rsp,%rsi)
	vbroadcastss	-64(%rdx,%rsi), %ymm1
	vfmadd213ps	96(%rsp,%rsi), %ymm0, %ymm1 ## ymm1 = (ymm0 * ymm1) + mem
	vmovaps	%ymm1, 96(%rsp,%rsi)
	vbroadcastss	-32(%rdx,%rsi), %ymm1
	vfmadd213ps	128(%rsp,%rsi), %ymm0, %ymm1 ## ymm1 = (ymm0 * ymm1) + mem
	vmovaps	%ymm1, 128(%rsp,%rsi)
	vbroadcastss	(%rdx,%rsi), %ymm1
	vfmadd213ps	160(%rsp,%rsi), %ymm0, %ymm1 ## ymm1 = (ymm0 * ymm1) + mem
	vmovaps	%ymm1, 160(%rsp,%rsi)
	addq	$4, %rcx
	subq	$-128, %rsi
	cmpq	%rcx, %rdi
	jne	LBB8_139
LBB8_140:                               ## %"end for conv1_stage1.s1.w.rebased10.loopexit.us.2.unr-lcssa"
	movl	%r8d, 8(%rsp)                   ## 4-byte Spill
	testq	%rax, %rax
	je	LBB8_143
## %bb.141:                             ## %"for conv1_stage1.s1.w.rebased9.us.2.epil.preheader"
	leaq	(%rcx,%r11), %rdx
	shlq	$5, %rdx
	addq	%r14, %rdx
	addq	$8, %rdx
	shlq	$5, %rcx
	addq	%rsp, %rcx
	addq	$160, %rcx
	shlq	$5, %rax
	xorl	%esi, %esi
	.p2align	4, 0x90
LBB8_142:                               ## %"for conv1_stage1.s1.w.rebased9.us.2.epil"
                                        ## =>This Inner Loop Header: Depth=1
	vbroadcastss	(%rdx,%rsi), %ymm1
	vfmadd213ps	(%rcx,%rsi), %ymm0, %ymm1 ## ymm1 = (ymm0 * ymm1) + mem
	vmovaps	%ymm1, (%rcx,%rsi)
	addq	$32, %rsi
	cmpq	%rsi, %rax
	jne	LBB8_142
LBB8_143:                               ## %"end for conv1_stage1.s1.w.rebased10.loopexit.us.2"
	leaq	(,%r13,2), %r8
	addq	%r13, %r8
	leaq	(%r8,%r9), %rax
	vmovups	(%r15,%rax,4), %ymm0
	movl	%ebx, %eax
	andl	$3, %eax
	cmpq	$3, %r12
	jae	LBB8_145
## %bb.144:
	xorl	%ecx, %ecx
	jmp	LBB8_147
LBB8_189:                               ## %"end for conv1_stage1.s1.w.rebased.loopexit.us.2.new"
	movl	%r14d, %esi
	andl	$-4, %esi
	movq	%r13, %rcx
	shlq	$5, %rcx
	movq	8(%rsp), %rdx                   ## 8-byte Reload
	leaq	(%rcx,%rdx), %rbx
	addq	$12, %rbx
	movl	$96, %edi
	xorl	%ecx, %ecx
	.p2align	4, 0x90
LBB8_190:                               ## %"for conv1_stage1.s1.w.rebased.us.3"
                                        ## =>This Inner Loop Header: Depth=1
	vbroadcastss	-96(%rbx,%rdi), %ymm1
	vfmadd213ps	64(%rsp,%rdi), %ymm0, %ymm1 ## ymm1 = (ymm0 * ymm1) + mem
	vmovaps	%ymm1, 64(%rsp,%rdi)
	vbroadcastss	-64(%rbx,%rdi), %ymm1
	vfmadd213ps	96(%rsp,%rdi), %ymm0, %ymm1 ## ymm1 = (ymm0 * ymm1) + mem
	vmovaps	%ymm1, 96(%rsp,%rdi)
	vbroadcastss	-32(%rbx,%rdi), %ymm1
	vfmadd213ps	128(%rsp,%rdi), %ymm0, %ymm1 ## ymm1 = (ymm0 * ymm1) + mem
	vmovaps	%ymm1, 128(%rsp,%rdi)
	vbroadcastss	(%rbx,%rdi), %ymm1
	vfmadd213ps	160(%rsp,%rdi), %ymm0, %ymm1 ## ymm1 = (ymm0 * ymm1) + mem
	vmovaps	%ymm1, 160(%rsp,%rdi)
	addq	$4, %rcx
	subq	$-128, %rdi
	cmpq	%rcx, %rsi
	jne	LBB8_190
LBB8_191:                               ## %"end for conv1_stage1.s1.w.rebased.loopexit.us.3.unr-lcssa"
	testq	%rax, %rax
	je	LBB8_194
## %bb.192:                             ## %"for conv1_stage1.s1.w.rebased.us.3.epil.preheader"
	leaq	(%rcx,%r13), %rsi
	shlq	$5, %rsi
	movq	8(%rsp), %rdx                   ## 8-byte Reload
	addq	%rdx, %rsi
	addq	$12, %rsi
	shlq	$5, %rcx
	addq	%rsp, %rcx
	addq	$160, %rcx
	shlq	$5, %rax
	xorl	%edi, %edi
	.p2align	4, 0x90
LBB8_193:                               ## %"for conv1_stage1.s1.w.rebased.us.3.epil"
                                        ## =>This Inner Loop Header: Depth=1
	vbroadcastss	(%rsi,%rdi), %ymm1
	vfmadd213ps	(%rcx,%rdi), %ymm0, %ymm1 ## ymm1 = (ymm0 * ymm1) + mem
	vmovaps	%ymm1, (%rcx,%rdi)
	addq	$32, %rdi
	cmpq	%rdi, %rax
	jne	LBB8_193
LBB8_194:                               ## %"end for conv1_stage1.s1.w.rebased.loopexit.us.3"
	leaq	(%r8,%r15,4), %rax
	movq	40(%rsp), %rcx                  ## 8-byte Reload
	vmovups	(%rcx,%rax,4), %ymm0
	movl	%r14d, %eax
	andl	$3, %eax
	cmpq	$3, %r12
	jae	LBB8_196
## %bb.195:
	xorl	%ecx, %ecx
	jmp	LBB8_198
LBB8_145:                               ## %"end for conv1_stage1.s1.w.rebased10.loopexit.us.2.new"
	movl	%ebx, %edx
	andl	$-4, %edx
	movq	%r11, %rcx
	shlq	$5, %rcx
	leaq	(%rcx,%r14), %rdi
	addq	$12, %rdi
	movl	$96, %esi
	xorl	%ecx, %ecx
	.p2align	4, 0x90
LBB8_146:                               ## %"for conv1_stage1.s1.w.rebased9.us.3"
                                        ## =>This Inner Loop Header: Depth=1
	vbroadcastss	-96(%rdi,%rsi), %ymm1
	vfmadd213ps	64(%rsp,%rsi), %ymm0, %ymm1 ## ymm1 = (ymm0 * ymm1) + mem
	vmovaps	%ymm1, 64(%rsp,%rsi)
	vbroadcastss	-64(%rdi,%rsi), %ymm1
	vfmadd213ps	96(%rsp,%rsi), %ymm0, %ymm1 ## ymm1 = (ymm0 * ymm1) + mem
	vmovaps	%ymm1, 96(%rsp,%rsi)
	vbroadcastss	-32(%rdi,%rsi), %ymm1
	vfmadd213ps	128(%rsp,%rsi), %ymm0, %ymm1 ## ymm1 = (ymm0 * ymm1) + mem
	vmovaps	%ymm1, 128(%rsp,%rsi)
	vbroadcastss	(%rdi,%rsi), %ymm1
	vfmadd213ps	160(%rsp,%rsi), %ymm0, %ymm1 ## ymm1 = (ymm0 * ymm1) + mem
	vmovaps	%ymm1, 160(%rsp,%rsi)
	addq	$4, %rcx
	subq	$-128, %rsi
	cmpq	%rcx, %rdx
	jne	LBB8_146
LBB8_147:                               ## %"end for conv1_stage1.s1.w.rebased10.loopexit.us.3.unr-lcssa"
	testq	%rax, %rax
	je	LBB8_150
## %bb.148:                             ## %"for conv1_stage1.s1.w.rebased9.us.3.epil.preheader"
	leaq	(%rcx,%r11), %rdx
	shlq	$5, %rdx
	addq	%r14, %rdx
	addq	$12, %rdx
	shlq	$5, %rcx
	addq	%rsp, %rcx
	addq	$160, %rcx
	shlq	$5, %rax
	xorl	%esi, %esi
	.p2align	4, 0x90
LBB8_149:                               ## %"for conv1_stage1.s1.w.rebased9.us.3.epil"
                                        ## =>This Inner Loop Header: Depth=1
	vbroadcastss	(%rdx,%rsi), %ymm1
	vfmadd213ps	(%rcx,%rsi), %ymm0, %ymm1 ## ymm1 = (ymm0 * ymm1) + mem
	vmovaps	%ymm1, (%rcx,%rsi)
	addq	$32, %rsi
	cmpq	%rsi, %rax
	jne	LBB8_149
LBB8_150:                               ## %"end for conv1_stage1.s1.w.rebased10.loopexit.us.3"
	leaq	(%r9,%r13,4), %rax
	vmovups	(%r15,%rax,4), %ymm0
	movl	%ebx, %eax
	andl	$3, %eax
	cmpq	$3, %r12
	jae	LBB8_152
## %bb.151:
	xorl	%ecx, %ecx
	jmp	LBB8_154
LBB8_196:                               ## %"end for conv1_stage1.s1.w.rebased.loopexit.us.3.new"
	movl	%r14d, %esi
	andl	$-4, %esi
	movq	%r13, %rcx
	shlq	$5, %rcx
	movq	8(%rsp), %rdx                   ## 8-byte Reload
	leaq	(%rcx,%rdx), %rbx
	addq	$16, %rbx
	movl	$96, %edi
	xorl	%ecx, %ecx
	.p2align	4, 0x90
LBB8_197:                               ## %"for conv1_stage1.s1.w.rebased.us.4"
                                        ## =>This Inner Loop Header: Depth=1
	vbroadcastss	-96(%rbx,%rdi), %ymm1
	vfmadd213ps	64(%rsp,%rdi), %ymm0, %ymm1 ## ymm1 = (ymm0 * ymm1) + mem
	vmovaps	%ymm1, 64(%rsp,%rdi)
	vbroadcastss	-64(%rbx,%rdi), %ymm1
	vfmadd213ps	96(%rsp,%rdi), %ymm0, %ymm1 ## ymm1 = (ymm0 * ymm1) + mem
	vmovaps	%ymm1, 96(%rsp,%rdi)
	vbroadcastss	-32(%rbx,%rdi), %ymm1
	vfmadd213ps	128(%rsp,%rdi), %ymm0, %ymm1 ## ymm1 = (ymm0 * ymm1) + mem
	vmovaps	%ymm1, 128(%rsp,%rdi)
	vbroadcastss	(%rbx,%rdi), %ymm1
	vfmadd213ps	160(%rsp,%rdi), %ymm0, %ymm1 ## ymm1 = (ymm0 * ymm1) + mem
	vmovaps	%ymm1, 160(%rsp,%rdi)
	addq	$4, %rcx
	subq	$-128, %rdi
	cmpq	%rcx, %rsi
	jne	LBB8_197
LBB8_198:                               ## %"end for conv1_stage1.s1.w.rebased.loopexit.us.4.unr-lcssa"
	testq	%rax, %rax
	je	LBB8_201
## %bb.199:                             ## %"for conv1_stage1.s1.w.rebased.us.4.epil.preheader"
	leaq	(%rcx,%r13), %rsi
	shlq	$5, %rsi
	movq	8(%rsp), %rdx                   ## 8-byte Reload
	addq	%rdx, %rsi
	addq	$16, %rsi
	shlq	$5, %rcx
	addq	%rsp, %rcx
	addq	$160, %rcx
	shlq	$5, %rax
	xorl	%edi, %edi
	.p2align	4, 0x90
LBB8_200:                               ## %"for conv1_stage1.s1.w.rebased.us.4.epil"
                                        ## =>This Inner Loop Header: Depth=1
	vbroadcastss	(%rsi,%rdi), %ymm1
	vfmadd213ps	(%rcx,%rdi), %ymm0, %ymm1 ## ymm1 = (ymm0 * ymm1) + mem
	vmovaps	%ymm1, (%rcx,%rdi)
	addq	$32, %rdi
	cmpq	%rdi, %rax
	jne	LBB8_200
LBB8_201:                               ## %"end for conv1_stage1.s1.w.rebased.loopexit.us.4"
	leaq	(%r15,%r15,4), %rax
	addq	%r8, %rax
	movq	40(%rsp), %rcx                  ## 8-byte Reload
	vmovups	(%rcx,%rax,4), %ymm0
	movl	%r14d, %eax
	andl	$3, %eax
	cmpq	$3, %r12
	jae	LBB8_203
## %bb.202:
	xorl	%ecx, %ecx
	jmp	LBB8_205
LBB8_152:                               ## %"end for conv1_stage1.s1.w.rebased10.loopexit.us.3.new"
	movl	%ebx, %edx
	andl	$-4, %edx
	movq	%r11, %rcx
	shlq	$5, %rcx
	leaq	(%rcx,%r14), %rdi
	addq	$16, %rdi
	movl	$96, %esi
	xorl	%ecx, %ecx
	.p2align	4, 0x90
LBB8_153:                               ## %"for conv1_stage1.s1.w.rebased9.us.4"
                                        ## =>This Inner Loop Header: Depth=1
	vbroadcastss	-96(%rdi,%rsi), %ymm1
	vfmadd213ps	64(%rsp,%rsi), %ymm0, %ymm1 ## ymm1 = (ymm0 * ymm1) + mem
	vmovaps	%ymm1, 64(%rsp,%rsi)
	vbroadcastss	-64(%rdi,%rsi), %ymm1
	vfmadd213ps	96(%rsp,%rsi), %ymm0, %ymm1 ## ymm1 = (ymm0 * ymm1) + mem
	vmovaps	%ymm1, 96(%rsp,%rsi)
	vbroadcastss	-32(%rdi,%rsi), %ymm1
	vfmadd213ps	128(%rsp,%rsi), %ymm0, %ymm1 ## ymm1 = (ymm0 * ymm1) + mem
	vmovaps	%ymm1, 128(%rsp,%rsi)
	vbroadcastss	(%rdi,%rsi), %ymm1
	vfmadd213ps	160(%rsp,%rsi), %ymm0, %ymm1 ## ymm1 = (ymm0 * ymm1) + mem
	vmovaps	%ymm1, 160(%rsp,%rsi)
	addq	$4, %rcx
	subq	$-128, %rsi
	cmpq	%rcx, %rdx
	jne	LBB8_153
LBB8_154:                               ## %"end for conv1_stage1.s1.w.rebased10.loopexit.us.4.unr-lcssa"
	testq	%rax, %rax
	je	LBB8_157
## %bb.155:                             ## %"for conv1_stage1.s1.w.rebased9.us.4.epil.preheader"
	leaq	(%rcx,%r11), %rdx
	shlq	$5, %rdx
	addq	%r14, %rdx
	addq	$16, %rdx
	shlq	$5, %rcx
	addq	%rsp, %rcx
	addq	$160, %rcx
	shlq	$5, %rax
	xorl	%esi, %esi
	.p2align	4, 0x90
LBB8_156:                               ## %"for conv1_stage1.s1.w.rebased9.us.4.epil"
                                        ## =>This Inner Loop Header: Depth=1
	vbroadcastss	(%rdx,%rsi), %ymm1
	vfmadd213ps	(%rcx,%rsi), %ymm0, %ymm1 ## ymm1 = (ymm0 * ymm1) + mem
	vmovaps	%ymm1, (%rcx,%rsi)
	addq	$32, %rsi
	cmpq	%rsi, %rax
	jne	LBB8_156
LBB8_157:                               ## %"end for conv1_stage1.s1.w.rebased10.loopexit.us.4"
	leaq	(,%r13,4), %rax
	addq	%r13, %rax
	addq	%r9, %rax
	vmovups	(%r15,%rax,4), %ymm0
	movl	%ebx, %eax
	andl	$3, %eax
	cmpq	$3, %r12
	jae	LBB8_159
## %bb.158:
	xorl	%ecx, %ecx
	jmp	LBB8_161
LBB8_203:                               ## %"end for conv1_stage1.s1.w.rebased.loopexit.us.4.new"
	movl	%r14d, %esi
	andl	$-4, %esi
	movq	%r13, %rcx
	shlq	$5, %rcx
	movq	8(%rsp), %rdx                   ## 8-byte Reload
	leaq	(%rcx,%rdx), %rbx
	addq	$20, %rbx
	movl	$96, %edi
	xorl	%ecx, %ecx
	.p2align	4, 0x90
LBB8_204:                               ## %"for conv1_stage1.s1.w.rebased.us.5"
                                        ## =>This Inner Loop Header: Depth=1
	vbroadcastss	-96(%rbx,%rdi), %ymm1
	vfmadd213ps	64(%rsp,%rdi), %ymm0, %ymm1 ## ymm1 = (ymm0 * ymm1) + mem
	vmovaps	%ymm1, 64(%rsp,%rdi)
	vbroadcastss	-64(%rbx,%rdi), %ymm1
	vfmadd213ps	96(%rsp,%rdi), %ymm0, %ymm1 ## ymm1 = (ymm0 * ymm1) + mem
	vmovaps	%ymm1, 96(%rsp,%rdi)
	vbroadcastss	-32(%rbx,%rdi), %ymm1
	vfmadd213ps	128(%rsp,%rdi), %ymm0, %ymm1 ## ymm1 = (ymm0 * ymm1) + mem
	vmovaps	%ymm1, 128(%rsp,%rdi)
	vbroadcastss	(%rbx,%rdi), %ymm1
	vfmadd213ps	160(%rsp,%rdi), %ymm0, %ymm1 ## ymm1 = (ymm0 * ymm1) + mem
	vmovaps	%ymm1, 160(%rsp,%rdi)
	addq	$4, %rcx
	subq	$-128, %rdi
	cmpq	%rcx, %rsi
	jne	LBB8_204
LBB8_205:                               ## %"end for conv1_stage1.s1.w.rebased.loopexit.us.5.unr-lcssa"
	testq	%rax, %rax
	je	LBB8_208
## %bb.206:                             ## %"for conv1_stage1.s1.w.rebased.us.5.epil.preheader"
	leaq	(%rcx,%r13), %rsi
	shlq	$5, %rsi
	movq	8(%rsp), %rdx                   ## 8-byte Reload
	addq	%rdx, %rsi
	addq	$20, %rsi
	shlq	$5, %rcx
	addq	%rsp, %rcx
	addq	$160, %rcx
	shlq	$5, %rax
	xorl	%edi, %edi
	.p2align	4, 0x90
LBB8_207:                               ## %"for conv1_stage1.s1.w.rebased.us.5.epil"
                                        ## =>This Inner Loop Header: Depth=1
	vbroadcastss	(%rsi,%rdi), %ymm1
	vfmadd213ps	(%rcx,%rdi), %ymm0, %ymm1 ## ymm1 = (ymm0 * ymm1) + mem
	vmovaps	%ymm1, (%rcx,%rdi)
	addq	$32, %rdi
	cmpq	%rdi, %rax
	jne	LBB8_207
LBB8_208:                               ## %"end for conv1_stage1.s1.w.rebased.loopexit.us.5"
	leaq	(%r8,%r10,2), %rax
	movq	40(%rsp), %rcx                  ## 8-byte Reload
	vmovups	(%rcx,%rax,4), %ymm0
	movl	%r14d, %eax
	andl	$3, %eax
	cmpq	$3, %r12
	jae	LBB8_210
## %bb.209:
	xorl	%ecx, %ecx
	movq	24(%rsp), %r10                  ## 8-byte Reload
	movl	20(%rsp), %r11d                 ## 4-byte Reload
	jmp	LBB8_212
LBB8_159:                               ## %"end for conv1_stage1.s1.w.rebased10.loopexit.us.4.new"
	movl	%ebx, %edx
	andl	$-4, %edx
	movq	%r11, %rcx
	shlq	$5, %rcx
	leaq	(%rcx,%r14), %rdi
	addq	$20, %rdi
	movl	$96, %esi
	xorl	%ecx, %ecx
	.p2align	4, 0x90
LBB8_160:                               ## %"for conv1_stage1.s1.w.rebased9.us.5"
                                        ## =>This Inner Loop Header: Depth=1
	vbroadcastss	-96(%rdi,%rsi), %ymm1
	vfmadd213ps	64(%rsp,%rsi), %ymm0, %ymm1 ## ymm1 = (ymm0 * ymm1) + mem
	vmovaps	%ymm1, 64(%rsp,%rsi)
	vbroadcastss	-64(%rdi,%rsi), %ymm1
	vfmadd213ps	96(%rsp,%rsi), %ymm0, %ymm1 ## ymm1 = (ymm0 * ymm1) + mem
	vmovaps	%ymm1, 96(%rsp,%rsi)
	vbroadcastss	-32(%rdi,%rsi), %ymm1
	vfmadd213ps	128(%rsp,%rsi), %ymm0, %ymm1 ## ymm1 = (ymm0 * ymm1) + mem
	vmovaps	%ymm1, 128(%rsp,%rsi)
	vbroadcastss	(%rdi,%rsi), %ymm1
	vfmadd213ps	160(%rsp,%rsi), %ymm0, %ymm1 ## ymm1 = (ymm0 * ymm1) + mem
	vmovaps	%ymm1, 160(%rsp,%rsi)
	addq	$4, %rcx
	subq	$-128, %rsi
	cmpq	%rcx, %rdx
	jne	LBB8_160
LBB8_161:                               ## %"end for conv1_stage1.s1.w.rebased10.loopexit.us.5.unr-lcssa"
	testq	%rax, %rax
	je	LBB8_164
## %bb.162:                             ## %"for conv1_stage1.s1.w.rebased9.us.5.epil.preheader"
	leaq	(%rcx,%r11), %rdx
	shlq	$5, %rdx
	addq	%r14, %rdx
	addq	$20, %rdx
	shlq	$5, %rcx
	addq	%rsp, %rcx
	addq	$160, %rcx
	shlq	$5, %rax
	xorl	%esi, %esi
	.p2align	4, 0x90
LBB8_163:                               ## %"for conv1_stage1.s1.w.rebased9.us.5.epil"
                                        ## =>This Inner Loop Header: Depth=1
	vbroadcastss	(%rdx,%rsi), %ymm1
	vfmadd213ps	(%rcx,%rsi), %ymm0, %ymm1 ## ymm1 = (ymm0 * ymm1) + mem
	vmovaps	%ymm1, (%rcx,%rsi)
	addq	$32, %rsi
	cmpq	%rsi, %rax
	jne	LBB8_163
LBB8_164:                               ## %"end for conv1_stage1.s1.w.rebased10.loopexit.us.5"
	leaq	(%r9,%r8,2), %rax
	vmovups	(%r15,%rax,4), %ymm0
	movl	%ebx, %eax
	andl	$3, %eax
	cmpq	$3, %r12
	jae	LBB8_166
## %bb.165:
	xorl	%ecx, %ecx
	movl	8(%rsp), %r8d                   ## 4-byte Reload
	jmp	LBB8_168
LBB8_210:                               ## %"end for conv1_stage1.s1.w.rebased.loopexit.us.5.new"
	movl	%r14d, %esi
	andl	$-4, %esi
	movq	%r13, %rcx
	shlq	$5, %rcx
	movq	8(%rsp), %rdx                   ## 8-byte Reload
	addq	%rcx, %rdx
	addq	$24, %rdx
	movl	$96, %edi
	xorl	%ecx, %ecx
	movq	24(%rsp), %r10                  ## 8-byte Reload
	movl	20(%rsp), %r11d                 ## 4-byte Reload
	.p2align	4, 0x90
LBB8_211:                               ## %"for conv1_stage1.s1.w.rebased.us.6"
                                        ## =>This Inner Loop Header: Depth=1
	vbroadcastss	-96(%rdx,%rdi), %ymm1
	vfmadd213ps	64(%rsp,%rdi), %ymm0, %ymm1 ## ymm1 = (ymm0 * ymm1) + mem
	vmovaps	%ymm1, 64(%rsp,%rdi)
	vbroadcastss	-64(%rdx,%rdi), %ymm1
	vfmadd213ps	96(%rsp,%rdi), %ymm0, %ymm1 ## ymm1 = (ymm0 * ymm1) + mem
	vmovaps	%ymm1, 96(%rsp,%rdi)
	vbroadcastss	-32(%rdx,%rdi), %ymm1
	vfmadd213ps	128(%rsp,%rdi), %ymm0, %ymm1 ## ymm1 = (ymm0 * ymm1) + mem
	vmovaps	%ymm1, 128(%rsp,%rdi)
	vbroadcastss	(%rdx,%rdi), %ymm1
	vfmadd213ps	160(%rsp,%rdi), %ymm0, %ymm1 ## ymm1 = (ymm0 * ymm1) + mem
	vmovaps	%ymm1, 160(%rsp,%rdi)
	addq	$4, %rcx
	subq	$-128, %rdi
	cmpq	%rcx, %rsi
	jne	LBB8_211
LBB8_212:                               ## %"end for conv1_stage1.s1.w.rebased.loopexit.us.6.unr-lcssa"
	testq	%rax, %rax
	je	LBB8_215
## %bb.213:                             ## %"for conv1_stage1.s1.w.rebased.us.6.epil.preheader"
	leaq	(%rcx,%r13), %rsi
	shlq	$5, %rsi
	movq	8(%rsp), %rdx                   ## 8-byte Reload
	addq	%rdx, %rsi
	addq	$24, %rsi
	shlq	$5, %rcx
	addq	%rsp, %rcx
	addq	$160, %rcx
	shlq	$5, %rax
	xorl	%edi, %edi
	.p2align	4, 0x90
LBB8_214:                               ## %"for conv1_stage1.s1.w.rebased.us.6.epil"
                                        ## =>This Inner Loop Header: Depth=1
	vbroadcastss	(%rsi,%rdi), %ymm1
	vfmadd213ps	(%rcx,%rdi), %ymm0, %ymm1 ## ymm1 = (ymm0 * ymm1) + mem
	vmovaps	%ymm1, (%rcx,%rdi)
	addq	$32, %rdi
	cmpq	%rdi, %rax
	jne	LBB8_214
LBB8_215:                               ## %"end for conv1_stage1.s1.w.rebased.loopexit.us.6"
	leaq	(,%r15,8), %rax
	subq	%r15, %rax
	addq	%r8, %rax
	movq	40(%rsp), %rcx                  ## 8-byte Reload
	vmovups	(%rcx,%rax,4), %ymm0
	movl	%r14d, %eax
	andl	$3, %eax
	cmpq	$3, %r12
	movq	(%rsp), %r9                     ## 8-byte Reload
	jae	LBB8_217
## %bb.216:
	xorl	%ecx, %ecx
	jmp	LBB8_22
LBB8_166:                               ## %"end for conv1_stage1.s1.w.rebased10.loopexit.us.5.new"
	movl	%ebx, %edx
	andl	$-4, %edx
	movq	%r11, %rcx
	shlq	$5, %rcx
	leaq	(%rcx,%r14), %rdi
	addq	$24, %rdi
	movl	$96, %esi
	xorl	%ecx, %ecx
	movl	8(%rsp), %r8d                   ## 4-byte Reload
	.p2align	4, 0x90
LBB8_167:                               ## %"for conv1_stage1.s1.w.rebased9.us.6"
                                        ## =>This Inner Loop Header: Depth=1
	vbroadcastss	-96(%rdi,%rsi), %ymm1
	vfmadd213ps	64(%rsp,%rsi), %ymm0, %ymm1 ## ymm1 = (ymm0 * ymm1) + mem
	vmovaps	%ymm1, 64(%rsp,%rsi)
	vbroadcastss	-64(%rdi,%rsi), %ymm1
	vfmadd213ps	96(%rsp,%rsi), %ymm0, %ymm1 ## ymm1 = (ymm0 * ymm1) + mem
	vmovaps	%ymm1, 96(%rsp,%rsi)
	vbroadcastss	-32(%rdi,%rsi), %ymm1
	vfmadd213ps	128(%rsp,%rsi), %ymm0, %ymm1 ## ymm1 = (ymm0 * ymm1) + mem
	vmovaps	%ymm1, 128(%rsp,%rsi)
	vbroadcastss	(%rdi,%rsi), %ymm1
	vfmadd213ps	160(%rsp,%rsi), %ymm0, %ymm1 ## ymm1 = (ymm0 * ymm1) + mem
	vmovaps	%ymm1, 160(%rsp,%rsi)
	addq	$4, %rcx
	subq	$-128, %rsi
	cmpq	%rcx, %rdx
	jne	LBB8_167
LBB8_168:                               ## %"end for conv1_stage1.s1.w.rebased10.loopexit.us.6.unr-lcssa"
	testq	%rax, %rax
	je	LBB8_171
## %bb.169:                             ## %"for conv1_stage1.s1.w.rebased9.us.6.epil.preheader"
	leaq	(%rcx,%r11), %rdx
	shlq	$5, %rdx
	addq	%r14, %rdx
	addq	$24, %rdx
	shlq	$5, %rcx
	addq	%rsp, %rcx
	addq	$160, %rcx
	shlq	$5, %rax
	xorl	%esi, %esi
	.p2align	4, 0x90
LBB8_170:                               ## %"for conv1_stage1.s1.w.rebased9.us.6.epil"
                                        ## =>This Inner Loop Header: Depth=1
	vbroadcastss	(%rdx,%rsi), %ymm1
	vfmadd213ps	(%rcx,%rsi), %ymm0, %ymm1 ## ymm1 = (ymm0 * ymm1) + mem
	vmovaps	%ymm1, (%rcx,%rsi)
	addq	$32, %rsi
	cmpq	%rsi, %rax
	jne	LBB8_170
LBB8_171:                               ## %"end for conv1_stage1.s1.w.rebased10.loopexit.us.6"
	leaq	(,%r13,8), %rax
	subq	%r13, %rax
	addq	%r9, %rax
	vmovups	(%r15,%rax,4), %ymm0
	movl	%ebx, %eax
	andl	$3, %eax
	cmpq	$3, %r12
	jae	LBB8_173
## %bb.172:
	xorl	%ecx, %ecx
	jmp	LBB8_101
LBB8_217:                               ## %"end for conv1_stage1.s1.w.rebased.loopexit.us.6.new"
	andl	$-4, %r14d
	movq	%r13, %rcx
	shlq	$5, %rcx
	movq	8(%rsp), %rdx                   ## 8-byte Reload
	leaq	(%rcx,%rdx), %rsi
	addq	$28, %rsi
	movl	$96, %edi
	xorl	%ecx, %ecx
	.p2align	4, 0x90
LBB8_218:                               ## %"for conv1_stage1.s1.w.rebased.us.7"
                                        ## =>This Inner Loop Header: Depth=1
	vbroadcastss	-96(%rsi,%rdi), %ymm1
	vfmadd213ps	64(%rsp,%rdi), %ymm0, %ymm1 ## ymm1 = (ymm0 * ymm1) + mem
	vmovaps	%ymm1, 64(%rsp,%rdi)
	vbroadcastss	-64(%rsi,%rdi), %ymm1
	vfmadd213ps	96(%rsp,%rdi), %ymm0, %ymm1 ## ymm1 = (ymm0 * ymm1) + mem
	vmovaps	%ymm1, 96(%rsp,%rdi)
	vbroadcastss	-32(%rsi,%rdi), %ymm1
	vfmadd213ps	128(%rsp,%rdi), %ymm0, %ymm1 ## ymm1 = (ymm0 * ymm1) + mem
	vmovaps	%ymm1, 128(%rsp,%rdi)
	vbroadcastss	(%rsi,%rdi), %ymm1
	vfmadd213ps	160(%rsp,%rdi), %ymm0, %ymm1 ## ymm1 = (ymm0 * ymm1) + mem
	vmovaps	%ymm1, 160(%rsp,%rdi)
	addq	$4, %rcx
	subq	$-128, %rdi
	cmpq	%rcx, %r14
	jne	LBB8_218
LBB8_22:                                ## %"consume conv1_stage1.loopexit.unr-lcssa"
	testq	%rax, %rax
	je	LBB8_25
## %bb.23:                              ## %"for conv1_stage1.s1.w.rebased.us.7.epil.preheader"
	addq	%rcx, %r13
	shlq	$5, %r13
	movq	8(%rsp), %rdx                   ## 8-byte Reload
	addq	%r13, %rdx
	addq	$28, %rdx
	shlq	$5, %rcx
	addq	%rsp, %rcx
	addq	$160, %rcx
	shlq	$5, %rax
	xorl	%esi, %esi
	.p2align	4, 0x90
LBB8_24:                                ## %"for conv1_stage1.s1.w.rebased.us.7.epil"
                                        ## =>This Inner Loop Header: Depth=1
	vbroadcastss	(%rdx,%rsi), %ymm1
	vfmadd213ps	(%rcx,%rsi), %ymm0, %ymm1 ## ymm1 = (ymm0 * ymm1) + mem
	vmovaps	%ymm1, (%rcx,%rsi)
	addq	$32, %rsi
	cmpq	%rsi, %rax
	jne	LBB8_24
LBB8_25:                                ## %"consume conv1_stage1"
	movl	304(%rsp), %edx                 ## 4-byte Reload
	movl	%edx, %ecx
	sarl	$3, %ecx
	movq	32(%rsp), %r13                  ## 8-byte Reload
	addl	$7, %r13d
	sarl	$3, %r13d
	movq	%rcx, %rax
	movq	%rcx, 8(%rsp)                   ## 8-byte Spill
	subl	%ecx, %r13d
	jle	LBB8_75
## %bb.26:                              ## %"consume conv1_stage1.split78.us"
	movl	%r13d, %eax
	movq	%rax, 40(%rsp)                  ## 8-byte Spill
	cmpl	$7, %edx
	movl	88(%rsp), %r8d                  ## 4-byte Reload
	movl	%edx, %eax
	jle	LBB8_27
## %bb.33:                              ## %"for conv1_stage2.s0.w.wi.wi.us.us.preheader"
	movq	8(%rsp), %rdx                   ## 8-byte Reload
	movl	%edx, %esi
	movl	%r11d, %edi
	subl	%r9d, %edi
	andl	$-8, %eax
	movl	%eax, 304(%rsp)                 ## 4-byte Spill
	subl	%eax, %edi
	leaq	-1(%rsi), %rax
	movq	%rax, 72(%rsp)                  ## 8-byte Spill
	movl	%esi, %ecx
	andl	$7, %ecx
	movq	%rsi, 64(%rsp)                  ## 8-byte Spill
                                        ## kill: def $esi killed $esi killed $rsi def $rsi
	andl	$-8, %esi
	leaq	224(%r10), %rax
	movq	%rax, 104(%rsp)                 ## 8-byte Spill
	movq	120(%rsp), %r15                 ## 8-byte Reload
	shll	$5, %r15d
	andl	$-128, %r15d
	addl	56(%rsp), %r15d                 ## 4-byte Folded Reload
	movl	144(%rsp), %eax                 ## 4-byte Reload
	shll	$5, %eax
	subl	%eax, %r15d
	movq	%r15, 120(%rsp)                 ## 8-byte Spill
	leal	32(%r15), %eax
	imull	%r8d, %eax
	movq	%rcx, 96(%rsp)                  ## 8-byte Spill
	movq	%rcx, %rbx
	shlq	$5, %rbx
	leaq	992(%r10), %rcx
	movq	%rcx, 112(%rsp)                 ## 8-byte Spill
	leal	(,%rdx,8), %ecx
	leal	(%rax,%rdx,8), %r8d
	movl	%eax, %edx
	subl	%r9d, %edx
	leaq	96(%r10), %rax
	movq	%rax, 32(%rsp)                  ## 8-byte Spill
	movl	%r11d, %eax
	subl	%r9d, %eax
	movl	%ecx, 144(%rsp)                 ## 4-byte Spill
	subl	%ecx, %eax
	movl	%eax, 152(%rsp)                 ## 4-byte Spill
	xorl	%eax, %eax
	movl	%edi, 48(%rsp)                  ## 4-byte Spill
	movq	%rsi, 136(%rsp)                 ## 8-byte Spill
	movq	%rbx, 128(%rsp)                 ## 8-byte Spill
	jmp	LBB8_34
	.p2align	4, 0x90
LBB8_47:                                ## %"end for conv1_stage2.s0.n.n.rebased.loopexit.us.us.us.us.us.us"
                                        ##   in Loop: Header=BB8_34 Depth=1
	movq	80(%rsp), %rax                  ## 8-byte Reload
	incq	%rax
	movl	88(%rsp), %ecx                  ## 4-byte Reload
	movl	56(%rsp), %edx                  ## 4-byte Reload
	addl	%ecx, %edx
	movl	%ebx, %r8d
	addl	%ecx, %r8d
	cmpq	$8, %rax
	movq	(%rsp), %r9                     ## 8-byte Reload
	movq	136(%rsp), %rsi                 ## 8-byte Reload
	movq	128(%rsp), %rbx                 ## 8-byte Reload
	je	LBB8_48
LBB8_34:                                ## %"for conv1_stage2.s0.c.ci.us.us.us.us.us.us"
                                        ## =>This Loop Header: Depth=1
                                        ##     Child Loop BB8_37 Depth 2
                                        ##     Child Loop BB8_40 Depth 2
                                        ##     Child Loop BB8_42 Depth 2
                                        ##       Child Loop BB8_54 Depth 3
                                        ##       Child Loop BB8_57 Depth 3
                                        ##       Child Loop BB8_45 Depth 3
	movl	%edx, 56(%rsp)                  ## 4-byte Spill
	movslq	%edx, %r14
	vbroadcastss	160(%rsp,%rax,4), %ymm0
	cmpq	$7, 72(%rsp)                    ## 8-byte Folded Reload
	jae	LBB8_36
## %bb.35:                              ##   in Loop: Header=BB8_34 Depth=1
	xorl	%ecx, %ecx
	jmp	LBB8_38
	.p2align	4, 0x90
LBB8_36:                                ## %"for conv1_stage2.s0.n.n.us.us.us.us.us.us.preheader"
                                        ##   in Loop: Header=BB8_34 Depth=1
	leaq	(%r9,%r14), %rcx
	movq	104(%rsp), %rdx                 ## 8-byte Reload
	leaq	(%rdx,%rcx,4), %rdx
	xorl	%ecx, %ecx
	.p2align	4, 0x90
LBB8_37:                                ## %"for conv1_stage2.s0.n.n.us.us.us.us.us.us"
                                        ##   Parent Loop BB8_34 Depth=1
                                        ## =>  This Inner Loop Header: Depth=2
	vmovups	%ymm0, -224(%rdx)
	vmovups	%ymm0, -192(%rdx)
	vmovups	%ymm0, -160(%rdx)
	vmovups	%ymm0, -128(%rdx)
	vmovups	%ymm0, -96(%rdx)
	vmovups	%ymm0, -64(%rdx)
	vmovups	%ymm0, -32(%rdx)
	vmovups	%ymm0, (%rdx)
	addq	$8, %rcx
	addq	$256, %rdx                      ## imm = 0x100
	cmpq	%rcx, %rsi
	jne	LBB8_37
LBB8_38:                                ## %"for conv1_stage2.s0.n.n.rebased.us.us.us.us.us.us.preheader.unr-lcssa"
                                        ##   in Loop: Header=BB8_34 Depth=1
	movq	%rax, 80(%rsp)                  ## 8-byte Spill
	cmpq	$0, 96(%rsp)                    ## 8-byte Folded Reload
	je	LBB8_41
## %bb.39:                              ## %"for conv1_stage2.s0.n.n.us.us.us.us.us.us.epil.preheader"
                                        ##   in Loop: Header=BB8_34 Depth=1
	leaq	(%r9,%rcx,8), %rcx
	addq	%r14, %rcx
	leaq	(%r10,%rcx,4), %rax
	xorl	%ecx, %ecx
	.p2align	4, 0x90
LBB8_40:                                ## %"for conv1_stage2.s0.n.n.us.us.us.us.us.us.epil"
                                        ##   Parent Loop BB8_34 Depth=1
                                        ## =>  This Inner Loop Header: Depth=2
	vmovups	%ymm0, (%rax,%rcx)
	addq	$32, %rcx
	cmpq	%rcx, %rbx
	jne	LBB8_40
LBB8_41:                                ## %"for conv1_stage2.s0.n.n.rebased.us.us.us.us.us.us.preheader"
                                        ##   in Loop: Header=BB8_34 Depth=1
	vbroadcastss	%xmm0, %ymm1
	movl	152(%rsp), %r13d                ## 4-byte Reload
	movl	%edi, %r15d
	movl	%r8d, %ebx
	movl	%r8d, %r9d
	xorl	%r12d, %r12d
	jmp	LBB8_42
	.p2align	4, 0x90
LBB8_46:                                ## %"end for conv1_stage2.s0.n.ni.us.us.us.us.us.us"
                                        ##   in Loop: Header=BB8_42 Depth=2
	incq	%r12
	addl	$8, %r9d
	addl	$-8, %r15d
	addl	$-8, %r13d
	cmpq	40(%rsp), %r12                  ## 8-byte Folded Reload
	je	LBB8_47
LBB8_42:                                ## %"for conv1_stage2.s0.n.n.rebased.us.us.us.us.us.us"
                                        ##   Parent Loop BB8_34 Depth=1
                                        ## =>  This Loop Header: Depth=2
                                        ##       Child Loop BB8_54 Depth 3
                                        ##       Child Loop BB8_57 Depth 3
                                        ##       Child Loop BB8_45 Depth 3
	cmpl	$8, %r13d
	movl	$8, %ecx
	cmovll	%r13d, %ecx
	cmpl	$8, %r15d
	movl	$8, %edx
	cmovll	%r15d, %edx
	leal	(,%r12,8), %eax
	movl	%edi, %r14d
	subl	%eax, %r14d
	cmpl	$8, %r14d
	movl	$8, %eax
	cmovgel	%eax, %r14d
	movslq	%r9d, %r9
	movq	8(%rsp), %rax                   ## 8-byte Reload
	addl	%r12d, %eax
	movq	(%rsp), %rsi                    ## 8-byte Reload
	leal	(%rsi,%rax,8), %eax
	cmpl	%eax, %r11d
	jle	LBB8_46
## %bb.43:                              ## %"for conv1_stage2.s0.n.ni.preheader.us.us.us.us.us.us"
                                        ##   in Loop: Header=BB8_42 Depth=2
	cmpl	$32, %r14d
	jae	LBB8_51
## %bb.44:                              ##   in Loop: Header=BB8_42 Depth=2
	xorl	%eax, %eax
	jmp	LBB8_59
	.p2align	4, 0x90
LBB8_51:                                ## %vector.ph368
                                        ##   in Loop: Header=BB8_42 Depth=2
	andl	$-32, %edx
	addq	$-32, %rdx
	shrq	$5, %rdx
	movl	%r14d, %eax
	andl	$-32, %eax
	addq	$-32, %rax
	movq	%rax, %rsi
	shrq	$5, %rsi
	incq	%rsi
	cmpq	$224, %rax
	jae	LBB8_53
## %bb.52:                              ##   in Loop: Header=BB8_42 Depth=2
	xorl	%r11d, %r11d
	jmp	LBB8_55
	.p2align	4, 0x90
LBB8_53:                                ## %vector.ph368.new
                                        ##   in Loop: Header=BB8_42 Depth=2
	movq	112(%rsp), %rax                 ## 8-byte Reload
	leaq	(%rax,%r9,4), %r10
	leaq	1(%rdx), %r8
	andq	$-8, %r8
	negq	%r8
	xorl	%r11d, %r11d
	.p2align	4, 0x90
LBB8_54:                                ## %vector.body365
                                        ##   Parent Loop BB8_34 Depth=1
                                        ##     Parent Loop BB8_42 Depth=2
                                        ## =>    This Inner Loop Header: Depth=3
	vmovups	%ymm1, -992(%r10,%r11,4)
	vmovups	%ymm1, -960(%r10,%r11,4)
	vmovups	%ymm1, -928(%r10,%r11,4)
	vmovups	%ymm1, -896(%r10,%r11,4)
	vmovups	%ymm1, -864(%r10,%r11,4)
	vmovups	%ymm1, -832(%r10,%r11,4)
	vmovups	%ymm1, -800(%r10,%r11,4)
	vmovups	%ymm1, -768(%r10,%r11,4)
	vmovups	%ymm1, -736(%r10,%r11,4)
	vmovups	%ymm1, -704(%r10,%r11,4)
	vmovups	%ymm1, -672(%r10,%r11,4)
	vmovups	%ymm1, -640(%r10,%r11,4)
	vmovups	%ymm1, -608(%r10,%r11,4)
	vmovups	%ymm1, -576(%r10,%r11,4)
	vmovups	%ymm1, -544(%r10,%r11,4)
	vmovups	%ymm1, -512(%r10,%r11,4)
	vmovups	%ymm1, -480(%r10,%r11,4)
	vmovups	%ymm1, -448(%r10,%r11,4)
	vmovups	%ymm1, -416(%r10,%r11,4)
	vmovups	%ymm1, -384(%r10,%r11,4)
	vmovups	%ymm1, -352(%r10,%r11,4)
	vmovups	%ymm1, -320(%r10,%r11,4)
	vmovups	%ymm1, -288(%r10,%r11,4)
	vmovups	%ymm1, -256(%r10,%r11,4)
	vmovups	%ymm1, -224(%r10,%r11,4)
	vmovups	%ymm1, -192(%r10,%r11,4)
	vmovups	%ymm1, -160(%r10,%r11,4)
	vmovups	%ymm1, -128(%r10,%r11,4)
	vmovups	%ymm1, -96(%r10,%r11,4)
	vmovups	%ymm1, -64(%r10,%r11,4)
	vmovups	%ymm1, -32(%r10,%r11,4)
	vmovups	%ymm1, (%r10,%r11,4)
	addq	$256, %r11                      ## imm = 0x100
	addq	$8, %r8
	jne	LBB8_54
LBB8_55:                                ## %middle.block363.unr-lcssa
                                        ##   in Loop: Header=BB8_42 Depth=2
	movl	%r14d, %eax
	andl	$-32, %eax
	testb	$7, %sil
	je	LBB8_58
## %bb.56:                              ## %vector.body365.epil.preheader
                                        ##   in Loop: Header=BB8_42 Depth=2
	incb	%dl
	movzbl	%dl, %edx
	andl	$7, %edx
	shlq	$7, %rdx
	addq	%r9, %r11
	movq	32(%rsp), %rsi                  ## 8-byte Reload
	leaq	(%rsi,%r11,4), %rsi
	xorl	%edi, %edi
	.p2align	4, 0x90
LBB8_57:                                ## %vector.body365.epil
                                        ##   Parent Loop BB8_34 Depth=1
                                        ##     Parent Loop BB8_42 Depth=2
                                        ## =>    This Inner Loop Header: Depth=3
	vmovups	%ymm1, -96(%rsi,%rdi)
	vmovups	%ymm1, -64(%rsi,%rdi)
	vmovups	%ymm1, -32(%rsi,%rdi)
	vmovups	%ymm1, (%rsi,%rdi)
	subq	$-128, %rdi
	cmpq	%rdi, %rdx
	jne	LBB8_57
LBB8_58:                                ## %middle.block363
                                        ##   in Loop: Header=BB8_42 Depth=2
	cmpq	%r14, %rax
	movq	24(%rsp), %r10                  ## 8-byte Reload
	movl	20(%rsp), %r11d                 ## 4-byte Reload
	movl	48(%rsp), %edi                  ## 4-byte Reload
	je	LBB8_46
LBB8_59:                                ## %"for conv1_stage2.s0.n.ni.us.us.us.us.us.us.preheader"
                                        ##   in Loop: Header=BB8_42 Depth=2
	subq	%rax, %rcx
	addq	%r9, %rax
	leaq	(%r10,%rax,4), %rax
	xorl	%edx, %edx
	.p2align	4, 0x90
LBB8_45:                                ## %"for conv1_stage2.s0.n.ni.us.us.us.us.us.us"
                                        ##   Parent Loop BB8_34 Depth=1
                                        ##     Parent Loop BB8_42 Depth=2
                                        ## =>    This Inner Loop Header: Depth=3
	vmovss	%xmm0, (%rax,%rdx,4)
	incq	%rdx
	cmpq	%rdx, %rcx
	jne	LBB8_45
	jmp	LBB8_46
LBB8_48:                                ## %"end for conv1_stage2.s0.c.ci.split.us.us.split.us.us.us.us"
	subl	%r9d, %r11d
	movl	%r11d, %r14d
	subl	304(%rsp), %r14d                ## 4-byte Folded Reload
	movq	64(%rsp), %rax                  ## 8-byte Reload
	movl	%eax, %edx
	andl	$7, %edx
	movl	%eax, %edi
	andl	$-8, %edi
	movq	120(%rsp), %rax                 ## 8-byte Reload
	addl	$64, %eax
	imull	%ecx, %eax
	movq	%rdx, 136(%rsp)                 ## 8-byte Spill
	movq	%rdx, %rsi
	shlq	$5, %rsi
	movq	8(%rsp), %rcx                   ## 8-byte Reload
	leal	(%rax,%rcx,8), %r12d
                                        ## kill: def $eax killed $eax killed $rax
	subl	%r9d, %eax
	subl	144(%rsp), %r11d                ## 4-byte Folded Reload
	movl	%r11d, 96(%rsp)                 ## 4-byte Spill
	xorl	%edx, %edx
	movl	$8, %ebx
	movq	%rdi, 128(%rsp)                 ## 8-byte Spill
	movq	%rsi, 152(%rsp)                 ## 8-byte Spill
	jmp	LBB8_49
	.p2align	4, 0x90
LBB8_446:                               ## %"end for conv1_stage2.s0.n.n.rebased.loopexit.us.us.us.us.us.us.1"
                                        ##   in Loop: Header=BB8_49 Depth=1
	movq	80(%rsp), %rdx                  ## 8-byte Reload
	incq	%rdx
	movl	88(%rsp), %ecx                  ## 4-byte Reload
	movl	56(%rsp), %eax                  ## 4-byte Reload
	addl	%ecx, %eax
	movl	48(%rsp), %r12d                 ## 4-byte Reload
	addl	%ecx, %r12d
	cmpq	$8, %rdx
	movl	20(%rsp), %r10d                 ## 4-byte Reload
	movq	128(%rsp), %rdi                 ## 8-byte Reload
	movq	152(%rsp), %rsi                 ## 8-byte Reload
	je	LBB8_447
LBB8_49:                                ## %"for conv1_stage2.s0.c.ci.us.us.us.us.us.us.1"
                                        ## =>This Loop Header: Depth=1
                                        ##     Child Loop BB8_427 Depth 2
                                        ##     Child Loop BB8_430 Depth 2
                                        ##     Child Loop BB8_432 Depth 2
                                        ##       Child Loop BB8_438 Depth 3
                                        ##       Child Loop BB8_441 Depth 3
                                        ##       Child Loop BB8_444 Depth 3
	movl	%eax, 56(%rsp)                  ## 4-byte Spill
	cltq
	movq	%rdx, 80(%rsp)                  ## 8-byte Spill
	vbroadcastss	192(%rsp,%rdx,4), %ymm0
	cmpq	$7, 72(%rsp)                    ## 8-byte Folded Reload
	jae	LBB8_426
## %bb.50:                              ##   in Loop: Header=BB8_49 Depth=1
	xorl	%ecx, %ecx
	movq	24(%rsp), %r8                   ## 8-byte Reload
	jmp	LBB8_428
	.p2align	4, 0x90
LBB8_426:                               ## %"for conv1_stage2.s0.n.n.us.us.us.us.us.us.1.preheader"
                                        ##   in Loop: Header=BB8_49 Depth=1
	leaq	(%r9,%rax), %rcx
	movq	104(%rsp), %rdx                 ## 8-byte Reload
	leaq	(%rdx,%rcx,4), %rdx
	xorl	%ecx, %ecx
	movq	24(%rsp), %r8                   ## 8-byte Reload
	.p2align	4, 0x90
LBB8_427:                               ## %"for conv1_stage2.s0.n.n.us.us.us.us.us.us.1"
                                        ##   Parent Loop BB8_49 Depth=1
                                        ## =>  This Inner Loop Header: Depth=2
	vmovups	%ymm0, -224(%rdx)
	vmovups	%ymm0, -192(%rdx)
	vmovups	%ymm0, -160(%rdx)
	vmovups	%ymm0, -128(%rdx)
	vmovups	%ymm0, -96(%rdx)
	vmovups	%ymm0, -64(%rdx)
	vmovups	%ymm0, -32(%rdx)
	vmovups	%ymm0, (%rdx)
	addq	$8, %rcx
	addq	$256, %rdx                      ## imm = 0x100
	cmpq	%rcx, %rdi
	jne	LBB8_427
LBB8_428:                               ## %"for conv1_stage2.s0.n.n.rebased.us.us.us.us.us.us.1.preheader.unr-lcssa"
                                        ##   in Loop: Header=BB8_49 Depth=1
	cmpq	$0, 136(%rsp)                   ## 8-byte Folded Reload
	je	LBB8_431
## %bb.429:                             ## %"for conv1_stage2.s0.n.n.us.us.us.us.us.us.1.epil.preheader"
                                        ##   in Loop: Header=BB8_49 Depth=1
	leaq	(%r9,%rcx,8), %rcx
	addq	%rax, %rcx
	leaq	(%r8,%rcx,4), %rax
	xorl	%ecx, %ecx
	.p2align	4, 0x90
LBB8_430:                               ## %"for conv1_stage2.s0.n.n.us.us.us.us.us.us.1.epil"
                                        ##   Parent Loop BB8_49 Depth=1
                                        ## =>  This Inner Loop Header: Depth=2
	vmovups	%ymm0, (%rax,%rcx)
	addq	$32, %rcx
	cmpq	%rcx, %rsi
	jne	LBB8_430
LBB8_431:                               ## %"for conv1_stage2.s0.n.n.rebased.us.us.us.us.us.us.1.preheader"
                                        ##   in Loop: Header=BB8_49 Depth=1
	vbroadcastss	%xmm0, %ymm1
	movl	96(%rsp), %r10d                 ## 4-byte Reload
	movl	%r14d, %r15d
	movl	%r12d, 48(%rsp)                 ## 4-byte Spill
                                        ## kill: def $r12d killed $r12d def $r12
	xorl	%r8d, %r8d
	jmp	LBB8_432
	.p2align	4, 0x90
LBB8_445:                               ## %"end for conv1_stage2.s0.n.ni.us.us.us.us.us.us.1"
                                        ##   in Loop: Header=BB8_432 Depth=2
	incq	%r8
	addl	$8, %r12d
	addl	$-8, %r15d
	addl	$-8, %r10d
	cmpq	40(%rsp), %r8                   ## 8-byte Folded Reload
	je	LBB8_446
LBB8_432:                               ## %"for conv1_stage2.s0.n.n.rebased.us.us.us.us.us.us.1"
                                        ##   Parent Loop BB8_49 Depth=1
                                        ## =>  This Loop Header: Depth=2
                                        ##       Child Loop BB8_438 Depth 3
                                        ##       Child Loop BB8_441 Depth 3
                                        ##       Child Loop BB8_444 Depth 3
	cmpl	$8, %r10d
	movl	$8, %ecx
	cmovll	%r10d, %ecx
	cmpl	$8, %r15d
	movl	$8, %esi
	cmovll	%r15d, %esi
	leal	(,%r8,8), %edx
	movl	%r14d, %eax
	subl	%edx, %eax
	cmpl	$8, %eax
	cmovgel	%ebx, %eax
	movslq	%r12d, %r12
	movq	8(%rsp), %rdx                   ## 8-byte Reload
	addl	%r8d, %edx
	leal	(%r9,%rdx,8), %edx
	cmpl	%edx, 20(%rsp)                  ## 4-byte Folded Reload
	jle	LBB8_445
## %bb.433:                             ## %"for conv1_stage2.s0.n.ni.preheader.us.us.us.us.us.us.1"
                                        ##   in Loop: Header=BB8_432 Depth=2
	cmpl	$32, %eax
	jae	LBB8_435
## %bb.434:                             ##   in Loop: Header=BB8_432 Depth=2
	xorl	%edi, %edi
	jmp	LBB8_443
	.p2align	4, 0x90
LBB8_435:                               ## %vector.ph388
                                        ##   in Loop: Header=BB8_432 Depth=2
	andl	$-32, %esi
	addq	$-32, %rsi
	shrq	$5, %rsi
	movl	%eax, %edx
	andl	$-32, %edx
	addq	$-32, %rdx
	movq	%rdx, %r11
	shrq	$5, %r11
	incq	%r11
	cmpq	$224, %rdx
	jae	LBB8_437
## %bb.436:                             ##   in Loop: Header=BB8_432 Depth=2
	xorl	%edx, %edx
	jmp	LBB8_439
	.p2align	4, 0x90
LBB8_437:                               ## %vector.ph388.new
                                        ##   in Loop: Header=BB8_432 Depth=2
	movq	112(%rsp), %rdx                 ## 8-byte Reload
	leaq	(%rdx,%r12,4), %rdi
	leaq	1(%rsi), %r13
	andq	$-8, %r13
	negq	%r13
	xorl	%edx, %edx
	.p2align	4, 0x90
LBB8_438:                               ## %vector.body385
                                        ##   Parent Loop BB8_49 Depth=1
                                        ##     Parent Loop BB8_432 Depth=2
                                        ## =>    This Inner Loop Header: Depth=3
	vmovups	%ymm1, -992(%rdi,%rdx,4)
	vmovups	%ymm1, -960(%rdi,%rdx,4)
	vmovups	%ymm1, -928(%rdi,%rdx,4)
	vmovups	%ymm1, -896(%rdi,%rdx,4)
	vmovups	%ymm1, -864(%rdi,%rdx,4)
	vmovups	%ymm1, -832(%rdi,%rdx,4)
	vmovups	%ymm1, -800(%rdi,%rdx,4)
	vmovups	%ymm1, -768(%rdi,%rdx,4)
	vmovups	%ymm1, -736(%rdi,%rdx,4)
	vmovups	%ymm1, -704(%rdi,%rdx,4)
	vmovups	%ymm1, -672(%rdi,%rdx,4)
	vmovups	%ymm1, -640(%rdi,%rdx,4)
	vmovups	%ymm1, -608(%rdi,%rdx,4)
	vmovups	%ymm1, -576(%rdi,%rdx,4)
	vmovups	%ymm1, -544(%rdi,%rdx,4)
	vmovups	%ymm1, -512(%rdi,%rdx,4)
	vmovups	%ymm1, -480(%rdi,%rdx,4)
	vmovups	%ymm1, -448(%rdi,%rdx,4)
	vmovups	%ymm1, -416(%rdi,%rdx,4)
	vmovups	%ymm1, -384(%rdi,%rdx,4)
	vmovups	%ymm1, -352(%rdi,%rdx,4)
	vmovups	%ymm1, -320(%rdi,%rdx,4)
	vmovups	%ymm1, -288(%rdi,%rdx,4)
	vmovups	%ymm1, -256(%rdi,%rdx,4)
	vmovups	%ymm1, -224(%rdi,%rdx,4)
	vmovups	%ymm1, -192(%rdi,%rdx,4)
	vmovups	%ymm1, -160(%rdi,%rdx,4)
	vmovups	%ymm1, -128(%rdi,%rdx,4)
	vmovups	%ymm1, -96(%rdi,%rdx,4)
	vmovups	%ymm1, -64(%rdi,%rdx,4)
	vmovups	%ymm1, -32(%rdi,%rdx,4)
	vmovups	%ymm1, (%rdi,%rdx,4)
	addq	$256, %rdx                      ## imm = 0x100
	addq	$8, %r13
	jne	LBB8_438
LBB8_439:                               ## %middle.block383.unr-lcssa
                                        ##   in Loop: Header=BB8_432 Depth=2
	movl	%eax, %edi
	andl	$-32, %edi
	testb	$7, %r11b
	je	LBB8_442
## %bb.440:                             ## %vector.body385.epil.preheader
                                        ##   in Loop: Header=BB8_432 Depth=2
	incb	%sil
	movzbl	%sil, %esi
	andl	$7, %esi
	shlq	$7, %rsi
	addq	%r12, %rdx
	movq	32(%rsp), %rbx                  ## 8-byte Reload
	leaq	(%rbx,%rdx,4), %rdx
	xorl	%ebx, %ebx
	.p2align	4, 0x90
LBB8_441:                               ## %vector.body385.epil
                                        ##   Parent Loop BB8_49 Depth=1
                                        ##     Parent Loop BB8_432 Depth=2
                                        ## =>    This Inner Loop Header: Depth=3
	vmovups	%ymm1, -96(%rdx,%rbx)
	vmovups	%ymm1, -64(%rdx,%rbx)
	vmovups	%ymm1, -32(%rdx,%rbx)
	vmovups	%ymm1, (%rdx,%rbx)
	subq	$-128, %rbx
	cmpq	%rbx, %rsi
	jne	LBB8_441
LBB8_442:                               ## %middle.block383
                                        ##   in Loop: Header=BB8_432 Depth=2
	cmpq	%rax, %rdi
	movq	(%rsp), %r9                     ## 8-byte Reload
	movl	$8, %ebx
	je	LBB8_445
LBB8_443:                               ## %"for conv1_stage2.s0.n.ni.us.us.us.us.us.us.1.preheader"
                                        ##   in Loop: Header=BB8_432 Depth=2
	subq	%rdi, %rcx
	addq	%r12, %rdi
	movq	24(%rsp), %rax                  ## 8-byte Reload
	leaq	(%rax,%rdi,4), %rax
	xorl	%edx, %edx
	.p2align	4, 0x90
LBB8_444:                               ## %"for conv1_stage2.s0.n.ni.us.us.us.us.us.us.1"
                                        ##   Parent Loop BB8_49 Depth=1
                                        ##     Parent Loop BB8_432 Depth=2
                                        ## =>    This Inner Loop Header: Depth=3
	vmovss	%xmm0, (%rax,%rdx,4)
	incq	%rdx
	cmpq	%rdx, %rcx
	jne	LBB8_444
	jmp	LBB8_445
LBB8_447:                               ## %"end for conv1_stage2.s0.c.ci.split.us.us.split.us.us.us.us.1"
	movl	%r10d, %r13d
	subl	%r9d, %r13d
	subl	304(%rsp), %r13d                ## 4-byte Folded Reload
	movq	64(%rsp), %rax                  ## 8-byte Reload
	movl	%eax, %esi
	andl	$7, %esi
	movl	%eax, %r11d
	andl	$-8, %r11d
	movq	120(%rsp), %rax                 ## 8-byte Reload
	addl	$96, %eax
	imull	%ecx, %eax
	movq	%rsi, 96(%rsp)                  ## 8-byte Spill
	shlq	$5, %rsi
	movq	8(%rsp), %rcx                   ## 8-byte Reload
	leal	(%rax,%rcx,8), %r8d
                                        ## kill: def $eax killed $eax killed $rax
	subl	%r9d, %eax
	movl	%r10d, %edx
	subl	%r9d, %edx
	subl	144(%rsp), %edx                 ## 4-byte Folded Reload
	movl	%edx, 152(%rsp)                 ## 4-byte Spill
	xorl	%ecx, %ecx
	movl	$8, %ebx
	movq	%r11, 136(%rsp)                 ## 8-byte Spill
	movq	%rsi, 128(%rsp)                 ## 8-byte Spill
	jmp	LBB8_448
	.p2align	4, 0x90
LBB8_470:                               ## %"end for conv1_stage2.s0.n.n.rebased.loopexit.us.us.us.us.us.us.1184"
                                        ##   in Loop: Header=BB8_448 Depth=1
	movq	80(%rsp), %rcx                  ## 8-byte Reload
	incq	%rcx
	movl	88(%rsp), %edx                  ## 4-byte Reload
	movl	56(%rsp), %eax                  ## 4-byte Reload
	addl	%edx, %eax
	movl	48(%rsp), %r8d                  ## 4-byte Reload
	addl	%edx, %r8d
	cmpq	$8, %rcx
	movl	20(%rsp), %r10d                 ## 4-byte Reload
	movq	64(%rsp), %rdi                  ## 8-byte Reload
	movq	136(%rsp), %r11                 ## 8-byte Reload
	movq	128(%rsp), %rsi                 ## 8-byte Reload
	je	LBB8_471
LBB8_448:                               ## %"for conv1_stage2.s0.c.ci.us.us.us.us.us.us.1161"
                                        ## =>This Loop Header: Depth=1
                                        ##     Child Loop BB8_451 Depth 2
                                        ##     Child Loop BB8_454 Depth 2
                                        ##     Child Loop BB8_456 Depth 2
                                        ##       Child Loop BB8_462 Depth 3
                                        ##       Child Loop BB8_465 Depth 3
                                        ##       Child Loop BB8_468 Depth 3
	movslq	%eax, %rdi
	movq	%rcx, 80(%rsp)                  ## 8-byte Spill
	vbroadcastss	224(%rsp,%rcx,4), %ymm0
	cmpq	$7, 72(%rsp)                    ## 8-byte Folded Reload
	jae	LBB8_450
## %bb.449:                             ##   in Loop: Header=BB8_448 Depth=1
	xorl	%ecx, %ecx
	movq	24(%rsp), %r10                  ## 8-byte Reload
	jmp	LBB8_452
	.p2align	4, 0x90
LBB8_450:                               ## %"for conv1_stage2.s0.n.n.us.us.us.us.us.us.1165.preheader"
                                        ##   in Loop: Header=BB8_448 Depth=1
	leaq	(%r9,%rdi), %rcx
	movq	104(%rsp), %rdx                 ## 8-byte Reload
	leaq	(%rdx,%rcx,4), %rdx
	xorl	%ecx, %ecx
	movq	24(%rsp), %r10                  ## 8-byte Reload
	.p2align	4, 0x90
LBB8_451:                               ## %"for conv1_stage2.s0.n.n.us.us.us.us.us.us.1165"
                                        ##   Parent Loop BB8_448 Depth=1
                                        ## =>  This Inner Loop Header: Depth=2
	vmovups	%ymm0, -224(%rdx)
	vmovups	%ymm0, -192(%rdx)
	vmovups	%ymm0, -160(%rdx)
	vmovups	%ymm0, -128(%rdx)
	vmovups	%ymm0, -96(%rdx)
	vmovups	%ymm0, -64(%rdx)
	vmovups	%ymm0, -32(%rdx)
	vmovups	%ymm0, (%rdx)
	addq	$8, %rcx
	addq	$256, %rdx                      ## imm = 0x100
	cmpq	%rcx, %r11
	jne	LBB8_451
LBB8_452:                               ## %"for conv1_stage2.s0.n.n.rebased.us.us.us.us.us.us.1171.preheader.unr-lcssa"
                                        ##   in Loop: Header=BB8_448 Depth=1
	movl	%eax, 56(%rsp)                  ## 4-byte Spill
	cmpq	$0, 96(%rsp)                    ## 8-byte Folded Reload
	je	LBB8_455
## %bb.453:                             ## %"for conv1_stage2.s0.n.n.us.us.us.us.us.us.1165.epil.preheader"
                                        ##   in Loop: Header=BB8_448 Depth=1
	leaq	(%r9,%rcx,8), %rcx
	addq	%rdi, %rcx
	leaq	(%r10,%rcx,4), %rax
	xorl	%ecx, %ecx
	.p2align	4, 0x90
LBB8_454:                               ## %"for conv1_stage2.s0.n.n.us.us.us.us.us.us.1165.epil"
                                        ##   Parent Loop BB8_448 Depth=1
                                        ## =>  This Inner Loop Header: Depth=2
	vmovups	%ymm0, (%rax,%rcx)
	addq	$32, %rcx
	cmpq	%rcx, %rsi
	jne	LBB8_454
LBB8_455:                               ## %"for conv1_stage2.s0.n.n.rebased.us.us.us.us.us.us.1171.preheader"
                                        ##   in Loop: Header=BB8_448 Depth=1
	vbroadcastss	%xmm0, %ymm1
	movl	152(%rsp), %r11d                ## 4-byte Reload
	movl	%r13d, %r10d
	movl	%r8d, 48(%rsp)                  ## 4-byte Spill
	movl	%r8d, %r14d
	xorl	%r12d, %r12d
	jmp	LBB8_456
	.p2align	4, 0x90
LBB8_469:                               ## %"end for conv1_stage2.s0.n.ni.us.us.us.us.us.us.1181"
                                        ##   in Loop: Header=BB8_456 Depth=2
	incq	%r12
	addl	$8, %r14d
	addl	$-8, %r10d
	addl	$-8, %r11d
	cmpq	40(%rsp), %r12                  ## 8-byte Folded Reload
	je	LBB8_470
LBB8_456:                               ## %"for conv1_stage2.s0.n.n.rebased.us.us.us.us.us.us.1171"
                                        ##   Parent Loop BB8_448 Depth=1
                                        ## =>  This Loop Header: Depth=2
                                        ##       Child Loop BB8_462 Depth 3
                                        ##       Child Loop BB8_465 Depth 3
                                        ##       Child Loop BB8_468 Depth 3
	cmpl	$8, %r11d
	movl	$8, %ecx
	cmovll	%r11d, %ecx
	cmpl	$8, %r10d
	movl	$8, %edx
	cmovll	%r10d, %edx
	leal	(,%r12,8), %esi
	movl	%r13d, %eax
	subl	%esi, %eax
	cmpl	$8, %eax
	cmovgel	%ebx, %eax
	movslq	%r14d, %r14
	movq	8(%rsp), %rsi                   ## 8-byte Reload
	addl	%r12d, %esi
	leal	(%r9,%rsi,8), %esi
	cmpl	%esi, 20(%rsp)                  ## 4-byte Folded Reload
	jle	LBB8_469
## %bb.457:                             ## %"for conv1_stage2.s0.n.ni.preheader.us.us.us.us.us.us.1173"
                                        ##   in Loop: Header=BB8_456 Depth=2
	cmpl	$32, %eax
	jae	LBB8_459
## %bb.458:                             ##   in Loop: Header=BB8_456 Depth=2
	xorl	%edi, %edi
	jmp	LBB8_467
	.p2align	4, 0x90
LBB8_459:                               ## %vector.ph408
                                        ##   in Loop: Header=BB8_456 Depth=2
	andl	$-32, %edx
	addq	$-32, %rdx
	shrq	$5, %rdx
	movl	%eax, %esi
	andl	$-32, %esi
	addq	$-32, %rsi
	movq	%rsi, %r15
	shrq	$5, %r15
	incq	%r15
	cmpq	$224, %rsi
	jae	LBB8_461
## %bb.460:                             ##   in Loop: Header=BB8_456 Depth=2
	xorl	%ebx, %ebx
	jmp	LBB8_463
	.p2align	4, 0x90
LBB8_461:                               ## %vector.ph408.new
                                        ##   in Loop: Header=BB8_456 Depth=2
	movq	112(%rsp), %rsi                 ## 8-byte Reload
	leaq	(%rsi,%r14,4), %rdi
	leaq	1(%rdx), %r8
	andq	$-8, %r8
	negq	%r8
	xorl	%ebx, %ebx
	.p2align	4, 0x90
LBB8_462:                               ## %vector.body405
                                        ##   Parent Loop BB8_448 Depth=1
                                        ##     Parent Loop BB8_456 Depth=2
                                        ## =>    This Inner Loop Header: Depth=3
	vmovups	%ymm1, -992(%rdi,%rbx,4)
	vmovups	%ymm1, -960(%rdi,%rbx,4)
	vmovups	%ymm1, -928(%rdi,%rbx,4)
	vmovups	%ymm1, -896(%rdi,%rbx,4)
	vmovups	%ymm1, -864(%rdi,%rbx,4)
	vmovups	%ymm1, -832(%rdi,%rbx,4)
	vmovups	%ymm1, -800(%rdi,%rbx,4)
	vmovups	%ymm1, -768(%rdi,%rbx,4)
	vmovups	%ymm1, -736(%rdi,%rbx,4)
	vmovups	%ymm1, -704(%rdi,%rbx,4)
	vmovups	%ymm1, -672(%rdi,%rbx,4)
	vmovups	%ymm1, -640(%rdi,%rbx,4)
	vmovups	%ymm1, -608(%rdi,%rbx,4)
	vmovups	%ymm1, -576(%rdi,%rbx,4)
	vmovups	%ymm1, -544(%rdi,%rbx,4)
	vmovups	%ymm1, -512(%rdi,%rbx,4)
	vmovups	%ymm1, -480(%rdi,%rbx,4)
	vmovups	%ymm1, -448(%rdi,%rbx,4)
	vmovups	%ymm1, -416(%rdi,%rbx,4)
	vmovups	%ymm1, -384(%rdi,%rbx,4)
	vmovups	%ymm1, -352(%rdi,%rbx,4)
	vmovups	%ymm1, -320(%rdi,%rbx,4)
	vmovups	%ymm1, -288(%rdi,%rbx,4)
	vmovups	%ymm1, -256(%rdi,%rbx,4)
	vmovups	%ymm1, -224(%rdi,%rbx,4)
	vmovups	%ymm1, -192(%rdi,%rbx,4)
	vmovups	%ymm1, -160(%rdi,%rbx,4)
	vmovups	%ymm1, -128(%rdi,%rbx,4)
	vmovups	%ymm1, -96(%rdi,%rbx,4)
	vmovups	%ymm1, -64(%rdi,%rbx,4)
	vmovups	%ymm1, -32(%rdi,%rbx,4)
	vmovups	%ymm1, (%rdi,%rbx,4)
	addq	$256, %rbx                      ## imm = 0x100
	addq	$8, %r8
	jne	LBB8_462
LBB8_463:                               ## %middle.block403.unr-lcssa
                                        ##   in Loop: Header=BB8_456 Depth=2
	movl	%eax, %edi
	andl	$-32, %edi
	testb	$7, %r15b
	je	LBB8_466
## %bb.464:                             ## %vector.body405.epil.preheader
                                        ##   in Loop: Header=BB8_456 Depth=2
	incb	%dl
	movzbl	%dl, %edx
	andl	$7, %edx
	shlq	$7, %rdx
	addq	%r14, %rbx
	movq	32(%rsp), %rsi                  ## 8-byte Reload
	leaq	(%rsi,%rbx,4), %rbx
	xorl	%esi, %esi
	.p2align	4, 0x90
LBB8_465:                               ## %vector.body405.epil
                                        ##   Parent Loop BB8_448 Depth=1
                                        ##     Parent Loop BB8_456 Depth=2
                                        ## =>    This Inner Loop Header: Depth=3
	vmovups	%ymm1, -96(%rbx,%rsi)
	vmovups	%ymm1, -64(%rbx,%rsi)
	vmovups	%ymm1, -32(%rbx,%rsi)
	vmovups	%ymm1, (%rbx,%rsi)
	subq	$-128, %rsi
	cmpq	%rsi, %rdx
	jne	LBB8_465
LBB8_466:                               ## %middle.block403
                                        ##   in Loop: Header=BB8_456 Depth=2
	cmpq	%rax, %rdi
	movq	(%rsp), %r9                     ## 8-byte Reload
	movl	$8, %ebx
	je	LBB8_469
LBB8_467:                               ## %"for conv1_stage2.s0.n.ni.us.us.us.us.us.us.1177.preheader"
                                        ##   in Loop: Header=BB8_456 Depth=2
	subq	%rdi, %rcx
	addq	%r14, %rdi
	movq	24(%rsp), %rax                  ## 8-byte Reload
	leaq	(%rax,%rdi,4), %rax
	xorl	%edx, %edx
	.p2align	4, 0x90
LBB8_468:                               ## %"for conv1_stage2.s0.n.ni.us.us.us.us.us.us.1177"
                                        ##   Parent Loop BB8_448 Depth=1
                                        ##     Parent Loop BB8_456 Depth=2
                                        ## =>    This Inner Loop Header: Depth=3
	vmovss	%xmm0, (%rax,%rdx,4)
	incq	%rdx
	cmpq	%rdx, %rcx
	jne	LBB8_468
	jmp	LBB8_469
LBB8_471:                               ## %"end for conv1_stage2.s0.c.ci.split.us.us.split.us.us.us.us.1185"
	movl	%r10d, %r13d
	subl	%r9d, %r13d
	subl	304(%rsp), %r13d                ## 4-byte Folded Reload
	movl	%edi, %esi
	andl	$7, %esi
	andl	$-8, %edi
	movq	120(%rsp), %r8                  ## 8-byte Reload
	subl	$-128, %r8d
	imull	%edx, %r8d
	movq	%rsi, 96(%rsp)                  ## 8-byte Spill
	shlq	$5, %rsi
	movl	%r8d, %r11d
	subl	%r9d, %r11d
	movq	8(%rsp), %rax                   ## 8-byte Reload
	leal	(%r8,%rax,8), %r14d
	movl	%r10d, %ecx
	subl	%r9d, %ecx
	subl	144(%rsp), %ecx                 ## 4-byte Folded Reload
	movl	%ecx, 128(%rsp)                 ## 4-byte Spill
	xorl	%ecx, %ecx
	movl	$8, %ebx
	movq	24(%rsp), %rax                  ## 8-byte Reload
	movq	%rdi, 64(%rsp)                  ## 8-byte Spill
	movq	%rsi, 136(%rsp)                 ## 8-byte Spill
	jmp	LBB8_472
	.p2align	4, 0x90
LBB8_494:                               ## %"end for conv1_stage2.s0.n.n.rebased.loopexit.us.us.us.us.us.us.1.1"
                                        ##   in Loop: Header=BB8_472 Depth=1
	movq	80(%rsp), %rcx                  ## 8-byte Reload
	incq	%rcx
	movl	88(%rsp), %eax                  ## 4-byte Reload
	movl	48(%rsp), %r11d                 ## 4-byte Reload
	addl	%eax, %r11d
	movl	56(%rsp), %r14d                 ## 4-byte Reload
	addl	%eax, %r14d
	cmpq	$8, %rcx
	movq	24(%rsp), %rax                  ## 8-byte Reload
	movq	64(%rsp), %rdi                  ## 8-byte Reload
	movq	136(%rsp), %rsi                 ## 8-byte Reload
	je	LBB8_118
LBB8_472:                               ## %"for conv1_stage2.s0.c.ci.us.us.us.us.us.us.1.1"
                                        ## =>This Loop Header: Depth=1
                                        ##     Child Loop BB8_475 Depth 2
                                        ##     Child Loop BB8_478 Depth 2
                                        ##     Child Loop BB8_480 Depth 2
                                        ##       Child Loop BB8_486 Depth 3
                                        ##       Child Loop BB8_489 Depth 3
                                        ##       Child Loop BB8_492 Depth 3
	movslq	%r11d, %r8
	movq	%rcx, 80(%rsp)                  ## 8-byte Spill
	vbroadcastss	256(%rsp,%rcx,4), %ymm0
	cmpq	$7, 72(%rsp)                    ## 8-byte Folded Reload
	jae	LBB8_474
## %bb.473:                             ##   in Loop: Header=BB8_472 Depth=1
	xorl	%ecx, %ecx
	jmp	LBB8_476
	.p2align	4, 0x90
LBB8_474:                               ## %"for conv1_stage2.s0.n.n.us.us.us.us.us.us.1.1.preheader"
                                        ##   in Loop: Header=BB8_472 Depth=1
	leaq	(%r9,%r8), %rcx
	movq	104(%rsp), %rdx                 ## 8-byte Reload
	leaq	(%rdx,%rcx,4), %rdx
	xorl	%ecx, %ecx
	.p2align	4, 0x90
LBB8_475:                               ## %"for conv1_stage2.s0.n.n.us.us.us.us.us.us.1.1"
                                        ##   Parent Loop BB8_472 Depth=1
                                        ## =>  This Inner Loop Header: Depth=2
	vmovups	%ymm0, -224(%rdx)
	vmovups	%ymm0, -192(%rdx)
	vmovups	%ymm0, -160(%rdx)
	vmovups	%ymm0, -128(%rdx)
	vmovups	%ymm0, -96(%rdx)
	vmovups	%ymm0, -64(%rdx)
	vmovups	%ymm0, -32(%rdx)
	vmovups	%ymm0, (%rdx)
	addq	$8, %rcx
	addq	$256, %rdx                      ## imm = 0x100
	cmpq	%rcx, %rdi
	jne	LBB8_475
LBB8_476:                               ## %"for conv1_stage2.s0.n.n.rebased.us.us.us.us.us.us.1.1.preheader.unr-lcssa"
                                        ##   in Loop: Header=BB8_472 Depth=1
	movl	%r11d, 48(%rsp)                 ## 4-byte Spill
	cmpq	$0, 96(%rsp)                    ## 8-byte Folded Reload
	je	LBB8_479
## %bb.477:                             ## %"for conv1_stage2.s0.n.n.us.us.us.us.us.us.1.1.epil.preheader"
                                        ##   in Loop: Header=BB8_472 Depth=1
	leaq	(%r9,%rcx,8), %rcx
	addq	%r8, %rcx
	leaq	(%rax,%rcx,4), %rax
	xorl	%ecx, %ecx
	.p2align	4, 0x90
LBB8_478:                               ## %"for conv1_stage2.s0.n.n.us.us.us.us.us.us.1.1.epil"
                                        ##   Parent Loop BB8_472 Depth=1
                                        ## =>  This Inner Loop Header: Depth=2
	vmovups	%ymm0, (%rax,%rcx)
	addq	$32, %rcx
	cmpq	%rcx, %rsi
	jne	LBB8_478
LBB8_479:                               ## %"for conv1_stage2.s0.n.n.rebased.us.us.us.us.us.us.1.1.preheader"
                                        ##   in Loop: Header=BB8_472 Depth=1
	vbroadcastss	%xmm0, %ymm1
	movl	128(%rsp), %r11d                ## 4-byte Reload
	movl	%r13d, %r10d
	movl	%r14d, 56(%rsp)                 ## 4-byte Spill
                                        ## kill: def $r14d killed $r14d def $r14
	xorl	%r12d, %r12d
	jmp	LBB8_480
	.p2align	4, 0x90
LBB8_493:                               ## %"end for conv1_stage2.s0.n.ni.us.us.us.us.us.us.1.1"
                                        ##   in Loop: Header=BB8_480 Depth=2
	incq	%r12
	addl	$8, %r14d
	addl	$-8, %r10d
	addl	$-8, %r11d
	cmpq	40(%rsp), %r12                  ## 8-byte Folded Reload
	je	LBB8_494
LBB8_480:                               ## %"for conv1_stage2.s0.n.n.rebased.us.us.us.us.us.us.1.1"
                                        ##   Parent Loop BB8_472 Depth=1
                                        ## =>  This Loop Header: Depth=2
                                        ##       Child Loop BB8_486 Depth 3
                                        ##       Child Loop BB8_489 Depth 3
                                        ##       Child Loop BB8_492 Depth 3
	cmpl	$8, %r11d
	movl	$8, %ecx
	cmovll	%r11d, %ecx
	cmpl	$8, %r10d
	movl	$8, %edx
	cmovll	%r10d, %edx
	leal	(,%r12,8), %esi
	movl	%r13d, %eax
	subl	%esi, %eax
	cmpl	$8, %eax
	cmovgel	%ebx, %eax
	movslq	%r14d, %r14
	movq	8(%rsp), %rsi                   ## 8-byte Reload
	addl	%r12d, %esi
	leal	(%r9,%rsi,8), %esi
	cmpl	%esi, 20(%rsp)                  ## 4-byte Folded Reload
	jle	LBB8_493
## %bb.481:                             ## %"for conv1_stage2.s0.n.ni.preheader.us.us.us.us.us.us.1.1"
                                        ##   in Loop: Header=BB8_480 Depth=2
	cmpl	$32, %eax
	jae	LBB8_483
## %bb.482:                             ##   in Loop: Header=BB8_480 Depth=2
	xorl	%edi, %edi
	jmp	LBB8_491
	.p2align	4, 0x90
LBB8_483:                               ## %vector.ph428
                                        ##   in Loop: Header=BB8_480 Depth=2
	andl	$-32, %edx
	addq	$-32, %rdx
	shrq	$5, %rdx
	movl	%eax, %esi
	andl	$-32, %esi
	addq	$-32, %rsi
	movq	%rsi, %r15
	shrq	$5, %r15
	incq	%r15
	cmpq	$224, %rsi
	jae	LBB8_485
## %bb.484:                             ##   in Loop: Header=BB8_480 Depth=2
	xorl	%ebx, %ebx
	jmp	LBB8_487
	.p2align	4, 0x90
LBB8_485:                               ## %vector.ph428.new
                                        ##   in Loop: Header=BB8_480 Depth=2
	movq	112(%rsp), %rsi                 ## 8-byte Reload
	leaq	(%rsi,%r14,4), %rdi
	leaq	1(%rdx), %r8
	andq	$-8, %r8
	negq	%r8
	xorl	%ebx, %ebx
	.p2align	4, 0x90
LBB8_486:                               ## %vector.body425
                                        ##   Parent Loop BB8_472 Depth=1
                                        ##     Parent Loop BB8_480 Depth=2
                                        ## =>    This Inner Loop Header: Depth=3
	vmovups	%ymm1, -992(%rdi,%rbx,4)
	vmovups	%ymm1, -960(%rdi,%rbx,4)
	vmovups	%ymm1, -928(%rdi,%rbx,4)
	vmovups	%ymm1, -896(%rdi,%rbx,4)
	vmovups	%ymm1, -864(%rdi,%rbx,4)
	vmovups	%ymm1, -832(%rdi,%rbx,4)
	vmovups	%ymm1, -800(%rdi,%rbx,4)
	vmovups	%ymm1, -768(%rdi,%rbx,4)
	vmovups	%ymm1, -736(%rdi,%rbx,4)
	vmovups	%ymm1, -704(%rdi,%rbx,4)
	vmovups	%ymm1, -672(%rdi,%rbx,4)
	vmovups	%ymm1, -640(%rdi,%rbx,4)
	vmovups	%ymm1, -608(%rdi,%rbx,4)
	vmovups	%ymm1, -576(%rdi,%rbx,4)
	vmovups	%ymm1, -544(%rdi,%rbx,4)
	vmovups	%ymm1, -512(%rdi,%rbx,4)
	vmovups	%ymm1, -480(%rdi,%rbx,4)
	vmovups	%ymm1, -448(%rdi,%rbx,4)
	vmovups	%ymm1, -416(%rdi,%rbx,4)
	vmovups	%ymm1, -384(%rdi,%rbx,4)
	vmovups	%ymm1, -352(%rdi,%rbx,4)
	vmovups	%ymm1, -320(%rdi,%rbx,4)
	vmovups	%ymm1, -288(%rdi,%rbx,4)
	vmovups	%ymm1, -256(%rdi,%rbx,4)
	vmovups	%ymm1, -224(%rdi,%rbx,4)
	vmovups	%ymm1, -192(%rdi,%rbx,4)
	vmovups	%ymm1, -160(%rdi,%rbx,4)
	vmovups	%ymm1, -128(%rdi,%rbx,4)
	vmovups	%ymm1, -96(%rdi,%rbx,4)
	vmovups	%ymm1, -64(%rdi,%rbx,4)
	vmovups	%ymm1, -32(%rdi,%rbx,4)
	vmovups	%ymm1, (%rdi,%rbx,4)
	addq	$256, %rbx                      ## imm = 0x100
	addq	$8, %r8
	jne	LBB8_486
LBB8_487:                               ## %middle.block423.unr-lcssa
                                        ##   in Loop: Header=BB8_480 Depth=2
	movl	%eax, %edi
	andl	$-32, %edi
	testb	$7, %r15b
	je	LBB8_490
## %bb.488:                             ## %vector.body425.epil.preheader
                                        ##   in Loop: Header=BB8_480 Depth=2
	incb	%dl
	movzbl	%dl, %edx
	andl	$7, %edx
	shlq	$7, %rdx
	addq	%r14, %rbx
	movq	32(%rsp), %rsi                  ## 8-byte Reload
	leaq	(%rsi,%rbx,4), %rbx
	xorl	%esi, %esi
	.p2align	4, 0x90
LBB8_489:                               ## %vector.body425.epil
                                        ##   Parent Loop BB8_472 Depth=1
                                        ##     Parent Loop BB8_480 Depth=2
                                        ## =>    This Inner Loop Header: Depth=3
	vmovups	%ymm1, -96(%rbx,%rsi)
	vmovups	%ymm1, -64(%rbx,%rsi)
	vmovups	%ymm1, -32(%rbx,%rsi)
	vmovups	%ymm1, (%rbx,%rsi)
	subq	$-128, %rsi
	cmpq	%rsi, %rdx
	jne	LBB8_489
LBB8_490:                               ## %middle.block423
                                        ##   in Loop: Header=BB8_480 Depth=2
	cmpq	%rax, %rdi
	movq	(%rsp), %r9                     ## 8-byte Reload
	movl	$8, %ebx
	je	LBB8_493
LBB8_491:                               ## %"for conv1_stage2.s0.n.ni.us.us.us.us.us.us.1.1.preheader"
                                        ##   in Loop: Header=BB8_480 Depth=2
	subq	%rdi, %rcx
	addq	%r14, %rdi
	movq	24(%rsp), %rax                  ## 8-byte Reload
	leaq	(%rax,%rdi,4), %rax
	xorl	%edx, %edx
	.p2align	4, 0x90
LBB8_492:                               ## %"for conv1_stage2.s0.n.ni.us.us.us.us.us.us.1.1"
                                        ##   Parent Loop BB8_472 Depth=1
                                        ##     Parent Loop BB8_480 Depth=2
                                        ## =>    This Inner Loop Header: Depth=3
	vmovss	%xmm0, (%rax,%rdx,4)
	incq	%rdx
	cmpq	%rdx, %rcx
	jne	LBB8_492
	jmp	LBB8_493
LBB8_173:                               ## %"end for conv1_stage1.s1.w.rebased10.loopexit.us.6.new"
	andl	$-4, %ebx
	movq	%r11, %rcx
	shlq	$5, %rcx
	leaq	(%rcx,%r14), %rdx
	addq	$28, %rdx
	movl	$96, %esi
	xorl	%ecx, %ecx
	.p2align	4, 0x90
LBB8_174:                               ## %"for conv1_stage1.s1.w.rebased9.us.7"
                                        ## =>This Inner Loop Header: Depth=1
	vbroadcastss	-96(%rdx,%rsi), %ymm1
	vfmadd213ps	64(%rsp,%rsi), %ymm0, %ymm1 ## ymm1 = (ymm0 * ymm1) + mem
	vmovaps	%ymm1, 64(%rsp,%rsi)
	vbroadcastss	-64(%rdx,%rsi), %ymm1
	vfmadd213ps	96(%rsp,%rsi), %ymm0, %ymm1 ## ymm1 = (ymm0 * ymm1) + mem
	vmovaps	%ymm1, 96(%rsp,%rsi)
	vbroadcastss	-32(%rdx,%rsi), %ymm1
	vfmadd213ps	128(%rsp,%rsi), %ymm0, %ymm1 ## ymm1 = (ymm0 * ymm1) + mem
	vmovaps	%ymm1, 128(%rsp,%rsi)
	vbroadcastss	(%rdx,%rsi), %ymm1
	vfmadd213ps	160(%rsp,%rsi), %ymm0, %ymm1 ## ymm1 = (ymm0 * ymm1) + mem
	vmovaps	%ymm1, 160(%rsp,%rsi)
	addq	$4, %rcx
	subq	$-128, %rsi
	cmpq	%rcx, %rbx
	jne	LBB8_174
LBB8_101:                               ## %"consume conv1_stage112.loopexit.unr-lcssa"
	testq	%rax, %rax
	je	LBB8_104
## %bb.102:                             ## %"for conv1_stage1.s1.w.rebased9.us.7.epil.preheader"
	addq	%rcx, %r11
	shlq	$5, %r11
	leaq	(%r11,%r14), %rdx
	addq	$28, %rdx
	shlq	$5, %rcx
	addq	%rsp, %rcx
	addq	$160, %rcx
	shlq	$5, %rax
	xorl	%esi, %esi
	.p2align	4, 0x90
LBB8_103:                               ## %"for conv1_stage1.s1.w.rebased9.us.7.epil"
                                        ## =>This Inner Loop Header: Depth=1
	vbroadcastss	(%rdx,%rsi), %ymm1
	vfmadd213ps	(%rcx,%rsi), %ymm0, %ymm1 ## ymm1 = (ymm0 * ymm1) + mem
	vmovaps	%ymm1, (%rcx,%rsi)
	addq	$32, %rsi
	cmpq	%rsi, %rax
	jne	LBB8_103
LBB8_104:                               ## %"consume conv1_stage112"
	movq	120(%rsp), %rax                 ## 8-byte Reload
                                        ## kill: def $eax killed $eax killed $rax def $rax
	sarl	%eax
	andl	$-2, %eax
	movq	%rax, 336(%rsp)                 ## 8-byte Spill
	subl	%eax, %r8d
	testl	%r8d, %r8d
	movl	$1, %eax
	cmovlel	%r8d, %eax
	testl	%eax, %eax
	movq	(%rsp), %rsi                    ## 8-byte Reload
	movl	88(%rsp), %edx                  ## 4-byte Reload
	movl	20(%rsp), %ebx                  ## 4-byte Reload
	movq	32(%rsp), %r12                  ## 8-byte Reload
	js	LBB8_118
## %bb.105:                             ## %"consume conv1_stage112"
	testl	%r12d, %r12d
	jle	LBB8_118
## %bb.106:                             ## %"for conv1_stage2.s0.w.wi.wi13.us.preheader"
	movl	%edx, %r9d
	shll	$5, %r9d
	addl	$7, %r12d
	sarl	$3, %r12d
	movl	%eax, %eax
	movq	%rax, 328(%rsp)                 ## 8-byte Spill
	movl	%ebx, %r14d
	subl	%esi, %r14d
	leaq	992(%r10), %rax
	movq	%rax, 112(%rsp)                 ## 8-byte Spill
	movq	120(%rsp), %r15                 ## 8-byte Reload
	shll	$5, %r15d
	andl	$-128, %r15d
	addl	56(%rsp), %r15d                 ## 4-byte Folded Reload
	movl	144(%rsp), %eax                 ## 4-byte Reload
	shll	$5, %eax
	subl	%eax, %r15d
	addl	$32, %r15d
	imull	%edx, %r15d
	movq	%r15, 120(%rsp)                 ## 8-byte Spill
	movl	%edx, %eax
	shll	$6, %eax
	movl	%eax, 320(%rsp)                 ## 4-byte Spill
	leaq	96(%r10), %rax
	movq	%rax, 32(%rsp)                  ## 8-byte Spill
	xorl	%ecx, %ecx
	movl	%r9d, 304(%rsp)                 ## 4-byte Spill
	movl	%r14d, 40(%rsp)                 ## 4-byte Spill
	jmp	LBB8_107
	.p2align	4, 0x90
LBB8_117:                               ## %"end for conv1_stage2.s0.w.wi.wii17.us"
                                        ##   in Loop: Header=BB8_107 Depth=1
	movq	312(%rsp), %rax                 ## 8-byte Reload
	leaq	1(%rax), %r8
	movq	120(%rsp), %rcx                 ## 8-byte Reload
	addl	320(%rsp), %ecx                 ## 4-byte Folded Reload
	movq	%rcx, 120(%rsp)                 ## 8-byte Spill
	cmpq	328(%rsp), %rax                 ## 8-byte Folded Reload
	movq	%r8, %rcx
	je	LBB8_118
LBB8_107:                               ## %"for conv1_stage2.s0.w.wi.wi13.us"
                                        ## =>This Loop Header: Depth=1
                                        ##     Child Loop BB8_109 Depth 2
                                        ##       Child Loop BB8_110 Depth 3
                                        ##         Child Loop BB8_111 Depth 4
                                        ##           Child Loop BB8_125 Depth 5
                                        ##           Child Loop BB8_128 Depth 5
                                        ##           Child Loop BB8_113 Depth 5
	movq	336(%rsp), %rax                 ## 8-byte Reload
	movq	%rcx, 312(%rsp)                 ## 8-byte Spill
	addl	%eax, %ecx
	addl	%ecx, %ecx
	movl	344(%rsp), %eax                 ## 4-byte Reload
	subl	%ecx, %eax
	testl	%eax, %eax
	jle	LBB8_117
## %bb.108:                             ## %"for conv1_stage2.s0.w.wi.wii16.us.us.preheader"
                                        ##   in Loop: Header=BB8_107 Depth=1
	xorl	%ecx, %ecx
	cmpl	$1, %eax
	sete	%cl
	movl	$2, %eax
	subq	%rcx, %rax
	movq	%rax, 136(%rsp)                 ## 8-byte Spill
	movq	312(%rsp), %rax                 ## 8-byte Reload
	addl	%eax, %eax
	movq	%rax, 128(%rsp)                 ## 8-byte Spill
	movq	120(%rsp), %rax                 ## 8-byte Reload
	movl	%eax, %r11d
	xorl	%edi, %edi
	jmp	LBB8_109
	.p2align	4, 0x90
LBB8_116:                               ## %"end for conv1_stage2.s0.c.ci21.split.us.us.us"
                                        ##   in Loop: Header=BB8_109 Depth=2
	movq	96(%rsp), %rdi                  ## 8-byte Reload
	incq	%rdi
	movl	304(%rsp), %r9d                 ## 4-byte Reload
	movl	80(%rsp), %r11d                 ## 4-byte Reload
	addl	%r9d, %r11d
	cmpq	136(%rsp), %rdi                 ## 8-byte Folded Reload
	je	LBB8_117
LBB8_109:                               ## %"for conv1_stage2.s0.w.wi.wii16.us.us"
                                        ##   Parent Loop BB8_107 Depth=1
                                        ## =>  This Loop Header: Depth=2
                                        ##       Child Loop BB8_110 Depth 3
                                        ##         Child Loop BB8_111 Depth 4
                                        ##           Child Loop BB8_125 Depth 5
                                        ##           Child Loop BB8_128 Depth 5
                                        ##           Child Loop BB8_113 Depth 5
	movq	128(%rsp), %rax                 ## 8-byte Reload
	movq	%rdi, 96(%rsp)                  ## 8-byte Spill
	addl	%edi, %eax
	leal	(,%rax,8), %edi
	movq	%rdi, 104(%rsp)                 ## 8-byte Spill
	addl	152(%rsp), %eax                 ## 4-byte Folded Reload
	subl	144(%rsp), %eax                 ## 4-byte Folded Reload
	incl	%eax
	imull	%r9d, %eax
	subl	%esi, %eax
	movq	%rax, 64(%rsp)                  ## 8-byte Spill
	movl	%r11d, 80(%rsp)                 ## 4-byte Spill
	xorl	%edi, %edi
	jmp	LBB8_110
	.p2align	4, 0x90
LBB8_115:                               ## %"end for conv1_stage2.s0.n.n24.loopexit.us.us.us"
                                        ##   in Loop: Header=BB8_110 Depth=3
	movq	72(%rsp), %rdi                  ## 8-byte Reload
	incq	%rdi
	movl	88(%rsp), %edx                  ## 4-byte Reload
	movl	48(%rsp), %r11d                 ## 4-byte Reload
	addl	%edx, %r11d
	cmpq	$8, %rdi
	je	LBB8_116
LBB8_110:                               ## %"for conv1_stage2.s0.c.ci20.us.us.us"
                                        ##   Parent Loop BB8_107 Depth=1
                                        ##     Parent Loop BB8_109 Depth=2
                                        ## =>    This Loop Header: Depth=3
                                        ##         Child Loop BB8_111 Depth 4
                                        ##           Child Loop BB8_125 Depth 5
                                        ##           Child Loop BB8_128 Depth 5
                                        ##           Child Loop BB8_113 Depth 5
	movq	56(%rsp), %rax                  ## 8-byte Reload
	addl	%edi, %eax
	imull	%edx, %eax
	addl	64(%rsp), %eax                  ## 4-byte Folded Reload
	movq	104(%rsp), %rcx                 ## 8-byte Reload
	movq	%rdi, 72(%rsp)                  ## 8-byte Spill
	leal	(%rcx,%rdi), %r9d
	cltq
	movq	%rax, 8(%rsp)                   ## 8-byte Spill
	movl	%r14d, %edi
	movl	%r11d, 48(%rsp)                 ## 4-byte Spill
	xorl	%r15d, %r15d
	jmp	LBB8_111
	.p2align	4, 0x90
LBB8_112:                               ## %then_bb28.us.us.us
                                        ##   in Loop: Header=BB8_111 Depth=4
	vbroadcastss	160(%rsp,%r9,4), %ymm0
	cltq
	addq	8(%rsp), %rax                   ## 8-byte Folded Reload
	vmovups	%ymm0, (%r10,%rax,4)
LBB8_114:                               ## %after_bb27.us.us.us
                                        ##   in Loop: Header=BB8_111 Depth=4
	incq	%r15
	addl	$8, %r11d
	addl	$-8, %edi
	cmpq	%r12, %r15
	je	LBB8_115
LBB8_111:                               ## %"for conv1_stage2.s0.n.n23.us.us.us"
                                        ##   Parent Loop BB8_107 Depth=1
                                        ##     Parent Loop BB8_109 Depth=2
                                        ##       Parent Loop BB8_110 Depth=3
                                        ## =>      This Loop Header: Depth=4
                                        ##           Child Loop BB8_125 Depth 5
                                        ##           Child Loop BB8_128 Depth 5
                                        ##           Child Loop BB8_113 Depth 5
	cmpl	$8, %edi
	movl	$8, %r13d
	cmovll	%edi, %r13d
	leal	(,%r15,8), %eax
	movl	%r14d, %r8d
	subl	%eax, %r8d
	cmpl	$8, %r8d
	movl	$8, %eax
	cmovgel	%eax, %r8d
	leal	(%rsi,%r15,8), %eax
	leal	(%rsi,%r15,8), %ecx
	addl	$8, %ecx
	cmpl	%ebx, %ecx
	jle	LBB8_112
## %bb.119:                             ## %next_bb29.us.us.us
                                        ##   in Loop: Header=BB8_111 Depth=4
	cmpl	%eax, %ebx
	jle	LBB8_114
## %bb.120:                             ## %"for conv1_stage2.s0.n.ni30.preheader.us.us.us"
                                        ##   in Loop: Header=BB8_111 Depth=4
	movslq	%r11d, %rdx
	vmovss	160(%rsp,%r9,4), %xmm0          ## xmm0 = mem[0],zero,zero,zero
	cmpl	$32, %r8d
	jae	LBB8_122
## %bb.121:                             ##   in Loop: Header=BB8_111 Depth=4
	xorl	%eax, %eax
	leaq	(%r10,%rdx,4), %rcx
	jmp	LBB8_113
	.p2align	4, 0x90
LBB8_122:                               ## %vector.ph
                                        ##   in Loop: Header=BB8_111 Depth=4
	movq	%r12, %r14
	movl	%r13d, %r12d
	andl	$-32, %r12d
	addq	$-32, %r12
	shrq	$5, %r12
	movl	%r8d, %eax
	andl	$-32, %eax
	addq	$-32, %rax
	movq	%rax, %rbx
	shrq	$5, %rbx
	incq	%rbx
	vbroadcastss	%xmm0, %ymm1
	cmpq	$224, %rax
	jae	LBB8_124
## %bb.123:                             ##   in Loop: Header=BB8_111 Depth=4
	xorl	%ecx, %ecx
	jmp	LBB8_126
LBB8_124:                               ## %vector.ph.new
                                        ##   in Loop: Header=BB8_111 Depth=4
	movq	112(%rsp), %rax                 ## 8-byte Reload
	leaq	(%rax,%rdx,4), %rax
	leaq	1(%r12), %r10
	andq	$-8, %r10
	negq	%r10
	xorl	%ecx, %ecx
	.p2align	4, 0x90
LBB8_125:                               ## %vector.body
                                        ##   Parent Loop BB8_107 Depth=1
                                        ##     Parent Loop BB8_109 Depth=2
                                        ##       Parent Loop BB8_110 Depth=3
                                        ##         Parent Loop BB8_111 Depth=4
                                        ## =>        This Inner Loop Header: Depth=5
	vmovups	%ymm1, -992(%rax,%rcx,4)
	vmovups	%ymm1, -960(%rax,%rcx,4)
	vmovups	%ymm1, -928(%rax,%rcx,4)
	vmovups	%ymm1, -896(%rax,%rcx,4)
	vmovups	%ymm1, -864(%rax,%rcx,4)
	vmovups	%ymm1, -832(%rax,%rcx,4)
	vmovups	%ymm1, -800(%rax,%rcx,4)
	vmovups	%ymm1, -768(%rax,%rcx,4)
	vmovups	%ymm1, -736(%rax,%rcx,4)
	vmovups	%ymm1, -704(%rax,%rcx,4)
	vmovups	%ymm1, -672(%rax,%rcx,4)
	vmovups	%ymm1, -640(%rax,%rcx,4)
	vmovups	%ymm1, -608(%rax,%rcx,4)
	vmovups	%ymm1, -576(%rax,%rcx,4)
	vmovups	%ymm1, -544(%rax,%rcx,4)
	vmovups	%ymm1, -512(%rax,%rcx,4)
	vmovups	%ymm1, -480(%rax,%rcx,4)
	vmovups	%ymm1, -448(%rax,%rcx,4)
	vmovups	%ymm1, -416(%rax,%rcx,4)
	vmovups	%ymm1, -384(%rax,%rcx,4)
	vmovups	%ymm1, -352(%rax,%rcx,4)
	vmovups	%ymm1, -320(%rax,%rcx,4)
	vmovups	%ymm1, -288(%rax,%rcx,4)
	vmovups	%ymm1, -256(%rax,%rcx,4)
	vmovups	%ymm1, -224(%rax,%rcx,4)
	vmovups	%ymm1, -192(%rax,%rcx,4)
	vmovups	%ymm1, -160(%rax,%rcx,4)
	vmovups	%ymm1, -128(%rax,%rcx,4)
	vmovups	%ymm1, -96(%rax,%rcx,4)
	vmovups	%ymm1, -64(%rax,%rcx,4)
	vmovups	%ymm1, -32(%rax,%rcx,4)
	vmovups	%ymm1, (%rax,%rcx,4)
	addq	$256, %rcx                      ## imm = 0x100
	addq	$8, %r10
	jne	LBB8_125
LBB8_126:                               ## %middle.block.unr-lcssa
                                        ##   in Loop: Header=BB8_111 Depth=4
	movl	%r8d, %eax
	andl	$-32, %eax
	testb	$7, %bl
	je	LBB8_129
## %bb.127:                             ## %vector.body.epil.preheader
                                        ##   in Loop: Header=BB8_111 Depth=4
	incb	%r12b
	movzbl	%r12b, %ebx
	andl	$7, %ebx
	shlq	$7, %rbx
	addq	%rdx, %rcx
	movq	32(%rsp), %rsi                  ## 8-byte Reload
	leaq	(%rsi,%rcx,4), %rcx
	xorl	%esi, %esi
	.p2align	4, 0x90
LBB8_128:                               ## %vector.body.epil
                                        ##   Parent Loop BB8_107 Depth=1
                                        ##     Parent Loop BB8_109 Depth=2
                                        ##       Parent Loop BB8_110 Depth=3
                                        ##         Parent Loop BB8_111 Depth=4
                                        ## =>        This Inner Loop Header: Depth=5
	vmovups	%ymm1, -96(%rcx,%rsi)
	vmovups	%ymm1, -64(%rcx,%rsi)
	vmovups	%ymm1, -32(%rcx,%rsi)
	vmovups	%ymm1, (%rcx,%rsi)
	subq	$-128, %rsi
	cmpq	%rsi, %rbx
	jne	LBB8_128
LBB8_129:                               ## %middle.block
                                        ##   in Loop: Header=BB8_111 Depth=4
	cmpq	%r8, %rax
	movq	(%rsp), %rsi                    ## 8-byte Reload
	movq	24(%rsp), %r10                  ## 8-byte Reload
	movl	20(%rsp), %ebx                  ## 4-byte Reload
	movq	%r14, %r12
	movl	40(%rsp), %r14d                 ## 4-byte Reload
	je	LBB8_114
## %bb.130:                             ## %"for conv1_stage2.s0.n.ni30.us.us.us.preheader"
                                        ##   in Loop: Header=BB8_111 Depth=4
	leaq	(%r10,%rdx,4), %rcx
	.p2align	4, 0x90
LBB8_113:                               ## %"for conv1_stage2.s0.n.ni30.us.us.us"
                                        ##   Parent Loop BB8_107 Depth=1
                                        ##     Parent Loop BB8_109 Depth=2
                                        ##       Parent Loop BB8_110 Depth=3
                                        ##         Parent Loop BB8_111 Depth=4
                                        ## =>        This Inner Loop Header: Depth=5
	vmovss	%xmm0, (%rcx,%rax,4)
	incq	%rax
	cmpq	%rax, %r13
	jne	LBB8_113
	jmp	LBB8_114
LBB8_75:                                ## %"consume conv1_stage1.split78"
	cmpl	$8, %edx
	movl	88(%rsp), %eax                  ## 4-byte Reload
	jl	LBB8_118
## %bb.76:                              ## %"for conv1_stage2.s0.w.wi.wi.us86.preheader"
	movl	$1, %ecx
	subl	144(%rsp), %ecx                 ## 4-byte Folded Reload
	movl	%ecx, 104(%rsp)                 ## 4-byte Spill
	movl	%eax, %ecx
	shll	$5, %ecx
	movl	%ecx, 20(%rsp)                  ## 4-byte Spill
	movl	8(%rsp), %r14d                  ## 4-byte Reload
	movq	56(%rsp), %rcx                  ## 8-byte Reload
	movl	%ecx, %edx
	imull	%eax, %edx
	movq	%rdx, 8(%rsp)                   ## 8-byte Spill
	movl	%ecx, %edx
	orl	$1, %edx
	imull	%eax, %edx
	movq	%rdx, 40(%rsp)                  ## 8-byte Spill
	movl	%ecx, %edx
	orl	$2, %edx
	imull	%eax, %edx
	movq	%rdx, 32(%rsp)                  ## 8-byte Spill
	movl	%ecx, %edx
	orl	$3, %edx
	imull	%eax, %edx
	movq	%rdx, 112(%rsp)                 ## 8-byte Spill
	movl	%ecx, %edx
	orl	$4, %edx
	imull	%eax, %edx
	movq	%rdx, 48(%rsp)                  ## 8-byte Spill
	movl	%ecx, %edx
	orl	$5, %edx
	imull	%eax, %edx
	movq	%rdx, 72(%rsp)                  ## 8-byte Spill
	movl	%ecx, %edx
	orl	$6, %edx
	imull	%eax, %edx
	movq	%rdx, 64(%rsp)                  ## 8-byte Spill
	orl	$7, %ecx
	imull	%eax, %ecx
	movq	%rcx, 56(%rsp)                  ## 8-byte Spill
	movl	%r14d, %r8d
	andl	$7, %r8d
	movl	%r14d, %edi
	andl	$-8, %edi
	movq	%r8, 88(%rsp)                   ## 8-byte Spill
	shlq	$5, %r8
	leaq	-1(%r14), %r11
	leaq	224(%r10), %r12
	xorl	%r13d, %r13d
	xorl	%r15d, %r15d
	jmp	LBB8_77
LBB8_322:                               ## %"end for conv1_stage2.s0.n.n.loopexit.us72.us.us.7.1"
                                        ##   in Loop: Header=BB8_77 Depth=1
	movl	$2, %r15d
	testb	$1, %r13b
	movb	$1, %r13b
	movq	(%rsp), %r9                     ## 8-byte Reload
	jne	LBB8_118
LBB8_77:                                ## %"for conv1_stage2.s0.w.wi.wi.us86"
                                        ## =>This Loop Header: Depth=1
                                        ##     Child Loop BB8_80 Depth 2
                                        ##     Child Loop BB8_83 Depth 2
                                        ##     Child Loop BB8_220 Depth 2
                                        ##     Child Loop BB8_223 Depth 2
                                        ##     Child Loop BB8_227 Depth 2
                                        ##     Child Loop BB8_230 Depth 2
                                        ##     Child Loop BB8_234 Depth 2
                                        ##     Child Loop BB8_237 Depth 2
                                        ##     Child Loop BB8_241 Depth 2
                                        ##     Child Loop BB8_244 Depth 2
                                        ##     Child Loop BB8_248 Depth 2
                                        ##     Child Loop BB8_251 Depth 2
                                        ##     Child Loop BB8_255 Depth 2
                                        ##     Child Loop BB8_258 Depth 2
                                        ##     Child Loop BB8_262 Depth 2
                                        ##     Child Loop BB8_265 Depth 2
                                        ##     Child Loop BB8_269 Depth 2
                                        ##     Child Loop BB8_272 Depth 2
                                        ##     Child Loop BB8_276 Depth 2
                                        ##     Child Loop BB8_279 Depth 2
                                        ##     Child Loop BB8_283 Depth 2
                                        ##     Child Loop BB8_286 Depth 2
                                        ##     Child Loop BB8_290 Depth 2
                                        ##     Child Loop BB8_293 Depth 2
                                        ##     Child Loop BB8_297 Depth 2
                                        ##     Child Loop BB8_300 Depth 2
                                        ##     Child Loop BB8_304 Depth 2
                                        ##     Child Loop BB8_307 Depth 2
                                        ##     Child Loop BB8_311 Depth 2
                                        ##     Child Loop BB8_314 Depth 2
                                        ##     Child Loop BB8_318 Depth 2
                                        ##     Child Loop BB8_321 Depth 2
	movl	152(%rsp), %eax                 ## 4-byte Reload
	movl	%eax, %r10d
	orl	%r15d, %r10d
	addl	104(%rsp), %r10d                ## 4-byte Folded Reload
	shll	$3, %r15d
	movl	20(%rsp), %eax                  ## 4-byte Reload
	movl	%eax, %esi
	imull	%r10d, %esi
	subl	%r9d, %esi
	movq	8(%rsp), %rax                   ## 8-byte Reload
	addl	%esi, %eax
	vbroadcastss	160(%rsp,%r15,4), %ymm0
	cltq
	cmpq	$7, %r11
	jae	LBB8_79
## %bb.78:                              ##   in Loop: Header=BB8_77 Depth=1
	xorl	%ecx, %ecx
	jmp	LBB8_81
LBB8_79:                                ## %"for conv1_stage2.s0.n.n.us65.us.us.preheader"
                                        ##   in Loop: Header=BB8_77 Depth=1
	movq	(%rsp), %rcx                    ## 8-byte Reload
	addq	%rax, %rcx
	leaq	(%r12,%rcx,4), %rdx
	xorl	%ecx, %ecx
LBB8_80:                                ## %"for conv1_stage2.s0.n.n.us65.us.us"
                                        ##   Parent Loop BB8_77 Depth=1
                                        ## =>  This Inner Loop Header: Depth=2
	vmovups	%ymm0, -224(%rdx)
	vmovups	%ymm0, -192(%rdx)
	vmovups	%ymm0, -160(%rdx)
	vmovups	%ymm0, -128(%rdx)
	vmovups	%ymm0, -96(%rdx)
	vmovups	%ymm0, -64(%rdx)
	vmovups	%ymm0, -32(%rdx)
	vmovups	%ymm0, (%rdx)
	addq	$8, %rcx
	addq	$256, %rdx                      ## imm = 0x100
	cmpq	%rcx, %rdi
	jne	LBB8_80
LBB8_81:                                ## %"end for conv1_stage2.s0.n.n.loopexit.us72.us.us.unr-lcssa"
                                        ##   in Loop: Header=BB8_77 Depth=1
	cmpq	$0, 88(%rsp)                    ## 8-byte Folded Reload
	je	LBB8_84
## %bb.82:                              ## %"for conv1_stage2.s0.n.n.us65.us.us.epil.preheader"
                                        ##   in Loop: Header=BB8_77 Depth=1
	movq	(%rsp), %rdx                    ## 8-byte Reload
	leaq	(%rdx,%rcx,8), %rcx
	addq	%rax, %rcx
	movq	24(%rsp), %rax                  ## 8-byte Reload
	leaq	(%rax,%rcx,4), %rax
	xorl	%ecx, %ecx
LBB8_83:                                ## %"for conv1_stage2.s0.n.n.us65.us.us.epil"
                                        ##   Parent Loop BB8_77 Depth=1
                                        ## =>  This Inner Loop Header: Depth=2
	vmovups	%ymm0, (%rax,%rcx)
	addq	$32, %rcx
	cmpq	%rcx, %r8
	jne	LBB8_83
LBB8_84:                                ## %"end for conv1_stage2.s0.n.n.loopexit.us72.us.us"
                                        ##   in Loop: Header=BB8_77 Depth=1
	movq	40(%rsp), %rax                  ## 8-byte Reload
	addl	%esi, %eax
	leaq	(,%r15,4), %rdx
	movq	%rdx, %rcx
	orq	$4, %rcx
	vbroadcastss	160(%rsp,%rcx), %ymm0
	cltq
	cmpq	$7, %r11
	jae	LBB8_219
## %bb.85:                              ##   in Loop: Header=BB8_77 Depth=1
	xorl	%ecx, %ecx
	jmp	LBB8_221
LBB8_219:                               ## %"for conv1_stage2.s0.n.n.us65.us.us.1.preheader"
                                        ##   in Loop: Header=BB8_77 Depth=1
	movq	(%rsp), %rcx                    ## 8-byte Reload
	addq	%rax, %rcx
	leaq	(%r12,%rcx,4), %rbx
	xorl	%ecx, %ecx
LBB8_220:                               ## %"for conv1_stage2.s0.n.n.us65.us.us.1"
                                        ##   Parent Loop BB8_77 Depth=1
                                        ## =>  This Inner Loop Header: Depth=2
	vmovups	%ymm0, -224(%rbx)
	vmovups	%ymm0, -192(%rbx)
	vmovups	%ymm0, -160(%rbx)
	vmovups	%ymm0, -128(%rbx)
	vmovups	%ymm0, -96(%rbx)
	vmovups	%ymm0, -64(%rbx)
	vmovups	%ymm0, -32(%rbx)
	vmovups	%ymm0, (%rbx)
	addq	$8, %rcx
	addq	$256, %rbx                      ## imm = 0x100
	cmpq	%rcx, %rdi
	jne	LBB8_220
LBB8_221:                               ## %"end for conv1_stage2.s0.n.n.loopexit.us72.us.us.1.unr-lcssa"
                                        ##   in Loop: Header=BB8_77 Depth=1
	testb	$7, %r14b
	je	LBB8_224
## %bb.222:                             ## %"for conv1_stage2.s0.n.n.us65.us.us.1.epil.preheader"
                                        ##   in Loop: Header=BB8_77 Depth=1
	movq	(%rsp), %rbx                    ## 8-byte Reload
	leaq	(%rbx,%rcx,8), %rcx
	addq	%rax, %rcx
	movq	24(%rsp), %rax                  ## 8-byte Reload
	leaq	(%rax,%rcx,4), %rax
	xorl	%ecx, %ecx
LBB8_223:                               ## %"for conv1_stage2.s0.n.n.us65.us.us.1.epil"
                                        ##   Parent Loop BB8_77 Depth=1
                                        ## =>  This Inner Loop Header: Depth=2
	vmovups	%ymm0, (%rax,%rcx)
	addq	$32, %rcx
	cmpq	%rcx, %r8
	jne	LBB8_223
LBB8_224:                               ## %"end for conv1_stage2.s0.n.n.loopexit.us72.us.us.1"
                                        ##   in Loop: Header=BB8_77 Depth=1
	movq	32(%rsp), %rax                  ## 8-byte Reload
	addl	%esi, %eax
	movq	%rdx, %rcx
	orq	$8, %rcx
	vbroadcastss	160(%rsp,%rcx), %ymm0
	cltq
	cmpq	$7, %r11
	jae	LBB8_226
## %bb.225:                             ##   in Loop: Header=BB8_77 Depth=1
	xorl	%ecx, %ecx
	jmp	LBB8_228
LBB8_226:                               ## %"for conv1_stage2.s0.n.n.us65.us.us.2.preheader"
                                        ##   in Loop: Header=BB8_77 Depth=1
	movq	(%rsp), %rcx                    ## 8-byte Reload
	addq	%rax, %rcx
	leaq	(%r12,%rcx,4), %rbx
	xorl	%ecx, %ecx
LBB8_227:                               ## %"for conv1_stage2.s0.n.n.us65.us.us.2"
                                        ##   Parent Loop BB8_77 Depth=1
                                        ## =>  This Inner Loop Header: Depth=2
	vmovups	%ymm0, -224(%rbx)
	vmovups	%ymm0, -192(%rbx)
	vmovups	%ymm0, -160(%rbx)
	vmovups	%ymm0, -128(%rbx)
	vmovups	%ymm0, -96(%rbx)
	vmovups	%ymm0, -64(%rbx)
	vmovups	%ymm0, -32(%rbx)
	vmovups	%ymm0, (%rbx)
	addq	$8, %rcx
	addq	$256, %rbx                      ## imm = 0x100
	cmpq	%rcx, %rdi
	jne	LBB8_227
LBB8_228:                               ## %"end for conv1_stage2.s0.n.n.loopexit.us72.us.us.2.unr-lcssa"
                                        ##   in Loop: Header=BB8_77 Depth=1
	testb	$7, %r14b
	je	LBB8_231
## %bb.229:                             ## %"for conv1_stage2.s0.n.n.us65.us.us.2.epil.preheader"
                                        ##   in Loop: Header=BB8_77 Depth=1
	movq	(%rsp), %rbx                    ## 8-byte Reload
	leaq	(%rbx,%rcx,8), %rcx
	addq	%rax, %rcx
	movq	24(%rsp), %rax                  ## 8-byte Reload
	leaq	(%rax,%rcx,4), %rax
	xorl	%ecx, %ecx
LBB8_230:                               ## %"for conv1_stage2.s0.n.n.us65.us.us.2.epil"
                                        ##   Parent Loop BB8_77 Depth=1
                                        ## =>  This Inner Loop Header: Depth=2
	vmovups	%ymm0, (%rax,%rcx)
	addq	$32, %rcx
	cmpq	%rcx, %r8
	jne	LBB8_230
LBB8_231:                               ## %"end for conv1_stage2.s0.n.n.loopexit.us72.us.us.2"
                                        ##   in Loop: Header=BB8_77 Depth=1
	movq	112(%rsp), %rax                 ## 8-byte Reload
	addl	%esi, %eax
	movq	%rdx, %rcx
	orq	$12, %rcx
	vbroadcastss	160(%rsp,%rcx), %ymm0
	cltq
	cmpq	$7, %r11
	jae	LBB8_233
## %bb.232:                             ##   in Loop: Header=BB8_77 Depth=1
	xorl	%ecx, %ecx
	jmp	LBB8_235
LBB8_233:                               ## %"for conv1_stage2.s0.n.n.us65.us.us.3.preheader"
                                        ##   in Loop: Header=BB8_77 Depth=1
	movq	(%rsp), %rcx                    ## 8-byte Reload
	addq	%rax, %rcx
	leaq	(%r12,%rcx,4), %rbx
	xorl	%ecx, %ecx
LBB8_234:                               ## %"for conv1_stage2.s0.n.n.us65.us.us.3"
                                        ##   Parent Loop BB8_77 Depth=1
                                        ## =>  This Inner Loop Header: Depth=2
	vmovups	%ymm0, -224(%rbx)
	vmovups	%ymm0, -192(%rbx)
	vmovups	%ymm0, -160(%rbx)
	vmovups	%ymm0, -128(%rbx)
	vmovups	%ymm0, -96(%rbx)
	vmovups	%ymm0, -64(%rbx)
	vmovups	%ymm0, -32(%rbx)
	vmovups	%ymm0, (%rbx)
	addq	$8, %rcx
	addq	$256, %rbx                      ## imm = 0x100
	cmpq	%rcx, %rdi
	jne	LBB8_234
LBB8_235:                               ## %"end for conv1_stage2.s0.n.n.loopexit.us72.us.us.3.unr-lcssa"
                                        ##   in Loop: Header=BB8_77 Depth=1
	testb	$7, %r14b
	je	LBB8_238
## %bb.236:                             ## %"for conv1_stage2.s0.n.n.us65.us.us.3.epil.preheader"
                                        ##   in Loop: Header=BB8_77 Depth=1
	movq	(%rsp), %rbx                    ## 8-byte Reload
	leaq	(%rbx,%rcx,8), %rcx
	addq	%rax, %rcx
	movq	24(%rsp), %rax                  ## 8-byte Reload
	leaq	(%rax,%rcx,4), %rax
	xorl	%ecx, %ecx
LBB8_237:                               ## %"for conv1_stage2.s0.n.n.us65.us.us.3.epil"
                                        ##   Parent Loop BB8_77 Depth=1
                                        ## =>  This Inner Loop Header: Depth=2
	vmovups	%ymm0, (%rax,%rcx)
	addq	$32, %rcx
	cmpq	%rcx, %r8
	jne	LBB8_237
LBB8_238:                               ## %"end for conv1_stage2.s0.n.n.loopexit.us72.us.us.3"
                                        ##   in Loop: Header=BB8_77 Depth=1
	movq	48(%rsp), %rax                  ## 8-byte Reload
	addl	%esi, %eax
	movq	%rdx, %rcx
	orq	$16, %rcx
	vbroadcastss	160(%rsp,%rcx), %ymm0
	cltq
	cmpq	$7, %r11
	jae	LBB8_240
## %bb.239:                             ##   in Loop: Header=BB8_77 Depth=1
	xorl	%ecx, %ecx
	jmp	LBB8_242
LBB8_240:                               ## %"for conv1_stage2.s0.n.n.us65.us.us.4.preheader"
                                        ##   in Loop: Header=BB8_77 Depth=1
	movq	(%rsp), %rcx                    ## 8-byte Reload
	addq	%rax, %rcx
	leaq	(%r12,%rcx,4), %rbx
	xorl	%ecx, %ecx
LBB8_241:                               ## %"for conv1_stage2.s0.n.n.us65.us.us.4"
                                        ##   Parent Loop BB8_77 Depth=1
                                        ## =>  This Inner Loop Header: Depth=2
	vmovups	%ymm0, -224(%rbx)
	vmovups	%ymm0, -192(%rbx)
	vmovups	%ymm0, -160(%rbx)
	vmovups	%ymm0, -128(%rbx)
	vmovups	%ymm0, -96(%rbx)
	vmovups	%ymm0, -64(%rbx)
	vmovups	%ymm0, -32(%rbx)
	vmovups	%ymm0, (%rbx)
	addq	$8, %rcx
	addq	$256, %rbx                      ## imm = 0x100
	cmpq	%rcx, %rdi
	jne	LBB8_241
LBB8_242:                               ## %"end for conv1_stage2.s0.n.n.loopexit.us72.us.us.4.unr-lcssa"
                                        ##   in Loop: Header=BB8_77 Depth=1
	testb	$7, %r14b
	je	LBB8_245
## %bb.243:                             ## %"for conv1_stage2.s0.n.n.us65.us.us.4.epil.preheader"
                                        ##   in Loop: Header=BB8_77 Depth=1
	movq	(%rsp), %rbx                    ## 8-byte Reload
	leaq	(%rbx,%rcx,8), %rcx
	addq	%rax, %rcx
	movq	24(%rsp), %rax                  ## 8-byte Reload
	leaq	(%rax,%rcx,4), %rax
	xorl	%ecx, %ecx
LBB8_244:                               ## %"for conv1_stage2.s0.n.n.us65.us.us.4.epil"
                                        ##   Parent Loop BB8_77 Depth=1
                                        ## =>  This Inner Loop Header: Depth=2
	vmovups	%ymm0, (%rax,%rcx)
	addq	$32, %rcx
	cmpq	%rcx, %r8
	jne	LBB8_244
LBB8_245:                               ## %"end for conv1_stage2.s0.n.n.loopexit.us72.us.us.4"
                                        ##   in Loop: Header=BB8_77 Depth=1
	movq	72(%rsp), %rax                  ## 8-byte Reload
	addl	%esi, %eax
	movq	%rdx, %rcx
	orq	$20, %rcx
	vbroadcastss	160(%rsp,%rcx), %ymm0
	cltq
	cmpq	$7, %r11
	jae	LBB8_247
## %bb.246:                             ##   in Loop: Header=BB8_77 Depth=1
	xorl	%ecx, %ecx
	jmp	LBB8_249
LBB8_247:                               ## %"for conv1_stage2.s0.n.n.us65.us.us.5.preheader"
                                        ##   in Loop: Header=BB8_77 Depth=1
	movq	(%rsp), %rcx                    ## 8-byte Reload
	addq	%rax, %rcx
	leaq	(%r12,%rcx,4), %rbx
	xorl	%ecx, %ecx
LBB8_248:                               ## %"for conv1_stage2.s0.n.n.us65.us.us.5"
                                        ##   Parent Loop BB8_77 Depth=1
                                        ## =>  This Inner Loop Header: Depth=2
	vmovups	%ymm0, -224(%rbx)
	vmovups	%ymm0, -192(%rbx)
	vmovups	%ymm0, -160(%rbx)
	vmovups	%ymm0, -128(%rbx)
	vmovups	%ymm0, -96(%rbx)
	vmovups	%ymm0, -64(%rbx)
	vmovups	%ymm0, -32(%rbx)
	vmovups	%ymm0, (%rbx)
	addq	$8, %rcx
	addq	$256, %rbx                      ## imm = 0x100
	cmpq	%rcx, %rdi
	jne	LBB8_248
LBB8_249:                               ## %"end for conv1_stage2.s0.n.n.loopexit.us72.us.us.5.unr-lcssa"
                                        ##   in Loop: Header=BB8_77 Depth=1
	testb	$7, %r14b
	je	LBB8_252
## %bb.250:                             ## %"for conv1_stage2.s0.n.n.us65.us.us.5.epil.preheader"
                                        ##   in Loop: Header=BB8_77 Depth=1
	movq	(%rsp), %rbx                    ## 8-byte Reload
	leaq	(%rbx,%rcx,8), %rcx
	addq	%rax, %rcx
	movq	24(%rsp), %rax                  ## 8-byte Reload
	leaq	(%rax,%rcx,4), %rax
	xorl	%ecx, %ecx
LBB8_251:                               ## %"for conv1_stage2.s0.n.n.us65.us.us.5.epil"
                                        ##   Parent Loop BB8_77 Depth=1
                                        ## =>  This Inner Loop Header: Depth=2
	vmovups	%ymm0, (%rax,%rcx)
	addq	$32, %rcx
	cmpq	%rcx, %r8
	jne	LBB8_251
LBB8_252:                               ## %"end for conv1_stage2.s0.n.n.loopexit.us72.us.us.5"
                                        ##   in Loop: Header=BB8_77 Depth=1
	movq	64(%rsp), %rax                  ## 8-byte Reload
	addl	%esi, %eax
	movq	%rdx, %rcx
	orq	$24, %rcx
	vbroadcastss	160(%rsp,%rcx), %ymm0
	cltq
	cmpq	$7, %r11
	jae	LBB8_254
## %bb.253:                             ##   in Loop: Header=BB8_77 Depth=1
	xorl	%ecx, %ecx
	jmp	LBB8_256
LBB8_254:                               ## %"for conv1_stage2.s0.n.n.us65.us.us.6.preheader"
                                        ##   in Loop: Header=BB8_77 Depth=1
	movq	(%rsp), %rcx                    ## 8-byte Reload
	addq	%rax, %rcx
	leaq	(%r12,%rcx,4), %rbx
	xorl	%ecx, %ecx
LBB8_255:                               ## %"for conv1_stage2.s0.n.n.us65.us.us.6"
                                        ##   Parent Loop BB8_77 Depth=1
                                        ## =>  This Inner Loop Header: Depth=2
	vmovups	%ymm0, -224(%rbx)
	vmovups	%ymm0, -192(%rbx)
	vmovups	%ymm0, -160(%rbx)
	vmovups	%ymm0, -128(%rbx)
	vmovups	%ymm0, -96(%rbx)
	vmovups	%ymm0, -64(%rbx)
	vmovups	%ymm0, -32(%rbx)
	vmovups	%ymm0, (%rbx)
	addq	$8, %rcx
	addq	$256, %rbx                      ## imm = 0x100
	cmpq	%rcx, %rdi
	jne	LBB8_255
LBB8_256:                               ## %"end for conv1_stage2.s0.n.n.loopexit.us72.us.us.6.unr-lcssa"
                                        ##   in Loop: Header=BB8_77 Depth=1
	testb	$7, %r14b
	je	LBB8_259
## %bb.257:                             ## %"for conv1_stage2.s0.n.n.us65.us.us.6.epil.preheader"
                                        ##   in Loop: Header=BB8_77 Depth=1
	movq	(%rsp), %rbx                    ## 8-byte Reload
	leaq	(%rbx,%rcx,8), %rcx
	addq	%rax, %rcx
	movq	24(%rsp), %rax                  ## 8-byte Reload
	leaq	(%rax,%rcx,4), %rax
	xorl	%ecx, %ecx
LBB8_258:                               ## %"for conv1_stage2.s0.n.n.us65.us.us.6.epil"
                                        ##   Parent Loop BB8_77 Depth=1
                                        ## =>  This Inner Loop Header: Depth=2
	vmovups	%ymm0, (%rax,%rcx)
	addq	$32, %rcx
	cmpq	%rcx, %r8
	jne	LBB8_258
LBB8_259:                               ## %"end for conv1_stage2.s0.n.n.loopexit.us72.us.us.6"
                                        ##   in Loop: Header=BB8_77 Depth=1
	addl	56(%rsp), %esi                  ## 4-byte Folded Reload
	orq	$28, %rdx
	vbroadcastss	160(%rsp,%rdx), %ymm0
	movslq	%esi, %rax
	cmpq	$7, %r11
	jae	LBB8_261
## %bb.260:                             ##   in Loop: Header=BB8_77 Depth=1
	xorl	%ecx, %ecx
	jmp	LBB8_263
LBB8_261:                               ## %"for conv1_stage2.s0.n.n.us65.us.us.7.preheader"
                                        ##   in Loop: Header=BB8_77 Depth=1
	movq	(%rsp), %rcx                    ## 8-byte Reload
	addq	%rax, %rcx
	leaq	(%r12,%rcx,4), %rdx
	xorl	%ecx, %ecx
LBB8_262:                               ## %"for conv1_stage2.s0.n.n.us65.us.us.7"
                                        ##   Parent Loop BB8_77 Depth=1
                                        ## =>  This Inner Loop Header: Depth=2
	vmovups	%ymm0, -224(%rdx)
	vmovups	%ymm0, -192(%rdx)
	vmovups	%ymm0, -160(%rdx)
	vmovups	%ymm0, -128(%rdx)
	vmovups	%ymm0, -96(%rdx)
	vmovups	%ymm0, -64(%rdx)
	vmovups	%ymm0, -32(%rdx)
	vmovups	%ymm0, (%rdx)
	addq	$8, %rcx
	addq	$256, %rdx                      ## imm = 0x100
	cmpq	%rcx, %rdi
	jne	LBB8_262
LBB8_263:                               ## %"end for conv1_stage2.s0.n.n.loopexit.us72.us.us.7.unr-lcssa"
                                        ##   in Loop: Header=BB8_77 Depth=1
	testb	$7, %r14b
	je	LBB8_266
## %bb.264:                             ## %"for conv1_stage2.s0.n.n.us65.us.us.7.epil.preheader"
                                        ##   in Loop: Header=BB8_77 Depth=1
	movq	(%rsp), %rdx                    ## 8-byte Reload
	leaq	(%rdx,%rcx,8), %rcx
	addq	%rax, %rcx
	movq	24(%rsp), %rax                  ## 8-byte Reload
	leaq	(%rax,%rcx,4), %rax
	xorl	%ecx, %ecx
LBB8_265:                               ## %"for conv1_stage2.s0.n.n.us65.us.us.7.epil"
                                        ##   Parent Loop BB8_77 Depth=1
                                        ## =>  This Inner Loop Header: Depth=2
	vmovups	%ymm0, (%rax,%rcx)
	addq	$32, %rcx
	cmpq	%rcx, %r8
	jne	LBB8_265
LBB8_266:                               ## %"end for conv1_stage2.s0.n.n.loopexit.us72.us.us.7"
                                        ##   in Loop: Header=BB8_77 Depth=1
	orl	$8, %r15d
	incl	%r10d
	imull	20(%rsp), %r10d                 ## 4-byte Folded Reload
	subl	(%rsp), %r10d                   ## 4-byte Folded Reload
	movq	8(%rsp), %rax                   ## 8-byte Reload
	addl	%r10d, %eax
	vbroadcastss	160(%rsp,%r15,4), %ymm0
	cltq
	cmpq	$7, %r11
	jae	LBB8_268
## %bb.267:                             ##   in Loop: Header=BB8_77 Depth=1
	xorl	%ecx, %ecx
	jmp	LBB8_270
LBB8_268:                               ## %"for conv1_stage2.s0.n.n.us65.us.us.1134.preheader"
                                        ##   in Loop: Header=BB8_77 Depth=1
	movq	(%rsp), %rcx                    ## 8-byte Reload
	addq	%rax, %rcx
	leaq	(%r12,%rcx,4), %rdx
	xorl	%ecx, %ecx
LBB8_269:                               ## %"for conv1_stage2.s0.n.n.us65.us.us.1134"
                                        ##   Parent Loop BB8_77 Depth=1
                                        ## =>  This Inner Loop Header: Depth=2
	vmovups	%ymm0, -224(%rdx)
	vmovups	%ymm0, -192(%rdx)
	vmovups	%ymm0, -160(%rdx)
	vmovups	%ymm0, -128(%rdx)
	vmovups	%ymm0, -96(%rdx)
	vmovups	%ymm0, -64(%rdx)
	vmovups	%ymm0, -32(%rdx)
	vmovups	%ymm0, (%rdx)
	addq	$8, %rcx
	addq	$256, %rdx                      ## imm = 0x100
	cmpq	%rcx, %rdi
	jne	LBB8_269
LBB8_270:                               ## %"end for conv1_stage2.s0.n.n.loopexit.us72.us.us.1135.unr-lcssa"
                                        ##   in Loop: Header=BB8_77 Depth=1
	testb	$7, %r14b
	je	LBB8_273
## %bb.271:                             ## %"for conv1_stage2.s0.n.n.us65.us.us.1134.epil.preheader"
                                        ##   in Loop: Header=BB8_77 Depth=1
	movq	(%rsp), %rdx                    ## 8-byte Reload
	leaq	(%rdx,%rcx,8), %rcx
	addq	%rax, %rcx
	movq	24(%rsp), %rax                  ## 8-byte Reload
	leaq	(%rax,%rcx,4), %rax
	xorl	%ecx, %ecx
LBB8_272:                               ## %"for conv1_stage2.s0.n.n.us65.us.us.1134.epil"
                                        ##   Parent Loop BB8_77 Depth=1
                                        ## =>  This Inner Loop Header: Depth=2
	vmovups	%ymm0, (%rax,%rcx)
	addq	$32, %rcx
	cmpq	%rcx, %r8
	jne	LBB8_272
LBB8_273:                               ## %"end for conv1_stage2.s0.n.n.loopexit.us72.us.us.1135"
                                        ##   in Loop: Header=BB8_77 Depth=1
	movq	40(%rsp), %rax                  ## 8-byte Reload
	addl	%r10d, %eax
	shlq	$2, %r15
	movq	%r15, %rcx
	orq	$4, %rcx
	vbroadcastss	160(%rsp,%rcx), %ymm0
	cltq
	cmpq	$7, %r11
	jae	LBB8_275
## %bb.274:                             ##   in Loop: Header=BB8_77 Depth=1
	xorl	%ecx, %ecx
	jmp	LBB8_277
LBB8_275:                               ## %"for conv1_stage2.s0.n.n.us65.us.us.1.1.preheader"
                                        ##   in Loop: Header=BB8_77 Depth=1
	movq	(%rsp), %rcx                    ## 8-byte Reload
	addq	%rax, %rcx
	leaq	(%r12,%rcx,4), %rdx
	xorl	%ecx, %ecx
LBB8_276:                               ## %"for conv1_stage2.s0.n.n.us65.us.us.1.1"
                                        ##   Parent Loop BB8_77 Depth=1
                                        ## =>  This Inner Loop Header: Depth=2
	vmovups	%ymm0, -224(%rdx)
	vmovups	%ymm0, -192(%rdx)
	vmovups	%ymm0, -160(%rdx)
	vmovups	%ymm0, -128(%rdx)
	vmovups	%ymm0, -96(%rdx)
	vmovups	%ymm0, -64(%rdx)
	vmovups	%ymm0, -32(%rdx)
	vmovups	%ymm0, (%rdx)
	addq	$8, %rcx
	addq	$256, %rdx                      ## imm = 0x100
	cmpq	%rcx, %rdi
	jne	LBB8_276
LBB8_277:                               ## %"end for conv1_stage2.s0.n.n.loopexit.us72.us.us.1.1.unr-lcssa"
                                        ##   in Loop: Header=BB8_77 Depth=1
	testb	$7, %r14b
	je	LBB8_280
## %bb.278:                             ## %"for conv1_stage2.s0.n.n.us65.us.us.1.1.epil.preheader"
                                        ##   in Loop: Header=BB8_77 Depth=1
	movq	(%rsp), %rdx                    ## 8-byte Reload
	leaq	(%rdx,%rcx,8), %rcx
	addq	%rax, %rcx
	movq	24(%rsp), %rax                  ## 8-byte Reload
	leaq	(%rax,%rcx,4), %rax
	xorl	%ecx, %ecx
LBB8_279:                               ## %"for conv1_stage2.s0.n.n.us65.us.us.1.1.epil"
                                        ##   Parent Loop BB8_77 Depth=1
                                        ## =>  This Inner Loop Header: Depth=2
	vmovups	%ymm0, (%rax,%rcx)
	addq	$32, %rcx
	cmpq	%rcx, %r8
	jne	LBB8_279
LBB8_280:                               ## %"end for conv1_stage2.s0.n.n.loopexit.us72.us.us.1.1"
                                        ##   in Loop: Header=BB8_77 Depth=1
	movq	32(%rsp), %rax                  ## 8-byte Reload
	addl	%r10d, %eax
	movq	%r15, %rcx
	orq	$8, %rcx
	vbroadcastss	160(%rsp,%rcx), %ymm0
	cltq
	cmpq	$7, %r11
	jae	LBB8_282
## %bb.281:                             ##   in Loop: Header=BB8_77 Depth=1
	xorl	%ecx, %ecx
	jmp	LBB8_284
LBB8_282:                               ## %"for conv1_stage2.s0.n.n.us65.us.us.2.1.preheader"
                                        ##   in Loop: Header=BB8_77 Depth=1
	movq	(%rsp), %rcx                    ## 8-byte Reload
	addq	%rax, %rcx
	leaq	(%r12,%rcx,4), %rdx
	xorl	%ecx, %ecx
LBB8_283:                               ## %"for conv1_stage2.s0.n.n.us65.us.us.2.1"
                                        ##   Parent Loop BB8_77 Depth=1
                                        ## =>  This Inner Loop Header: Depth=2
	vmovups	%ymm0, -224(%rdx)
	vmovups	%ymm0, -192(%rdx)
	vmovups	%ymm0, -160(%rdx)
	vmovups	%ymm0, -128(%rdx)
	vmovups	%ymm0, -96(%rdx)
	vmovups	%ymm0, -64(%rdx)
	vmovups	%ymm0, -32(%rdx)
	vmovups	%ymm0, (%rdx)
	addq	$8, %rcx
	addq	$256, %rdx                      ## imm = 0x100
	cmpq	%rcx, %rdi
	jne	LBB8_283
LBB8_284:                               ## %"end for conv1_stage2.s0.n.n.loopexit.us72.us.us.2.1.unr-lcssa"
                                        ##   in Loop: Header=BB8_77 Depth=1
	testb	$7, %r14b
	je	LBB8_287
## %bb.285:                             ## %"for conv1_stage2.s0.n.n.us65.us.us.2.1.epil.preheader"
                                        ##   in Loop: Header=BB8_77 Depth=1
	movq	(%rsp), %rdx                    ## 8-byte Reload
	leaq	(%rdx,%rcx,8), %rcx
	addq	%rax, %rcx
	movq	24(%rsp), %rax                  ## 8-byte Reload
	leaq	(%rax,%rcx,4), %rax
	xorl	%ecx, %ecx
LBB8_286:                               ## %"for conv1_stage2.s0.n.n.us65.us.us.2.1.epil"
                                        ##   Parent Loop BB8_77 Depth=1
                                        ## =>  This Inner Loop Header: Depth=2
	vmovups	%ymm0, (%rax,%rcx)
	addq	$32, %rcx
	cmpq	%rcx, %r8
	jne	LBB8_286
LBB8_287:                               ## %"end for conv1_stage2.s0.n.n.loopexit.us72.us.us.2.1"
                                        ##   in Loop: Header=BB8_77 Depth=1
	movq	112(%rsp), %rax                 ## 8-byte Reload
	addl	%r10d, %eax
	movq	%r15, %rcx
	orq	$12, %rcx
	vbroadcastss	160(%rsp,%rcx), %ymm0
	cltq
	cmpq	$7, %r11
	jae	LBB8_289
## %bb.288:                             ##   in Loop: Header=BB8_77 Depth=1
	xorl	%ecx, %ecx
	jmp	LBB8_291
LBB8_289:                               ## %"for conv1_stage2.s0.n.n.us65.us.us.3.1.preheader"
                                        ##   in Loop: Header=BB8_77 Depth=1
	movq	(%rsp), %rcx                    ## 8-byte Reload
	addq	%rax, %rcx
	leaq	(%r12,%rcx,4), %rdx
	xorl	%ecx, %ecx
LBB8_290:                               ## %"for conv1_stage2.s0.n.n.us65.us.us.3.1"
                                        ##   Parent Loop BB8_77 Depth=1
                                        ## =>  This Inner Loop Header: Depth=2
	vmovups	%ymm0, -224(%rdx)
	vmovups	%ymm0, -192(%rdx)
	vmovups	%ymm0, -160(%rdx)
	vmovups	%ymm0, -128(%rdx)
	vmovups	%ymm0, -96(%rdx)
	vmovups	%ymm0, -64(%rdx)
	vmovups	%ymm0, -32(%rdx)
	vmovups	%ymm0, (%rdx)
	addq	$8, %rcx
	addq	$256, %rdx                      ## imm = 0x100
	cmpq	%rcx, %rdi
	jne	LBB8_290
LBB8_291:                               ## %"end for conv1_stage2.s0.n.n.loopexit.us72.us.us.3.1.unr-lcssa"
                                        ##   in Loop: Header=BB8_77 Depth=1
	testb	$7, %r14b
	je	LBB8_294
## %bb.292:                             ## %"for conv1_stage2.s0.n.n.us65.us.us.3.1.epil.preheader"
                                        ##   in Loop: Header=BB8_77 Depth=1
	movq	(%rsp), %rdx                    ## 8-byte Reload
	leaq	(%rdx,%rcx,8), %rcx
	addq	%rax, %rcx
	movq	24(%rsp), %rax                  ## 8-byte Reload
	leaq	(%rax,%rcx,4), %rax
	xorl	%ecx, %ecx
LBB8_293:                               ## %"for conv1_stage2.s0.n.n.us65.us.us.3.1.epil"
                                        ##   Parent Loop BB8_77 Depth=1
                                        ## =>  This Inner Loop Header: Depth=2
	vmovups	%ymm0, (%rax,%rcx)
	addq	$32, %rcx
	cmpq	%rcx, %r8
	jne	LBB8_293
LBB8_294:                               ## %"end for conv1_stage2.s0.n.n.loopexit.us72.us.us.3.1"
                                        ##   in Loop: Header=BB8_77 Depth=1
	movq	48(%rsp), %rax                  ## 8-byte Reload
	addl	%r10d, %eax
	movq	%r15, %rcx
	orq	$16, %rcx
	vbroadcastss	160(%rsp,%rcx), %ymm0
	cltq
	cmpq	$7, %r11
	jae	LBB8_296
## %bb.295:                             ##   in Loop: Header=BB8_77 Depth=1
	xorl	%ecx, %ecx
	jmp	LBB8_298
LBB8_296:                               ## %"for conv1_stage2.s0.n.n.us65.us.us.4.1.preheader"
                                        ##   in Loop: Header=BB8_77 Depth=1
	movq	(%rsp), %rcx                    ## 8-byte Reload
	addq	%rax, %rcx
	leaq	(%r12,%rcx,4), %rdx
	xorl	%ecx, %ecx
LBB8_297:                               ## %"for conv1_stage2.s0.n.n.us65.us.us.4.1"
                                        ##   Parent Loop BB8_77 Depth=1
                                        ## =>  This Inner Loop Header: Depth=2
	vmovups	%ymm0, -224(%rdx)
	vmovups	%ymm0, -192(%rdx)
	vmovups	%ymm0, -160(%rdx)
	vmovups	%ymm0, -128(%rdx)
	vmovups	%ymm0, -96(%rdx)
	vmovups	%ymm0, -64(%rdx)
	vmovups	%ymm0, -32(%rdx)
	vmovups	%ymm0, (%rdx)
	addq	$8, %rcx
	addq	$256, %rdx                      ## imm = 0x100
	cmpq	%rcx, %rdi
	jne	LBB8_297
LBB8_298:                               ## %"end for conv1_stage2.s0.n.n.loopexit.us72.us.us.4.1.unr-lcssa"
                                        ##   in Loop: Header=BB8_77 Depth=1
	testb	$7, %r14b
	je	LBB8_301
## %bb.299:                             ## %"for conv1_stage2.s0.n.n.us65.us.us.4.1.epil.preheader"
                                        ##   in Loop: Header=BB8_77 Depth=1
	movq	(%rsp), %rdx                    ## 8-byte Reload
	leaq	(%rdx,%rcx,8), %rcx
	addq	%rax, %rcx
	movq	24(%rsp), %rax                  ## 8-byte Reload
	leaq	(%rax,%rcx,4), %rax
	xorl	%ecx, %ecx
LBB8_300:                               ## %"for conv1_stage2.s0.n.n.us65.us.us.4.1.epil"
                                        ##   Parent Loop BB8_77 Depth=1
                                        ## =>  This Inner Loop Header: Depth=2
	vmovups	%ymm0, (%rax,%rcx)
	addq	$32, %rcx
	cmpq	%rcx, %r8
	jne	LBB8_300
LBB8_301:                               ## %"end for conv1_stage2.s0.n.n.loopexit.us72.us.us.4.1"
                                        ##   in Loop: Header=BB8_77 Depth=1
	movq	72(%rsp), %rax                  ## 8-byte Reload
	addl	%r10d, %eax
	movq	%r15, %rcx
	orq	$20, %rcx
	vbroadcastss	160(%rsp,%rcx), %ymm0
	cltq
	cmpq	$7, %r11
	jae	LBB8_303
## %bb.302:                             ##   in Loop: Header=BB8_77 Depth=1
	xorl	%ecx, %ecx
	jmp	LBB8_305
LBB8_303:                               ## %"for conv1_stage2.s0.n.n.us65.us.us.5.1.preheader"
                                        ##   in Loop: Header=BB8_77 Depth=1
	movq	(%rsp), %rcx                    ## 8-byte Reload
	addq	%rax, %rcx
	leaq	(%r12,%rcx,4), %rdx
	xorl	%ecx, %ecx
LBB8_304:                               ## %"for conv1_stage2.s0.n.n.us65.us.us.5.1"
                                        ##   Parent Loop BB8_77 Depth=1
                                        ## =>  This Inner Loop Header: Depth=2
	vmovups	%ymm0, -224(%rdx)
	vmovups	%ymm0, -192(%rdx)
	vmovups	%ymm0, -160(%rdx)
	vmovups	%ymm0, -128(%rdx)
	vmovups	%ymm0, -96(%rdx)
	vmovups	%ymm0, -64(%rdx)
	vmovups	%ymm0, -32(%rdx)
	vmovups	%ymm0, (%rdx)
	addq	$8, %rcx
	addq	$256, %rdx                      ## imm = 0x100
	cmpq	%rcx, %rdi
	jne	LBB8_304
LBB8_305:                               ## %"end for conv1_stage2.s0.n.n.loopexit.us72.us.us.5.1.unr-lcssa"
                                        ##   in Loop: Header=BB8_77 Depth=1
	testb	$7, %r14b
	je	LBB8_308
## %bb.306:                             ## %"for conv1_stage2.s0.n.n.us65.us.us.5.1.epil.preheader"
                                        ##   in Loop: Header=BB8_77 Depth=1
	movq	(%rsp), %rdx                    ## 8-byte Reload
	leaq	(%rdx,%rcx,8), %rcx
	addq	%rax, %rcx
	movq	24(%rsp), %rax                  ## 8-byte Reload
	leaq	(%rax,%rcx,4), %rax
	xorl	%ecx, %ecx
LBB8_307:                               ## %"for conv1_stage2.s0.n.n.us65.us.us.5.1.epil"
                                        ##   Parent Loop BB8_77 Depth=1
                                        ## =>  This Inner Loop Header: Depth=2
	vmovups	%ymm0, (%rax,%rcx)
	addq	$32, %rcx
	cmpq	%rcx, %r8
	jne	LBB8_307
LBB8_308:                               ## %"end for conv1_stage2.s0.n.n.loopexit.us72.us.us.5.1"
                                        ##   in Loop: Header=BB8_77 Depth=1
	movq	64(%rsp), %rax                  ## 8-byte Reload
	addl	%r10d, %eax
	movq	%r15, %rcx
	orq	$24, %rcx
	vbroadcastss	160(%rsp,%rcx), %ymm0
	cltq
	cmpq	$7, %r11
	jae	LBB8_310
## %bb.309:                             ##   in Loop: Header=BB8_77 Depth=1
	xorl	%ecx, %ecx
	jmp	LBB8_312
LBB8_310:                               ## %"for conv1_stage2.s0.n.n.us65.us.us.6.1.preheader"
                                        ##   in Loop: Header=BB8_77 Depth=1
	movq	(%rsp), %rcx                    ## 8-byte Reload
	addq	%rax, %rcx
	leaq	(%r12,%rcx,4), %rdx
	xorl	%ecx, %ecx
LBB8_311:                               ## %"for conv1_stage2.s0.n.n.us65.us.us.6.1"
                                        ##   Parent Loop BB8_77 Depth=1
                                        ## =>  This Inner Loop Header: Depth=2
	vmovups	%ymm0, -224(%rdx)
	vmovups	%ymm0, -192(%rdx)
	vmovups	%ymm0, -160(%rdx)
	vmovups	%ymm0, -128(%rdx)
	vmovups	%ymm0, -96(%rdx)
	vmovups	%ymm0, -64(%rdx)
	vmovups	%ymm0, -32(%rdx)
	vmovups	%ymm0, (%rdx)
	addq	$8, %rcx
	addq	$256, %rdx                      ## imm = 0x100
	cmpq	%rcx, %rdi
	jne	LBB8_311
LBB8_312:                               ## %"end for conv1_stage2.s0.n.n.loopexit.us72.us.us.6.1.unr-lcssa"
                                        ##   in Loop: Header=BB8_77 Depth=1
	testb	$7, %r14b
	je	LBB8_315
## %bb.313:                             ## %"for conv1_stage2.s0.n.n.us65.us.us.6.1.epil.preheader"
                                        ##   in Loop: Header=BB8_77 Depth=1
	movq	(%rsp), %rdx                    ## 8-byte Reload
	leaq	(%rdx,%rcx,8), %rcx
	addq	%rax, %rcx
	movq	24(%rsp), %rax                  ## 8-byte Reload
	leaq	(%rax,%rcx,4), %rax
	xorl	%ecx, %ecx
LBB8_314:                               ## %"for conv1_stage2.s0.n.n.us65.us.us.6.1.epil"
                                        ##   Parent Loop BB8_77 Depth=1
                                        ## =>  This Inner Loop Header: Depth=2
	vmovups	%ymm0, (%rax,%rcx)
	addq	$32, %rcx
	cmpq	%rcx, %r8
	jne	LBB8_314
LBB8_315:                               ## %"end for conv1_stage2.s0.n.n.loopexit.us72.us.us.6.1"
                                        ##   in Loop: Header=BB8_77 Depth=1
	addl	56(%rsp), %r10d                 ## 4-byte Folded Reload
	orq	$28, %r15
	vbroadcastss	160(%rsp,%r15), %ymm0
	movslq	%r10d, %rax
	cmpq	$7, %r11
	jae	LBB8_317
## %bb.316:                             ##   in Loop: Header=BB8_77 Depth=1
	xorl	%ecx, %ecx
	jmp	LBB8_319
LBB8_317:                               ## %"for conv1_stage2.s0.n.n.us65.us.us.7.1.preheader"
                                        ##   in Loop: Header=BB8_77 Depth=1
	movq	(%rsp), %rcx                    ## 8-byte Reload
	addq	%rax, %rcx
	leaq	(%r12,%rcx,4), %rdx
	xorl	%ecx, %ecx
LBB8_318:                               ## %"for conv1_stage2.s0.n.n.us65.us.us.7.1"
                                        ##   Parent Loop BB8_77 Depth=1
                                        ## =>  This Inner Loop Header: Depth=2
	vmovups	%ymm0, -224(%rdx)
	vmovups	%ymm0, -192(%rdx)
	vmovups	%ymm0, -160(%rdx)
	vmovups	%ymm0, -128(%rdx)
	vmovups	%ymm0, -96(%rdx)
	vmovups	%ymm0, -64(%rdx)
	vmovups	%ymm0, -32(%rdx)
	vmovups	%ymm0, (%rdx)
	addq	$8, %rcx
	addq	$256, %rdx                      ## imm = 0x100
	cmpq	%rcx, %rdi
	jne	LBB8_318
LBB8_319:                               ## %"end for conv1_stage2.s0.n.n.loopexit.us72.us.us.7.1.unr-lcssa"
                                        ##   in Loop: Header=BB8_77 Depth=1
	testb	$7, %r14b
	je	LBB8_322
## %bb.320:                             ## %"for conv1_stage2.s0.n.n.us65.us.us.7.1.epil.preheader"
                                        ##   in Loop: Header=BB8_77 Depth=1
	movq	(%rsp), %rdx                    ## 8-byte Reload
	leaq	(%rdx,%rcx,8), %rcx
	addq	%rax, %rcx
	movq	24(%rsp), %rax                  ## 8-byte Reload
	leaq	(%rax,%rcx,4), %rax
	xorl	%ecx, %ecx
LBB8_321:                               ## %"for conv1_stage2.s0.n.n.us65.us.us.7.1.epil"
                                        ##   Parent Loop BB8_77 Depth=1
                                        ## =>  This Inner Loop Header: Depth=2
	vmovups	%ymm0, (%rax,%rcx)
	addq	$32, %rcx
	cmpq	%rcx, %r8
	jne	LBB8_321
	jmp	LBB8_322
LBB8_27:                                ## %"for conv1_stage2.s0.w.wi.wi.us.preheader"
	movl	%r11d, %edx
	subl	%r9d, %edx
	andl	$-8, %eax
	movl	%edx, %ecx
	subl	%eax, %ecx
	movl	%ecx, 112(%rsp)                 ## 4-byte Spill
	subl	%r9d, %r11d
	subl	%eax, %r11d
	movl	%r11d, 32(%rsp)                 ## 4-byte Spill
	leaq	992(%r10), %rax
	movq	%rax, 64(%rsp)                  ## 8-byte Spill
	movq	8(%rsp), %rax                   ## 8-byte Reload
	leal	(,%rax,8), %esi
	movl	144(%rsp), %ecx                 ## 4-byte Reload
	shll	$5, %ecx
	movq	56(%rsp), %rax                  ## 8-byte Reload
	subl	%ecx, %eax
	leal	32(%rax), %ecx
	movq	%rcx, 304(%rsp)                 ## 8-byte Spill
	leaq	96(%r10), %rcx
	movq	%rcx, 48(%rsp)                  ## 8-byte Spill
	movl	%esi, 80(%rsp)                  ## 4-byte Spill
	subl	%esi, %edx
	movl	%edx, 104(%rsp)                 ## 4-byte Spill
	leal	33(%rax), %ecx
	movq	%rcx, 120(%rsp)                 ## 8-byte Spill
	leal	34(%rax), %ecx
	movq	%rcx, 312(%rsp)                 ## 8-byte Spill
	leal	35(%rax), %ecx
	movq	%rcx, 344(%rsp)                 ## 8-byte Spill
	leal	36(%rax), %ecx
	movq	%rcx, 336(%rsp)                 ## 8-byte Spill
	leal	37(%rax), %ecx
	movq	%rcx, 328(%rsp)                 ## 8-byte Spill
	leal	38(%rax), %ecx
	movq	%rcx, 320(%rsp)                 ## 8-byte Spill
	addl	$39, %eax
	movq	%rax, 56(%rsp)                  ## 8-byte Spill
	xorl	%eax, %eax
	movq	%rax, 352(%rsp)                 ## 8-byte Spill
	movl	$0, 128(%rsp)                   ## 4-byte Folded Spill
	jmp	LBB8_28
LBB8_425:                               ## %"end for conv1_stage2.s0.w.wi.wii.split.us.split.us84"
                                        ##   in Loop: Header=BB8_28 Depth=1
	movl	$2, 128(%rsp)                   ## 4-byte Folded Spill
	testb	$1, 352(%rsp)                   ## 1-byte Folded Reload
	movb	$1, %al
	movq	%rax, 352(%rsp)                 ## 8-byte Spill
	jne	LBB8_118
LBB8_28:                                ## %"for conv1_stage2.s0.w.wi.wi.us"
                                        ## =>This Loop Header: Depth=1
                                        ##     Child Loop BB8_29 Depth 2
                                        ##       Child Loop BB8_30 Depth 3
                                        ##         Child Loop BB8_69 Depth 4
                                        ##         Child Loop BB8_72 Depth 4
                                        ##         Child Loop BB8_60 Depth 4
                                        ##       Child Loop BB8_63 Depth 3
                                        ##         Child Loop BB8_326 Depth 4
                                        ##         Child Loop BB8_329 Depth 4
                                        ##         Child Loop BB8_332 Depth 4
                                        ##       Child Loop BB8_335 Depth 3
                                        ##         Child Loop BB8_341 Depth 4
                                        ##         Child Loop BB8_344 Depth 4
                                        ##         Child Loop BB8_347 Depth 4
                                        ##       Child Loop BB8_350 Depth 3
                                        ##         Child Loop BB8_356 Depth 4
                                        ##         Child Loop BB8_359 Depth 4
                                        ##         Child Loop BB8_362 Depth 4
                                        ##       Child Loop BB8_365 Depth 3
                                        ##         Child Loop BB8_371 Depth 4
                                        ##         Child Loop BB8_374 Depth 4
                                        ##         Child Loop BB8_377 Depth 4
                                        ##       Child Loop BB8_380 Depth 3
                                        ##         Child Loop BB8_386 Depth 4
                                        ##         Child Loop BB8_389 Depth 4
                                        ##         Child Loop BB8_392 Depth 4
                                        ##       Child Loop BB8_395 Depth 3
                                        ##         Child Loop BB8_401 Depth 4
                                        ##         Child Loop BB8_404 Depth 4
                                        ##         Child Loop BB8_407 Depth 4
                                        ##       Child Loop BB8_410 Depth 3
                                        ##         Child Loop BB8_416 Depth 4
                                        ##         Child Loop BB8_419 Depth 4
                                        ##         Child Loop BB8_422 Depth 4
	movl	152(%rsp), %eax                 ## 4-byte Reload
                                        ## kill: def $eax killed $eax def $rax
	orl	128(%rsp), %eax                 ## 4-byte Folded Reload
	movq	%rax, 144(%rsp)                 ## 8-byte Spill
	xorl	%eax, %eax
	movq	%rax, 136(%rsp)                 ## 8-byte Spill
	xorl	%ecx, %ecx
	jmp	LBB8_29
LBB8_424:                               ## %"end for conv1_stage2.s0.n.n.rebased.loopexit.us.us.us.7"
                                        ##   in Loop: Header=BB8_29 Depth=2
	movl	$1, %ecx
	testb	$1, 136(%rsp)                   ## 1-byte Folded Reload
	movb	$1, %al
	movq	%rax, 136(%rsp)                 ## 8-byte Spill
	jne	LBB8_425
LBB8_29:                                ## %"for conv1_stage2.s0.w.wi.wii.us.us79"
                                        ##   Parent Loop BB8_28 Depth=1
                                        ## =>  This Loop Header: Depth=2
                                        ##       Child Loop BB8_30 Depth 3
                                        ##         Child Loop BB8_69 Depth 4
                                        ##         Child Loop BB8_72 Depth 4
                                        ##         Child Loop BB8_60 Depth 4
                                        ##       Child Loop BB8_63 Depth 3
                                        ##         Child Loop BB8_326 Depth 4
                                        ##         Child Loop BB8_329 Depth 4
                                        ##         Child Loop BB8_332 Depth 4
                                        ##       Child Loop BB8_335 Depth 3
                                        ##         Child Loop BB8_341 Depth 4
                                        ##         Child Loop BB8_344 Depth 4
                                        ##         Child Loop BB8_347 Depth 4
                                        ##       Child Loop BB8_350 Depth 3
                                        ##         Child Loop BB8_356 Depth 4
                                        ##         Child Loop BB8_359 Depth 4
                                        ##         Child Loop BB8_362 Depth 4
                                        ##       Child Loop BB8_365 Depth 3
                                        ##         Child Loop BB8_371 Depth 4
                                        ##         Child Loop BB8_374 Depth 4
                                        ##         Child Loop BB8_377 Depth 4
                                        ##       Child Loop BB8_380 Depth 3
                                        ##         Child Loop BB8_386 Depth 4
                                        ##         Child Loop BB8_389 Depth 4
                                        ##         Child Loop BB8_392 Depth 4
                                        ##       Child Loop BB8_395 Depth 3
                                        ##         Child Loop BB8_401 Depth 4
                                        ##         Child Loop BB8_404 Depth 4
                                        ##         Child Loop BB8_407 Depth 4
                                        ##       Child Loop BB8_410 Depth 3
                                        ##         Child Loop BB8_416 Depth 4
                                        ##         Child Loop BB8_419 Depth 4
                                        ##         Child Loop BB8_422 Depth 4
	movq	144(%rsp), %rax                 ## 8-byte Reload
	leal	(%rax,%rcx), %edx
	orl	128(%rsp), %ecx                 ## 4-byte Folded Reload
	shll	$3, %ecx
	movq	%rcx, 72(%rsp)                  ## 8-byte Spill
	shll	$5, %edx
	movq	304(%rsp), %rax                 ## 8-byte Reload
	movq	%rdx, 96(%rsp)                  ## 8-byte Spill
	leal	(%rax,%rdx), %r12d
	imull	88(%rsp), %r12d                 ## 4-byte Folded Reload
	addl	80(%rsp), %r12d                 ## 4-byte Folded Reload
	movl	104(%rsp), %r8d                 ## 4-byte Reload
	movl	32(%rsp), %r11d                 ## 4-byte Reload
	xorl	%r9d, %r9d
	jmp	LBB8_30
LBB8_61:                                ## %"end for conv1_stage2.s0.n.ni.us.us.us"
                                        ##   in Loop: Header=BB8_30 Depth=3
	incq	%r9
	addl	$8, %r12d
	addl	$-8, %r11d
	addl	$-8, %r8d
	cmpq	40(%rsp), %r9                   ## 8-byte Folded Reload
	je	LBB8_62
LBB8_30:                                ## %"for conv1_stage2.s0.n.n.rebased.us.us.us"
                                        ##   Parent Loop BB8_28 Depth=1
                                        ##     Parent Loop BB8_29 Depth=2
                                        ## =>    This Loop Header: Depth=3
                                        ##         Child Loop BB8_69 Depth 4
                                        ##         Child Loop BB8_72 Depth 4
                                        ##         Child Loop BB8_60 Depth 4
	cmpl	$8, %r8d
	movl	$8, %ebx
	cmovll	%r8d, %ebx
	cmpl	$8, %r11d
	movl	$8, %edx
	cmovll	%r11d, %edx
	leal	(,%r9,8), %eax
	movl	32(%rsp), %ecx                  ## 4-byte Reload
	movl	%ecx, %esi
	subl	%eax, %esi
	cmpl	$8, %esi
	movl	$8, %eax
	cmovgel	%eax, %esi
	movq	8(%rsp), %rax                   ## 8-byte Reload
	addl	%r9d, %eax
	movq	(%rsp), %rcx                    ## 8-byte Reload
	leal	(%rcx,%rax,8), %eax
	cmpl	%eax, 20(%rsp)                  ## 4-byte Folded Reload
	jle	LBB8_61
## %bb.31:                              ## %"for conv1_stage2.s0.n.ni.preheader.us.us.us"
                                        ##   in Loop: Header=BB8_30 Depth=3
	movslq	%r12d, %r15
	movq	72(%rsp), %rax                  ## 8-byte Reload
	vmovss	160(%rsp,%rax,4), %xmm0         ## xmm0 = mem[0],zero,zero,zero
	cmpl	$32, %esi
	jae	LBB8_66
## %bb.32:                              ##   in Loop: Header=BB8_30 Depth=3
	xorl	%eax, %eax
	jmp	LBB8_74
LBB8_66:                                ## %vector.ph348
                                        ##   in Loop: Header=BB8_30 Depth=3
	andl	$-32, %edx
	addq	$-32, %rdx
	shrq	$5, %rdx
	movl	%esi, %eax
	andl	$-32, %eax
	addq	$-32, %rax
	movq	%rax, %r13
	shrq	$5, %r13
	incq	%r13
	vbroadcastss	%xmm0, %ymm1
	cmpq	$224, %rax
	jae	LBB8_68
## %bb.67:                              ##   in Loop: Header=BB8_30 Depth=3
	xorl	%edi, %edi
	jmp	LBB8_70
LBB8_68:                                ## %vector.ph348.new
                                        ##   in Loop: Header=BB8_30 Depth=3
	movq	64(%rsp), %rax                  ## 8-byte Reload
	leaq	(%rax,%r15,4), %rax
	leaq	1(%rdx), %rcx
	andq	$-8, %rcx
	negq	%rcx
	xorl	%edi, %edi
LBB8_69:                                ## %vector.body345
                                        ##   Parent Loop BB8_28 Depth=1
                                        ##     Parent Loop BB8_29 Depth=2
                                        ##       Parent Loop BB8_30 Depth=3
                                        ## =>      This Inner Loop Header: Depth=4
	vmovups	%ymm1, -992(%rax,%rdi,4)
	vmovups	%ymm1, -960(%rax,%rdi,4)
	vmovups	%ymm1, -928(%rax,%rdi,4)
	vmovups	%ymm1, -896(%rax,%rdi,4)
	vmovups	%ymm1, -864(%rax,%rdi,4)
	vmovups	%ymm1, -832(%rax,%rdi,4)
	vmovups	%ymm1, -800(%rax,%rdi,4)
	vmovups	%ymm1, -768(%rax,%rdi,4)
	vmovups	%ymm1, -736(%rax,%rdi,4)
	vmovups	%ymm1, -704(%rax,%rdi,4)
	vmovups	%ymm1, -672(%rax,%rdi,4)
	vmovups	%ymm1, -640(%rax,%rdi,4)
	vmovups	%ymm1, -608(%rax,%rdi,4)
	vmovups	%ymm1, -576(%rax,%rdi,4)
	vmovups	%ymm1, -544(%rax,%rdi,4)
	vmovups	%ymm1, -512(%rax,%rdi,4)
	vmovups	%ymm1, -480(%rax,%rdi,4)
	vmovups	%ymm1, -448(%rax,%rdi,4)
	vmovups	%ymm1, -416(%rax,%rdi,4)
	vmovups	%ymm1, -384(%rax,%rdi,4)
	vmovups	%ymm1, -352(%rax,%rdi,4)
	vmovups	%ymm1, -320(%rax,%rdi,4)
	vmovups	%ymm1, -288(%rax,%rdi,4)
	vmovups	%ymm1, -256(%rax,%rdi,4)
	vmovups	%ymm1, -224(%rax,%rdi,4)
	vmovups	%ymm1, -192(%rax,%rdi,4)
	vmovups	%ymm1, -160(%rax,%rdi,4)
	vmovups	%ymm1, -128(%rax,%rdi,4)
	vmovups	%ymm1, -96(%rax,%rdi,4)
	vmovups	%ymm1, -64(%rax,%rdi,4)
	vmovups	%ymm1, -32(%rax,%rdi,4)
	vmovups	%ymm1, (%rax,%rdi,4)
	addq	$256, %rdi                      ## imm = 0x100
	addq	$8, %rcx
	jne	LBB8_69
LBB8_70:                                ## %middle.block343.unr-lcssa
                                        ##   in Loop: Header=BB8_30 Depth=3
	movl	%esi, %eax
	andl	$-32, %eax
	testb	$7, %r13b
	je	LBB8_73
## %bb.71:                              ## %vector.body345.epil.preheader
                                        ##   in Loop: Header=BB8_30 Depth=3
	incb	%dl
	movzbl	%dl, %ecx
	andl	$7, %ecx
	shlq	$7, %rcx
	addq	%r15, %rdi
	movq	48(%rsp), %rdx                  ## 8-byte Reload
	leaq	(%rdx,%rdi,4), %rdx
	xorl	%edi, %edi
LBB8_72:                                ## %vector.body345.epil
                                        ##   Parent Loop BB8_28 Depth=1
                                        ##     Parent Loop BB8_29 Depth=2
                                        ##       Parent Loop BB8_30 Depth=3
                                        ## =>      This Inner Loop Header: Depth=4
	vmovups	%ymm1, -96(%rdx,%rdi)
	vmovups	%ymm1, -64(%rdx,%rdi)
	vmovups	%ymm1, -32(%rdx,%rdi)
	vmovups	%ymm1, (%rdx,%rdi)
	subq	$-128, %rdi
	cmpq	%rdi, %rcx
	jne	LBB8_72
LBB8_73:                                ## %middle.block343
                                        ##   in Loop: Header=BB8_30 Depth=3
	cmpq	%rsi, %rax
	je	LBB8_61
LBB8_74:                                ## %"for conv1_stage2.s0.n.ni.us.us.us.preheader"
                                        ##   in Loop: Header=BB8_30 Depth=3
	movq	24(%rsp), %rcx                  ## 8-byte Reload
	leaq	(%rcx,%r15,4), %rcx
LBB8_60:                                ## %"for conv1_stage2.s0.n.ni.us.us.us"
                                        ##   Parent Loop BB8_28 Depth=1
                                        ##     Parent Loop BB8_29 Depth=2
                                        ##       Parent Loop BB8_30 Depth=3
                                        ## =>      This Inner Loop Header: Depth=4
	vmovss	%xmm0, (%rcx,%rax,4)
	incq	%rax
	cmpq	%rax, %rbx
	jne	LBB8_60
	jmp	LBB8_61
LBB8_62:                                ## %"end for conv1_stage2.s0.n.n.rebased.loopexit.us.us.us"
                                        ##   in Loop: Header=BB8_29 Depth=2
	movq	72(%rsp), %r15                  ## 8-byte Reload
	orq	$1, %r15
	movq	120(%rsp), %rax                 ## 8-byte Reload
	movq	96(%rsp), %rcx                  ## 8-byte Reload
	leal	(%rax,%rcx), %r12d
	imull	88(%rsp), %r12d                 ## 4-byte Folded Reload
	addl	80(%rsp), %r12d                 ## 4-byte Folded Reload
	movl	104(%rsp), %r11d                ## 4-byte Reload
	movl	32(%rsp), %r10d                 ## 4-byte Reload
	xorl	%esi, %esi
	jmp	LBB8_63
LBB8_333:                               ## %"end for conv1_stage2.s0.n.ni.us.us.us.1"
                                        ##   in Loop: Header=BB8_63 Depth=3
	incq	%rsi
	addl	$8, %r12d
	addl	$-8, %r10d
	addl	$-8, %r11d
	cmpq	40(%rsp), %rsi                  ## 8-byte Folded Reload
	je	LBB8_334
LBB8_63:                                ## %"for conv1_stage2.s0.n.n.rebased.us.us.us.1"
                                        ##   Parent Loop BB8_28 Depth=1
                                        ##     Parent Loop BB8_29 Depth=2
                                        ## =>    This Loop Header: Depth=3
                                        ##         Child Loop BB8_326 Depth 4
                                        ##         Child Loop BB8_329 Depth 4
                                        ##         Child Loop BB8_332 Depth 4
	cmpl	$8, %r11d
	movl	$8, %ecx
	cmovll	%r11d, %ecx
	cmpl	$8, %r10d
	movl	$8, %eax
	cmovll	%r10d, %eax
	leal	(,%rsi,8), %edx
	movl	32(%rsp), %edi                  ## 4-byte Reload
	movl	%edi, %r9d
	subl	%edx, %r9d
	cmpl	$8, %r9d
	movl	$8, %edx
	cmovll	%r9d, %edx
	cmpl	$8, %r9d
	movl	$8, %edi
	cmovgel	%edi, %r9d
	movq	8(%rsp), %rdi                   ## 8-byte Reload
	addl	%esi, %edi
	movq	(%rsp), %rbx                    ## 8-byte Reload
	leal	(%rbx,%rdi,8), %edi
	cmpl	%edi, 20(%rsp)                  ## 4-byte Folded Reload
	jle	LBB8_333
## %bb.64:                              ## %"for conv1_stage2.s0.n.ni.preheader.us.us.us.1"
                                        ##   in Loop: Header=BB8_63 Depth=3
	movslq	%r12d, %r13
	vmovss	160(%rsp,%r15,4), %xmm0         ## xmm0 = mem[0],zero,zero,zero
	cmpl	$32, %r9d
	jae	LBB8_323
## %bb.65:                              ##   in Loop: Header=BB8_63 Depth=3
	xorl	%edx, %edx
	jmp	LBB8_331
LBB8_323:                               ## %vector.ph328
                                        ##   in Loop: Header=BB8_63 Depth=3
	andl	$-32, %eax
	addq	$-32, %rax
	shrq	$5, %rax
	andl	$-32, %edx
	addq	$-32, %rdx
	movq	%rdx, %r8
	shrq	$5, %r8
	incq	%r8
	vbroadcastss	%xmm0, %ymm1
	cmpq	$224, %rdx
	jae	LBB8_325
## %bb.324:                             ##   in Loop: Header=BB8_63 Depth=3
	xorl	%edi, %edi
	jmp	LBB8_327
LBB8_325:                               ## %vector.ph328.new
                                        ##   in Loop: Header=BB8_63 Depth=3
	movq	64(%rsp), %rdx                  ## 8-byte Reload
	leaq	(%rdx,%r13,4), %rdx
	leaq	1(%rax), %rbx
	andq	$-8, %rbx
	negq	%rbx
	xorl	%edi, %edi
LBB8_326:                               ## %vector.body325
                                        ##   Parent Loop BB8_28 Depth=1
                                        ##     Parent Loop BB8_29 Depth=2
                                        ##       Parent Loop BB8_63 Depth=3
                                        ## =>      This Inner Loop Header: Depth=4
	vmovups	%ymm1, -992(%rdx,%rdi,4)
	vmovups	%ymm1, -960(%rdx,%rdi,4)
	vmovups	%ymm1, -928(%rdx,%rdi,4)
	vmovups	%ymm1, -896(%rdx,%rdi,4)
	vmovups	%ymm1, -864(%rdx,%rdi,4)
	vmovups	%ymm1, -832(%rdx,%rdi,4)
	vmovups	%ymm1, -800(%rdx,%rdi,4)
	vmovups	%ymm1, -768(%rdx,%rdi,4)
	vmovups	%ymm1, -736(%rdx,%rdi,4)
	vmovups	%ymm1, -704(%rdx,%rdi,4)
	vmovups	%ymm1, -672(%rdx,%rdi,4)
	vmovups	%ymm1, -640(%rdx,%rdi,4)
	vmovups	%ymm1, -608(%rdx,%rdi,4)
	vmovups	%ymm1, -576(%rdx,%rdi,4)
	vmovups	%ymm1, -544(%rdx,%rdi,4)
	vmovups	%ymm1, -512(%rdx,%rdi,4)
	vmovups	%ymm1, -480(%rdx,%rdi,4)
	vmovups	%ymm1, -448(%rdx,%rdi,4)
	vmovups	%ymm1, -416(%rdx,%rdi,4)
	vmovups	%ymm1, -384(%rdx,%rdi,4)
	vmovups	%ymm1, -352(%rdx,%rdi,4)
	vmovups	%ymm1, -320(%rdx,%rdi,4)
	vmovups	%ymm1, -288(%rdx,%rdi,4)
	vmovups	%ymm1, -256(%rdx,%rdi,4)
	vmovups	%ymm1, -224(%rdx,%rdi,4)
	vmovups	%ymm1, -192(%rdx,%rdi,4)
	vmovups	%ymm1, -160(%rdx,%rdi,4)
	vmovups	%ymm1, -128(%rdx,%rdi,4)
	vmovups	%ymm1, -96(%rdx,%rdi,4)
	vmovups	%ymm1, -64(%rdx,%rdi,4)
	vmovups	%ymm1, -32(%rdx,%rdi,4)
	vmovups	%ymm1, (%rdx,%rdi,4)
	addq	$256, %rdi                      ## imm = 0x100
	addq	$8, %rbx
	jne	LBB8_326
LBB8_327:                               ## %middle.block323.unr-lcssa
                                        ##   in Loop: Header=BB8_63 Depth=3
	movl	%r9d, %edx
	andl	$-32, %edx
	testb	$7, %r8b
	je	LBB8_330
## %bb.328:                             ## %vector.body325.epil.preheader
                                        ##   in Loop: Header=BB8_63 Depth=3
	incb	%al
	movzbl	%al, %eax
	andl	$7, %eax
	shlq	$7, %rax
	addq	%r13, %rdi
	movq	48(%rsp), %rbx                  ## 8-byte Reload
	leaq	(%rbx,%rdi,4), %rdi
	xorl	%ebx, %ebx
LBB8_329:                               ## %vector.body325.epil
                                        ##   Parent Loop BB8_28 Depth=1
                                        ##     Parent Loop BB8_29 Depth=2
                                        ##       Parent Loop BB8_63 Depth=3
                                        ## =>      This Inner Loop Header: Depth=4
	vmovups	%ymm1, -96(%rdi,%rbx)
	vmovups	%ymm1, -64(%rdi,%rbx)
	vmovups	%ymm1, -32(%rdi,%rbx)
	vmovups	%ymm1, (%rdi,%rbx)
	subq	$-128, %rbx
	cmpq	%rbx, %rax
	jne	LBB8_329
LBB8_330:                               ## %middle.block323
                                        ##   in Loop: Header=BB8_63 Depth=3
	cmpq	%r9, %rdx
	je	LBB8_333
LBB8_331:                               ## %"for conv1_stage2.s0.n.ni.us.us.us.1.preheader"
                                        ##   in Loop: Header=BB8_63 Depth=3
	movq	24(%rsp), %rax                  ## 8-byte Reload
	leaq	(%rax,%r13,4), %rax
LBB8_332:                               ## %"for conv1_stage2.s0.n.ni.us.us.us.1"
                                        ##   Parent Loop BB8_28 Depth=1
                                        ##     Parent Loop BB8_29 Depth=2
                                        ##       Parent Loop BB8_63 Depth=3
                                        ## =>      This Inner Loop Header: Depth=4
	vmovss	%xmm0, (%rax,%rdx,4)
	incq	%rdx
	cmpq	%rdx, %rcx
	jne	LBB8_332
	jmp	LBB8_333
LBB8_334:                               ## %"end for conv1_stage2.s0.n.n.rebased.loopexit.us.us.us.1"
                                        ##   in Loop: Header=BB8_29 Depth=2
	movq	72(%rsp), %r13                  ## 8-byte Reload
	orq	$2, %r13
	movq	312(%rsp), %rax                 ## 8-byte Reload
	movq	96(%rsp), %rcx                  ## 8-byte Reload
	leal	(%rax,%rcx), %r11d
	imull	88(%rsp), %r11d                 ## 4-byte Folded Reload
	addl	80(%rsp), %r11d                 ## 4-byte Folded Reload
	movl	104(%rsp), %r10d                ## 4-byte Reload
	movl	32(%rsp), %r15d                 ## 4-byte Reload
	xorl	%r12d, %r12d
	jmp	LBB8_335
LBB8_348:                               ## %"end for conv1_stage2.s0.n.ni.us.us.us.2"
                                        ##   in Loop: Header=BB8_335 Depth=3
	incq	%r12
	addl	$8, %r11d
	addl	$-8, %r15d
	addl	$-8, %r10d
	cmpq	40(%rsp), %r12                  ## 8-byte Folded Reload
	je	LBB8_349
LBB8_335:                               ## %"for conv1_stage2.s0.n.n.rebased.us.us.us.2"
                                        ##   Parent Loop BB8_28 Depth=1
                                        ##     Parent Loop BB8_29 Depth=2
                                        ## =>    This Loop Header: Depth=3
                                        ##         Child Loop BB8_341 Depth 4
                                        ##         Child Loop BB8_344 Depth 4
                                        ##         Child Loop BB8_347 Depth 4
	cmpl	$8, %r10d
	movl	$8, %ecx
	cmovll	%r10d, %ecx
	cmpl	$8, %r15d
	movl	$8, %ebx
	cmovll	%r15d, %ebx
	leal	(,%r12,8), %edx
	movl	32(%rsp), %eax                  ## 4-byte Reload
                                        ## kill: def $eax killed $eax def $rax
	subl	%edx, %eax
	cmpl	$8, %eax
	movl	$8, %edi
	cmovgel	%edi, %eax
	movl	112(%rsp), %esi                 ## 4-byte Reload
                                        ## kill: def $esi killed $esi def $rsi
	subl	%edx, %esi
	cmpl	$8, %esi
	cmovgel	%edi, %esi
	movq	8(%rsp), %rdx                   ## 8-byte Reload
	addl	%r12d, %edx
	movq	(%rsp), %rdi                    ## 8-byte Reload
	leal	(%rdi,%rdx,8), %edx
	cmpl	%edx, 20(%rsp)                  ## 4-byte Folded Reload
	jle	LBB8_348
## %bb.336:                             ## %"for conv1_stage2.s0.n.ni.preheader.us.us.us.2"
                                        ##   in Loop: Header=BB8_335 Depth=3
	movslq	%r11d, %r8
	vmovss	160(%rsp,%r13,4), %xmm0         ## xmm0 = mem[0],zero,zero,zero
	cmpl	$32, %esi
	jae	LBB8_338
## %bb.337:                             ##   in Loop: Header=BB8_335 Depth=3
	xorl	%eax, %eax
	jmp	LBB8_346
LBB8_338:                               ## %vector.ph308
                                        ##   in Loop: Header=BB8_335 Depth=3
	andl	$-32, %ebx
	addq	$-32, %rbx
	shrq	$5, %rbx
	andl	$-32, %eax
	addq	$-32, %rax
	movq	%rax, %r9
	shrq	$5, %r9
	incq	%r9
	vbroadcastss	%xmm0, %ymm1
	cmpq	$224, %rax
	jae	LBB8_340
## %bb.339:                             ##   in Loop: Header=BB8_335 Depth=3
	xorl	%edx, %edx
	jmp	LBB8_342
LBB8_340:                               ## %vector.ph308.new
                                        ##   in Loop: Header=BB8_335 Depth=3
	movq	64(%rsp), %rax                  ## 8-byte Reload
	leaq	(%rax,%r8,4), %rax
	leaq	1(%rbx), %rdi
	andq	$-8, %rdi
	negq	%rdi
	xorl	%edx, %edx
LBB8_341:                               ## %vector.body305
                                        ##   Parent Loop BB8_28 Depth=1
                                        ##     Parent Loop BB8_29 Depth=2
                                        ##       Parent Loop BB8_335 Depth=3
                                        ## =>      This Inner Loop Header: Depth=4
	vmovups	%ymm1, -992(%rax,%rdx,4)
	vmovups	%ymm1, -960(%rax,%rdx,4)
	vmovups	%ymm1, -928(%rax,%rdx,4)
	vmovups	%ymm1, -896(%rax,%rdx,4)
	vmovups	%ymm1, -864(%rax,%rdx,4)
	vmovups	%ymm1, -832(%rax,%rdx,4)
	vmovups	%ymm1, -800(%rax,%rdx,4)
	vmovups	%ymm1, -768(%rax,%rdx,4)
	vmovups	%ymm1, -736(%rax,%rdx,4)
	vmovups	%ymm1, -704(%rax,%rdx,4)
	vmovups	%ymm1, -672(%rax,%rdx,4)
	vmovups	%ymm1, -640(%rax,%rdx,4)
	vmovups	%ymm1, -608(%rax,%rdx,4)
	vmovups	%ymm1, -576(%rax,%rdx,4)
	vmovups	%ymm1, -544(%rax,%rdx,4)
	vmovups	%ymm1, -512(%rax,%rdx,4)
	vmovups	%ymm1, -480(%rax,%rdx,4)
	vmovups	%ymm1, -448(%rax,%rdx,4)
	vmovups	%ymm1, -416(%rax,%rdx,4)
	vmovups	%ymm1, -384(%rax,%rdx,4)
	vmovups	%ymm1, -352(%rax,%rdx,4)
	vmovups	%ymm1, -320(%rax,%rdx,4)
	vmovups	%ymm1, -288(%rax,%rdx,4)
	vmovups	%ymm1, -256(%rax,%rdx,4)
	vmovups	%ymm1, -224(%rax,%rdx,4)
	vmovups	%ymm1, -192(%rax,%rdx,4)
	vmovups	%ymm1, -160(%rax,%rdx,4)
	vmovups	%ymm1, -128(%rax,%rdx,4)
	vmovups	%ymm1, -96(%rax,%rdx,4)
	vmovups	%ymm1, -64(%rax,%rdx,4)
	vmovups	%ymm1, -32(%rax,%rdx,4)
	vmovups	%ymm1, (%rax,%rdx,4)
	addq	$256, %rdx                      ## imm = 0x100
	addq	$8, %rdi
	jne	LBB8_341
LBB8_342:                               ## %middle.block303.unr-lcssa
                                        ##   in Loop: Header=BB8_335 Depth=3
	movl	%esi, %eax
	andl	$-32, %eax
	testb	$7, %r9b
	je	LBB8_345
## %bb.343:                             ## %vector.body305.epil.preheader
                                        ##   in Loop: Header=BB8_335 Depth=3
	incb	%bl
	movzbl	%bl, %edi
	andl	$7, %edi
	shlq	$7, %rdi
	addq	%r8, %rdx
	movq	48(%rsp), %rbx                  ## 8-byte Reload
	leaq	(%rbx,%rdx,4), %rdx
	xorl	%ebx, %ebx
LBB8_344:                               ## %vector.body305.epil
                                        ##   Parent Loop BB8_28 Depth=1
                                        ##     Parent Loop BB8_29 Depth=2
                                        ##       Parent Loop BB8_335 Depth=3
                                        ## =>      This Inner Loop Header: Depth=4
	vmovups	%ymm1, -96(%rdx,%rbx)
	vmovups	%ymm1, -64(%rdx,%rbx)
	vmovups	%ymm1, -32(%rdx,%rbx)
	vmovups	%ymm1, (%rdx,%rbx)
	subq	$-128, %rbx
	cmpq	%rbx, %rdi
	jne	LBB8_344
LBB8_345:                               ## %middle.block303
                                        ##   in Loop: Header=BB8_335 Depth=3
	cmpq	%rsi, %rax
	je	LBB8_348
LBB8_346:                               ## %"for conv1_stage2.s0.n.ni.us.us.us.2.preheader"
                                        ##   in Loop: Header=BB8_335 Depth=3
	movq	24(%rsp), %rdx                  ## 8-byte Reload
	leaq	(%rdx,%r8,4), %rdx
LBB8_347:                               ## %"for conv1_stage2.s0.n.ni.us.us.us.2"
                                        ##   Parent Loop BB8_28 Depth=1
                                        ##     Parent Loop BB8_29 Depth=2
                                        ##       Parent Loop BB8_335 Depth=3
                                        ## =>      This Inner Loop Header: Depth=4
	vmovss	%xmm0, (%rdx,%rax,4)
	incq	%rax
	cmpq	%rax, %rcx
	jne	LBB8_347
	jmp	LBB8_348
LBB8_349:                               ## %"end for conv1_stage2.s0.n.n.rebased.loopexit.us.us.us.2"
                                        ##   in Loop: Header=BB8_29 Depth=2
	movq	72(%rsp), %r15                  ## 8-byte Reload
	orq	$3, %r15
	movq	344(%rsp), %rax                 ## 8-byte Reload
	movq	96(%rsp), %rcx                  ## 8-byte Reload
	leal	(%rax,%rcx), %r12d
	imull	88(%rsp), %r12d                 ## 4-byte Folded Reload
	addl	80(%rsp), %r12d                 ## 4-byte Folded Reload
	movl	104(%rsp), %r11d                ## 4-byte Reload
	movl	32(%rsp), %r9d                  ## 4-byte Reload
	xorl	%r10d, %r10d
	jmp	LBB8_350
LBB8_363:                               ## %"end for conv1_stage2.s0.n.ni.us.us.us.3"
                                        ##   in Loop: Header=BB8_350 Depth=3
	incq	%r10
	addl	$8, %r12d
	addl	$-8, %r9d
	addl	$-8, %r11d
	cmpq	40(%rsp), %r10                  ## 8-byte Folded Reload
	je	LBB8_364
LBB8_350:                               ## %"for conv1_stage2.s0.n.n.rebased.us.us.us.3"
                                        ##   Parent Loop BB8_28 Depth=1
                                        ##     Parent Loop BB8_29 Depth=2
                                        ## =>    This Loop Header: Depth=3
                                        ##         Child Loop BB8_356 Depth 4
                                        ##         Child Loop BB8_359 Depth 4
                                        ##         Child Loop BB8_362 Depth 4
	cmpl	$8, %r11d
	movl	$8, %ecx
	cmovll	%r11d, %ecx
	cmpl	$8, %r9d
	movl	$8, %r14d
	cmovll	%r9d, %r14d
	leal	(,%r10,8), %edx
	movl	32(%rsp), %eax                  ## 4-byte Reload
                                        ## kill: def $eax killed $eax def $rax
	subl	%edx, %eax
	cmpl	$8, %eax
	movl	$8, %esi
	cmovgel	%esi, %eax
	movl	112(%rsp), %edi                 ## 4-byte Reload
	movl	%edi, %ebx
	subl	%edx, %ebx
	cmpl	$8, %ebx
	cmovgel	%esi, %ebx
	movq	8(%rsp), %rdx                   ## 8-byte Reload
	addl	%r10d, %edx
	movq	(%rsp), %rdi                    ## 8-byte Reload
	leal	(%rdi,%rdx,8), %edx
	cmpl	%edx, 20(%rsp)                  ## 4-byte Folded Reload
	jle	LBB8_363
## %bb.351:                             ## %"for conv1_stage2.s0.n.ni.preheader.us.us.us.3"
                                        ##   in Loop: Header=BB8_350 Depth=3
	movslq	%r12d, %r13
	vmovss	160(%rsp,%r15,4), %xmm0         ## xmm0 = mem[0],zero,zero,zero
	cmpl	$32, %ebx
	jae	LBB8_353
## %bb.352:                             ##   in Loop: Header=BB8_350 Depth=3
	xorl	%eax, %eax
	jmp	LBB8_361
LBB8_353:                               ## %vector.ph288
                                        ##   in Loop: Header=BB8_350 Depth=3
	andl	$-32, %r14d
	addq	$-32, %r14
	shrq	$5, %r14
	andl	$-32, %eax
	addq	$-32, %rax
	movq	%rax, %r8
	shrq	$5, %r8
	incq	%r8
	vbroadcastss	%xmm0, %ymm1
	cmpq	$224, %rax
	jae	LBB8_355
## %bb.354:                             ##   in Loop: Header=BB8_350 Depth=3
	xorl	%edi, %edi
	jmp	LBB8_357
LBB8_355:                               ## %vector.ph288.new
                                        ##   in Loop: Header=BB8_350 Depth=3
	movq	64(%rsp), %rax                  ## 8-byte Reload
	leaq	(%rax,%r13,4), %rax
	leaq	1(%r14), %rdx
	andq	$-8, %rdx
	negq	%rdx
	xorl	%edi, %edi
LBB8_356:                               ## %vector.body285
                                        ##   Parent Loop BB8_28 Depth=1
                                        ##     Parent Loop BB8_29 Depth=2
                                        ##       Parent Loop BB8_350 Depth=3
                                        ## =>      This Inner Loop Header: Depth=4
	vmovups	%ymm1, -992(%rax,%rdi,4)
	vmovups	%ymm1, -960(%rax,%rdi,4)
	vmovups	%ymm1, -928(%rax,%rdi,4)
	vmovups	%ymm1, -896(%rax,%rdi,4)
	vmovups	%ymm1, -864(%rax,%rdi,4)
	vmovups	%ymm1, -832(%rax,%rdi,4)
	vmovups	%ymm1, -800(%rax,%rdi,4)
	vmovups	%ymm1, -768(%rax,%rdi,4)
	vmovups	%ymm1, -736(%rax,%rdi,4)
	vmovups	%ymm1, -704(%rax,%rdi,4)
	vmovups	%ymm1, -672(%rax,%rdi,4)
	vmovups	%ymm1, -640(%rax,%rdi,4)
	vmovups	%ymm1, -608(%rax,%rdi,4)
	vmovups	%ymm1, -576(%rax,%rdi,4)
	vmovups	%ymm1, -544(%rax,%rdi,4)
	vmovups	%ymm1, -512(%rax,%rdi,4)
	vmovups	%ymm1, -480(%rax,%rdi,4)
	vmovups	%ymm1, -448(%rax,%rdi,4)
	vmovups	%ymm1, -416(%rax,%rdi,4)
	vmovups	%ymm1, -384(%rax,%rdi,4)
	vmovups	%ymm1, -352(%rax,%rdi,4)
	vmovups	%ymm1, -320(%rax,%rdi,4)
	vmovups	%ymm1, -288(%rax,%rdi,4)
	vmovups	%ymm1, -256(%rax,%rdi,4)
	vmovups	%ymm1, -224(%rax,%rdi,4)
	vmovups	%ymm1, -192(%rax,%rdi,4)
	vmovups	%ymm1, -160(%rax,%rdi,4)
	vmovups	%ymm1, -128(%rax,%rdi,4)
	vmovups	%ymm1, -96(%rax,%rdi,4)
	vmovups	%ymm1, -64(%rax,%rdi,4)
	vmovups	%ymm1, -32(%rax,%rdi,4)
	vmovups	%ymm1, (%rax,%rdi,4)
	addq	$256, %rdi                      ## imm = 0x100
	addq	$8, %rdx
	jne	LBB8_356
LBB8_357:                               ## %middle.block283.unr-lcssa
                                        ##   in Loop: Header=BB8_350 Depth=3
	movl	%ebx, %eax
	andl	$-32, %eax
	testb	$7, %r8b
	je	LBB8_360
## %bb.358:                             ## %vector.body285.epil.preheader
                                        ##   in Loop: Header=BB8_350 Depth=3
	incb	%r14b
	movzbl	%r14b, %edx
	andl	$7, %edx
	shlq	$7, %rdx
	addq	%r13, %rdi
	movq	48(%rsp), %rsi                  ## 8-byte Reload
	leaq	(%rsi,%rdi,4), %rsi
	xorl	%edi, %edi
LBB8_359:                               ## %vector.body285.epil
                                        ##   Parent Loop BB8_28 Depth=1
                                        ##     Parent Loop BB8_29 Depth=2
                                        ##       Parent Loop BB8_350 Depth=3
                                        ## =>      This Inner Loop Header: Depth=4
	vmovups	%ymm1, -96(%rsi,%rdi)
	vmovups	%ymm1, -64(%rsi,%rdi)
	vmovups	%ymm1, -32(%rsi,%rdi)
	vmovups	%ymm1, (%rsi,%rdi)
	subq	$-128, %rdi
	cmpq	%rdi, %rdx
	jne	LBB8_359
LBB8_360:                               ## %middle.block283
                                        ##   in Loop: Header=BB8_350 Depth=3
	cmpq	%rbx, %rax
	je	LBB8_363
LBB8_361:                               ## %"for conv1_stage2.s0.n.ni.us.us.us.3.preheader"
                                        ##   in Loop: Header=BB8_350 Depth=3
	movq	24(%rsp), %rdx                  ## 8-byte Reload
	leaq	(%rdx,%r13,4), %rdx
LBB8_362:                               ## %"for conv1_stage2.s0.n.ni.us.us.us.3"
                                        ##   Parent Loop BB8_28 Depth=1
                                        ##     Parent Loop BB8_29 Depth=2
                                        ##       Parent Loop BB8_350 Depth=3
                                        ## =>      This Inner Loop Header: Depth=4
	vmovss	%xmm0, (%rdx,%rax,4)
	incq	%rax
	cmpq	%rax, %rcx
	jne	LBB8_362
	jmp	LBB8_363
LBB8_364:                               ## %"end for conv1_stage2.s0.n.n.rebased.loopexit.us.us.us.3"
                                        ##   in Loop: Header=BB8_29 Depth=2
	movq	72(%rsp), %r15                  ## 8-byte Reload
	orq	$4, %r15
	movq	336(%rsp), %rax                 ## 8-byte Reload
	movq	96(%rsp), %rcx                  ## 8-byte Reload
	leal	(%rax,%rcx), %r12d
	imull	88(%rsp), %r12d                 ## 4-byte Folded Reload
	addl	80(%rsp), %r12d                 ## 4-byte Folded Reload
	movl	104(%rsp), %r11d                ## 4-byte Reload
	movl	32(%rsp), %r9d                  ## 4-byte Reload
	xorl	%r10d, %r10d
	jmp	LBB8_365
LBB8_378:                               ## %"end for conv1_stage2.s0.n.ni.us.us.us.4"
                                        ##   in Loop: Header=BB8_365 Depth=3
	incq	%r10
	addl	$8, %r12d
	addl	$-8, %r9d
	addl	$-8, %r11d
	cmpq	40(%rsp), %r10                  ## 8-byte Folded Reload
	je	LBB8_379
LBB8_365:                               ## %"for conv1_stage2.s0.n.n.rebased.us.us.us.4"
                                        ##   Parent Loop BB8_28 Depth=1
                                        ##     Parent Loop BB8_29 Depth=2
                                        ## =>    This Loop Header: Depth=3
                                        ##         Child Loop BB8_371 Depth 4
                                        ##         Child Loop BB8_374 Depth 4
                                        ##         Child Loop BB8_377 Depth 4
	cmpl	$8, %r11d
	movl	$8, %esi
	cmovll	%r11d, %esi
	cmpl	$8, %r9d
	movl	$8, %eax
	cmovll	%r9d, %eax
	leal	(,%r10,8), %ecx
	movl	32(%rsp), %edx                  ## 4-byte Reload
	movl	%edx, %ebx
	subl	%ecx, %ebx
	cmpl	$8, %ebx
	movl	$8, %ecx
	cmovll	%ebx, %ecx
	cmpl	$8, %ebx
	movl	$8, %edx
	cmovgel	%edx, %ebx
	movq	8(%rsp), %rdx                   ## 8-byte Reload
	addl	%r10d, %edx
	movq	(%rsp), %rdi                    ## 8-byte Reload
	leal	(%rdi,%rdx,8), %edx
	cmpl	%edx, 20(%rsp)                  ## 4-byte Folded Reload
	jle	LBB8_378
## %bb.366:                             ## %"for conv1_stage2.s0.n.ni.preheader.us.us.us.4"
                                        ##   in Loop: Header=BB8_365 Depth=3
	movslq	%r12d, %r13
	vmovss	160(%rsp,%r15,4), %xmm0         ## xmm0 = mem[0],zero,zero,zero
	cmpl	$32, %ebx
	jae	LBB8_368
## %bb.367:                             ##   in Loop: Header=BB8_365 Depth=3
	xorl	%edx, %edx
	jmp	LBB8_376
LBB8_368:                               ## %vector.ph268
                                        ##   in Loop: Header=BB8_365 Depth=3
	andl	$-32, %eax
	addq	$-32, %rax
	shrq	$5, %rax
	andl	$-32, %ecx
	addq	$-32, %rcx
	movq	%rcx, %r8
	shrq	$5, %r8
	incq	%r8
	vbroadcastss	%xmm0, %ymm1
	cmpq	$224, %rcx
	jae	LBB8_370
## %bb.369:                             ##   in Loop: Header=BB8_365 Depth=3
	xorl	%edi, %edi
	jmp	LBB8_372
LBB8_370:                               ## %vector.ph268.new
                                        ##   in Loop: Header=BB8_365 Depth=3
	movq	64(%rsp), %rcx                  ## 8-byte Reload
	leaq	(%rcx,%r13,4), %rdx
	leaq	1(%rax), %rcx
	andq	$-8, %rcx
	negq	%rcx
	xorl	%edi, %edi
LBB8_371:                               ## %vector.body265
                                        ##   Parent Loop BB8_28 Depth=1
                                        ##     Parent Loop BB8_29 Depth=2
                                        ##       Parent Loop BB8_365 Depth=3
                                        ## =>      This Inner Loop Header: Depth=4
	vmovups	%ymm1, -992(%rdx,%rdi,4)
	vmovups	%ymm1, -960(%rdx,%rdi,4)
	vmovups	%ymm1, -928(%rdx,%rdi,4)
	vmovups	%ymm1, -896(%rdx,%rdi,4)
	vmovups	%ymm1, -864(%rdx,%rdi,4)
	vmovups	%ymm1, -832(%rdx,%rdi,4)
	vmovups	%ymm1, -800(%rdx,%rdi,4)
	vmovups	%ymm1, -768(%rdx,%rdi,4)
	vmovups	%ymm1, -736(%rdx,%rdi,4)
	vmovups	%ymm1, -704(%rdx,%rdi,4)
	vmovups	%ymm1, -672(%rdx,%rdi,4)
	vmovups	%ymm1, -640(%rdx,%rdi,4)
	vmovups	%ymm1, -608(%rdx,%rdi,4)
	vmovups	%ymm1, -576(%rdx,%rdi,4)
	vmovups	%ymm1, -544(%rdx,%rdi,4)
	vmovups	%ymm1, -512(%rdx,%rdi,4)
	vmovups	%ymm1, -480(%rdx,%rdi,4)
	vmovups	%ymm1, -448(%rdx,%rdi,4)
	vmovups	%ymm1, -416(%rdx,%rdi,4)
	vmovups	%ymm1, -384(%rdx,%rdi,4)
	vmovups	%ymm1, -352(%rdx,%rdi,4)
	vmovups	%ymm1, -320(%rdx,%rdi,4)
	vmovups	%ymm1, -288(%rdx,%rdi,4)
	vmovups	%ymm1, -256(%rdx,%rdi,4)
	vmovups	%ymm1, -224(%rdx,%rdi,4)
	vmovups	%ymm1, -192(%rdx,%rdi,4)
	vmovups	%ymm1, -160(%rdx,%rdi,4)
	vmovups	%ymm1, -128(%rdx,%rdi,4)
	vmovups	%ymm1, -96(%rdx,%rdi,4)
	vmovups	%ymm1, -64(%rdx,%rdi,4)
	vmovups	%ymm1, -32(%rdx,%rdi,4)
	vmovups	%ymm1, (%rdx,%rdi,4)
	addq	$256, %rdi                      ## imm = 0x100
	addq	$8, %rcx
	jne	LBB8_371
LBB8_372:                               ## %middle.block263.unr-lcssa
                                        ##   in Loop: Header=BB8_365 Depth=3
	movl	%ebx, %edx
	andl	$-32, %edx
	testb	$7, %r8b
	je	LBB8_375
## %bb.373:                             ## %vector.body265.epil.preheader
                                        ##   in Loop: Header=BB8_365 Depth=3
	incb	%al
	movzbl	%al, %eax
	andl	$7, %eax
	shlq	$7, %rax
	addq	%r13, %rdi
	movq	48(%rsp), %rcx                  ## 8-byte Reload
	leaq	(%rcx,%rdi,4), %rcx
	xorl	%edi, %edi
LBB8_374:                               ## %vector.body265.epil
                                        ##   Parent Loop BB8_28 Depth=1
                                        ##     Parent Loop BB8_29 Depth=2
                                        ##       Parent Loop BB8_365 Depth=3
                                        ## =>      This Inner Loop Header: Depth=4
	vmovups	%ymm1, -96(%rcx,%rdi)
	vmovups	%ymm1, -64(%rcx,%rdi)
	vmovups	%ymm1, -32(%rcx,%rdi)
	vmovups	%ymm1, (%rcx,%rdi)
	subq	$-128, %rdi
	cmpq	%rdi, %rax
	jne	LBB8_374
LBB8_375:                               ## %middle.block263
                                        ##   in Loop: Header=BB8_365 Depth=3
	cmpq	%rbx, %rdx
	je	LBB8_378
LBB8_376:                               ## %"for conv1_stage2.s0.n.ni.us.us.us.4.preheader"
                                        ##   in Loop: Header=BB8_365 Depth=3
	movq	24(%rsp), %rax                  ## 8-byte Reload
	leaq	(%rax,%r13,4), %rax
LBB8_377:                               ## %"for conv1_stage2.s0.n.ni.us.us.us.4"
                                        ##   Parent Loop BB8_28 Depth=1
                                        ##     Parent Loop BB8_29 Depth=2
                                        ##       Parent Loop BB8_365 Depth=3
                                        ## =>      This Inner Loop Header: Depth=4
	vmovss	%xmm0, (%rax,%rdx,4)
	incq	%rdx
	cmpq	%rdx, %rsi
	jne	LBB8_377
	jmp	LBB8_378
LBB8_379:                               ## %"end for conv1_stage2.s0.n.n.rebased.loopexit.us.us.us.4"
                                        ##   in Loop: Header=BB8_29 Depth=2
	movq	72(%rsp), %r15                  ## 8-byte Reload
	orq	$5, %r15
	movq	328(%rsp), %rax                 ## 8-byte Reload
	movq	96(%rsp), %rcx                  ## 8-byte Reload
	leal	(%rax,%rcx), %r12d
	imull	88(%rsp), %r12d                 ## 4-byte Folded Reload
	addl	80(%rsp), %r12d                 ## 4-byte Folded Reload
	movl	104(%rsp), %r11d                ## 4-byte Reload
	movl	32(%rsp), %r9d                  ## 4-byte Reload
	xorl	%r10d, %r10d
	jmp	LBB8_380
LBB8_393:                               ## %"end for conv1_stage2.s0.n.ni.us.us.us.5"
                                        ##   in Loop: Header=BB8_380 Depth=3
	incq	%r10
	addl	$8, %r12d
	addl	$-8, %r9d
	addl	$-8, %r11d
	cmpq	40(%rsp), %r10                  ## 8-byte Folded Reload
	je	LBB8_394
LBB8_380:                               ## %"for conv1_stage2.s0.n.n.rebased.us.us.us.5"
                                        ##   Parent Loop BB8_28 Depth=1
                                        ##     Parent Loop BB8_29 Depth=2
                                        ## =>    This Loop Header: Depth=3
                                        ##         Child Loop BB8_386 Depth 4
                                        ##         Child Loop BB8_389 Depth 4
                                        ##         Child Loop BB8_392 Depth 4
	cmpl	$8, %r11d
	movl	$8, %esi
	cmovll	%r11d, %esi
	cmpl	$8, %r9d
	movl	$8, %eax
	cmovll	%r9d, %eax
	leal	(,%r10,8), %ecx
	movl	32(%rsp), %edx                  ## 4-byte Reload
	movl	%edx, %ebx
	subl	%ecx, %ebx
	cmpl	$8, %ebx
	movl	$8, %ecx
	cmovll	%ebx, %ecx
	cmpl	$8, %ebx
	movl	$8, %edx
	cmovgel	%edx, %ebx
	movq	8(%rsp), %rdx                   ## 8-byte Reload
	addl	%r10d, %edx
	movq	(%rsp), %rdi                    ## 8-byte Reload
	leal	(%rdi,%rdx,8), %edx
	cmpl	%edx, 20(%rsp)                  ## 4-byte Folded Reload
	jle	LBB8_393
## %bb.381:                             ## %"for conv1_stage2.s0.n.ni.preheader.us.us.us.5"
                                        ##   in Loop: Header=BB8_380 Depth=3
	movslq	%r12d, %r13
	vmovss	160(%rsp,%r15,4), %xmm0         ## xmm0 = mem[0],zero,zero,zero
	cmpl	$32, %ebx
	jae	LBB8_383
## %bb.382:                             ##   in Loop: Header=BB8_380 Depth=3
	xorl	%edx, %edx
	jmp	LBB8_391
LBB8_383:                               ## %vector.ph248
                                        ##   in Loop: Header=BB8_380 Depth=3
	andl	$-32, %eax
	addq	$-32, %rax
	shrq	$5, %rax
	andl	$-32, %ecx
	addq	$-32, %rcx
	movq	%rcx, %r8
	shrq	$5, %r8
	incq	%r8
	vbroadcastss	%xmm0, %ymm1
	cmpq	$224, %rcx
	jae	LBB8_385
## %bb.384:                             ##   in Loop: Header=BB8_380 Depth=3
	xorl	%edi, %edi
	jmp	LBB8_387
LBB8_385:                               ## %vector.ph248.new
                                        ##   in Loop: Header=BB8_380 Depth=3
	movq	64(%rsp), %rcx                  ## 8-byte Reload
	leaq	(%rcx,%r13,4), %rdx
	leaq	1(%rax), %rcx
	andq	$-8, %rcx
	negq	%rcx
	xorl	%edi, %edi
LBB8_386:                               ## %vector.body245
                                        ##   Parent Loop BB8_28 Depth=1
                                        ##     Parent Loop BB8_29 Depth=2
                                        ##       Parent Loop BB8_380 Depth=3
                                        ## =>      This Inner Loop Header: Depth=4
	vmovups	%ymm1, -992(%rdx,%rdi,4)
	vmovups	%ymm1, -960(%rdx,%rdi,4)
	vmovups	%ymm1, -928(%rdx,%rdi,4)
	vmovups	%ymm1, -896(%rdx,%rdi,4)
	vmovups	%ymm1, -864(%rdx,%rdi,4)
	vmovups	%ymm1, -832(%rdx,%rdi,4)
	vmovups	%ymm1, -800(%rdx,%rdi,4)
	vmovups	%ymm1, -768(%rdx,%rdi,4)
	vmovups	%ymm1, -736(%rdx,%rdi,4)
	vmovups	%ymm1, -704(%rdx,%rdi,4)
	vmovups	%ymm1, -672(%rdx,%rdi,4)
	vmovups	%ymm1, -640(%rdx,%rdi,4)
	vmovups	%ymm1, -608(%rdx,%rdi,4)
	vmovups	%ymm1, -576(%rdx,%rdi,4)
	vmovups	%ymm1, -544(%rdx,%rdi,4)
	vmovups	%ymm1, -512(%rdx,%rdi,4)
	vmovups	%ymm1, -480(%rdx,%rdi,4)
	vmovups	%ymm1, -448(%rdx,%rdi,4)
	vmovups	%ymm1, -416(%rdx,%rdi,4)
	vmovups	%ymm1, -384(%rdx,%rdi,4)
	vmovups	%ymm1, -352(%rdx,%rdi,4)
	vmovups	%ymm1, -320(%rdx,%rdi,4)
	vmovups	%ymm1, -288(%rdx,%rdi,4)
	vmovups	%ymm1, -256(%rdx,%rdi,4)
	vmovups	%ymm1, -224(%rdx,%rdi,4)
	vmovups	%ymm1, -192(%rdx,%rdi,4)
	vmovups	%ymm1, -160(%rdx,%rdi,4)
	vmovups	%ymm1, -128(%rdx,%rdi,4)
	vmovups	%ymm1, -96(%rdx,%rdi,4)
	vmovups	%ymm1, -64(%rdx,%rdi,4)
	vmovups	%ymm1, -32(%rdx,%rdi,4)
	vmovups	%ymm1, (%rdx,%rdi,4)
	addq	$256, %rdi                      ## imm = 0x100
	addq	$8, %rcx
	jne	LBB8_386
LBB8_387:                               ## %middle.block243.unr-lcssa
                                        ##   in Loop: Header=BB8_380 Depth=3
	movl	%ebx, %edx
	andl	$-32, %edx
	testb	$7, %r8b
	je	LBB8_390
## %bb.388:                             ## %vector.body245.epil.preheader
                                        ##   in Loop: Header=BB8_380 Depth=3
	incb	%al
	movzbl	%al, %eax
	andl	$7, %eax
	shlq	$7, %rax
	addq	%r13, %rdi
	movq	48(%rsp), %rcx                  ## 8-byte Reload
	leaq	(%rcx,%rdi,4), %rcx
	xorl	%edi, %edi
LBB8_389:                               ## %vector.body245.epil
                                        ##   Parent Loop BB8_28 Depth=1
                                        ##     Parent Loop BB8_29 Depth=2
                                        ##       Parent Loop BB8_380 Depth=3
                                        ## =>      This Inner Loop Header: Depth=4
	vmovups	%ymm1, -96(%rcx,%rdi)
	vmovups	%ymm1, -64(%rcx,%rdi)
	vmovups	%ymm1, -32(%rcx,%rdi)
	vmovups	%ymm1, (%rcx,%rdi)
	subq	$-128, %rdi
	cmpq	%rdi, %rax
	jne	LBB8_389
LBB8_390:                               ## %middle.block243
                                        ##   in Loop: Header=BB8_380 Depth=3
	cmpq	%rbx, %rdx
	je	LBB8_393
LBB8_391:                               ## %"for conv1_stage2.s0.n.ni.us.us.us.5.preheader"
                                        ##   in Loop: Header=BB8_380 Depth=3
	movq	24(%rsp), %rax                  ## 8-byte Reload
	leaq	(%rax,%r13,4), %rax
LBB8_392:                               ## %"for conv1_stage2.s0.n.ni.us.us.us.5"
                                        ##   Parent Loop BB8_28 Depth=1
                                        ##     Parent Loop BB8_29 Depth=2
                                        ##       Parent Loop BB8_380 Depth=3
                                        ## =>      This Inner Loop Header: Depth=4
	vmovss	%xmm0, (%rax,%rdx,4)
	incq	%rdx
	cmpq	%rdx, %rsi
	jne	LBB8_392
	jmp	LBB8_393
LBB8_394:                               ## %"end for conv1_stage2.s0.n.n.rebased.loopexit.us.us.us.5"
                                        ##   in Loop: Header=BB8_29 Depth=2
	movq	72(%rsp), %r15                  ## 8-byte Reload
	orq	$6, %r15
	movq	320(%rsp), %rax                 ## 8-byte Reload
	movq	96(%rsp), %rcx                  ## 8-byte Reload
	leal	(%rax,%rcx), %r12d
	imull	88(%rsp), %r12d                 ## 4-byte Folded Reload
	addl	80(%rsp), %r12d                 ## 4-byte Folded Reload
	movl	104(%rsp), %r11d                ## 4-byte Reload
	movl	32(%rsp), %r9d                  ## 4-byte Reload
	xorl	%r10d, %r10d
	jmp	LBB8_395
LBB8_408:                               ## %"end for conv1_stage2.s0.n.ni.us.us.us.6"
                                        ##   in Loop: Header=BB8_395 Depth=3
	incq	%r10
	addl	$8, %r12d
	addl	$-8, %r9d
	addl	$-8, %r11d
	cmpq	40(%rsp), %r10                  ## 8-byte Folded Reload
	je	LBB8_409
LBB8_395:                               ## %"for conv1_stage2.s0.n.n.rebased.us.us.us.6"
                                        ##   Parent Loop BB8_28 Depth=1
                                        ##     Parent Loop BB8_29 Depth=2
                                        ## =>    This Loop Header: Depth=3
                                        ##         Child Loop BB8_401 Depth 4
                                        ##         Child Loop BB8_404 Depth 4
                                        ##         Child Loop BB8_407 Depth 4
	cmpl	$8, %r11d
	movl	$8, %ecx
	cmovll	%r11d, %ecx
	cmpl	$8, %r9d
	movl	$8, %r14d
	cmovll	%r9d, %r14d
	leal	(,%r10,8), %edx
	movl	32(%rsp), %eax                  ## 4-byte Reload
                                        ## kill: def $eax killed $eax def $rax
	subl	%edx, %eax
	cmpl	$8, %eax
	movl	$8, %esi
	cmovgel	%esi, %eax
	movl	112(%rsp), %edi                 ## 4-byte Reload
	movl	%edi, %ebx
	subl	%edx, %ebx
	cmpl	$8, %ebx
	cmovgel	%esi, %ebx
	movq	8(%rsp), %rdx                   ## 8-byte Reload
	addl	%r10d, %edx
	movq	(%rsp), %rdi                    ## 8-byte Reload
	leal	(%rdi,%rdx,8), %edx
	cmpl	%edx, 20(%rsp)                  ## 4-byte Folded Reload
	jle	LBB8_408
## %bb.396:                             ## %"for conv1_stage2.s0.n.ni.preheader.us.us.us.6"
                                        ##   in Loop: Header=BB8_395 Depth=3
	movslq	%r12d, %r13
	vmovss	160(%rsp,%r15,4), %xmm0         ## xmm0 = mem[0],zero,zero,zero
	cmpl	$32, %ebx
	jae	LBB8_398
## %bb.397:                             ##   in Loop: Header=BB8_395 Depth=3
	xorl	%eax, %eax
	jmp	LBB8_406
LBB8_398:                               ## %vector.ph228
                                        ##   in Loop: Header=BB8_395 Depth=3
	andl	$-32, %r14d
	addq	$-32, %r14
	shrq	$5, %r14
	andl	$-32, %eax
	addq	$-32, %rax
	movq	%rax, %r8
	shrq	$5, %r8
	incq	%r8
	vbroadcastss	%xmm0, %ymm1
	cmpq	$224, %rax
	jae	LBB8_400
## %bb.399:                             ##   in Loop: Header=BB8_395 Depth=3
	xorl	%edi, %edi
	jmp	LBB8_402
LBB8_400:                               ## %vector.ph228.new
                                        ##   in Loop: Header=BB8_395 Depth=3
	movq	64(%rsp), %rax                  ## 8-byte Reload
	leaq	(%rax,%r13,4), %rax
	leaq	1(%r14), %rdx
	andq	$-8, %rdx
	negq	%rdx
	xorl	%edi, %edi
LBB8_401:                               ## %vector.body225
                                        ##   Parent Loop BB8_28 Depth=1
                                        ##     Parent Loop BB8_29 Depth=2
                                        ##       Parent Loop BB8_395 Depth=3
                                        ## =>      This Inner Loop Header: Depth=4
	vmovups	%ymm1, -992(%rax,%rdi,4)
	vmovups	%ymm1, -960(%rax,%rdi,4)
	vmovups	%ymm1, -928(%rax,%rdi,4)
	vmovups	%ymm1, -896(%rax,%rdi,4)
	vmovups	%ymm1, -864(%rax,%rdi,4)
	vmovups	%ymm1, -832(%rax,%rdi,4)
	vmovups	%ymm1, -800(%rax,%rdi,4)
	vmovups	%ymm1, -768(%rax,%rdi,4)
	vmovups	%ymm1, -736(%rax,%rdi,4)
	vmovups	%ymm1, -704(%rax,%rdi,4)
	vmovups	%ymm1, -672(%rax,%rdi,4)
	vmovups	%ymm1, -640(%rax,%rdi,4)
	vmovups	%ymm1, -608(%rax,%rdi,4)
	vmovups	%ymm1, -576(%rax,%rdi,4)
	vmovups	%ymm1, -544(%rax,%rdi,4)
	vmovups	%ymm1, -512(%rax,%rdi,4)
	vmovups	%ymm1, -480(%rax,%rdi,4)
	vmovups	%ymm1, -448(%rax,%rdi,4)
	vmovups	%ymm1, -416(%rax,%rdi,4)
	vmovups	%ymm1, -384(%rax,%rdi,4)
	vmovups	%ymm1, -352(%rax,%rdi,4)
	vmovups	%ymm1, -320(%rax,%rdi,4)
	vmovups	%ymm1, -288(%rax,%rdi,4)
	vmovups	%ymm1, -256(%rax,%rdi,4)
	vmovups	%ymm1, -224(%rax,%rdi,4)
	vmovups	%ymm1, -192(%rax,%rdi,4)
	vmovups	%ymm1, -160(%rax,%rdi,4)
	vmovups	%ymm1, -128(%rax,%rdi,4)
	vmovups	%ymm1, -96(%rax,%rdi,4)
	vmovups	%ymm1, -64(%rax,%rdi,4)
	vmovups	%ymm1, -32(%rax,%rdi,4)
	vmovups	%ymm1, (%rax,%rdi,4)
	addq	$256, %rdi                      ## imm = 0x100
	addq	$8, %rdx
	jne	LBB8_401
LBB8_402:                               ## %middle.block223.unr-lcssa
                                        ##   in Loop: Header=BB8_395 Depth=3
	movl	%ebx, %eax
	andl	$-32, %eax
	testb	$7, %r8b
	je	LBB8_405
## %bb.403:                             ## %vector.body225.epil.preheader
                                        ##   in Loop: Header=BB8_395 Depth=3
	incb	%r14b
	movzbl	%r14b, %edx
	andl	$7, %edx
	shlq	$7, %rdx
	addq	%r13, %rdi
	movq	48(%rsp), %rsi                  ## 8-byte Reload
	leaq	(%rsi,%rdi,4), %rsi
	xorl	%edi, %edi
LBB8_404:                               ## %vector.body225.epil
                                        ##   Parent Loop BB8_28 Depth=1
                                        ##     Parent Loop BB8_29 Depth=2
                                        ##       Parent Loop BB8_395 Depth=3
                                        ## =>      This Inner Loop Header: Depth=4
	vmovups	%ymm1, -96(%rsi,%rdi)
	vmovups	%ymm1, -64(%rsi,%rdi)
	vmovups	%ymm1, -32(%rsi,%rdi)
	vmovups	%ymm1, (%rsi,%rdi)
	subq	$-128, %rdi
	cmpq	%rdi, %rdx
	jne	LBB8_404
LBB8_405:                               ## %middle.block223
                                        ##   in Loop: Header=BB8_395 Depth=3
	cmpq	%rbx, %rax
	je	LBB8_408
LBB8_406:                               ## %"for conv1_stage2.s0.n.ni.us.us.us.6.preheader"
                                        ##   in Loop: Header=BB8_395 Depth=3
	movq	24(%rsp), %rdx                  ## 8-byte Reload
	leaq	(%rdx,%r13,4), %rdx
LBB8_407:                               ## %"for conv1_stage2.s0.n.ni.us.us.us.6"
                                        ##   Parent Loop BB8_28 Depth=1
                                        ##     Parent Loop BB8_29 Depth=2
                                        ##       Parent Loop BB8_395 Depth=3
                                        ## =>      This Inner Loop Header: Depth=4
	vmovss	%xmm0, (%rdx,%rax,4)
	incq	%rax
	cmpq	%rax, %rcx
	jne	LBB8_407
	jmp	LBB8_408
LBB8_409:                               ## %"end for conv1_stage2.s0.n.n.rebased.loopexit.us.us.us.6"
                                        ##   in Loop: Header=BB8_29 Depth=2
	orq	$7, 72(%rsp)                    ## 8-byte Folded Spill
	movq	96(%rsp), %r14                  ## 8-byte Reload
	addl	56(%rsp), %r14d                 ## 4-byte Folded Reload
	imull	88(%rsp), %r14d                 ## 4-byte Folded Reload
	addl	80(%rsp), %r14d                 ## 4-byte Folded Reload
	movl	104(%rsp), %r11d                ## 4-byte Reload
	movl	32(%rsp), %r9d                  ## 4-byte Reload
	xorl	%r10d, %r10d
	jmp	LBB8_410
LBB8_423:                               ## %"end for conv1_stage2.s0.n.ni.us.us.us.7"
                                        ##   in Loop: Header=BB8_410 Depth=3
	incq	%r10
	addl	$8, %r14d
	addl	$-8, %r9d
	addl	$-8, %r11d
	cmpq	40(%rsp), %r10                  ## 8-byte Folded Reload
	je	LBB8_424
LBB8_410:                               ## %"for conv1_stage2.s0.n.n.rebased.us.us.us.7"
                                        ##   Parent Loop BB8_28 Depth=1
                                        ##     Parent Loop BB8_29 Depth=2
                                        ## =>    This Loop Header: Depth=3
                                        ##         Child Loop BB8_416 Depth 4
                                        ##         Child Loop BB8_419 Depth 4
                                        ##         Child Loop BB8_422 Depth 4
	cmpl	$8, %r11d
	movl	$8, %ecx
	cmovll	%r11d, %ecx
	cmpl	$8, %r9d
	movl	$8, %esi
	cmovll	%r9d, %esi
	leal	(,%r10,8), %edx
	movl	32(%rsp), %eax                  ## 4-byte Reload
                                        ## kill: def $eax killed $eax def $rax
	subl	%edx, %eax
	cmpl	$8, %eax
	movl	$8, %ebx
	cmovgel	%ebx, %eax
	movl	112(%rsp), %edi                 ## 4-byte Reload
                                        ## kill: def $edi killed $edi def $rdi
	subl	%edx, %edi
	cmpl	$8, %edi
	cmovgel	%ebx, %edi
	movq	8(%rsp), %rdx                   ## 8-byte Reload
	addl	%r10d, %edx
	movq	(%rsp), %rbx                    ## 8-byte Reload
	leal	(%rbx,%rdx,8), %edx
	cmpl	%edx, 20(%rsp)                  ## 4-byte Folded Reload
	jle	LBB8_423
## %bb.411:                             ## %"for conv1_stage2.s0.n.ni.preheader.us.us.us.7"
                                        ##   in Loop: Header=BB8_410 Depth=3
	movslq	%r14d, %r15
	movq	72(%rsp), %rdx                  ## 8-byte Reload
	vmovss	160(%rsp,%rdx,4), %xmm0         ## xmm0 = mem[0],zero,zero,zero
	cmpl	$32, %edi
	jae	LBB8_413
## %bb.412:                             ##   in Loop: Header=BB8_410 Depth=3
	xorl	%eax, %eax
	jmp	LBB8_421
LBB8_413:                               ## %vector.ph208
                                        ##   in Loop: Header=BB8_410 Depth=3
	andl	$-32, %esi
	addq	$-32, %rsi
	shrq	$5, %rsi
	andl	$-32, %eax
	addq	$-32, %rax
	movq	%rax, %r8
	shrq	$5, %r8
	incq	%r8
	vbroadcastss	%xmm0, %ymm1
	cmpq	$224, %rax
	jae	LBB8_415
## %bb.414:                             ##   in Loop: Header=BB8_410 Depth=3
	xorl	%edx, %edx
	jmp	LBB8_417
LBB8_415:                               ## %vector.ph208.new
                                        ##   in Loop: Header=BB8_410 Depth=3
	movq	64(%rsp), %rax                  ## 8-byte Reload
	leaq	(%rax,%r15,4), %rax
	leaq	1(%rsi), %rbx
	andq	$-8, %rbx
	negq	%rbx
	xorl	%edx, %edx
LBB8_416:                               ## %vector.body205
                                        ##   Parent Loop BB8_28 Depth=1
                                        ##     Parent Loop BB8_29 Depth=2
                                        ##       Parent Loop BB8_410 Depth=3
                                        ## =>      This Inner Loop Header: Depth=4
	vmovups	%ymm1, -992(%rax,%rdx,4)
	vmovups	%ymm1, -960(%rax,%rdx,4)
	vmovups	%ymm1, -928(%rax,%rdx,4)
	vmovups	%ymm1, -896(%rax,%rdx,4)
	vmovups	%ymm1, -864(%rax,%rdx,4)
	vmovups	%ymm1, -832(%rax,%rdx,4)
	vmovups	%ymm1, -800(%rax,%rdx,4)
	vmovups	%ymm1, -768(%rax,%rdx,4)
	vmovups	%ymm1, -736(%rax,%rdx,4)
	vmovups	%ymm1, -704(%rax,%rdx,4)
	vmovups	%ymm1, -672(%rax,%rdx,4)
	vmovups	%ymm1, -640(%rax,%rdx,4)
	vmovups	%ymm1, -608(%rax,%rdx,4)
	vmovups	%ymm1, -576(%rax,%rdx,4)
	vmovups	%ymm1, -544(%rax,%rdx,4)
	vmovups	%ymm1, -512(%rax,%rdx,4)
	vmovups	%ymm1, -480(%rax,%rdx,4)
	vmovups	%ymm1, -448(%rax,%rdx,4)
	vmovups	%ymm1, -416(%rax,%rdx,4)
	vmovups	%ymm1, -384(%rax,%rdx,4)
	vmovups	%ymm1, -352(%rax,%rdx,4)
	vmovups	%ymm1, -320(%rax,%rdx,4)
	vmovups	%ymm1, -288(%rax,%rdx,4)
	vmovups	%ymm1, -256(%rax,%rdx,4)
	vmovups	%ymm1, -224(%rax,%rdx,4)
	vmovups	%ymm1, -192(%rax,%rdx,4)
	vmovups	%ymm1, -160(%rax,%rdx,4)
	vmovups	%ymm1, -128(%rax,%rdx,4)
	vmovups	%ymm1, -96(%rax,%rdx,4)
	vmovups	%ymm1, -64(%rax,%rdx,4)
	vmovups	%ymm1, -32(%rax,%rdx,4)
	vmovups	%ymm1, (%rax,%rdx,4)
	addq	$256, %rdx                      ## imm = 0x100
	addq	$8, %rbx
	jne	LBB8_416
LBB8_417:                               ## %middle.block203.unr-lcssa
                                        ##   in Loop: Header=BB8_410 Depth=3
	movl	%edi, %eax
	andl	$-32, %eax
	testb	$7, %r8b
	je	LBB8_420
## %bb.418:                             ## %vector.body205.epil.preheader
                                        ##   in Loop: Header=BB8_410 Depth=3
	incb	%sil
	movzbl	%sil, %esi
	andl	$7, %esi
	shlq	$7, %rsi
	addq	%r15, %rdx
	movq	48(%rsp), %rbx                  ## 8-byte Reload
	leaq	(%rbx,%rdx,4), %rdx
	xorl	%ebx, %ebx
LBB8_419:                               ## %vector.body205.epil
                                        ##   Parent Loop BB8_28 Depth=1
                                        ##     Parent Loop BB8_29 Depth=2
                                        ##       Parent Loop BB8_410 Depth=3
                                        ## =>      This Inner Loop Header: Depth=4
	vmovups	%ymm1, -96(%rdx,%rbx)
	vmovups	%ymm1, -64(%rdx,%rbx)
	vmovups	%ymm1, -32(%rdx,%rbx)
	vmovups	%ymm1, (%rdx,%rbx)
	subq	$-128, %rbx
	cmpq	%rbx, %rsi
	jne	LBB8_419
LBB8_420:                               ## %middle.block203
                                        ##   in Loop: Header=BB8_410 Depth=3
	cmpq	%rdi, %rax
	je	LBB8_423
LBB8_421:                               ## %"for conv1_stage2.s0.n.ni.us.us.us.7.preheader"
                                        ##   in Loop: Header=BB8_410 Depth=3
	movq	24(%rsp), %rdx                  ## 8-byte Reload
	leaq	(%rdx,%r15,4), %rdx
LBB8_422:                               ## %"for conv1_stage2.s0.n.ni.us.us.us.7"
                                        ##   Parent Loop BB8_28 Depth=1
                                        ##     Parent Loop BB8_29 Depth=2
                                        ##       Parent Loop BB8_410 Depth=3
                                        ## =>      This Inner Loop Header: Depth=4
	vmovss	%xmm0, (%rdx,%rax,4)
	incq	%rax
	cmpq	%rax, %rcx
	jne	LBB8_422
	jmp	LBB8_423
LBB8_118:                               ## %destructor_block
	xorl	%eax, %eax
	leaq	-40(%rbp), %rsp
	popq	%rbx
	popq	%r12
	popq	%r13
	popq	%r14
	popq	%r15
	popq	%rbp
	vzeroupper
	retq
                                        ## -- End function
	.p2align	4, 0x90                         ## -- Begin function train_cost_model.par_for.conv1_stage2.s1.c.c.c
_train_cost_model.par_for.conv1_stage2.s1.c.c.c: ## @train_cost_model.par_for.conv1_stage2.s1.c.c.c
## %bb.0:                               ## %entry
	pushq	%rbp
	pushq	%r15
	pushq	%r14
	pushq	%r13
	pushq	%r12
	pushq	%rbx
	subq	$1096, %rsp                     ## imm = 0x448
                                        ## kill: def $esi killed $esi def $rsi
	movl	(%rdx), %r9d
	movslq	4(%rdx), %rax
	movq	%rax, 640(%rsp)                 ## 8-byte Spill
	movslq	8(%rdx), %r15
	movl	20(%rdx), %r8d
	movl	24(%rdx), %r11d
	movl	32(%rdx), %edi
	movl	36(%rdx), %ebx
	movq	40(%rdx), %rcx
	movq	%rcx, -104(%rsp)                ## 8-byte Spill
	movq	56(%rdx), %rcx
	movq	%rcx, 104(%rsp)                 ## 8-byte Spill
	movq	72(%rdx), %rcx
	cmpl	%esi, 12(%rdx)
	movq	%r9, 112(%rsp)                  ## 8-byte Spill
	movq	%r15, 1056(%rsp)                ## 8-byte Spill
	jle	LBB9_31
## %bb.1:                               ## %then_bb
	movl	16(%rdx), %r10d
	movl	%esi, %edx
	sarl	%edx
	movl	%edx, %ebp
	andl	$-2, %ebp
	movl	%ebp, 128(%rsp)                 ## 4-byte Spill
	movl	%r10d, %ebp
	sarl	$3, %ebp
	shll	$3, %esi
	andl	$24, %esi
	addl	$7, %r11d
	sarl	$3, %r11d
	orl	$1, %edx
	subl	%r8d, %edx
	movq	%rdx, 136(%rsp)                 ## 8-byte Spill
	leal	(,%r15,8), %edx
	leal	(%rdx,%rdx,2), %edx
	movl	%edx, 120(%rsp)                 ## 4-byte Spill
	subl	%ebp, %r11d
	movq	%rsi, 160(%rsp)                 ## 8-byte Spill
	jle	LBB9_24
## %bb.2:                               ## %"for conv1_stage2.s1.w.wi.us.preheader"
	subl	%edi, %ebx
	movl	%r11d, %eax
	movq	%rax, 72(%rsp)                  ## 8-byte Spill
	movq	640(%rsp), %rdi                 ## 8-byte Reload
	leaq	(%rdi,%rdi), %r13
	leaq	(%r13,%r13,4), %rax
	movq	%rax, 24(%rsp)                  ## 8-byte Spill
	leaq	(%rdi,%rdi,4), %r14
	leaq	(%rdi,%r14,2), %rax
	movq	%rax, 16(%rsp)                  ## 8-byte Spill
	leaq	(,%rdi,4), %r11
	leaq	(%r11,%r11,2), %rax
	movq	%rax, 8(%rsp)                   ## 8-byte Spill
	leaq	(%rdi,%rdi,2), %r12
	leaq	(%rdi,%r12,4), %rdx
	movq	%rdx, -8(%rsp)                  ## 8-byte Spill
	movq	%rdi, %r8
	shlq	$4, %r8
	movq	%r8, %rax
	subq	%rdi, %rax
	subq	%rdi, %rax
	movq	%rax, -24(%rsp)                 ## 8-byte Spill
	leaq	(%r14,%r14,2), %rax
	movq	%rax, -32(%rsp)                 ## 8-byte Spill
	leaq	(,%r13,8), %rax
	addq	%r13, %rax
	movq	%rax, -40(%rsp)                 ## 8-byte Spill
	leaq	(%r11,%r11,4), %rax
	movq	%rax, -48(%rsp)                 ## 8-byte Spill
	leaq	(%rdi,%r14,4), %rax
	movq	%rax, -72(%rsp)                 ## 8-byte Spill
	shlq	$3, %r12
	subq	%rdi, %r12
	movq	%r12, (%rsp)                    ## 8-byte Spill
	leaq	(%r14,%r14,4), %r11
	movl	%r10d, %eax
	andl	$-8, %eax
	movl	%ebx, %edx
	subl	%eax, %edx
	movl	%edx, -64(%rsp)                 ## 4-byte Spill
	movq	%rdi, %rax
	shlq	$5, %rax
	subq	%rdi, %rax
	movq	%rax, 288(%rsp)                 ## 8-byte Spill
	subq	%rdi, %rax
	movq	%rax, 280(%rsp)                 ## 8-byte Spill
	leal	(,%rbp,8), %eax
	movl	%ebx, %edx
	movl	%eax, 272(%rsp)                 ## 4-byte Spill
	subl	%eax, %edx
	movl	%edx, 264(%rsp)                 ## 4-byte Spill
	movl	%ebp, %eax
	movq	%rax, 152(%rsp)                 ## 8-byte Spill
	movl	%esi, %eax
	movq	%rax, 144(%rsp)                 ## 8-byte Spill
	leaq	(,%rdi,8), %r14
	leaq	(%rdi,%rdi,8), %rdx
	leaq	(,%r15,4), %r13
	movq	%r8, -16(%rsp)                  ## 8-byte Spill
	leaq	(%rdi,%r8), %rax
	movq	%rax, 240(%rsp)                 ## 8-byte Spill
	leaq	(%rdi,%rdx,2), %rax
	movq	%rax, 232(%rsp)                 ## 8-byte Spill
	movq	-72(%rsp), %rax                 ## 8-byte Reload
	leaq	(%rdi,%rax), %rax
	movq	%rax, 224(%rsp)                 ## 8-byte Spill
	movq	%r14, 256(%rsp)                 ## 8-byte Spill
	leaq	(%r14,%r14,2), %rax
	movq	%rax, 216(%rsp)                 ## 8-byte Spill
	movq	%r11, -80(%rsp)                 ## 8-byte Spill
	leaq	(%r11,%rdi), %rax
	movq	%rax, 208(%rsp)                 ## 8-byte Spill
	movq	%rdx, 248(%rsp)                 ## 8-byte Spill
	leaq	(%rdx,%rdx,2), %rax
	movq	%rax, 200(%rsp)                 ## 8-byte Spill
	addq	%rdi, %rax
	movq	%rax, 192(%rsp)                 ## 8-byte Spill
	addq	%rdi, %rax
	movq	%rax, 184(%rsp)                 ## 8-byte Spill
	leaq	96(%rcx), %rax
	movq	%rax, 168(%rsp)                 ## 8-byte Spill
	movq	-104(%rsp), %rax                ## 8-byte Reload
	addq	$96, %rax
	movq	%rax, 64(%rsp)                  ## 8-byte Spill
	xorl	%edx, %edx
	xorl	%eax, %eax
	movl	%ebx, -88(%rsp)                 ## 4-byte Spill
	movl	%r10d, 32(%rsp)                 ## 4-byte Spill
	movq	%rbp, 80(%rsp)                  ## 8-byte Spill
	jmp	LBB9_3
	.p2align	4, 0x90
LBB9_14:                                ## %"end for conv1_stage2.s1.c.ci.split.us.us"
                                        ##   in Loop: Header=BB9_3 Depth=1
	movl	$1, %eax
	testb	$1, 176(%rsp)                   ## 1-byte Folded Reload
	movb	$1, %dl
	jne	LBB9_41
LBB9_3:                                 ## %"for conv1_stage2.s1.w.wi.us"
                                        ## =>This Loop Header: Depth=1
                                        ##     Child Loop BB9_4 Depth 2
                                        ##       Child Loop BB9_6 Depth 3
                                        ##       Child Loop BB9_8 Depth 3
                                        ##         Child Loop BB9_10 Depth 4
                                        ##           Child Loop BB9_18 Depth 5
                                        ##           Child Loop BB9_22 Depth 5
	movq	%rdx, 176(%rsp)                 ## 8-byte Spill
	movq	136(%rsp), %rdx                 ## 8-byte Reload
	addl	%eax, %edx
	shll	$5, %edx
	orl	128(%rsp), %eax                 ## 4-byte Folded Reload
	imull	120(%rsp), %eax                 ## 4-byte Folded Reload
	movslq	%eax, %rdi
	leaq	(%rcx,%rdi,4), %rax
	movq	%rax, -96(%rsp)                 ## 8-byte Spill
	movq	%rdx, 40(%rsp)                  ## 8-byte Spill
	leal	(%rsi,%rdx), %eax
	imull	%r9d, %eax
	movq	168(%rsp), %rdx                 ## 8-byte Reload
	movq	%rdi, 1024(%rsp)                ## 8-byte Spill
	leaq	(%rdx,%rdi,4), %rdx
	movq	%rdx, 88(%rsp)                  ## 8-byte Spill
	xorl	%edi, %edi
	jmp	LBB9_4
	.p2align	4, 0x90
LBB9_13:                                ## %"end for conv1_stage2.s1.n.n.rebased.loopexit.us.us"
                                        ##   in Loop: Header=BB9_4 Depth=2
	movq	56(%rsp), %rdi                  ## 8-byte Reload
	incq	%rdi
	movq	112(%rsp), %r9                  ## 8-byte Reload
	movq	-56(%rsp), %rax                 ## 8-byte Reload
	addl	%r9d, %eax
	cmpq	$8, %rdi
	movq	160(%rsp), %rsi                 ## 8-byte Reload
	movl	32(%rsp), %r10d                 ## 4-byte Reload
	je	LBB9_14
LBB9_4:                                 ## %"for conv1_stage2.s1.c.ci.us.us"
                                        ##   Parent Loop BB9_3 Depth=1
                                        ## =>  This Loop Header: Depth=2
                                        ##       Child Loop BB9_6 Depth 3
                                        ##       Child Loop BB9_8 Depth 3
                                        ##         Child Loop BB9_10 Depth 4
                                        ##           Child Loop BB9_18 Depth 5
                                        ##           Child Loop BB9_22 Depth 5
	movq	144(%rsp), %rdx                 ## 8-byte Reload
	addq	%rdi, %rdx
	movq	%rdx, 608(%rsp)                 ## 8-byte Spill
	movq	%rdi, 56(%rsp)                  ## 8-byte Spill
	leal	(%rdi,%rsi), %edx
	addl	40(%rsp), %edx                  ## 4-byte Folded Reload
	imull	%r9d, %edx
	movl	%edx, 48(%rsp)                  ## 4-byte Spill
	movslq	%eax, %rdx
	movq	-104(%rsp), %rax                ## 8-byte Reload
	movq	%rdx, -56(%rsp)                 ## 8-byte Spill
	leaq	(%rax,%rdx,4), %rsi
	cmpl	$7, %r10d
	jle	LBB9_7
## %bb.5:                               ## %"for conv1_stage2.s1.n.n.preheader.us.us"
                                        ##   in Loop: Header=BB9_4 Depth=2
	movq	256(%rsp), %rax                 ## 8-byte Reload
	movq	608(%rsp), %rdx                 ## 8-byte Reload
	addq	%rdx, %rax
	movq	104(%rsp), %rdi                 ## 8-byte Reload
	vbroadcastss	(%rdi,%rax,4), %ymm0
	vmovups	%ymm0, 576(%rsp)                ## 32-byte Spill
	movq	248(%rsp), %rax                 ## 8-byte Reload
	leaq	(%rax,%rdx), %rax
	vbroadcastss	(%rdi,%rax,4), %ymm0
	vmovups	%ymm0, 544(%rsp)                ## 32-byte Spill
	movq	24(%rsp), %rax                  ## 8-byte Reload
	leaq	(%rax,%rdx), %rax
	vbroadcastss	(%rdi,%rax,4), %ymm0
	vmovups	%ymm0, 512(%rsp)                ## 32-byte Spill
	movq	16(%rsp), %rax                  ## 8-byte Reload
	leaq	(%rax,%rdx), %rax
	vbroadcastss	(%rdi,%rax,4), %ymm0
	vmovups	%ymm0, 480(%rsp)                ## 32-byte Spill
	movq	8(%rsp), %rax                   ## 8-byte Reload
	leaq	(%rax,%rdx), %rax
	vbroadcastss	(%rdi,%rax,4), %ymm0
	vmovups	%ymm0, 384(%rsp)                ## 32-byte Spill
	movq	-8(%rsp), %rax                  ## 8-byte Reload
	leaq	(%rax,%rdx), %rax
	vbroadcastss	(%rdi,%rax,4), %ymm0
	vmovups	%ymm0, 448(%rsp)                ## 32-byte Spill
	movq	-24(%rsp), %rax                 ## 8-byte Reload
	leaq	(%rax,%rdx), %rax
	vbroadcastss	(%rdi,%rax,4), %ymm0
	vmovups	%ymm0, 352(%rsp)                ## 32-byte Spill
	movq	-32(%rsp), %rax                 ## 8-byte Reload
	leaq	(%rax,%rdx), %rax
	vbroadcastss	(%rdi,%rax,4), %ymm0
	vmovups	%ymm0, 416(%rsp)                ## 32-byte Spill
	movq	-16(%rsp), %rax                 ## 8-byte Reload
	leaq	(%rax,%rdx), %rax
	vbroadcastss	(%rdi,%rax,4), %ymm0
	vmovups	%ymm0, 992(%rsp)                ## 32-byte Spill
	movq	240(%rsp), %rax                 ## 8-byte Reload
	leaq	(%rax,%rdx), %rax
	vbroadcastss	(%rdi,%rax,4), %ymm0
	vmovups	%ymm0, 960(%rsp)                ## 32-byte Spill
	movq	-40(%rsp), %rax                 ## 8-byte Reload
	leaq	(%rax,%rdx), %rax
	vbroadcastss	(%rdi,%rax,4), %ymm10
	movq	232(%rsp), %rax                 ## 8-byte Reload
	leaq	(%rax,%rdx), %rax
	vbroadcastss	(%rdi,%rax,4), %ymm11
	movq	-48(%rsp), %rax                 ## 8-byte Reload
	leaq	(%rax,%rdx), %rax
	vbroadcastss	(%rdi,%rax,4), %ymm12
	movq	-72(%rsp), %rax                 ## 8-byte Reload
	leaq	(%rax,%rdx), %rax
	vbroadcastss	(%rdi,%rax,4), %ymm13
	movq	224(%rsp), %rax                 ## 8-byte Reload
	leaq	(%rax,%rdx), %rax
	vbroadcastss	(%rdi,%rax,4), %ymm14
	movq	(%rsp), %rax                    ## 8-byte Reload
	leaq	(%rax,%rdx), %rax
	vbroadcastss	(%rdi,%rax,4), %ymm15
	movq	216(%rsp), %rax                 ## 8-byte Reload
	leaq	(%rax,%rdx), %rbx
	movq	-80(%rsp), %rax                 ## 8-byte Reload
	leaq	(%rax,%rdx), %r10
	movq	208(%rsp), %rax                 ## 8-byte Reload
	leaq	(%rax,%rdx), %r11
	movq	200(%rsp), %rax                 ## 8-byte Reload
	leaq	(%rax,%rdx), %r14
	movq	192(%rsp), %rax                 ## 8-byte Reload
	leaq	(%rax,%rdx), %r15
	movq	184(%rsp), %rax                 ## 8-byte Reload
	leaq	(%rax,%rdx), %r12
	movq	280(%rsp), %rax                 ## 8-byte Reload
	leaq	(%rax,%rdx), %r8
	movq	288(%rsp), %rax                 ## 8-byte Reload
	leaq	(%rax,%rdx), %r9
	xorl	%eax, %eax
	movq	152(%rsp), %rdx                 ## 8-byte Reload
	vbroadcastss	(%rdi,%rbx,4), %ymm0
	vbroadcastss	(%rdi,%r10,4), %ymm1
	vbroadcastss	(%rdi,%r11,4), %ymm2
	movq	-96(%rsp), %rbx                 ## 8-byte Reload
	vbroadcastss	(%rdi,%r14,4), %ymm3
	vbroadcastss	(%rdi,%r15,4), %ymm4
	vbroadcastss	(%rdi,%r12,4), %ymm5
	vbroadcastss	(%rdi,%r8,4), %ymm6
	vbroadcastss	(%rdi,%r9,4), %ymm7
	.p2align	4, 0x90
LBB9_6:                                 ## %"for conv1_stage2.s1.n.n.us.us"
                                        ##   Parent Loop BB9_3 Depth=1
                                        ##     Parent Loop BB9_4 Depth=2
                                        ## =>    This Inner Loop Header: Depth=3
	leaq	(%rbx,%rax), %rdi
	vmovups	(%rbx,%rax), %ymm8
	vmovups	576(%rsp), %ymm9                ## 32-byte Reload
	vfmadd213ps	(%rsi,%rax), %ymm9, %ymm8 ## ymm8 = (ymm9 * ymm8) + mem
	vmovups	544(%rsp), %ymm9                ## 32-byte Reload
	vfmadd231ps	(%r13,%rdi), %ymm9, %ymm8 ## ymm8 = (ymm9 * mem) + ymm8
	addq	%r13, %rdi
	vmovups	512(%rsp), %ymm9                ## 32-byte Reload
	vfmadd231ps	(%r13,%rdi), %ymm9, %ymm8 ## ymm8 = (ymm9 * mem) + ymm8
	addq	%r13, %rdi
	vmovups	480(%rsp), %ymm9                ## 32-byte Reload
	vfmadd231ps	(%r13,%rdi), %ymm9, %ymm8 ## ymm8 = (ymm9 * mem) + ymm8
	addq	%r13, %rdi
	vmovups	384(%rsp), %ymm9                ## 32-byte Reload
	vfmadd231ps	(%r13,%rdi), %ymm9, %ymm8 ## ymm8 = (ymm9 * mem) + ymm8
	addq	%r13, %rdi
	vmovups	448(%rsp), %ymm9                ## 32-byte Reload
	vfmadd231ps	(%r13,%rdi), %ymm9, %ymm8 ## ymm8 = (ymm9 * mem) + ymm8
	addq	%r13, %rdi
	vmovups	352(%rsp), %ymm9                ## 32-byte Reload
	vfmadd231ps	(%r13,%rdi), %ymm9, %ymm8 ## ymm8 = (ymm9 * mem) + ymm8
	addq	%r13, %rdi
	vmovups	416(%rsp), %ymm9                ## 32-byte Reload
	vfmadd231ps	(%r13,%rdi), %ymm9, %ymm8 ## ymm8 = (ymm9 * mem) + ymm8
	addq	%r13, %rdi
	vmovups	992(%rsp), %ymm9                ## 32-byte Reload
	vfmadd231ps	(%r13,%rdi), %ymm9, %ymm8 ## ymm8 = (ymm9 * mem) + ymm8
	addq	%r13, %rdi
	vmovups	960(%rsp), %ymm9                ## 32-byte Reload
	vfmadd231ps	(%r13,%rdi), %ymm9, %ymm8 ## ymm8 = (ymm9 * mem) + ymm8
	addq	%r13, %rdi
	vfmadd231ps	(%r13,%rdi), %ymm10, %ymm8 ## ymm8 = (ymm10 * mem) + ymm8
	addq	%r13, %rdi
	vfmadd231ps	(%r13,%rdi), %ymm11, %ymm8 ## ymm8 = (ymm11 * mem) + ymm8
	addq	%r13, %rdi
	vfmadd231ps	(%r13,%rdi), %ymm12, %ymm8 ## ymm8 = (ymm12 * mem) + ymm8
	addq	%r13, %rdi
	vfmadd231ps	(%r13,%rdi), %ymm13, %ymm8 ## ymm8 = (ymm13 * mem) + ymm8
	addq	%r13, %rdi
	vfmadd231ps	(%r13,%rdi), %ymm14, %ymm8 ## ymm8 = (ymm14 * mem) + ymm8
	addq	%r13, %rdi
	vfmadd231ps	(%r13,%rdi), %ymm15, %ymm8 ## ymm8 = (ymm15 * mem) + ymm8
	addq	%r13, %rdi
	vfmadd231ps	(%r13,%rdi), %ymm0, %ymm8 ## ymm8 = (ymm0 * mem) + ymm8
	addq	%r13, %rdi
	vfmadd231ps	(%r13,%rdi), %ymm1, %ymm8 ## ymm8 = (ymm1 * mem) + ymm8
	addq	%r13, %rdi
	vfmadd231ps	(%r13,%rdi), %ymm2, %ymm8 ## ymm8 = (ymm2 * mem) + ymm8
	addq	%r13, %rdi
	vfmadd231ps	(%r13,%rdi), %ymm3, %ymm8 ## ymm8 = (ymm3 * mem) + ymm8
	addq	%r13, %rdi
	vfmadd231ps	(%r13,%rdi), %ymm4, %ymm8 ## ymm8 = (ymm4 * mem) + ymm8
	addq	%r13, %rdi
	vfmadd231ps	(%r13,%rdi), %ymm5, %ymm8 ## ymm8 = (ymm5 * mem) + ymm8
	addq	%r13, %rdi
	vfmadd231ps	(%r13,%rdi), %ymm6, %ymm8 ## ymm8 = (ymm6 * mem) + ymm8
	addq	%r13, %rdi
	vfmadd231ps	(%r13,%rdi), %ymm7, %ymm8 ## ymm8 = (ymm7 * mem) + ymm8
	vmovups	%ymm8, (%rsi,%rax)
	addq	$32, %rax
	decq	%rdx
	jne	LBB9_6
LBB9_7:                                 ## %"end for conv1_stage2.s1.n.n.us.us"
                                        ##   in Loop: Header=BB9_4 Depth=2
	movq	%rsi, 96(%rsp)                  ## 8-byte Spill
	movslq	48(%rsp), %rax                  ## 4-byte Folded Reload
	movq	%rax, 480(%rsp)                 ## 8-byte Spill
	movl	264(%rsp), %eax                 ## 4-byte Reload
	movl	-64(%rsp), %edx                 ## 4-byte Reload
	movl	272(%rsp), %esi                 ## 4-byte Reload
	movl	%esi, 384(%rsp)                 ## 4-byte Spill
	xorl	%esi, %esi
	jmp	LBB9_8
	.p2align	4, 0x90
LBB9_12:                                ## %"end for conv1_stage2.s1.r63$x3.us.us"
                                        ##   in Loop: Header=BB9_8 Depth=3
	movq	416(%rsp), %rsi                 ## 8-byte Reload
	incq	%rsi
	addl	$8, 384(%rsp)                   ## 4-byte Folded Spill
	movl	352(%rsp), %edx                 ## 4-byte Reload
	addl	$-8, %edx
	movl	448(%rsp), %eax                 ## 4-byte Reload
	addl	$-8, %eax
	cmpq	72(%rsp), %rsi                  ## 8-byte Folded Reload
	movq	80(%rsp), %rbp                  ## 8-byte Reload
	je	LBB9_13
LBB9_8:                                 ## %"for conv1_stage2.s1.n.n.rebased.us.us"
                                        ##   Parent Loop BB9_3 Depth=1
                                        ##     Parent Loop BB9_4 Depth=2
                                        ## =>    This Loop Header: Depth=3
                                        ##         Child Loop BB9_10 Depth 4
                                        ##           Child Loop BB9_18 Depth 5
                                        ##           Child Loop BB9_22 Depth 5
	cmpl	$8, %eax
	movl	$8, %edi
	movl	%eax, 448(%rsp)                 ## 4-byte Spill
	cmovll	%eax, %edi
	cmpl	$8, %edx
	movl	$8, %ebx
	movl	%edx, 352(%rsp)                 ## 4-byte Spill
	cmovll	%edx, %ebx
	leal	(,%rsi,8), %eax
	movl	-64(%rsp), %edx                 ## 4-byte Reload
	movl	%edx, %r9d
	subl	%eax, %r9d
	cmpl	$8, %r9d
	movl	$8, %eax
	cmovgel	%eax, %r9d
	movq	%rsi, 416(%rsp)                 ## 8-byte Spill
	leal	(%rsi,%rbp), %eax
	shll	$3, %eax
	movl	-88(%rsp), %edx                 ## 4-byte Reload
	subl	%eax, %edx
	testl	%edx, %edx
	jle	LBB9_12
## %bb.9:                               ## %"for conv1_stage2.s1.r63$x2.us.us.us.preheader"
                                        ##   in Loop: Header=BB9_8 Depth=3
	movslq	384(%rsp), %rsi                 ## 4-byte Folded Reload
	movq	-96(%rsp), %rdx                 ## 8-byte Reload
	leaq	(%rdx,%rsi,4), %r12
	movq	96(%rsp), %rdx                  ## 8-byte Reload
	leaq	(%rdx,%rsi,4), %r10
	movq	88(%rsp), %rdx                  ## 8-byte Reload
	leaq	(%rdx,%rsi,4), %rdx
	addq	-56(%rsp), %rsi                 ## 8-byte Folded Reload
	movq	64(%rsp), %rbp                  ## 8-byte Reload
	leaq	(,%rsi,4), %r8
	addq	%rbp, %r8
	andl	$-16, %ebx
	addq	$-16, %rbx
	shrq	$4, %rbx
	incq	%rbx
	andq	$-2, %rbx
	negq	%rbx
	movq	%rbx, 544(%rsp)                 ## 8-byte Spill
	movl	%r9d, %r15d
	andl	$-16, %r15d
	addq	$-16, %r15
	movq	%r15, %rsi
	shrq	$4, %rsi
	incq	%rsi
	movq	%rsi, 576(%rsp)                 ## 8-byte Spill
	cltq
	movq	%rax, 512(%rsp)                 ## 8-byte Spill
	movl	%r9d, %r11d
	andl	$-16, %r11d
	xorl	%eax, %eax
	jmp	LBB9_10
	.p2align	4, 0x90
LBB9_23:                                ## %"end for conv1_stage2.s1.n.ni.loopexit.us.us.us"
                                        ##   in Loop: Header=BB9_10 Depth=4
	incq	%rax
	addq	%r13, %rdx
	addq	%r13, %r12
	cmpq	$24, %rax
	je	LBB9_12
LBB9_10:                                ## %"for conv1_stage2.s1.r63$x2.us.us.us"
                                        ##   Parent Loop BB9_3 Depth=1
                                        ##     Parent Loop BB9_4 Depth=2
                                        ##       Parent Loop BB9_8 Depth=3
                                        ## =>      This Loop Header: Depth=4
                                        ##           Child Loop BB9_18 Depth 5
                                        ##           Child Loop BB9_22 Depth 5
	leaq	8(%rax), %rbp
	imulq	640(%rsp), %rbp                 ## 8-byte Folded Reload
	addq	608(%rsp), %rbp                 ## 8-byte Folded Reload
	movq	104(%rsp), %rsi                 ## 8-byte Reload
	vmovss	(%rsi,%rbp,4), %xmm0            ## xmm0 = mem[0],zero,zero,zero
	cmpl	$16, %r9d
	jae	LBB9_15
## %bb.11:                              ##   in Loop: Header=BB9_10 Depth=4
	xorl	%ebp, %ebp
	jmp	LBB9_22
	.p2align	4, 0x90
LBB9_15:                                ## %vector.ph151
                                        ##   in Loop: Header=BB9_10 Depth=4
	vbroadcastss	%xmm0, %ymm1
	testq	%r15, %r15
	je	LBB9_16
## %bb.17:                              ## %vector.body148.preheader
                                        ##   in Loop: Header=BB9_10 Depth=4
	movq	544(%rsp), %r14                 ## 8-byte Reload
	xorl	%ebp, %ebp
	.p2align	4, 0x90
LBB9_18:                                ## %vector.body148
                                        ##   Parent Loop BB9_3 Depth=1
                                        ##     Parent Loop BB9_4 Depth=2
                                        ##       Parent Loop BB9_8 Depth=3
                                        ##         Parent Loop BB9_10 Depth=4
                                        ## =>        This Inner Loop Header: Depth=5
	vmovups	-96(%rdx,%rbp,4), %ymm2
	vmovups	-64(%rdx,%rbp,4), %ymm3
	vfmadd213ps	-96(%r8,%rbp,4), %ymm1, %ymm2 ## ymm2 = (ymm1 * ymm2) + mem
	vfmadd213ps	-64(%r8,%rbp,4), %ymm1, %ymm3 ## ymm3 = (ymm1 * ymm3) + mem
	vmovups	%ymm2, -96(%r8,%rbp,4)
	vmovups	%ymm3, -64(%r8,%rbp,4)
	vmovups	-32(%rdx,%rbp,4), %ymm2
	vmovups	(%rdx,%rbp,4), %ymm3
	vfmadd213ps	-32(%r8,%rbp,4), %ymm1, %ymm2 ## ymm2 = (ymm1 * ymm2) + mem
	vfmadd213ps	(%r8,%rbp,4), %ymm1, %ymm3 ## ymm3 = (ymm1 * ymm3) + mem
	vmovups	%ymm2, -32(%r8,%rbp,4)
	vmovups	%ymm3, (%r8,%rbp,4)
	addq	$32, %rbp
	addq	$2, %r14
	jne	LBB9_18
## %bb.19:                              ## %middle.block146.unr-lcssa
                                        ##   in Loop: Header=BB9_10 Depth=4
	testb	$1, 576(%rsp)                   ## 1-byte Folded Reload
	je	LBB9_21
LBB9_20:                                ## %vector.body148.epil
                                        ##   in Loop: Header=BB9_10 Depth=4
	movq	%rax, %rbx
	imulq	1056(%rsp), %rbx                ## 8-byte Folded Reload
	addq	1024(%rsp), %rbx                ## 8-byte Folded Reload
	addq	512(%rsp), %rbp                 ## 8-byte Folded Reload
	movq	480(%rsp), %rsi                 ## 8-byte Reload
	addq	%rbp, %rsi
	addq	%rbx, %rbp
	vmovups	(%rcx,%rbp,4), %ymm2
	vmovups	32(%rcx,%rbp,4), %ymm3
	movq	-104(%rsp), %rbx                ## 8-byte Reload
	vfmadd213ps	(%rbx,%rsi,4), %ymm1, %ymm2 ## ymm2 = (ymm1 * ymm2) + mem
	vfmadd213ps	32(%rbx,%rsi,4), %ymm1, %ymm3 ## ymm3 = (ymm1 * ymm3) + mem
	vmovups	%ymm2, (%rbx,%rsi,4)
	vmovups	%ymm3, 32(%rbx,%rsi,4)
LBB9_21:                                ## %middle.block146
                                        ##   in Loop: Header=BB9_10 Depth=4
	movq	%r11, %rbp
	cmpq	%r9, %r11
	je	LBB9_23
	.p2align	4, 0x90
LBB9_22:                                ## %"for conv1_stage2.s1.n.ni.us.us.us"
                                        ##   Parent Loop BB9_3 Depth=1
                                        ##     Parent Loop BB9_4 Depth=2
                                        ##       Parent Loop BB9_8 Depth=3
                                        ##         Parent Loop BB9_10 Depth=4
                                        ## =>        This Inner Loop Header: Depth=5
	vmovss	(%r12,%rbp,4), %xmm1            ## xmm1 = mem[0],zero,zero,zero
	vfmadd213ss	(%r10,%rbp,4), %xmm0, %xmm1 ## xmm1 = (xmm0 * xmm1) + mem
	vmovss	%xmm1, (%r10,%rbp,4)
	incq	%rbp
	cmpq	%rbp, %rdi
	jne	LBB9_22
	jmp	LBB9_23
LBB9_16:                                ##   in Loop: Header=BB9_10 Depth=4
	xorl	%ebp, %ebp
	testb	$1, 576(%rsp)                   ## 1-byte Folded Reload
	jne	LBB9_20
	jmp	LBB9_21
LBB9_31:                                ## %next_bb
	movl	28(%rdx), %ebp
	movl	%esi, %edx
	sarl	%edx
	movl	%edx, %eax
	andl	$-2, %eax
	movq	%rax, 704(%rsp)                 ## 8-byte Spill
	subl	%eax, %ebp
	cmpl	$2, %ebp
	movl	$2, %r10d
	cmovll	%ebp, %r10d
	testl	%ebp, %ebp
	jle	LBB9_41
## %bb.32:                              ## %"for conv1_stage2.s1.w.wi5.preheader"
	testl	%r11d, %r11d
	jle	LBB9_41
## %bb.33:                              ## %"for conv1_stage2.s1.w.wi5.us.preheader"
	addl	$7, %r11d
	sarl	$3, %r11d
	shll	$3, %esi
	andl	$24, %esi
	leal	(,%r15,8), %ebp
	leal	(%rbp,%rbp,2), %eax
	movl	%eax, 304(%rsp)                 ## 4-byte Spill
	movl	%r11d, %eax
	movq	%rax, 96(%rsp)                  ## 8-byte Spill
	movl	%esi, %eax
	movq	%rax, 896(%rsp)                 ## 8-byte Spill
	movl	%r10d, %eax
	movq	%rax, 696(%rsp)                 ## 8-byte Spill
	movq	640(%rsp), %rbp                 ## 8-byte Reload
	leaq	(%rbp,%rbp), %r10
	leaq	(%r10,%r10,4), %rax
	movq	%rax, 888(%rsp)                 ## 8-byte Spill
	leaq	(,%rbp,4), %r11
	leaq	(%r11,%r11,2), %rax
	movq	%rax, 880(%rsp)                 ## 8-byte Spill
	leaq	(,%rbp,2), %r14
	addq	%rbp, %r14
	movq	%rbp, %rax
	shlq	$4, %rax
	movq	%rax, 344(%rsp)                 ## 8-byte Spill
	subq	%rbp, %rax
	subq	%rbp, %rax
	movq	%rax, 864(%rsp)                 ## 8-byte Spill
	leaq	(,%r15,8), %r13
	movq	%r13, %rax
	movq	%r13, 256(%rsp)                 ## 8-byte Spill
	subq	%r15, %r13
	movq	%r13, 248(%rsp)                 ## 8-byte Spill
	leaq	(%r10,%r10,8), %rax
	movq	%rax, 856(%rsp)                 ## 8-byte Spill
	leaq	(%r11,%r11,4), %rax
	movq	%rax, 848(%rsp)                 ## 8-byte Spill
	movq	%r15, %rax
	shlq	$4, %rax
	movq	%rax, %r10
	movq	%rax, 152(%rsp)                 ## 8-byte Spill
	shll	$5, %edx
	andl	$-64, %edx
	orl	%esi, %edx
	movl	%r8d, %eax
	shll	$5, %eax
	subl	%eax, %edx
	movq	%r10, %rax
	subq	%r15, %rax
	subq	%r15, %rax
	movq	%rax, 240(%rsp)                 ## 8-byte Spill
	leaq	(,%r14,4), %rax
	addq	%rbp, %rax
	movq	%rax, 840(%rsp)                 ## 8-byte Spill
	shlq	$3, %r14
	subq	%rbp, %r14
	movq	%r14, 872(%rsp)                 ## 8-byte Spill
	leaq	(,%rbp,4), %rax
	addq	%rbp, %rax
	leaq	(%rbp,%rax,2), %rsi
	movq	%rsi, 832(%rsp)                 ## 8-byte Spill
	leaq	(%rax,%rax,2), %rsi
	movq	%rsi, 824(%rsp)                 ## 8-byte Spill
	leaq	(%rbp,%rax,4), %rsi
	movq	%rsi, 336(%rsp)                 ## 8-byte Spill
	leaq	(%rax,%rax,4), %rax
	movq	%rax, 328(%rsp)                 ## 8-byte Spill
	movq	%rbp, %rax
	shlq	$5, %rax
	subq	%rbp, %rax
	movq	%rax, 816(%rsp)                 ## 8-byte Spill
	subq	%rbp, %rax
	movq	%rax, 808(%rsp)                 ## 8-byte Spill
	leaq	(%r15,%r15,2), %r14
	leaq	(,%r14,8), %rsi
	subq	%r15, %rsi
	movq	%rsi, 224(%rsp)                 ## 8-byte Spill
	movl	%ebx, %esi
	subl	%edi, %esi
	movl	%esi, -96(%rsp)                 ## 4-byte Spill
	addl	$32, %edx
	imull	%r9d, %edx
	movl	%r9d, %esi
	shll	$5, %esi
	movl	%esi, 300(%rsp)                 ## 4-byte Spill
	leaq	(,%rbp,8), %rax
	movq	%rax, 320(%rsp)                 ## 8-byte Spill
	leaq	(,%rbp,8), %r13
	addq	%rbp, %r13
	leaq	(%r15,%r15), %rax
	leaq	(,%r15,4), %r11
	leaq	(%r15,%r15,4), %r10
	leaq	(%rax,%rax,2), %r12
	movq	%r12, 192(%rsp)                 ## 8-byte Spill
	leaq	(%r15,%r15,8), %rsi
	movq	%rsi, 144(%rsp)                 ## 8-byte Spill
	movq	344(%rsp), %rsi                 ## 8-byte Reload
	leaq	(%rbp,%rsi), %r12
	movq	%r12, 792(%rsp)                 ## 8-byte Spill
	leaq	(%rax,%rax,4), %rsi
	movq	%rsi, 184(%rsp)                 ## 8-byte Spill
	leaq	(%r15,%r10,2), %rsi
	movq	%rsi, 176(%rsp)                 ## 8-byte Spill
	leaq	(%rbp,%r13,2), %rsi
	movq	%rsi, 784(%rsp)                 ## 8-byte Spill
	leaq	(%r11,%r11,2), %rsi
	movq	%rsi, 136(%rsp)                 ## 8-byte Spill
	movq	%r14, 232(%rsp)                 ## 8-byte Spill
	leaq	(%r15,%r14,4), %rsi
	movq	%rsi, 128(%rsp)                 ## 8-byte Spill
	movq	336(%rsp), %rsi                 ## 8-byte Reload
	leaq	(%rbp,%rsi), %rsi
	movq	%rsi, 776(%rsp)                 ## 8-byte Spill
	leaq	(%r10,%r10,2), %rsi
	movq	%rsi, 120(%rsp)                 ## 8-byte Spill
	movq	320(%rsp), %rsi                 ## 8-byte Reload
	leaq	(%rsi,%rsi,2), %rsi
	movq	%rsi, 768(%rsp)                 ## 8-byte Spill
	movq	152(%rsp), %rsi                 ## 8-byte Reload
	leaq	(%r15,%rsi), %rsi
	movq	%rsi, 168(%rsp)                 ## 8-byte Spill
	movq	%rax, 216(%rsp)                 ## 8-byte Spill
	leaq	(%rax,%rax,8), %rax
	movq	%rax, 952(%rsp)                 ## 8-byte Spill
	movq	328(%rsp), %rax                 ## 8-byte Reload
	leaq	(%rax,%rbp), %rax
	movq	%rax, 760(%rsp)                 ## 8-byte Spill
	movq	144(%rsp), %rax                 ## 8-byte Reload
	leaq	(%r15,%rax,2), %rax
	movq	%rax, 944(%rsp)                 ## 8-byte Spill
	movq	%r13, 800(%rsp)                 ## 8-byte Spill
	leaq	(%r13,%r13,2), %rax
	movq	%r11, 208(%rsp)                 ## 8-byte Spill
	leaq	(%r11,%r11,4), %rsi
	movq	%rsi, 936(%rsp)                 ## 8-byte Spill
	movq	%rax, 752(%rsp)                 ## 8-byte Spill
	leaq	(%rax,%rbp), %rax
	movq	%r10, 200(%rsp)                 ## 8-byte Spill
	leaq	(%r15,%r10,4), %rsi
	movq	%rax, 744(%rsp)                 ## 8-byte Spill
	leaq	(%rbp,%rax), %rax
	movq	%rax, 736(%rsp)                 ## 8-byte Spill
	movq	%rsi, 928(%rsp)                 ## 8-byte Spill
	leaq	(%r15,%rsi), %rax
	movq	%rax, 920(%rsp)                 ## 8-byte Spill
	movq	-104(%rsp), %rax                ## 8-byte Reload
	leaq	96(%rax), %rsi
	movq	%rsi, 728(%rsp)                 ## 8-byte Spill
	addq	$12, %rax
	movq	%rax, 720(%rsp)                 ## 8-byte Spill
	xorl	%eax, %eax
	movl	%ebx, -88(%rsp)                 ## 4-byte Spill
	movq	%rdi, -64(%rsp)                 ## 8-byte Spill
	movl	%r8d, 308(%rsp)                 ## 4-byte Spill
	jmp	LBB9_34
	.p2align	4, 0x90
LBB9_40:                                ## %"end for conv1_stage2.s1.c.ci9.split.us.us"
                                        ##   in Loop: Header=BB9_34 Depth=1
	movl	312(%rsp), %edx                 ## 4-byte Reload
	addl	300(%rsp), %edx                 ## 4-byte Folded Reload
	movq	712(%rsp), %rax                 ## 8-byte Reload
	cmpq	696(%rsp), %rax                 ## 8-byte Folded Reload
	movl	308(%rsp), %r8d                 ## 4-byte Reload
	je	LBB9_41
LBB9_34:                                ## %"for conv1_stage2.s1.w.wi5.us"
                                        ## =>This Loop Header: Depth=1
                                        ##     Child Loop BB9_35 Depth 2
                                        ##       Child Loop BB9_36 Depth 3
                                        ##         Child Loop BB9_44 Depth 4
                                        ##           Child Loop BB9_49 Depth 5
                                        ##           Child Loop BB9_56 Depth 5
                                        ##           Child Loop BB9_58 Depth 5
	leaq	1(%rax), %rbp
	movq	704(%rsp), %rsi                 ## 8-byte Reload
	movq	%rbp, 712(%rsp)                 ## 8-byte Spill
	addl	%esi, %ebp
	subl	%r8d, %ebp
	shll	$5, %ebp
	movq	%rbp, 904(%rsp)                 ## 8-byte Spill
	addl	%esi, %eax
	imull	304(%rsp), %eax                 ## 4-byte Folded Reload
	cltq
	movq	%rax, 608(%rsp)                 ## 8-byte Spill
	movl	%edx, 312(%rsp)                 ## 4-byte Spill
	xorl	%esi, %esi
	jmp	LBB9_35
	.p2align	4, 0x90
LBB9_39:                                ## %"end for conv1_stage2.s1.n.n12.loopexit.us.us"
                                        ##   in Loop: Header=BB9_35 Depth=2
	movq	912(%rsp), %rsi                 ## 8-byte Reload
	incq	%rsi
	movq	112(%rsp), %r9                  ## 8-byte Reload
	movl	316(%rsp), %edx                 ## 4-byte Reload
	addl	%r9d, %edx
	cmpq	$8, %rsi
	je	LBB9_40
LBB9_35:                                ## %"for conv1_stage2.s1.c.ci8.us.us"
                                        ##   Parent Loop BB9_34 Depth=1
                                        ## =>  This Loop Header: Depth=2
                                        ##       Child Loop BB9_36 Depth 3
                                        ##         Child Loop BB9_44 Depth 4
                                        ##           Child Loop BB9_49 Depth 5
                                        ##           Child Loop BB9_56 Depth 5
                                        ##           Child Loop BB9_58 Depth 5
	movl	%edx, 316(%rsp)                 ## 4-byte Spill
	movslq	%edx, %rax
	movq	728(%rsp), %rdx                 ## 8-byte Reload
	leaq	(%rdx,%rax,4), %r14
	movq	896(%rsp), %rdx                 ## 8-byte Reload
	movq	%rsi, 912(%rsp)                 ## 8-byte Spill
	addq	%rdx, %rsi
	movq	904(%rsp), %rdx                 ## 8-byte Reload
	addl	%esi, %edx
	imull	%r9d, %edx
	movq	%rdx, 992(%rsp)                 ## 8-byte Spill
	movslq	%edx, %rdx
	movq	%rdx, 352(%rsp)                 ## 8-byte Spill
	movq	320(%rsp), %rdx                 ## 8-byte Reload
	addq	%rsi, %rdx
	movq	%rdx, 960(%rsp)                 ## 8-byte Spill
	movq	800(%rsp), %rdx                 ## 8-byte Reload
	leaq	(%rdx,%rsi), %rdx
	movq	%rdx, 88(%rsp)                  ## 8-byte Spill
	movq	888(%rsp), %rdx                 ## 8-byte Reload
	leaq	(%rdx,%rsi), %rdx
	movq	%rdx, 80(%rsp)                  ## 8-byte Spill
	movq	832(%rsp), %rdx                 ## 8-byte Reload
	leaq	(%rdx,%rsi), %rdx
	movq	%rdx, 72(%rsp)                  ## 8-byte Spill
	movq	880(%rsp), %rdx                 ## 8-byte Reload
	leaq	(%rdx,%rsi), %rdx
	movq	%rdx, 64(%rsp)                  ## 8-byte Spill
	movq	840(%rsp), %rdx                 ## 8-byte Reload
	leaq	(%rdx,%rsi), %rdx
	movq	%rdx, 56(%rsp)                  ## 8-byte Spill
	movq	864(%rsp), %rdx                 ## 8-byte Reload
	leaq	(%rdx,%rsi), %rdx
	movq	%rdx, 48(%rsp)                  ## 8-byte Spill
	movq	824(%rsp), %rdx                 ## 8-byte Reload
	leaq	(%rdx,%rsi), %rdx
	movq	%rdx, 40(%rsp)                  ## 8-byte Spill
	movq	344(%rsp), %rdx                 ## 8-byte Reload
	leaq	(%rdx,%rsi), %rdx
	movq	%rdx, -72(%rsp)                 ## 8-byte Spill
	movq	-104(%rsp), %rdx                ## 8-byte Reload
	leaq	(%rdx,%rax,4), %rdx
	movq	%rdx, 160(%rsp)                 ## 8-byte Spill
	movq	720(%rsp), %rdx                 ## 8-byte Reload
	leaq	(%rdx,%rax,4), %rax
	movq	%rax, 32(%rsp)                  ## 8-byte Spill
	movq	792(%rsp), %rax                 ## 8-byte Reload
	leaq	(%rax,%rsi), %rax
	movq	%rax, 24(%rsp)                  ## 8-byte Spill
	movq	856(%rsp), %rax                 ## 8-byte Reload
	leaq	(%rax,%rsi), %rax
	movq	%rax, 16(%rsp)                  ## 8-byte Spill
	movq	784(%rsp), %rax                 ## 8-byte Reload
	leaq	(%rax,%rsi), %rax
	movq	%rax, 8(%rsp)                   ## 8-byte Spill
	movq	848(%rsp), %rax                 ## 8-byte Reload
	leaq	(%rax,%rsi), %rax
	movq	%rax, (%rsp)                    ## 8-byte Spill
	movq	336(%rsp), %rax                 ## 8-byte Reload
	leaq	(%rax,%rsi), %rax
	movq	%rax, -8(%rsp)                  ## 8-byte Spill
	movq	776(%rsp), %rax                 ## 8-byte Reload
	leaq	(%rax,%rsi), %rax
	movq	%rax, -16(%rsp)                 ## 8-byte Spill
	movq	872(%rsp), %rax                 ## 8-byte Reload
	leaq	(%rax,%rsi), %rax
	movq	%rax, -24(%rsp)                 ## 8-byte Spill
	movq	768(%rsp), %rax                 ## 8-byte Reload
	leaq	(%rax,%rsi), %rax
	movq	%rax, -32(%rsp)                 ## 8-byte Spill
	movq	328(%rsp), %rax                 ## 8-byte Reload
	leaq	(%rax,%rsi), %rax
	movq	%rax, -40(%rsp)                 ## 8-byte Spill
	movq	760(%rsp), %rax                 ## 8-byte Reload
	leaq	(%rax,%rsi), %rax
	movq	%rax, -48(%rsp)                 ## 8-byte Spill
	movq	752(%rsp), %rax                 ## 8-byte Reload
	leaq	(%rax,%rsi), %rax
	movq	%rax, -80(%rsp)                 ## 8-byte Spill
	movq	744(%rsp), %rax                 ## 8-byte Reload
	leaq	(%rax,%rsi), %rax
	movq	%rax, 288(%rsp)                 ## 8-byte Spill
	movq	736(%rsp), %rax                 ## 8-byte Reload
	addq	%rsi, %rax
	movq	%rax, 280(%rsp)                 ## 8-byte Spill
	movq	808(%rsp), %rax                 ## 8-byte Reload
	addq	%rsi, %rax
	movq	%rax, 272(%rsp)                 ## 8-byte Spill
	movq	816(%rsp), %rax                 ## 8-byte Reload
	movq	%rsi, 544(%rsp)                 ## 8-byte Spill
	addq	%rsi, %rax
	movq	%rax, 264(%rsp)                 ## 8-byte Spill
	xorl	%r12d, %r12d
	movl	-96(%rsp), %eax                 ## 4-byte Reload
	xorl	%edx, %edx
	jmp	LBB9_36
	.p2align	4, 0x90
LBB9_37:                                ## %"for conv1_stage2.s1.n.n11.split.us51.us"
                                        ##   in Loop: Header=BB9_36 Depth=3
	addq	608(%rsp), %rsi                 ## 8-byte Folded Reload
	movq	992(%rsp), %rax                 ## 8-byte Reload
	leal	(%rax,%rdx,8), %eax
	cltq
	vmovups	(%rcx,%rsi,4), %ymm0
	movq	104(%rsp), %rbp                 ## 8-byte Reload
	movq	960(%rsp), %rdx                 ## 8-byte Reload
	vbroadcastss	(%rbp,%rdx,4), %ymm1
	movq	-104(%rsp), %rdi                ## 8-byte Reload
	vfmadd213ps	(%rdi,%rax,4), %ymm0, %ymm1 ## ymm1 = (ymm0 * ymm1) + mem
	leaq	(%rsi,%r15), %rdx
	movq	88(%rsp), %rbx                  ## 8-byte Reload
	vbroadcastss	(%rbp,%rbx,4), %ymm0
	vfmadd132ps	(%rcx,%rdx,4), %ymm1, %ymm0 ## ymm0 = (ymm0 * mem) + ymm1
	movq	216(%rsp), %rdx                 ## 8-byte Reload
	leaq	(%rsi,%rdx), %rdx
	movq	80(%rsp), %rbx                  ## 8-byte Reload
	vbroadcastss	(%rbp,%rbx,4), %ymm1
	vfmadd132ps	(%rcx,%rdx,4), %ymm0, %ymm1 ## ymm1 = (ymm1 * mem) + ymm0
	movq	232(%rsp), %rdx                 ## 8-byte Reload
	leaq	(%rsi,%rdx), %rdx
	movq	72(%rsp), %rbx                  ## 8-byte Reload
	vbroadcastss	(%rbp,%rbx,4), %ymm0
	vfmadd132ps	(%rcx,%rdx,4), %ymm1, %ymm0 ## ymm0 = (ymm0 * mem) + ymm1
	movq	208(%rsp), %rdx                 ## 8-byte Reload
	leaq	(%rsi,%rdx), %rdx
	movq	64(%rsp), %rbx                  ## 8-byte Reload
	vbroadcastss	(%rbp,%rbx,4), %ymm1
	vfmadd132ps	(%rcx,%rdx,4), %ymm0, %ymm1 ## ymm1 = (ymm1 * mem) + ymm0
	movq	200(%rsp), %rdx                 ## 8-byte Reload
	leaq	(%rsi,%rdx), %rdx
	movq	56(%rsp), %rbx                  ## 8-byte Reload
	vbroadcastss	(%rbp,%rbx,4), %ymm0
	vfmadd132ps	(%rcx,%rdx,4), %ymm1, %ymm0 ## ymm0 = (ymm0 * mem) + ymm1
	movq	192(%rsp), %rdx                 ## 8-byte Reload
	leaq	(%rsi,%rdx), %rdx
	movq	48(%rsp), %rbx                  ## 8-byte Reload
	vbroadcastss	(%rbp,%rbx,4), %ymm1
	vfmadd132ps	(%rcx,%rdx,4), %ymm0, %ymm1 ## ymm1 = (ymm1 * mem) + ymm0
	movq	248(%rsp), %rdx                 ## 8-byte Reload
	leaq	(%rsi,%rdx), %rdx
	movq	40(%rsp), %rbx                  ## 8-byte Reload
	vbroadcastss	(%rbp,%rbx,4), %ymm0
	vfmadd132ps	(%rcx,%rdx,4), %ymm1, %ymm0 ## ymm0 = (ymm0 * mem) + ymm1
	movq	256(%rsp), %rdx                 ## 8-byte Reload
	leaq	(%rsi,%rdx), %rdx
	movq	-72(%rsp), %rbx                 ## 8-byte Reload
	vbroadcastss	(%rbp,%rbx,4), %ymm1
	vfmadd132ps	(%rcx,%rdx,4), %ymm0, %ymm1 ## ymm1 = (ymm1 * mem) + ymm0
	movq	144(%rsp), %rdx                 ## 8-byte Reload
	leaq	(%rsi,%rdx), %rdx
	movq	24(%rsp), %rbx                  ## 8-byte Reload
	vbroadcastss	(%rbp,%rbx,4), %ymm0
	vfmadd132ps	(%rcx,%rdx,4), %ymm1, %ymm0 ## ymm0 = (ymm0 * mem) + ymm1
	movq	184(%rsp), %rdx                 ## 8-byte Reload
	leaq	(%rsi,%rdx), %rdx
	movq	16(%rsp), %rbx                  ## 8-byte Reload
	vbroadcastss	(%rbp,%rbx,4), %ymm1
	vfmadd132ps	(%rcx,%rdx,4), %ymm0, %ymm1 ## ymm1 = (ymm1 * mem) + ymm0
	movq	176(%rsp), %rdx                 ## 8-byte Reload
	leaq	(%rsi,%rdx), %rdx
	movq	8(%rsp), %rbx                   ## 8-byte Reload
	vbroadcastss	(%rbp,%rbx,4), %ymm0
	vfmadd132ps	(%rcx,%rdx,4), %ymm1, %ymm0 ## ymm0 = (ymm0 * mem) + ymm1
	movq	136(%rsp), %rdx                 ## 8-byte Reload
	leaq	(%rsi,%rdx), %rdx
	movq	(%rsp), %rbx                    ## 8-byte Reload
	vbroadcastss	(%rbp,%rbx,4), %ymm1
	vfmadd132ps	(%rcx,%rdx,4), %ymm0, %ymm1 ## ymm1 = (ymm1 * mem) + ymm0
	movq	128(%rsp), %rdx                 ## 8-byte Reload
	leaq	(%rsi,%rdx), %rdx
	movq	-8(%rsp), %rbx                  ## 8-byte Reload
	vbroadcastss	(%rbp,%rbx,4), %ymm0
	vfmadd132ps	(%rcx,%rdx,4), %ymm1, %ymm0 ## ymm0 = (ymm0 * mem) + ymm1
	movq	240(%rsp), %rdx                 ## 8-byte Reload
	leaq	(%rsi,%rdx), %rdx
	movq	-16(%rsp), %rbx                 ## 8-byte Reload
	vbroadcastss	(%rbp,%rbx,4), %ymm1
	vfmadd132ps	(%rcx,%rdx,4), %ymm0, %ymm1 ## ymm1 = (ymm1 * mem) + ymm0
	movq	120(%rsp), %rdx                 ## 8-byte Reload
	leaq	(%rsi,%rdx), %rdx
	movq	-24(%rsp), %rbx                 ## 8-byte Reload
	vbroadcastss	(%rbp,%rbx,4), %ymm0
	vfmadd132ps	(%rcx,%rdx,4), %ymm1, %ymm0 ## ymm0 = (ymm0 * mem) + ymm1
	movq	152(%rsp), %rdx                 ## 8-byte Reload
	leaq	(%rsi,%rdx), %rdx
	movq	-32(%rsp), %rbx                 ## 8-byte Reload
	vbroadcastss	(%rbp,%rbx,4), %ymm1
	vfmadd132ps	(%rcx,%rdx,4), %ymm0, %ymm1 ## ymm1 = (ymm1 * mem) + ymm0
	movq	168(%rsp), %rdx                 ## 8-byte Reload
	leaq	(%rsi,%rdx), %rdx
	movq	-40(%rsp), %rbx                 ## 8-byte Reload
	vbroadcastss	(%rbp,%rbx,4), %ymm0
	vfmadd132ps	(%rcx,%rdx,4), %ymm1, %ymm0 ## ymm0 = (ymm0 * mem) + ymm1
	movq	952(%rsp), %rdx                 ## 8-byte Reload
	leaq	(%rsi,%rdx), %rdx
	movq	-48(%rsp), %rbx                 ## 8-byte Reload
	vbroadcastss	(%rbp,%rbx,4), %ymm1
	vfmadd132ps	(%rcx,%rdx,4), %ymm0, %ymm1 ## ymm1 = (ymm1 * mem) + ymm0
	movq	944(%rsp), %rdx                 ## 8-byte Reload
	leaq	(%rsi,%rdx), %rdx
	movq	-80(%rsp), %rbx                 ## 8-byte Reload
	vbroadcastss	(%rbp,%rbx,4), %ymm0
	vfmadd132ps	(%rcx,%rdx,4), %ymm1, %ymm0 ## ymm0 = (ymm0 * mem) + ymm1
	movq	936(%rsp), %rdx                 ## 8-byte Reload
	leaq	(%rsi,%rdx), %rdx
	movq	288(%rsp), %rbx                 ## 8-byte Reload
	vbroadcastss	(%rbp,%rbx,4), %ymm1
	vfmadd132ps	(%rcx,%rdx,4), %ymm0, %ymm1 ## ymm1 = (ymm1 * mem) + ymm0
	movq	928(%rsp), %rdx                 ## 8-byte Reload
	addq	%rsi, %rdx
	movq	280(%rsp), %rbx                 ## 8-byte Reload
	vbroadcastss	(%rbp,%rbx,4), %ymm0
	vfmadd132ps	(%rcx,%rdx,4), %ymm1, %ymm0 ## ymm0 = (ymm0 * mem) + ymm1
	movq	920(%rsp), %rdx                 ## 8-byte Reload
	addq	%rsi, %rdx
	movq	272(%rsp), %rbx                 ## 8-byte Reload
	vbroadcastss	(%rbp,%rbx,4), %ymm1
	vfmadd132ps	(%rcx,%rdx,4), %ymm0, %ymm1 ## ymm1 = (ymm1 * mem) + ymm0
	addq	224(%rsp), %rsi                 ## 8-byte Folded Reload
	movq	264(%rsp), %rdx                 ## 8-byte Reload
	vbroadcastss	(%rbp,%rdx,4), %ymm0
	vfmadd132ps	(%rcx,%rsi,4), %ymm1, %ymm0 ## ymm0 = (ymm0 * mem) + ymm1
	vmovups	%ymm0, (%rdi,%rax,4)
LBB9_38:                                ## %"end for conv1_stage2.s1.r63$x16.us.us"
                                        ##   in Loop: Header=BB9_36 Depth=3
	movq	-56(%rsp), %rdx                 ## 8-byte Reload
	incq	%rdx
	addl	$8, %r12d
	movl	416(%rsp), %eax                 ## 4-byte Reload
	addl	$-8, %eax
	cmpq	96(%rsp), %rdx                  ## 8-byte Folded Reload
	movl	-88(%rsp), %ebx                 ## 4-byte Reload
	movq	-64(%rsp), %rdi                 ## 8-byte Reload
	je	LBB9_39
LBB9_36:                                ## %"for conv1_stage2.s1.n.n11.us.us"
                                        ##   Parent Loop BB9_34 Depth=1
                                        ##     Parent Loop BB9_35 Depth=2
                                        ## =>    This Loop Header: Depth=3
                                        ##         Child Loop BB9_44 Depth 4
                                        ##           Child Loop BB9_49 Depth 5
                                        ##           Child Loop BB9_56 Depth 5
                                        ##           Child Loop BB9_58 Depth 5
	cmpl	$8, %eax
	movl	$8, %r13d
	movl	%eax, 416(%rsp)                 ## 4-byte Spill
	cmovll	%eax, %r13d
	movslq	%r12d, %r12
	leal	(,%rdx,8), %esi
	movl	-96(%rsp), %eax                 ## 4-byte Reload
	movl	%eax, %r8d
	subl	%esi, %r8d
	cmpl	$8, %r8d
	movl	$8, %eax
	cmovgel	%eax, %r8d
	leal	(%rdi,%rdx,8), %eax
	addl	$8, %eax
	cmpl	%ebx, %eax
	movq	%rdx, -56(%rsp)                 ## 8-byte Spill
	jle	LBB9_37
## %bb.42:                              ## %"for conv1_stage2.s1.n.n11.split.us.us.us"
                                        ##   in Loop: Header=BB9_36 Depth=3
	leal	(%rdi,%rdx,8), %eax
	movl	%ebx, %edx
	subl	%eax, %edx
	testl	%edx, %edx
	jle	LBB9_38
## %bb.43:                              ## %"for conv1_stage2.s1.r63$x15.us.us.us.us.preheader"
                                        ##   in Loop: Header=BB9_36 Depth=3
	movq	32(%rsp), %rax                  ## 8-byte Reload
	leaq	(%rax,%r12,4), %rbp
	movq	160(%rsp), %rax                 ## 8-byte Reload
	leaq	(%rax,%r12,4), %rdx
	movl	%r13d, %eax
	andl	$3, %eax
	negq	%rax
	movq	%rax, 1024(%rsp)                ## 8-byte Spill
	movl	%r13d, %eax
	andl	$-16, %eax
	addq	$-16, %rax
	shrq	$4, %rax
	incq	%rax
	andq	$-2, %rax
	negq	%rax
	movq	%rax, 384(%rsp)                 ## 8-byte Spill
	movl	%r8d, %eax
	andl	$-16, %eax
	addq	$-16, %rax
	movq	%rax, 512(%rsp)                 ## 8-byte Spill
	shrq	$4, %rax
	incq	%rax
	movq	%rax, 480(%rsp)                 ## 8-byte Spill
	leaq	-1(%r8), %rax
	movslq	%esi, %rdi
	movq	%rdi, 448(%rsp)                 ## 8-byte Spill
	movl	%eax, %edi
	addl	%esi, %edi
	setb	%bl
	shrq	$32, %rax
	setne	%al
	orb	%bl, %al
	movb	%al, 576(%rsp)                  ## 1-byte Spill
	movl	%r8d, %r11d
	andl	$-16, %r11d
	xorl	%r10d, %r10d
	jmp	LBB9_44
	.p2align	4, 0x90
LBB9_59:                                ## %after_bb18.loopexit.us.us.us.us
                                        ##   in Loop: Header=BB9_44 Depth=4
	incq	%r10
	cmpq	$24, %r10
	je	LBB9_38
LBB9_44:                                ## %"for conv1_stage2.s1.r63$x15.us.us.us.us"
                                        ##   Parent Loop BB9_34 Depth=1
                                        ##     Parent Loop BB9_35 Depth=2
                                        ##       Parent Loop BB9_36 Depth=3
                                        ## =>      This Loop Header: Depth=4
                                        ##           Child Loop BB9_49 Depth 5
                                        ##           Child Loop BB9_56 Depth 5
                                        ##           Child Loop BB9_58 Depth 5
	cmpl	$16, %r8d
	setb	%al
	leaq	8(%r10), %rsi
	imulq	640(%rsp), %rsi                 ## 8-byte Folded Reload
	addq	544(%rsp), %rsi                 ## 8-byte Folded Reload
	movq	%r10, %r9
	imulq	%r15, %r9
	addq	608(%rsp), %r9                  ## 8-byte Folded Reload
	movq	104(%rsp), %rdi                 ## 8-byte Reload
	vmovss	(%rdi,%rsi,4), %xmm0            ## xmm0 = mem[0],zero,zero,zero
	orb	576(%rsp), %al                  ## 1-byte Folded Reload
	je	LBB9_46
## %bb.45:                              ##   in Loop: Header=BB9_44 Depth=4
	xorl	%eax, %eax
	jmp	LBB9_54
	.p2align	4, 0x90
LBB9_46:                                ## %vector.ph
                                        ##   in Loop: Header=BB9_44 Depth=4
	vbroadcastss	%xmm0, %ymm1
	cmpq	$0, 512(%rsp)                   ## 8-byte Folded Reload
	je	LBB9_47
## %bb.48:                              ## %vector.body.preheader
                                        ##   in Loop: Header=BB9_44 Depth=4
	movq	384(%rsp), %rbx                 ## 8-byte Reload
	movq	%r12, %r15
	xorl	%eax, %eax
	.p2align	4, 0x90
LBB9_49:                                ## %vector.body
                                        ##   Parent Loop BB9_34 Depth=1
                                        ##     Parent Loop BB9_35 Depth=2
                                        ##       Parent Loop BB9_36 Depth=3
                                        ##         Parent Loop BB9_44 Depth=4
                                        ## =>        This Inner Loop Header: Depth=5
	movl	%r15d, %esi
	andl	$-8, %esi
	addq	%r9, %rsi
	vmovups	(%rcx,%rsi,4), %ymm2
	vmovups	32(%rcx,%rsi,4), %ymm3
	vfmadd213ps	-96(%r14,%r15,4), %ymm1, %ymm2 ## ymm2 = (ymm1 * ymm2) + mem
	vfmadd213ps	-64(%r14,%r15,4), %ymm1, %ymm3 ## ymm3 = (ymm1 * ymm3) + mem
	vmovups	%ymm2, -96(%r14,%r15,4)
	vmovups	%ymm3, -64(%r14,%r15,4)
	leal	16(%r15), %esi
	andl	$-8, %esi
	addq	%r9, %rsi
	vmovups	(%rcx,%rsi,4), %ymm2
	vmovups	32(%rcx,%rsi,4), %ymm3
	vfmadd213ps	-32(%r14,%r15,4), %ymm1, %ymm2 ## ymm2 = (ymm1 * ymm2) + mem
	vfmadd213ps	(%r14,%r15,4), %ymm1, %ymm3 ## ymm3 = (ymm1 * ymm3) + mem
	vmovups	%ymm2, -32(%r14,%r15,4)
	vmovups	%ymm3, (%r14,%r15,4)
	addq	$-32, %rax
	addq	$32, %r15
	addq	$2, %rbx
	jne	LBB9_49
## %bb.50:                              ## %middle.block.unr-lcssa.loopexit
                                        ##   in Loop: Header=BB9_44 Depth=4
	negq	%rax
	movq	1056(%rsp), %r15                ## 8-byte Reload
	testb	$1, 480(%rsp)                   ## 1-byte Folded Reload
	je	LBB9_53
LBB9_52:                                ## %vector.body.epil
                                        ##   in Loop: Header=BB9_44 Depth=4
	addq	448(%rsp), %rax                 ## 8-byte Folded Reload
	movq	352(%rsp), %rsi                 ## 8-byte Reload
	addq	%rax, %rsi
	andl	$-8, %eax
	addq	%r9, %rax
	vmovups	(%rcx,%rax,4), %ymm2
	vmovups	32(%rcx,%rax,4), %ymm3
	movq	-104(%rsp), %rax                ## 8-byte Reload
	vfmadd213ps	(%rax,%rsi,4), %ymm1, %ymm2 ## ymm2 = (ymm1 * ymm2) + mem
	vfmadd213ps	32(%rax,%rsi,4), %ymm1, %ymm3 ## ymm3 = (ymm1 * ymm3) + mem
	vmovups	%ymm2, (%rax,%rsi,4)
	vmovups	%ymm3, 32(%rax,%rsi,4)
LBB9_53:                                ## %middle.block
                                        ##   in Loop: Header=BB9_44 Depth=4
	movq	%r11, %rax
	cmpq	%r8, %r11
	je	LBB9_59
LBB9_54:                                ## %"for conv1_stage2.s1.n.ni21.us.us.us.us.preheader"
                                        ##   in Loop: Header=BB9_44 Depth=4
	movq	%rax, %rbx
	notq	%rbx
	addq	%r8, %rbx
	testb	$3, %r8b
	je	LBB9_57
## %bb.55:                              ## %"for conv1_stage2.s1.n.ni21.us.us.us.us.prol.preheader"
                                        ##   in Loop: Header=BB9_44 Depth=4
	movq	1024(%rsp), %rsi                ## 8-byte Reload
	.p2align	4, 0x90
LBB9_56:                                ## %"for conv1_stage2.s1.n.ni21.us.us.us.us.prol"
                                        ##   Parent Loop BB9_34 Depth=1
                                        ##     Parent Loop BB9_35 Depth=2
                                        ##       Parent Loop BB9_36 Depth=3
                                        ##         Parent Loop BB9_44 Depth=4
                                        ## =>        This Inner Loop Header: Depth=5
	leal	(%r12,%rax), %edi
	addq	%r9, %rdi
	vmovss	(%rcx,%rdi,4), %xmm1            ## xmm1 = mem[0],zero,zero,zero
	vfmadd213ss	(%rdx,%rax,4), %xmm0, %xmm1 ## xmm1 = (xmm0 * xmm1) + mem
	vmovss	%xmm1, (%rdx,%rax,4)
	incq	%rax
	incq	%rsi
	jne	LBB9_56
LBB9_57:                                ## %"for conv1_stage2.s1.n.ni21.us.us.us.us.prol.loopexit"
                                        ##   in Loop: Header=BB9_44 Depth=4
	cmpq	$3, %rbx
	jb	LBB9_59
	.p2align	4, 0x90
LBB9_58:                                ## %"for conv1_stage2.s1.n.ni21.us.us.us.us"
                                        ##   Parent Loop BB9_34 Depth=1
                                        ##     Parent Loop BB9_35 Depth=2
                                        ##       Parent Loop BB9_36 Depth=3
                                        ##         Parent Loop BB9_44 Depth=4
                                        ## =>        This Inner Loop Header: Depth=5
	leaq	(%r12,%rax), %rbx
	movl	%ebx, %esi
	addq	%r9, %rsi
	vmovss	(%rcx,%rsi,4), %xmm1            ## xmm1 = mem[0],zero,zero,zero
	vfmadd213ss	-12(%rbp,%rax,4), %xmm0, %xmm1 ## xmm1 = (xmm0 * xmm1) + mem
	vmovss	%xmm1, -12(%rbp,%rax,4)
	leal	1(%rbx), %esi
	addq	%r9, %rsi
	vmovss	(%rcx,%rsi,4), %xmm1            ## xmm1 = mem[0],zero,zero,zero
	vfmadd213ss	-8(%rbp,%rax,4), %xmm0, %xmm1 ## xmm1 = (xmm0 * xmm1) + mem
	vmovss	%xmm1, -8(%rbp,%rax,4)
	leal	2(%rbx), %esi
	addq	%r9, %rsi
	vmovss	(%rcx,%rsi,4), %xmm1            ## xmm1 = mem[0],zero,zero,zero
	vfmadd213ss	-4(%rbp,%rax,4), %xmm0, %xmm1 ## xmm1 = (xmm0 * xmm1) + mem
	vmovss	%xmm1, -4(%rbp,%rax,4)
	addl	$3, %ebx
	addq	%r9, %rbx
	vmovss	(%rcx,%rbx,4), %xmm1            ## xmm1 = mem[0],zero,zero,zero
	vfmadd213ss	(%rbp,%rax,4), %xmm0, %xmm1 ## xmm1 = (xmm0 * xmm1) + mem
	vmovss	%xmm1, (%rbp,%rax,4)
	addq	$4, %rax
	cmpq	%rax, %r13
	jne	LBB9_58
	jmp	LBB9_59
LBB9_47:                                ##   in Loop: Header=BB9_44 Depth=4
	xorl	%eax, %eax
	testb	$1, 480(%rsp)                   ## 1-byte Folded Reload
	jne	LBB9_52
	jmp	LBB9_53
LBB9_24:                                ## %then_bb.split
	cmpl	$8, %r10d
	jl	LBB9_41
## %bb.25:                              ## %"for conv1_stage2.s1.w.wi.us83.preheader"
	movl	%ebp, %eax
	movq	%rax, 416(%rsp)                 ## 8-byte Spill
	movq	640(%rsp), %rbp                 ## 8-byte Reload
	movq	%rbp, %r8
	addq	%rbp, %r8
	leaq	(%r8,%r8,4), %rdx
	movq	%rdx, -56(%rsp)                 ## 8-byte Spill
	leaq	(,%rbp,4), %rdx
	addq	%rbp, %rdx
	leaq	(,%rdx,2), %rdi
	addq	%rbp, %rdi
	movq	%rdi, -96(%rsp)                 ## 8-byte Spill
	shlq	$2, %r15
	leaq	(,%rbp,4), %r10
	leaq	(%r10,%r10,2), %rdi
	movq	%rdi, -64(%rsp)                 ## 8-byte Spill
	leaq	(,%rbp,2), %rdi
	addq	%rbp, %rdi
	leaq	(,%rdi,4), %rax
	addq	%rbp, %rax
	movq	%rax, 992(%rsp)                 ## 8-byte Spill
	movq	%rbp, %rax
	shlq	$4, %rax
	movq	%rax, %rbx
	subq	%rbp, %rbx
	subq	%rbp, %rbx
	movq	%rbx, 88(%rsp)                  ## 8-byte Spill
	leaq	(%rdx,%rdx,2), %rbx
	movq	%rbx, 80(%rsp)                  ## 8-byte Spill
	leaq	(%r8,%r8,8), %rbx
	movq	%rbx, 72(%rsp)                  ## 8-byte Spill
	leaq	(%r10,%r10,4), %rbx
	movq	%rbx, 64(%rsp)                  ## 8-byte Spill
	leaq	(,%rdx,4), %r8
	addq	%rbp, %r8
	shlq	$3, %rdi
	subq	%rbp, %rdi
	movq	%rdi, 96(%rsp)                  ## 8-byte Spill
	leaq	(%rdx,%rdx,4), %rdx
	movq	%rbp, %rdi
	shlq	$5, %rdi
	subq	%rbp, %rdi
	movq	%rdi, 40(%rsp)                  ## 8-byte Spill
	subq	%rbp, %rdi
	movq	%rdi, -72(%rsp)                 ## 8-byte Spill
	leaq	(,%rbp,8), %rdi
	leaq	(,%rbp,8), %rbx
	addq	%rbp, %rbx
	movq	%rax, 960(%rsp)                 ## 8-byte Spill
	leaq	(%rbp,%rax), %rax
	movq	%rax, 16(%rsp)                  ## 8-byte Spill
	leaq	(%rbp,%rbx,2), %rax
	movq	%rax, 8(%rsp)                   ## 8-byte Spill
	movq	%r8, 56(%rsp)                   ## 8-byte Spill
	leaq	(%rbp,%r8), %rax
	movq	%rax, (%rsp)                    ## 8-byte Spill
	movq	%rdx, 48(%rsp)                  ## 8-byte Spill
	leaq	(%rdx,%rbp), %rax
	movq	%rax, -8(%rsp)                  ## 8-byte Spill
	movq	%rbx, 24(%rsp)                  ## 8-byte Spill
	leaq	(%rbx,%rbx,2), %rax
	movq	%rax, -16(%rsp)                 ## 8-byte Spill
	addq	%rbp, %rax
	movq	%rax, -24(%rsp)                 ## 8-byte Spill
	addq	%rbp, %rax
	movq	%rax, -32(%rsp)                 ## 8-byte Spill
	movl	%esi, %eax
	movq	%rax, -40(%rsp)                 ## 8-byte Spill
	movq	%rdi, 32(%rsp)                  ## 8-byte Spill
	leaq	(%rdi,%rdi,2), %rax
	movq	%rax, -48(%rsp)                 ## 8-byte Spill
	xorl	%eax, %eax
	movq	%rax, -80(%rsp)                 ## 8-byte Spill
	xorl	%eax, %eax
	movq	104(%rsp), %r11                 ## 8-byte Reload
LBB9_26:                                ## %"for conv1_stage2.s1.w.wi.us83"
                                        ## =>This Loop Header: Depth=1
                                        ##     Child Loop BB9_27 Depth 2
                                        ##       Child Loop BB9_28 Depth 3
	movq	136(%rsp), %rdx                 ## 8-byte Reload
	addl	%eax, %edx
	shll	$5, %edx
	orl	128(%rsp), %eax                 ## 4-byte Folded Reload
	imull	120(%rsp), %eax                 ## 4-byte Folded Reload
	cltq
	leaq	(%rcx,%rax,4), %r14
	orl	%esi, %edx
	imull	%r9d, %edx
	movl	%edx, 352(%rsp)                 ## 4-byte Spill
	xorl	%eax, %eax
	movq	%rax, -88(%rsp)                 ## 8-byte Spill
LBB9_27:                                ## %"for conv1_stage2.s1.c.ci.us62.us"
                                        ##   Parent Loop BB9_26 Depth=1
                                        ## =>  This Loop Header: Depth=2
                                        ##       Child Loop BB9_28 Depth 3
	movslq	352(%rsp), %rax                 ## 4-byte Folded Reload
	movq	-104(%rsp), %rdx                ## 8-byte Reload
	leaq	(%rdx,%rax,4), %r12
	movq	-40(%rsp), %rax                 ## 8-byte Reload
	movq	-88(%rsp), %rdx                 ## 8-byte Reload
	leaq	(%rdx,%rax), %rsi
	movq	32(%rsp), %rax                  ## 8-byte Reload
	addq	%rsi, %rax
	vbroadcastss	(%r11,%rax,4), %ymm0
	vmovups	%ymm0, 640(%rsp)                ## 32-byte Spill
	movq	24(%rsp), %rax                  ## 8-byte Reload
	leaq	(%rax,%rsi), %rax
	vbroadcastss	(%r11,%rax,4), %ymm0
	vmovups	%ymm0, 608(%rsp)                ## 32-byte Spill
	movq	-56(%rsp), %rax                 ## 8-byte Reload
	leaq	(%rax,%rsi), %rax
	vbroadcastss	(%r11,%rax,4), %ymm0
	vmovups	%ymm0, 576(%rsp)                ## 32-byte Spill
	movq	-96(%rsp), %rax                 ## 8-byte Reload
	leaq	(%rax,%rsi), %rax
	vbroadcastss	(%r11,%rax,4), %ymm0
	vmovups	%ymm0, 544(%rsp)                ## 32-byte Spill
	movq	-64(%rsp), %rax                 ## 8-byte Reload
	leaq	(%rax,%rsi), %rax
	vbroadcastss	(%r11,%rax,4), %ymm0
	vmovups	%ymm0, 1056(%rsp)               ## 32-byte Spill
	movq	992(%rsp), %rax                 ## 8-byte Reload
	leaq	(%rax,%rsi), %rax
	vbroadcastss	(%r11,%rax,4), %ymm0
	vmovups	%ymm0, 512(%rsp)                ## 32-byte Spill
	movq	88(%rsp), %rax                  ## 8-byte Reload
	leaq	(%rax,%rsi), %rax
	vbroadcastss	(%r11,%rax,4), %ymm0
	vmovups	%ymm0, 480(%rsp)                ## 32-byte Spill
	movq	80(%rsp), %rax                  ## 8-byte Reload
	leaq	(%rax,%rsi), %rax
	vbroadcastss	(%r11,%rax,4), %ymm0
	vmovups	%ymm0, 1024(%rsp)               ## 32-byte Spill
	movq	960(%rsp), %rax                 ## 8-byte Reload
	leaq	(%rax,%rsi), %rax
	vbroadcastss	(%r11,%rax,4), %ymm0
	vmovups	%ymm0, 384(%rsp)                ## 32-byte Spill
	movq	16(%rsp), %rax                  ## 8-byte Reload
	leaq	(%rax,%rsi), %rax
	vbroadcastss	(%r11,%rax,4), %ymm0
	vmovups	%ymm0, 448(%rsp)                ## 32-byte Spill
	movq	72(%rsp), %rax                  ## 8-byte Reload
	leaq	(%rax,%rsi), %rax
	vbroadcastss	(%r11,%rax,4), %ymm10
	movq	8(%rsp), %rax                   ## 8-byte Reload
	leaq	(%rax,%rsi), %rax
	vbroadcastss	(%r11,%rax,4), %ymm11
	movq	64(%rsp), %rax                  ## 8-byte Reload
	leaq	(%rax,%rsi), %rax
	vbroadcastss	(%r11,%rax,4), %ymm12
	movq	56(%rsp), %rax                  ## 8-byte Reload
	leaq	(%rax,%rsi), %rax
	vbroadcastss	(%r11,%rax,4), %ymm13
	movq	(%rsp), %rax                    ## 8-byte Reload
	leaq	(%rax,%rsi), %rax
	vbroadcastss	(%r11,%rax,4), %ymm14
	movq	96(%rsp), %rax                  ## 8-byte Reload
	leaq	(%rax,%rsi), %rax
	vbroadcastss	(%r11,%rax,4), %ymm15
	movq	-48(%rsp), %rax                 ## 8-byte Reload
	leaq	(%rax,%rsi), %r13
	movq	48(%rsp), %rax                  ## 8-byte Reload
	leaq	(%rax,%rsi), %rbx
	movq	-8(%rsp), %rax                  ## 8-byte Reload
	leaq	(%rax,%rsi), %rdi
	movq	-16(%rsp), %rax                 ## 8-byte Reload
	leaq	(%rax,%rsi), %rbp
	movq	-24(%rsp), %rax                 ## 8-byte Reload
	leaq	(%rax,%rsi), %rdx
	movq	-32(%rsp), %rax                 ## 8-byte Reload
	leaq	(%rax,%rsi), %r8
	movq	-72(%rsp), %rax                 ## 8-byte Reload
	leaq	(%rax,%rsi), %r10
	addq	40(%rsp), %rsi                  ## 8-byte Folded Reload
	xorl	%eax, %eax
	movq	416(%rsp), %r9                  ## 8-byte Reload
	vbroadcastss	(%r11,%r13,4), %ymm0
	vbroadcastss	(%r11,%rbx,4), %ymm1
	vbroadcastss	(%r11,%rdi,4), %ymm2
	vbroadcastss	(%r11,%rbp,4), %ymm3
	vbroadcastss	(%r11,%rdx,4), %ymm4
	vbroadcastss	(%r11,%r8,4), %ymm5
	vbroadcastss	(%r11,%r10,4), %ymm6
	vbroadcastss	(%r11,%rsi,4), %ymm7
LBB9_28:                                ## %"for conv1_stage2.s1.n.n.us67.us"
                                        ##   Parent Loop BB9_26 Depth=1
                                        ##     Parent Loop BB9_27 Depth=2
                                        ## =>    This Inner Loop Header: Depth=3
	leaq	(%r14,%rax), %r13
	vmovups	(%r14,%rax), %ymm8
	vmovups	640(%rsp), %ymm9                ## 32-byte Reload
	vfmadd213ps	(%r12,%rax), %ymm9, %ymm8 ## ymm8 = (ymm9 * ymm8) + mem
	movq	%r15, %rdx
	vmovups	608(%rsp), %ymm9                ## 32-byte Reload
	vfmadd231ps	(%r15,%r13), %ymm9, %ymm8 ## ymm8 = (ymm9 * mem) + ymm8
	addq	%r15, %r13
	vmovups	576(%rsp), %ymm9                ## 32-byte Reload
	vfmadd231ps	(%r15,%r13), %ymm9, %ymm8 ## ymm8 = (ymm9 * mem) + ymm8
	addq	%r15, %r13
	vmovups	544(%rsp), %ymm9                ## 32-byte Reload
	vfmadd231ps	(%r15,%r13), %ymm9, %ymm8 ## ymm8 = (ymm9 * mem) + ymm8
	addq	%r15, %r13
	vmovups	1056(%rsp), %ymm9               ## 32-byte Reload
	vfmadd231ps	(%r15,%r13), %ymm9, %ymm8 ## ymm8 = (ymm9 * mem) + ymm8
	addq	%r15, %r13
	vmovups	512(%rsp), %ymm9                ## 32-byte Reload
	vfmadd231ps	(%r15,%r13), %ymm9, %ymm8 ## ymm8 = (ymm9 * mem) + ymm8
	addq	%r15, %r13
	vmovups	480(%rsp), %ymm9                ## 32-byte Reload
	vfmadd231ps	(%r15,%r13), %ymm9, %ymm8 ## ymm8 = (ymm9 * mem) + ymm8
	addq	%r15, %r13
	vmovups	1024(%rsp), %ymm9               ## 32-byte Reload
	vfmadd231ps	(%r15,%r13), %ymm9, %ymm8 ## ymm8 = (ymm9 * mem) + ymm8
	addq	%r15, %r13
	vmovups	384(%rsp), %ymm9                ## 32-byte Reload
	vfmadd231ps	(%r15,%r13), %ymm9, %ymm8 ## ymm8 = (ymm9 * mem) + ymm8
	addq	%r15, %r13
	vmovups	448(%rsp), %ymm9                ## 32-byte Reload
	vfmadd231ps	(%r15,%r13), %ymm9, %ymm8 ## ymm8 = (ymm9 * mem) + ymm8
	addq	%r15, %r13
	vfmadd231ps	(%r15,%r13), %ymm10, %ymm8 ## ymm8 = (ymm10 * mem) + ymm8
	addq	%r15, %r13
	vfmadd231ps	(%r15,%r13), %ymm11, %ymm8 ## ymm8 = (ymm11 * mem) + ymm8
	addq	%r15, %r13
	vfmadd231ps	(%r15,%r13), %ymm12, %ymm8 ## ymm8 = (ymm12 * mem) + ymm8
	addq	%r15, %r13
	vfmadd231ps	(%r15,%r13), %ymm13, %ymm8 ## ymm8 = (ymm13 * mem) + ymm8
	addq	%r15, %r13
	vfmadd231ps	(%r15,%r13), %ymm14, %ymm8 ## ymm8 = (ymm14 * mem) + ymm8
	addq	%r15, %r13
	vfmadd231ps	(%r15,%r13), %ymm15, %ymm8 ## ymm8 = (ymm15 * mem) + ymm8
	addq	%r15, %r13
	vfmadd231ps	(%r15,%r13), %ymm0, %ymm8 ## ymm8 = (ymm0 * mem) + ymm8
	addq	%r15, %r13
	vfmadd231ps	(%r15,%r13), %ymm1, %ymm8 ## ymm8 = (ymm1 * mem) + ymm8
	addq	%r15, %r13
	vfmadd231ps	(%r15,%r13), %ymm2, %ymm8 ## ymm8 = (ymm2 * mem) + ymm8
	addq	%r15, %r13
	vfmadd231ps	(%r15,%r13), %ymm3, %ymm8 ## ymm8 = (ymm3 * mem) + ymm8
	addq	%r15, %r13
	vfmadd231ps	(%r15,%r13), %ymm4, %ymm8 ## ymm8 = (ymm4 * mem) + ymm8
	addq	%r15, %r13
	vfmadd231ps	(%r15,%r13), %ymm5, %ymm8 ## ymm8 = (ymm5 * mem) + ymm8
	addq	%r15, %r13
	vfmadd231ps	(%r15,%r13), %ymm6, %ymm8 ## ymm8 = (ymm6 * mem) + ymm8
	addq	%r15, %r13
	vfmadd231ps	(%r15,%r13), %ymm7, %ymm8 ## ymm8 = (ymm7 * mem) + ymm8
	vmovups	%ymm8, (%r12,%rax)
	addq	$32, %rax
	decq	%r9
	jne	LBB9_28
## %bb.29:                              ## %"end for conv1_stage2.s1.n.n.loopexit.us80.us"
                                        ##   in Loop: Header=BB9_27 Depth=2
	movq	-88(%rsp), %rdx                 ## 8-byte Reload
	incq	%rdx
	movl	352(%rsp), %eax                 ## 4-byte Reload
	addl	112(%rsp), %eax                 ## 4-byte Folded Reload
	movl	%eax, 352(%rsp)                 ## 4-byte Spill
	movq	%rdx, %rax
	movq	%rdx, -88(%rsp)                 ## 8-byte Spill
	cmpq	$8, %rdx
	jne	LBB9_27
## %bb.30:                              ## %"end for conv1_stage2.s1.c.ci.split.split.us.us"
                                        ##   in Loop: Header=BB9_26 Depth=1
	movl	$1, %eax
	testb	$1, -80(%rsp)                   ## 1-byte Folded Reload
	movb	$1, %dl
	movq	%rdx, -80(%rsp)                 ## 8-byte Spill
	movq	112(%rsp), %r9                  ## 8-byte Reload
	movq	160(%rsp), %rsi                 ## 8-byte Reload
	je	LBB9_26
LBB9_41:                                ## %destructor_block
	xorl	%eax, %eax
	addq	$1096, %rsp                     ## imm = 0x448
	popq	%rbx
	popq	%r12
	popq	%r13
	popq	%r14
	popq	%r15
	popq	%rbp
	vzeroupper
	retq
                                        ## -- End function
	.p2align	4, 0x90                         ## -- Begin function train_cost_model.par_for.f1.s0.n.n
_train_cost_model.par_for.f1.s0.n.n:    ## @train_cost_model.par_for.f1.s0.n.n
## %bb.0:                               ## %entry
	pushq	%rax
                                        ## kill: def $esi killed $esi def $rsi
	movslq	4(%rdx), %rcx
	movq	16(%rdx), %rax
	cmpl	%esi, (%rdx)
	jle	LBB10_2
## %bb.1:                               ## %then_bb
	leal	(%rcx,%rsi,8), %edx
	movslq	%edx, %rdx
	subq	%rcx, %rdx
	vxorps	%xmm0, %xmm0, %xmm0
	vmovups	%ymm0, (%rax,%rdx,4)
LBB10_4:                                ## %destructor_block
	xorl	%eax, %eax
	popq	%rcx
	vzeroupper
	retq
LBB10_2:                                ## %next_bb
	movl	8(%rdx), %edx
	leal	(%rcx,%rsi,8), %ecx
	subl	%ecx, %edx
	testl	%edx, %edx
	jle	LBB10_4
## %bb.3:                               ## %"for f1.s0.n.ni.preheader"
	shll	$3, %esi
	cmpl	$8, %edx
	movl	$8, %ecx
	cmovll	%edx, %ecx
	movslq	%esi, %rdx
	leaq	(%rax,%rdx,4), %rdi
	decl	%ecx
	leaq	4(,%rcx,4), %rdx
	xorl	%esi, %esi
	callq	_memset
	xorl	%eax, %eax
	popq	%rcx
	retq
                                        ## -- End function
	.section	__TEXT,__literal4,4byte_literals
	.p2align	2                               ## -- Begin function train_cost_model.par_for.f1.s1.n.n
LCPI11_0:
	.long	0x3f800000                      ## float 1
LCPI11_1:
	.long	0x45800000                      ## float 4096
LCPI11_2:
	.long	0x40000000                      ## float 2
LCPI11_3:
	.long	0xbf800000                      ## float -1
LCPI11_4:
	.long	0x3089705f                      ## float 9.99999971E-10
	.section	__TEXT,__literal16,16byte_literals
	.p2align	4
LCPI11_5:
	.long	24                              ## 0x18
	.long	22                              ## 0x16
	.long	25                              ## 0x19
	.long	9                               ## 0x9
LCPI11_6:
	.long	21                              ## 0x15
	.long	17                              ## 0x11
	.long	12                              ## 0xc
	.long	29                              ## 0x1d
LCPI11_7:
	.long	33                              ## 0x21
	.long	27                              ## 0x1b
	.long	28                              ## 0x1c
	.long	26                              ## 0x1a
	.section	__TEXT,__text,regular,pure_instructions
	.p2align	4, 0x90
_train_cost_model.par_for.f1.s1.n.n:    ## @train_cost_model.par_for.f1.s1.n.n
## %bb.0:                               ## %entry
	pushq	%rbp
	pushq	%r15
	pushq	%r14
	pushq	%r13
	pushq	%r12
	pushq	%rbx
	subq	$680, %rsp                      ## imm = 0x2A8
                                        ## kill: def $esi killed $esi def $rsi
	movslq	(%rdx), %r8
	movl	4(%rdx), %ebx
	movl	8(%rdx), %ebp
	movslq	12(%rdx), %r14
	movl	20(%rdx), %edi
	movl	24(%rdx), %r12d
	vmovss	28(%rdx), %xmm0                 ## xmm0 = mem[0],zero,zero,zero
	vmovaps	%xmm0, 448(%rsp)                ## 16-byte Spill
	movq	40(%rdx), %rcx
	movq	56(%rdx), %rax
	movq	%rax, 24(%rsp)                  ## 8-byte Spill
	movq	72(%rdx), %rax
	movq	%rax, 208(%rsp)                 ## 8-byte Spill
	cmpl	%esi, 16(%rdx)
	movq	%rbx, 264(%rsp)                 ## 8-byte Spill
	movq	%r14, 216(%rsp)                 ## 8-byte Spill
	movq	%rcx, 200(%rsp)                 ## 8-byte Spill
	jle	LBB11_5
## %bb.1:                               ## %then_bb
	testl	%ebx, %ebx
	jle	LBB11_20
## %bb.2:                               ## %"for f1.s1.r79$x.preheader"
	movq	%r8, %r9
	movl	%esi, %r8d
	shll	$3, %r8d
	leal	(%rdi,%rsi,8), %edi
	movl	%ebp, %r11d
	subl	%r12d, %r11d
	addl	%edi, %r11d
	leal	(%rbp,%rbp), %edx
	leal	(%rdx,%rdx,4), %r15d
	subl	%r12d, %r15d
	addl	%edi, %r15d
	leal	(%rbp,%rbp,4), %esi
	leal	(%rbp,%rsi,2), %r13d
	subl	%r12d, %r13d
	addl	%edi, %r13d
	leal	(%rdx,%rdx,8), %edx
	subl	%r12d, %edx
	addl	%edi, %edx
	leal	(%rbp,%rbp,8), %esi
	leal	(%rbp,%rsi,2), %esi
	subl	%r12d, %esi
	addl	%edi, %esi
	vmovd	%ebp, %xmm0
	vpbroadcastd	%xmm0, %xmm1
	vpmulld	LCPI11_5(%rip), %xmm1, %xmm0
	vmovd	%r12d, %xmm2
	vpbroadcastd	%xmm2, %xmm3
	vpsubd	%xmm3, %xmm0, %xmm0
	vmovd	%edi, %xmm2
	vpbroadcastd	%xmm2, %xmm4
	vpaddd	%xmm4, %xmm0, %xmm2
	vpmulld	LCPI11_6(%rip), %xmm1, %xmm0
	vpsubd	%xmm3, %xmm0, %xmm0
	vpaddd	%xmm4, %xmm0, %xmm0
	movl	%ebp, %ecx
	shll	$5, %ecx
	movl	%ecx, %r14d
	leal	(%rcx,%rbp,2), %r10d
	movl	%ecx, %ebx
	subl	%ebp, %ebx
	movl	%ebx, %eax
	subl	%ebp, %eax
	subl	%r12d, %eax
	addl	%edi, %eax
	subl	%r12d, %ebx
	addl	%edi, %ebx
	subl	%r12d, %r14d
	addl	%edi, %r14d
	vpmulld	LCPI11_7(%rip), %xmm1, %xmm1
	vpsubd	%xmm3, %xmm1, %xmm1
	vpaddd	%xmm4, %xmm1, %xmm1
	subl	%r12d, %r10d
	addl	%edi, %r10d
	subl	%r12d, %edi
	movslq	%edi, %rcx
	movq	%rcx, 320(%rsp)                 ## 8-byte Spill
	vpmovsxdq	%xmm2, %ymm2
	vmovdqu	%ymm2, 288(%rsp)                ## 32-byte Spill
	movslq	%r15d, %rcx
	movq	%rcx, 224(%rsp)                 ## 8-byte Spill
	vbroadcastss	448(%rsp), %ymm2        ## 16-byte Folded Reload
	vmovups	%ymm2, 352(%rsp)                ## 32-byte Spill
	cltq
	movq	%rax, 416(%rsp)                 ## 8-byte Spill
	movslq	%r14d, %r12
	movslq	%ebx, %r15
	movslq	%r11d, %r14
	movslq	%r8d, %rdi
	vpmovsxdq	%xmm0, %ymm0
	vmovdqu	%ymm0, 544(%rsp)                ## 32-byte Spill
	movslq	%r13d, %r11
	movslq	%r10d, %r10
	vpmovsxdq	%xmm1, %ymm0
	vmovdqu	%ymm0, 512(%rsp)                ## 32-byte Spill
	movslq	%esi, %rcx
	movslq	%edx, %r13
	movq	24(%rsp), %rax                  ## 8-byte Reload
	vmovups	(%rax,%rdi,4), %ymm6
	movq	%r9, %rdx
	shlq	$5, %rdx
	leaq	(%rdx,%rdi,4), %rax
	movq	%rax, 168(%rsp)                 ## 8-byte Spill
	movq	%r9, %rax
	shlq	$7, %rax
	movq	%rax, 160(%rsp)                 ## 8-byte Spill
	movq	%r9, %rdx
	shlq	$6, %rdx
	imulq	$100, %r9, %rbx
	leaq	(%rbx,%rdi,4), %rax
	movq	%rax, 152(%rsp)                 ## 8-byte Spill
	imulq	$104, %r9, %rbx
	leaq	(%rbx,%rdi,4), %rax
	movq	%rax, 144(%rsp)                 ## 8-byte Spill
	leaq	(%r9,%r9,8), %r8
	leaq	(%r8,%r8,2), %rbx
	addq	%r9, %rbx
	leaq	(%rbx,%rdi,4), %rax
	movq	%rax, 136(%rsp)                 ## 8-byte Spill
	imulq	$44, %r9, %rbx
	leaq	(%rbx,%rdi,4), %rax
	movq	%rax, 128(%rsp)                 ## 8-byte Spill
	leaq	(%r9,%r9,2), %rsi
	movq	%rsi, %rbx
	shlq	$4, %rbx
	leaq	(%rbx,%rdi,4), %rax
	movq	%rax, 120(%rsp)                 ## 8-byte Spill
	imulq	$52, %r9, %rbx
	leaq	(%rbx,%rdi,4), %rax
	movq	%rax, 112(%rsp)                 ## 8-byte Spill
	imulq	$56, %r9, %rbx
	leaq	(%rbx,%rdi,4), %rax
	movq	%rax, 104(%rsp)                 ## 8-byte Spill
	imulq	$60, %r9, %rbx
	leaq	(%rbx,%rdi,4), %rax
	movq	%rax, 96(%rsp)                  ## 8-byte Spill
	imulq	$84, %r9, %rbx
	leaq	(%rbx,%rdi,4), %rax
	movq	%rax, 88(%rsp)                  ## 8-byte Spill
	leaq	(%r9,%r9,4), %rax
	leaq	(,%rdi,4), %rbx
	leaq	(%rbx,%rax,4), %rbp
	movq	%rbp, 80(%rsp)                  ## 8-byte Spill
	leaq	(%rbx,%rax,8), %rbp
	movq	%rbp, 72(%rsp)                  ## 8-byte Spill
	shlq	$4, %rax
	leaq	(%rax,%rdi,4), %rax
	movq	%rax, 64(%rsp)                  ## 8-byte Spill
	imulq	$76, %r9, %rax
	leaq	(%rax,%rdi,4), %rax
	movq	%rax, 56(%rsp)                  ## 8-byte Spill
	leaq	(%rbx,%r8,4), %rax
	movq	%rax, -24(%rsp)                 ## 8-byte Spill
	leaq	(%rbx,%r8,8), %rax
	movq	%rax, 48(%rsp)                  ## 8-byte Spill
	leaq	(%rdx,%rdi,4), %rax
	movq	%rax, -32(%rsp)                 ## 8-byte Spill
	leaq	(%rdx,%r9,4), %rax
	leaq	(%rax,%rdi,4), %rax
	movq	%rax, -40(%rsp)                 ## 8-byte Spill
	imulq	$88, %r9, %rax
	leaq	(%rax,%rdi,4), %rax
	movq	%rax, -48(%rsp)                 ## 8-byte Spill
	imulq	$92, %r9, %rax
	leaq	(%rax,%rdi,4), %rax
	movq	%rax, -56(%rsp)                 ## 8-byte Spill
	leaq	(%rbx,%rsi,8), %rax
	movq	%rax, -64(%rsp)                 ## 8-byte Spill
	leaq	(%rbx,%rsi,4), %rax
	movq	%rax, -72(%rsp)                 ## 8-byte Spill
	shlq	$5, %rsi
	leaq	(%rsi,%rdi,4), %rax
	movq	%rax, -80(%rsp)                 ## 8-byte Spill
	leaq	(%rbx,%r9,8), %rax
	movq	%rax, -88(%rsp)                 ## 8-byte Spill
	imulq	$108, %r9, %rax
	addq	%rdi, %r9
	movq	%rdi, 192(%rsp)                 ## 8-byte Spill
	leaq	(%rax,%rdi,4), %rax
	movq	%rax, -96(%rsp)                 ## 8-byte Spill
	movq	208(%rsp), %r8                  ## 8-byte Reload
	leaq	(%r8,%r13,4), %rax
	movq	%rax, -104(%rsp)                ## 8-byte Spill
	leaq	(%r8,%rcx,4), %rax
	movq	%rax, 40(%rsp)                  ## 8-byte Spill
	leaq	(%r8,%r10,4), %rax
	movq	%rax, 32(%rsp)                  ## 8-byte Spill
	leaq	(%r8,%r11,4), %rax
	movq	%rax, -112(%rsp)                ## 8-byte Spill
	leaq	(%r8,%r14,4), %r11
	leaq	(%r8,%r15,4), %r14
	leaq	(%r8,%r12,4), %r15
	movq	416(%rsp), %rax                 ## 8-byte Reload
	leaq	(%r8,%rax,4), %r12
	movq	224(%rsp), %rax                 ## 8-byte Reload
	leaq	(%r8,%rax,4), %r13
	movq	320(%rsp), %rax                 ## 8-byte Reload
	leaq	(%r8,%rax,4), %rcx
	vbroadcastss	LCPI11_0(%rip), %ymm0   ## ymm0 = [1.0E+0,1.0E+0,1.0E+0,1.0E+0,1.0E+0,1.0E+0,1.0E+0,1.0E+0]
	vmovups	%ymm0, 416(%rsp)                ## 32-byte Spill
	vxorps	%xmm11, %xmm11, %xmm11
	vbroadcastss	LCPI11_1(%rip), %ymm0   ## ymm0 = [4.096E+3,4.096E+3,4.096E+3,4.096E+3,4.096E+3,4.096E+3,4.096E+3,4.096E+3]
	vmovups	%ymm0, 480(%rsp)                ## 32-byte Spill
	vbroadcastss	LCPI11_2(%rip), %ymm0   ## ymm0 = [2.0E+0,2.0E+0,2.0E+0,2.0E+0,2.0E+0,2.0E+0,2.0E+0,2.0E+0]
	vmovups	%ymm0, 384(%rsp)                ## 32-byte Spill
	vbroadcastss	LCPI11_3(%rip), %ymm0   ## ymm0 = [-1.0E+0,-1.0E+0,-1.0E+0,-1.0E+0,-1.0E+0,-1.0E+0,-1.0E+0,-1.0E+0]
	vmovups	%ymm0, 608(%rsp)                ## 32-byte Spill
	vbroadcastss	LCPI11_4(%rip), %ymm0   ## ymm0 = [9.99999971E-10,9.99999971E-10,9.99999971E-10,9.99999971E-10,9.99999971E-10,9.99999971E-10,9.99999971E-10,9.99999971E-10]
	vmovups	%ymm0, 576(%rsp)                ## 32-byte Spill
	xorl	%esi, %esi
	xorl	%edi, %edi
	movq	200(%rsp), %r10                 ## 8-byte Reload
	.p2align	4, 0x90
LBB11_3:                                ## %"for f1.s1.r79$x"
                                        ## =>This Inner Loop Header: Depth=1
	vmovq	%rsi, %xmm0
	vpbroadcastq	%xmm0, %ymm1
	vpaddq	288(%rsp), %ymm1, %ymm0         ## 32-byte Folded Reload
	vmovdqu	%ymm1, 320(%rsp)                ## 32-byte Spill
	vmovq	%xmm0, %rax
	vpextrq	$1, %xmm0, %rdx
	vextracti128	$1, %ymm0, %xmm2
	movq	-40(%rsp), %rbp                 ## 8-byte Reload
	addq	%r10, %rbp
	vpaddq	544(%rsp), %ymm1, %ymm0         ## 32-byte Folded Reload
	vmovups	(%r8,%rax,4), %ymm13
	vpextrq	$1, %xmm0, %rax
	vmovdqa	%ymm0, %ymm4
	vmovdqu	%ymm0, 224(%rsp)                ## 32-byte Spill
	vmulps	(%r8,%rdx,4), %ymm13, %ymm3
	movq	48(%rsp), %rdx                  ## 8-byte Reload
	addq	%r10, %rdx
	vcmpeqps	(%r8,%rax,4), %ymm11, %ymm1
	movq	64(%rsp), %rax                  ## 8-byte Reload
	addq	%r10, %rax
	testq	%rdi, %rdi
	cmoveq	%rbp, %rdx
	movq	88(%rsp), %rbp                  ## 8-byte Reload
	leaq	(%r10,%rbp), %rbp
	cmoveq	%rax, %rbp
	vmovq	%xmm2, %rax
	vmovups	(%r8,%rax,4), %ymm0
	vpextrq	$1, %xmm2, %rax
	vextracti128	$1, %ymm4, %xmm5
	vmovups	(%r8,%rax,4), %ymm14
	vpextrq	$1, %xmm5, %rax
	vmulps	(%r8,%rax,4), %ymm14, %ymm2
	vmovups	(%rdx), %ymm7
	vmovups	(%rcx,%rsi,4), %ymm15
	vmovups	(%r10,%rbx), %ymm8
	vmaxps	%ymm11, %ymm8, %ymm10
	vmovups	(%r10,%r9,4), %ymm8
	vmaxps	%ymm11, %ymm8, %ymm9
	vmovups	(%rbp), %ymm8
	vmulps	%ymm0, %ymm9, %ymm9
	vmovq	%xmm5, %rbp
	movq	-88(%rsp), %rax                 ## 8-byte Reload
	vmovups	(%r10,%rax), %ymm5
	vmaxps	%ymm11, %ymm5, %ymm5
	vfmadd231ps	%ymm10, %ymm3, %ymm9    ## ymm9 = (ymm3 * ymm10) + ymm9
	movq	-72(%rsp), %rax                 ## 8-byte Reload
	vmovups	(%r10,%rax), %ymm10
	vmaxps	%ymm11, %ymm10, %ymm10
	vmulps	%ymm0, %ymm10, %ymm10
	vfmadd231ps	%ymm5, %ymm3, %ymm10    ## ymm10 = (ymm3 * ymm5) + ymm10
	movq	-48(%rsp), %rax                 ## 8-byte Reload
	vmovups	(%r10,%rax), %ymm3
	movq	80(%rsp), %rax                  ## 8-byte Reload
	vmovups	(%r10,%rax), %ymm5
	vblendvps	%ymm1, %ymm9, %ymm10, %ymm12
	movq	-64(%rsp), %rax                 ## 8-byte Reload
	vmovups	(%r10,%rax), %ymm9
	movq	-104(%rsp), %rax                ## 8-byte Reload
	vmulps	(%rax,%rsi,4), %ymm15, %ymm10
	vmaxps	%ymm11, %ymm9, %ymm9
	vmulps	%ymm10, %ymm9, %ymm9
	movq	40(%rsp), %rax                  ## 8-byte Reload
	vmulps	(%rax,%rsi,4), %ymm15, %ymm10
	vmaxps	%ymm11, %ymm3, %ymm4
	vmaxps	%ymm11, %ymm5, %ymm3
	vfmadd231ps	%ymm10, %ymm3, %ymm9    ## ymm9 = (ymm3 * ymm10) + ymm9
	vaddps	%ymm0, %ymm13, %ymm5
	vmovups	416(%rsp), %ymm1                ## 32-byte Reload
	vcmpltps	%ymm14, %ymm1, %ymm3
	vmulps	%ymm4, %ymm5, %ymm4
	movq	-32(%rsp), %rax                 ## 8-byte Reload
	vblendvps	%ymm3, (%r10,%rax), %ymm7, %ymm5
	vmulps	(%r13,%rsi,4), %ymm14, %ymm7
	vmaxps	%ymm11, %ymm5, %ymm5
	vmulps	%ymm5, %ymm2, %ymm2
	vmovups	(%r12,%rsi,4), %ymm5
	movq	56(%rsp), %rax                  ## 8-byte Reload
	vblendvps	%ymm3, (%r10,%rax), %ymm8, %ymm8
	vmaxps	%ymm1, %ymm5, %ymm5
	vdivps	%ymm5, %ymm2, %ymm10
	vmaxps	%ymm11, %ymm8, %ymm2
	movq	-112(%rsp), %rax                ## 8-byte Reload
	vfmadd132ps	(%rax,%rsi,4), %ymm10, %ymm2 ## ymm2 = (ymm2 * mem) + ymm10
	movq	-56(%rsp), %rax                 ## 8-byte Reload
	vmovups	(%r10,%rax), %ymm8
	vmaxps	%ymm11, %ymm8, %ymm8
	vdivps	%ymm5, %ymm4, %ymm4
	vmovups	480(%rsp), %ymm10               ## 32-byte Reload
	vdivps	%ymm5, %ymm10, %ymm5
	vminps	%ymm14, %ymm5, %ymm5
	vmulps	(%r8,%rbp,4), %ymm5, %ymm5
	vmulps	%ymm7, %ymm5, %ymm5
	vandps	%ymm4, %ymm3, %ymm4
	vfmadd213ps	%ymm4, %ymm15, %ymm2    ## ymm2 = (ymm15 * ymm2) + ymm4
	vmovdqu	320(%rsp), %ymm4                ## 32-byte Reload
	vpaddq	512(%rsp), %ymm4, %ymm4         ## 32-byte Folded Reload
	vmovaps	%ymm6, %ymm10
	vextracti128	$1, %ymm4, %xmm6
	vfmadd231ps	%ymm5, %ymm8, %ymm2     ## ymm2 = (ymm8 * ymm5) + ymm2
	movq	136(%rsp), %rax                 ## 8-byte Reload
	vmovups	(%r10,%rax), %ymm5
	vmaxps	%ymm11, %ymm5, %ymm5
	vpextrq	$1, %xmm6, %rax
	vmulps	(%r8,%rax,4), %ymm13, %ymm8
	vfmadd231ps	%ymm8, %ymm5, %ymm9     ## ymm9 = (ymm5 * ymm8) + ymm9
	vmovups	(%r14,%rsi,4), %ymm5
	movq	168(%rsp), %rax                 ## 8-byte Reload
	vmovups	(%r10,%rax), %ymm8
	vmovq	%xmm6, %rax
	vmulps	(%r8,%rax,4), %ymm0, %ymm6
	vmaxps	%ymm11, %ymm8, %ymm8
	vfmadd231ps	%ymm6, %ymm8, %ymm9     ## ymm9 = (ymm8 * ymm6) + ymm9
	movq	-24(%rsp), %rax                 ## 8-byte Reload
	vmovups	(%r10,%rax), %ymm6
	vmaxps	%ymm11, %ymm6, %ymm6
	vpextrq	$1, %xmm4, %rax
	vmulps	(%r8,%rax,4), %ymm13, %ymm8
	vfmadd231ps	%ymm8, %ymm6, %ymm9     ## ymm9 = (ymm6 * ymm8) + ymm9
	movq	72(%rsp), %rax                  ## 8-byte Reload
	vmovups	(%r10,%rax), %ymm6
	vmaxps	%ymm11, %ymm6, %ymm6
	vmulps	%ymm5, %ymm0, %ymm8
	vfmadd231ps	%ymm8, %ymm6, %ymm9     ## ymm9 = (ymm6 * ymm8) + ymm9
	movq	128(%rsp), %rax                 ## 8-byte Reload
	vmovups	(%r10,%rax), %ymm6
	vmaxps	%ymm11, %ymm6, %ymm6
	vmulps	%ymm5, %ymm13, %ymm5
	vfmadd231ps	%ymm5, %ymm6, %ymm9     ## ymm9 = (ymm6 * ymm5) + ymm9
	vmovups	(%r15,%rsi,4), %ymm5
	movq	120(%rsp), %rax                 ## 8-byte Reload
	vmovups	(%r10,%rax), %ymm6
	vmaxps	%ymm11, %ymm6, %ymm6
	vmulps	%ymm5, %ymm0, %ymm0
	vfmadd231ps	%ymm0, %ymm6, %ymm9     ## ymm9 = (ymm6 * ymm0) + ymm9
	vmovaps	%ymm10, %ymm6
	vmulps	%ymm5, %ymm13, %ymm0
	movq	112(%rsp), %rax                 ## 8-byte Reload
	vmovups	(%r10,%rax), %ymm5
	vmaxps	%ymm11, %ymm5, %ymm5
	vfmadd231ps	%ymm0, %ymm5, %ymm9     ## ymm9 = (ymm5 * ymm0) + ymm9
	vmaxps	%ymm1, %ymm7, %ymm0
	movq	104(%rsp), %rax                 ## 8-byte Reload
	vmovups	(%r10,%rax), %ymm5
	vmaxps	%ymm11, %ymm5, %ymm5
	vmovq	%xmm4, %rax
	vmulps	(%r8,%rax,4), %ymm0, %ymm4
	vfmadd231ps	%ymm4, %ymm5, %ymm9     ## ymm9 = (ymm5 * ymm4) + ymm9
	vdivps	352(%rsp), %ymm0, %ymm4         ## 32-byte Folded Reload
	movq	96(%rsp), %rax                  ## 8-byte Reload
	vmovups	(%r10,%rax), %ymm5
	vmaxps	%ymm11, %ymm5, %ymm5
	movq	32(%rsp), %rax                  ## 8-byte Reload
	vmulps	(%rax,%rsi,4), %ymm0, %ymm0
	vfmadd231ps	%ymm0, %ymm5, %ymm9     ## ymm9 = (ymm5 * ymm0) + ymm9
	vroundps	$2, %ymm4, %ymm0
	vmulps	%ymm0, %ymm12, %ymm0
	vmaxps	%ymm1, %ymm4, %ymm1
	vdivps	%ymm1, %ymm0, %ymm0
	vfmadd132ps	384(%rsp), %ymm0, %ymm2 ## 32-byte Folded Reload
                                        ## ymm2 = (ymm2 * mem) + ymm0
	vaddps	%ymm2, %ymm9, %ymm0
	movq	-80(%rsp), %rax                 ## 8-byte Reload
	vmovups	(%r10,%rax), %ymm1
	vmaxps	%ymm11, %ymm1, %ymm1
	vfmadd231ps	%ymm1, %ymm15, %ymm0    ## ymm0 = (ymm15 * ymm1) + ymm0
	movq	152(%rsp), %rax                 ## 8-byte Reload
	vmovups	(%r10,%rax), %ymm1
	vmaxps	%ymm11, %ymm1, %ymm1
	vandps	%ymm1, %ymm3, %ymm1
	vmovups	(%r11,%rsi,4), %ymm2
	vfmadd213ps	%ymm0, %ymm2, %ymm1     ## ymm1 = (ymm2 * ymm1) + ymm0
	vaddps	608(%rsp), %ymm14, %ymm0        ## 32-byte Folded Reload
	vmulps	%ymm2, %ymm0, %ymm0
	movq	144(%rsp), %rax                 ## 8-byte Reload
	vmovups	(%r10,%rax), %ymm2
	vmaxps	%ymm11, %ymm2, %ymm2
	vfmadd231ps	%ymm0, %ymm2, %ymm1     ## ymm1 = (ymm2 * ymm0) + ymm1
	vmovdqu	224(%rsp), %ymm0                ## 32-byte Reload
	vmovq	%xmm0, %rax
	movq	-96(%rsp), %rdx                 ## 8-byte Reload
	vmovups	(%r10,%rdx), %ymm0
	vmaxps	%ymm11, %ymm0, %ymm0
	vfmadd231ps	(%r8,%rax,4), %ymm0, %ymm1 ## ymm1 = (ymm0 * mem) + ymm1
	vfmadd231ps	576(%rsp), %ymm1, %ymm6 ## 32-byte Folded Reload
                                        ## ymm6 = (ymm1 * mem) + ymm6
	incq	%rdi
	addq	160(%rsp), %r10                 ## 8-byte Folded Reload
	addq	216(%rsp), %rsi                 ## 8-byte Folded Reload
	cmpq	%rdi, 264(%rsp)                 ## 8-byte Folded Reload
	jne	LBB11_3
## %bb.4:                               ## %destructor_block.loopexit
	movq	24(%rsp), %rax                  ## 8-byte Reload
	movq	192(%rsp), %rcx                 ## 8-byte Reload
	vmovups	%ymm6, (%rax,%rcx,4)
LBB11_20:                               ## %destructor_block
	xorl	%eax, %eax
	addq	$680, %rsp                      ## imm = 0x2A8
	popq	%rbx
	popq	%r12
	popq	%r13
	popq	%r14
	popq	%r15
	popq	%rbp
	vzeroupper
	retq
LBB11_5:                                ## %next_bb
	movl	32(%rdx), %edx
	leal	(%rdi,%rsi,8), %r10d
	subl	%r10d, %edx
	cmpl	$8, %edx
	movl	$8, %eax
	cmovll	%edx, %eax
	xorl	%ecx, %ecx
	testl	%edx, %edx
	cmovgl	%eax, %ecx
	movl	%ecx, 176(%rsp)                 ## 4-byte Spill
	testl	%ebx, %ebx
	jle	LBB11_20
## %bb.6:                               ## %"for f1.s1.r79$x2.preheader"
	testl	%edx, %edx
	jle	LBB11_20
## %bb.7:                               ## %"for f1.s1.r79$x2.us.preheader"
	leal	(%rbp,%rbp,4), %edx
	leal	(%rbp,%rdx,2), %eax
	subl	%r12d, %eax
	addl	%r10d, %eax
	movl	%eax, 352(%rsp)                 ## 4-byte Spill
	leal	(,%rbp,4), %eax
	leal	(%rax,%rax,2), %eax
	subl	%r12d, %eax
	addl	%r10d, %eax
	movl	%eax, -32(%rsp)                 ## 4-byte Spill
	movl	%ebp, %eax
	shll	$4, %eax
	addl	%ebp, %eax
	subl	%r12d, %eax
	addl	%r10d, %eax
	movl	%eax, -40(%rsp)                 ## 4-byte Spill
	leal	(%rbp,%rbp), %eax
	leal	(%rax,%rax,8), %eax
	subl	%r12d, %eax
	addl	%r10d, %eax
	movl	%eax, -48(%rsp)                 ## 4-byte Spill
	leal	(%rbp,%rbp,8), %eax
	leal	(%rbp,%rax,2), %ecx
	subl	%r12d, %ecx
	addl	%r10d, %ecx
	movl	%ecx, -56(%rsp)                 ## 4-byte Spill
	leal	(%rbp,%rdx,4), %ecx
	subl	%r12d, %ecx
	addl	%r10d, %ecx
	movl	%ecx, -64(%rsp)                 ## 4-byte Spill
	leal	(%rdx,%rdx,4), %ecx
	addl	%ebp, %ecx
	subl	%r12d, %ecx
	addl	%r10d, %ecx
	movl	%ecx, -72(%rsp)                 ## 4-byte Spill
	leal	(%rax,%rax,2), %eax
	movl	%eax, %ecx
	subl	%r12d, %ecx
	addl	%r10d, %ecx
	movl	%ecx, -80(%rsp)                 ## 4-byte Spill
	addl	%ebp, %eax
	leal	(%rax,%rbp), %ecx
	subl	%r12d, %eax
	addl	%r10d, %eax
	movq	%rax, -88(%rsp)                 ## 8-byte Spill
	subl	%r12d, %ecx
	addl	%r10d, %ecx
	movl	%ecx, -96(%rsp)                 ## 4-byte Spill
	movl	%ebp, %edx
	shll	$5, %edx
	leal	(%rdx,%rbp), %eax
	subl	%r12d, %eax
	addl	%r10d, %eax
	movl	%eax, -104(%rsp)                ## 4-byte Spill
	leal	(%rdx,%rbp,2), %eax
	subl	%r12d, %eax
	addl	%r10d, %eax
	movl	%eax, -112(%rsp)                ## 4-byte Spill
	leal	(%rdi,%rbp), %eax
	leal	(%rax,%rsi,8), %eax
	subl	%r12d, %eax
	movl	%eax, 20(%rsp)                  ## 4-byte Spill
	leal	(%rdi,%rdx), %eax
	subl	%ebp, %edx
	leal	(%rdi,%rdx), %ecx
	leal	(%rcx,%rsi,8), %ecx
	subl	%r12d, %ecx
	movl	%ecx, 16(%rsp)                  ## 4-byte Spill
	leal	(%rax,%rsi,8), %eax
	subl	%r12d, %eax
	movl	%eax, 12(%rsp)                  ## 4-byte Spill
	subl	%ebp, %edx
	addl	%edi, %edx
	leal	(%rdx,%rsi,8), %eax
	subl	%r12d, %eax
	movl	%eax, 8(%rsp)                   ## 4-byte Spill
	leal	(%rbp,%rbp,4), %eax
	leal	(%rdi,%rax,2), %ecx
	leal	(%rcx,%rsi,8), %ecx
	subl	%r12d, %ecx
	movl	%ecx, 4(%rsp)                   ## 4-byte Spill
	leal	(%rbp,%rbp,8), %ecx
	addl	%edi, %ecx
	leal	(%rcx,%rsi,8), %ecx
	subl	%r12d, %ecx
	movl	%ecx, (%rsp)                    ## 4-byte Spill
	leal	(%rax,%rax,4), %ecx
	addl	%edi, %ecx
	leal	(%rcx,%rsi,8), %ecx
	subl	%r12d, %ecx
	movl	%ecx, -4(%rsp)                  ## 4-byte Spill
	leal	(%rbp,%rax,4), %eax
	addl	%ebp, %eax
	addl	%edi, %eax
	leal	(%rax,%rsi,8), %eax
	subl	%r12d, %eax
	movl	%eax, -8(%rsp)                  ## 4-byte Spill
	leal	(%rbp,%rbp,2), %eax
	leal	(%rdi,%rax,8), %eax
	leal	(%rax,%rsi,8), %eax
	subl	%r12d, %eax
	movl	%eax, -12(%rsp)                 ## 4-byte Spill
	leal	(%rdi,%rsi,8), %eax
	subl	%r12d, %eax
	movl	%eax, 180(%rsp)                 ## 4-byte Spill
	leal	(,%rsi,8), %ebx
	leaq	(%r8,%r8,2), %r12
	leaq	(,%r12,8), %r15
	subq	%r8, %r15
	movq	%r8, %rax
	shlq	$4, %rax
	movq	%rax, %rbp
	subq	%r8, %rbp
	subq	%r8, %rbp
	leaq	(,%r8,8), %rdi
	leaq	(%rdi,%rdi,2), %rcx
	movq	%rcx, -24(%rsp)                 ## 8-byte Spill
	subq	%r8, %rdi
	leaq	(,%r8,4), %rcx
	leaq	(%rcx,%rcx,4), %rdx
	movq	%rdx, 224(%rsp)                 ## 8-byte Spill
	leaq	(%rcx,%rcx,2), %r14
	leaq	(%r8,%r8), %rcx
	leaq	(%rcx,%rcx,8), %rdx
	movq	%rdx, 288(%rsp)                 ## 8-byte Spill
	leaq	(%rcx,%rcx,4), %r11
	leaq	(%rcx,%rcx,2), %r13
	movslq	%ebx, %rcx
	leaq	(%rcx,%r8,8), %rbx
	movq	200(%rsp), %rdx                 ## 8-byte Reload
	leaq	(%rdx,%rbx,4), %rsi
	movq	%rsi, 544(%rsp)                 ## 8-byte Spill
	leaq	(%r8,%rax), %rbx
	addq	%rcx, %rax
	leaq	(%rdx,%rax,4), %rax
	movq	%rax, 512(%rsp)                 ## 8-byte Spill
	leaq	(%rcx,%r8,2), %rax
	leaq	(%rdx,%rax,4), %rax
	movq	%rax, 184(%rsp)                 ## 8-byte Spill
	leaq	(%r8,%r8,4), %rax
	leaq	(%rax,%rax,4), %rsi
	leaq	(%rsi,%r8), %r10
	addq	%rcx, %rsi
	leaq	(%rdx,%rsi,4), %rsi
	movq	%rsi, 416(%rsp)                 ## 8-byte Spill
	addq	%rcx, %r10
	leaq	(%rdx,%r10,4), %rsi
	movq	%rsi, 168(%rsp)                 ## 8-byte Spill
	addq	%rcx, %r13
	leaq	(%rdx,%r13,4), %rsi
	movq	%rsi, 160(%rsp)                 ## 8-byte Spill
	leaq	(%r8,%rax,4), %r9
	leaq	(%rax,%rax,2), %r10
	leaq	(%r8,%rax,2), %rsi
	addq	%rcx, %rax
	leaq	(%rdx,%rax,4), %rax
	movq	%rax, 152(%rsp)                 ## 8-byte Spill
	addq	%rcx, %rdi
	leaq	(%rdx,%rdi,4), %rax
	movq	%rax, 144(%rsp)                 ## 8-byte Spill
	leaq	(%r8,%r8,8), %rdi
	leaq	(%rdi,%rdi,2), %r13
	leaq	(%r8,%rdi,2), %rax
	addq	%rcx, %rdi
	leaq	(%rdx,%rdi,4), %rdi
	movq	%rdi, 136(%rsp)                 ## 8-byte Spill
	addq	%rcx, %r11
	leaq	(%rdx,%r11,4), %rdi
	movq	%rdi, 128(%rsp)                 ## 8-byte Spill
	addq	%rcx, %rsi
	leaq	(%rdx,%rsi,4), %rsi
	movq	%rsi, 120(%rsp)                 ## 8-byte Spill
	addq	%rcx, %r14
	leaq	(%rdx,%r14,4), %rsi
	movq	%rsi, 112(%rsp)                 ## 8-byte Spill
	leaq	(%r8,%r12,4), %rsi
	addq	%rcx, %rsi
	leaq	(%rdx,%rsi,4), %rsi
	movq	%rsi, 104(%rsp)                 ## 8-byte Spill
	addq	%rcx, %rbp
	leaq	(%rdx,%rbp,4), %rsi
	movq	%rsi, 96(%rsp)                  ## 8-byte Spill
	addq	%rcx, %r10
	leaq	(%rdx,%r10,4), %rsi
	movq	%rsi, 88(%rsp)                  ## 8-byte Spill
	leaq	(%r8,%r9), %rsi
	addq	%rcx, %r9
	leaq	(%rdx,%r9,4), %rdi
	movq	%rdi, 320(%rsp)                 ## 8-byte Spill
	movq	224(%rsp), %rdi                 ## 8-byte Reload
	addq	%rcx, %rdi
	leaq	(%rdx,%rdi,4), %rdi
	movq	%rdi, 224(%rsp)                 ## 8-byte Spill
	addq	%rcx, %rax
	leaq	(%rdx,%rax,4), %rax
	movq	%rax, 40(%rsp)                  ## 8-byte Spill
	movq	288(%rsp), %rax                 ## 8-byte Reload
	addq	%rcx, %rax
	leaq	(%rdx,%rax,4), %rax
	movq	%rax, 288(%rsp)                 ## 8-byte Spill
	addq	%rcx, %rbx
	leaq	(%rdx,%rbx,4), %rax
	movq	%rax, 80(%rsp)                  ## 8-byte Spill
	addq	%rcx, %rsi
	leaq	(%rdx,%rsi,4), %rax
	movq	%rax, 480(%rsp)                 ## 8-byte Spill
	addq	%rcx, %r15
	leaq	(%rdx,%r15,4), %rax
	movq	%rax, 72(%rsp)                  ## 8-byte Spill
	addq	%rcx, %r12
	leaq	(%rdx,%r12,4), %rax
	movq	%rax, 64(%rsp)                  ## 8-byte Spill
	movq	%r8, %rsi
	addq	%rcx, %r8
	leaq	(%rdx,%r8,4), %rax
	movq	%rax, 56(%rsp)                  ## 8-byte Spill
	movq	-24(%rsp), %rax                 ## 8-byte Reload
	addq	%rcx, %rax
	leaq	(%rdx,%rax,4), %rax
	movq	%rax, -24(%rsp)                 ## 8-byte Spill
	addq	%rcx, %r13
	leaq	(%rdx,%r13,4), %rax
	movq	%rax, 32(%rsp)                  ## 8-byte Spill
	leaq	(%rdx,%rcx,4), %rax
	movq	%rax, 48(%rsp)                  ## 8-byte Spill
	movq	24(%rsp), %rax                  ## 8-byte Reload
	leaq	(%rax,%rcx,4), %rdx
	movslq	-64(%rsp), %rax                 ## 4-byte Folded Reload
	movq	%rax, 384(%rsp)                 ## 8-byte Spill
	movslq	-40(%rsp), %r8                  ## 4-byte Folded Reload
	movslq	-32(%rsp), %r10                 ## 4-byte Folded Reload
	movslq	-96(%rsp), %r11                 ## 4-byte Folded Reload
	movslq	352(%rsp), %r14                 ## 4-byte Folded Reload
	movslq	-112(%rsp), %r15                ## 4-byte Folded Reload
	movslq	-104(%rsp), %r12                ## 4-byte Folded Reload
	movslq	-80(%rsp), %r13                 ## 4-byte Folded Reload
	movslq	-88(%rsp), %rcx                 ## 4-byte Folded Reload
	movslq	-72(%rsp), %rdi                 ## 4-byte Folded Reload
	movslq	-56(%rsp), %rbp                 ## 4-byte Folded Reload
	movslq	-48(%rsp), %rbx                 ## 4-byte Folded Reload
	movl	176(%rsp), %eax                 ## 4-byte Reload
	movq	%rax, 192(%rsp)                 ## 8-byte Spill
                                        ## kill: def $eax killed $eax killed $rax def $rax
	andl	$2147483640, %eax               ## imm = 0x7FFFFFF8
	movq	%rax, 272(%rsp)                 ## 8-byte Spill
	vbroadcastss	448(%rsp), %ymm12       ## 16-byte Folded Reload
	movq	208(%rsp), %rax                 ## 8-byte Reload
	leaq	(%rax,%rbx,4), %rbx
	movq	%rbx, -112(%rsp)                ## 8-byte Spill
	leaq	(%rax,%rbp,4), %rbp
	movq	%rbp, -32(%rsp)                 ## 8-byte Spill
	leaq	(%rax,%rdi,4), %rdi
	movq	%rdi, -40(%rsp)                 ## 8-byte Spill
	movl	12(%rsp), %r9d                  ## 4-byte Reload
	leaq	(%rax,%rcx,4), %rcx
	movq	%rcx, -48(%rsp)                 ## 8-byte Spill
	leaq	(%rax,%r13,4), %rcx
	movq	%rcx, -56(%rsp)                 ## 8-byte Spill
	leaq	(%rax,%r12,4), %rcx
	movq	%rcx, -64(%rsp)                 ## 8-byte Spill
	movl	16(%rsp), %edi                  ## 4-byte Reload
	movl	20(%rsp), %r13d                 ## 4-byte Reload
	leaq	(%rax,%r15,4), %rcx
	movq	%rcx, -72(%rsp)                 ## 8-byte Spill
	movl	-12(%rsp), %r15d                ## 4-byte Reload
	leaq	(%rax,%r14,4), %rcx
	movq	%rcx, -80(%rsp)                 ## 8-byte Spill
	movl	-8(%rsp), %r14d                 ## 4-byte Reload
	leaq	(%rax,%r11,4), %rcx
	movq	%rcx, -88(%rsp)                 ## 8-byte Spill
	leaq	(%rax,%r10,4), %rcx
	movq	%rcx, -96(%rsp)                 ## 8-byte Spill
	movl	-4(%rsp), %r10d                 ## 4-byte Reload
	leaq	(%rax,%r8,4), %rcx
	movq	%rcx, -104(%rsp)                ## 8-byte Spill
	movq	384(%rsp), %rcx                 ## 8-byte Reload
	leaq	(%rax,%rcx,4), %rax
	movq	%rax, 352(%rsp)                 ## 8-byte Spill
	movl	(%rsp), %r8d                    ## 4-byte Reload
	movl	4(%rsp), %ebx                   ## 4-byte Reload
	movl	8(%rsp), %r11d                  ## 4-byte Reload
	shlq	$7, %rsi
	movq	%rsi, 472(%rsp)                 ## 8-byte Spill
	vxorps	%xmm15, %xmm15, %xmm15
	movq	216(%rsp), %rax                 ## 8-byte Reload
	leaq	(,%rax,4), %rax
	movq	%rax, 464(%rsp)                 ## 8-byte Spill
	xorl	%r12d, %r12d
	vmovups	%ymm12, 640(%rsp)               ## 32-byte Spill
	jmp	LBB11_8
	.p2align	4, 0x90
LBB11_19:                               ## %"end for f1.s1.n.ni.loopexit.us"
                                        ##   in Loop: Header=BB11_8 Depth=1
	incq	384(%rsp)                       ## 8-byte Folded Spill
	movq	464(%rsp), %rbp                 ## 8-byte Reload
	addq	%rbp, -112(%rsp)                ## 8-byte Folded Spill
	addq	%rbp, -32(%rsp)                 ## 8-byte Folded Spill
	addq	%rbp, -40(%rsp)                 ## 8-byte Folded Spill
	addq	%rbp, -48(%rsp)                 ## 8-byte Folded Spill
	addq	%rbp, -56(%rsp)                 ## 8-byte Folded Spill
	addq	%rbp, -64(%rsp)                 ## 8-byte Folded Spill
	addq	%rbp, -72(%rsp)                 ## 8-byte Folded Spill
	addq	%rbp, -80(%rsp)                 ## 8-byte Folded Spill
	addq	%rbp, -88(%rsp)                 ## 8-byte Folded Spill
	addq	%rbp, -96(%rsp)                 ## 8-byte Folded Spill
	addq	%rbp, -104(%rsp)                ## 8-byte Folded Spill
	addq	%rbp, %rax
	movq	%rax, 352(%rsp)                 ## 8-byte Spill
	movl	20(%rsp), %r13d                 ## 4-byte Reload
	movq	216(%rsp), %rbp                 ## 8-byte Reload
	addl	%ebp, %r13d
	movq	%rdi, %r12
	movl	16(%rsp), %edi                  ## 4-byte Reload
	addl	%ebp, %edi
	movl	12(%rsp), %r9d                  ## 4-byte Reload
	addl	%ebp, %r9d
	movl	8(%rsp), %r11d                  ## 4-byte Reload
	addl	%ebp, %r11d
	movq	%rbx, %rax
	movl	4(%rsp), %ebx                   ## 4-byte Reload
	addl	%ebp, %ebx
	movl	(%rsp), %r8d                    ## 4-byte Reload
	addl	%ebp, %r8d
	movl	-4(%rsp), %r10d                 ## 4-byte Reload
	addl	%ebp, %r10d
	movl	-8(%rsp), %r14d                 ## 4-byte Reload
	addl	%ebp, %r14d
	movl	-12(%rsp), %r15d                ## 4-byte Reload
	addl	%ebp, %r15d
	addl	%ebp, 180(%rsp)                 ## 4-byte Folded Spill
	movq	472(%rsp), %rbp                 ## 8-byte Reload
	addq	%rbp, 544(%rsp)                 ## 8-byte Folded Spill
	addq	%rbp, 512(%rsp)                 ## 8-byte Folded Spill
	addq	%rbp, %rax
	movq	%rax, 184(%rsp)                 ## 8-byte Spill
	addq	%rbp, 416(%rsp)                 ## 8-byte Folded Spill
	addq	%rbp, 168(%rsp)                 ## 8-byte Folded Spill
	addq	%rbp, 160(%rsp)                 ## 8-byte Folded Spill
	addq	%rbp, 152(%rsp)                 ## 8-byte Folded Spill
	addq	%rbp, 144(%rsp)                 ## 8-byte Folded Spill
	addq	%rbp, 136(%rsp)                 ## 8-byte Folded Spill
	addq	%rbp, 128(%rsp)                 ## 8-byte Folded Spill
	addq	%rbp, 120(%rsp)                 ## 8-byte Folded Spill
	addq	%rbp, 112(%rsp)                 ## 8-byte Folded Spill
	addq	%rbp, 104(%rsp)                 ## 8-byte Folded Spill
	addq	%rbp, 96(%rsp)                  ## 8-byte Folded Spill
	addq	%rbp, 88(%rsp)                  ## 8-byte Folded Spill
	addq	%rbp, %rcx
	movq	%rcx, 320(%rsp)                 ## 8-byte Spill
	addq	%rbp, %rsi
	movq	%rsi, 224(%rsp)                 ## 8-byte Spill
	addq	%rbp, 40(%rsp)                  ## 8-byte Folded Spill
	addq	%rbp, %r12
	movq	%r12, 288(%rsp)                 ## 8-byte Spill
	movq	384(%rsp), %r12                 ## 8-byte Reload
	addq	%rbp, 80(%rsp)                  ## 8-byte Folded Spill
	addq	%rbp, 480(%rsp)                 ## 8-byte Folded Spill
	addq	%rbp, 72(%rsp)                  ## 8-byte Folded Spill
	addq	%rbp, 64(%rsp)                  ## 8-byte Folded Spill
	addq	%rbp, 56(%rsp)                  ## 8-byte Folded Spill
	addq	%rbp, 48(%rsp)                  ## 8-byte Folded Spill
	addq	%rbp, -24(%rsp)                 ## 8-byte Folded Spill
	addq	%rbp, 32(%rsp)                  ## 8-byte Folded Spill
	cmpq	264(%rsp), %r12                 ## 8-byte Folded Reload
	vmovups	640(%rsp), %ymm12               ## 32-byte Reload
	je	LBB11_20
LBB11_8:                                ## %"for f1.s1.r79$x2.us"
                                        ## =>This Loop Header: Depth=1
                                        ##     Child Loop BB11_11 Depth 2
                                        ##     Child Loop BB11_13 Depth 2
	movslq	%r13d, %rcx
	movq	208(%rsp), %rax                 ## 8-byte Reload
	leaq	(%rax,%rcx,4), %rcx
	movq	%rcx, 576(%rsp)                 ## 8-byte Spill
	movslq	%edi, %rsi
	leaq	(%rax,%rsi,4), %rcx
	movq	%rcx, 24(%rsp)                  ## 8-byte Spill
	movl	%r9d, 12(%rsp)                  ## 4-byte Spill
	movslq	%r9d, %rsi
	leaq	(%rax,%rsi,4), %rcx
	movl	%r11d, 8(%rsp)                  ## 4-byte Spill
	movslq	%r11d, %rsi
	leaq	(%rax,%rsi,4), %rsi
	movq	%rsi, 608(%rsp)                 ## 8-byte Spill
	movl	%ebx, 4(%rsp)                   ## 4-byte Spill
	movslq	%ebx, %rsi
	leaq	(%rax,%rsi,4), %r11
	movl	%r8d, (%rsp)                    ## 4-byte Spill
	movslq	%r8d, %rsi
	leaq	(%rax,%rsi,4), %r9
	movl	%r10d, -4(%rsp)                 ## 4-byte Spill
	movslq	%r10d, %rbp
	leaq	(%rax,%rbp,4), %r8
	movl	%r14d, -8(%rsp)                 ## 4-byte Spill
	movslq	%r14d, %rbp
	leaq	(%rax,%rbp,4), %r10
	movl	%r15d, -12(%rsp)                ## 4-byte Spill
	movslq	%r15d, %rbp
	leaq	(%rax,%rbp,4), %r14
	movl	180(%rsp), %esi                 ## 4-byte Reload
	movslq	%esi, %rbp
	leaq	(%rax,%rbp,4), %r15
	cmpl	$8, 176(%rsp)                   ## 4-byte Folded Reload
	movq	%rcx, 200(%rsp)                 ## 8-byte Spill
	movl	%r13d, 20(%rsp)                 ## 4-byte Spill
	movl	%edi, 16(%rsp)                  ## 4-byte Spill
	movq	%r12, 384(%rsp)                 ## 8-byte Spill
	jae	LBB11_10
## %bb.9:                               ##   in Loop: Header=BB11_8 Depth=1
	xorl	%r12d, %r12d
	movq	184(%rsp), %rbx                 ## 8-byte Reload
	movq	320(%rsp), %rcx                 ## 8-byte Reload
	movq	224(%rsp), %rsi                 ## 8-byte Reload
	movq	288(%rsp), %rdi                 ## 8-byte Reload
	jmp	LBB11_13
	.p2align	4, 0x90
LBB11_10:                               ## %vector.body.preheader
                                        ##   in Loop: Header=BB11_8 Depth=1
	xorl	%ebp, %ebp
	movq	%r12, %rdi
	movq	%rcx, %rsi
	movq	608(%rsp), %r12                 ## 8-byte Reload
	movq	576(%rsp), %r13                 ## 8-byte Reload
	movq	%r11, 280(%rsp)                 ## 8-byte Spill
	movq	24(%rsp), %r11                  ## 8-byte Reload
	.p2align	4, 0x90
LBB11_11:                               ## %vector.body
                                        ##   Parent Loop BB11_8 Depth=1
                                        ## =>  This Inner Loop Header: Depth=2
	testq	%rdi, %rdi
	movq	80(%rsp), %rax                  ## 8-byte Reload
	leaq	(%rax,%rbp,4), %rax
	movq	288(%rsp), %rcx                 ## 8-byte Reload
	leaq	(%rcx,%rbp,4), %rcx
	cmoveq	%rax, %rcx
	movq	224(%rsp), %rax                 ## 8-byte Reload
	leaq	(%rax,%rbp,4), %rax
	movq	320(%rsp), %rbx                 ## 8-byte Reload
	leaq	(%rbx,%rbp,4), %rbx
	cmoveq	%rax, %rbx
	vmovups	(%r9,%rbp,4), %ymm0
	movq	280(%rsp), %rax                 ## 8-byte Reload
	vmulps	(%rax,%rbp,4), %ymm0, %ymm6
	vbroadcastss	LCPI11_0(%rip), %ymm4   ## ymm4 = [1.0E+0,1.0E+0,1.0E+0,1.0E+0,1.0E+0,1.0E+0,1.0E+0,1.0E+0]
	vmaxps	%ymm4, %ymm6, %ymm1
	vdivps	%ymm12, %ymm1, %ymm5
	vmovups	(%r14,%rbp,4), %ymm2
	vmovups	(%r8,%rbp,4), %ymm3
	vmulps	(%r10,%rbp,4), %ymm2, %ymm7
	vmovups	(%r12,%rbp,4), %ymm8
	vmaxps	%ymm4, %ymm8, %ymm8
	movq	48(%rsp), %rax                  ## 8-byte Reload
	vmovups	(%rax,%rbp,4), %ymm9
	vmaxps	%ymm15, %ymm9, %ymm9
	movq	56(%rsp), %rax                  ## 8-byte Reload
	vmovups	(%rax,%rbp,4), %ymm10
	vmaxps	%ymm15, %ymm10, %ymm10
	vmulps	%ymm3, %ymm10, %ymm10
	vfmadd231ps	%ymm9, %ymm7, %ymm10    ## ymm10 = (ymm7 * ymm9) + ymm10
	movq	184(%rsp), %rax                 ## 8-byte Reload
	vmovups	(%rax,%rbp,4), %ymm9
	vmaxps	%ymm15, %ymm9, %ymm9
	movq	64(%rsp), %rax                  ## 8-byte Reload
	vmovups	(%rax,%rbp,4), %ymm11
	vmaxps	%ymm15, %ymm11, %ymm11
	vmulps	%ymm3, %ymm11, %ymm11
	vfmadd231ps	%ymm9, %ymm7, %ymm11    ## ymm11 = (ymm7 * ymm9) + ymm11
	movq	-104(%rsp), %rax                ## 8-byte Reload
	vcmpeqps	(%rax,%rbp,4), %ymm15, %ymm7
	vblendvps	%ymm7, %ymm10, %ymm11, %ymm7
	vroundps	$10, %ymm5, %ymm9
	vmulps	%ymm7, %ymm9, %ymm7
	vmaxps	%ymm4, %ymm5, %ymm5
	vdivps	%ymm5, %ymm7, %ymm9
	vbroadcastss	LCPI11_1(%rip), %ymm5   ## ymm5 = [4.096E+3,4.096E+3,4.096E+3,4.096E+3,4.096E+3,4.096E+3,4.096E+3,4.096E+3]
	vdivps	%ymm8, %ymm5, %ymm7
	vmovups	(%rcx), %ymm5
	movq	480(%rsp), %rax                 ## 8-byte Reload
	vmovups	(%rax,%rbp,4), %ymm10
	vmaxps	%ymm15, %ymm10, %ymm10
	vaddps	%ymm3, %ymm2, %ymm11
	vmulps	%ymm10, %ymm11, %ymm10
	vdivps	%ymm8, %ymm10, %ymm10
	vcmpltps	%ymm0, %ymm4, %ymm4
	movq	512(%rsp), %rax                 ## 8-byte Reload
	vblendvps	%ymm4, (%rax,%rbp,4), %ymm5, %ymm5
	vmaxps	%ymm15, %ymm5, %ymm5
	movq	-88(%rsp), %rax                 ## 8-byte Reload
	vmulps	(%rax,%rbp,4), %ymm0, %ymm11
	vmulps	%ymm5, %ymm11, %ymm5
	vdivps	%ymm8, %ymm5, %ymm5
	vmovups	(%rbx), %ymm8
	movq	40(%rsp), %rax                  ## 8-byte Reload
	vblendvps	%ymm4, (%rax,%rbp,4), %ymm8, %ymm8
	vmaxps	%ymm15, %ymm8, %ymm8
	movq	-80(%rsp), %rax                 ## 8-byte Reload
	vfmadd132ps	(%rax,%rbp,4), %ymm5, %ymm8 ## ymm8 = (ymm8 * mem) + ymm5
	vmovups	(%r15,%rbp,4), %ymm5
	vminps	%ymm0, %ymm7, %ymm7
	movq	-96(%rsp), %rax                 ## 8-byte Reload
	vmulps	(%rax,%rbp,4), %ymm7, %ymm7
	vmulps	%ymm6, %ymm7, %ymm6
	vmovups	(%r11,%rbp,4), %ymm7
	vandps	%ymm4, %ymm10, %ymm10
	vfmadd213ps	%ymm10, %ymm5, %ymm8    ## ymm8 = (ymm5 * ymm8) + ymm10
	movq	72(%rsp), %rax                  ## 8-byte Reload
	vmovups	(%rax,%rbp,4), %ymm10
	vmaxps	%ymm15, %ymm10, %ymm10
	vfmadd231ps	%ymm6, %ymm10, %ymm8    ## ymm8 = (ymm10 * ymm6) + ymm8
	vbroadcastss	LCPI11_2(%rip), %ymm6   ## ymm6 = [2.0E+0,2.0E+0,2.0E+0,2.0E+0,2.0E+0,2.0E+0,2.0E+0,2.0E+0]
	vfmadd213ps	%ymm9, %ymm8, %ymm6     ## ymm6 = (ymm8 * ymm6) + ymm9
	movq	152(%rsp), %rax                 ## 8-byte Reload
	vmovups	(%rax,%rbp,4), %ymm8
	vmaxps	%ymm15, %ymm8, %ymm8
	movq	160(%rsp), %rax                 ## 8-byte Reload
	vmovups	(%rax,%rbp,4), %ymm9
	vmaxps	%ymm15, %ymm9, %ymm9
	movq	-112(%rsp), %rax                ## 8-byte Reload
	vmulps	(%rax,%rbp,4), %ymm5, %ymm10
	vmulps	%ymm10, %ymm9, %ymm9
	movq	-32(%rsp), %rax                 ## 8-byte Reload
	vmulps	(%rax,%rbp,4), %ymm5, %ymm10
	vfmadd231ps	%ymm10, %ymm8, %ymm9    ## ymm9 = (ymm8 * ymm10) + ymm9
	movq	144(%rsp), %rax                 ## 8-byte Reload
	vmovups	(%rax,%rbp,4), %ymm8
	vmaxps	%ymm15, %ymm8, %ymm8
	movq	-40(%rsp), %rax                 ## 8-byte Reload
	vmulps	(%rax,%rbp,4), %ymm2, %ymm10
	vfmadd231ps	%ymm10, %ymm8, %ymm9    ## ymm9 = (ymm8 * ymm10) + ymm9
	movq	544(%rsp), %rax                 ## 8-byte Reload
	vmovups	(%rax,%rbp,4), %ymm8
	vmaxps	%ymm15, %ymm8, %ymm8
	movq	-48(%rsp), %rax                 ## 8-byte Reload
	vmulps	(%rax,%rbp,4), %ymm3, %ymm10
	vfmadd231ps	%ymm10, %ymm8, %ymm9    ## ymm9 = (ymm8 * ymm10) + ymm9
	movq	136(%rsp), %rax                 ## 8-byte Reload
	vmovups	(%rax,%rbp,4), %ymm8
	movq	-56(%rsp), %rax                 ## 8-byte Reload
	vmulps	(%rax,%rbp,4), %ymm2, %ymm10
	vmaxps	%ymm15, %ymm8, %ymm8
	vfmadd231ps	%ymm10, %ymm8, %ymm9    ## ymm9 = (ymm8 * ymm10) + ymm9
	movq	128(%rsp), %rax                 ## 8-byte Reload
	vmovups	(%rax,%rbp,4), %ymm8
	vmaxps	%ymm15, %ymm8, %ymm8
	vmulps	%ymm7, %ymm3, %ymm10
	vfmadd231ps	%ymm10, %ymm8, %ymm9    ## ymm9 = (ymm8 * ymm10) + ymm9
	movq	120(%rsp), %rax                 ## 8-byte Reload
	vmovups	(%rax,%rbp,4), %ymm8
	vmaxps	%ymm15, %ymm8, %ymm8
	vmulps	%ymm7, %ymm2, %ymm7
	vfmadd231ps	%ymm7, %ymm8, %ymm9     ## ymm9 = (ymm8 * ymm7) + ymm9
	vmovups	(%rsi,%rbp,4), %ymm7
	movq	112(%rsp), %rax                 ## 8-byte Reload
	vmovups	(%rax,%rbp,4), %ymm8
	vmaxps	%ymm15, %ymm8, %ymm8
	vmulps	%ymm7, %ymm3, %ymm3
	vfmadd231ps	%ymm3, %ymm8, %ymm9     ## ymm9 = (ymm8 * ymm3) + ymm9
	vmulps	%ymm7, %ymm2, %ymm2
	movq	104(%rsp), %rax                 ## 8-byte Reload
	vmovups	(%rax,%rbp,4), %ymm3
	vmaxps	%ymm15, %ymm3, %ymm3
	vfmadd231ps	%ymm2, %ymm3, %ymm9     ## ymm9 = (ymm3 * ymm2) + ymm9
	movq	96(%rsp), %rax                  ## 8-byte Reload
	vmovups	(%rax,%rbp,4), %ymm2
	vmaxps	%ymm15, %ymm2, %ymm2
	movq	-64(%rsp), %rax                 ## 8-byte Reload
	vmulps	(%rax,%rbp,4), %ymm1, %ymm3
	vfmadd231ps	%ymm3, %ymm2, %ymm9     ## ymm9 = (ymm2 * ymm3) + ymm9
	movq	88(%rsp), %rax                  ## 8-byte Reload
	vmovups	(%rax,%rbp,4), %ymm2
	vmaxps	%ymm15, %ymm2, %ymm2
	movq	-72(%rsp), %rax                 ## 8-byte Reload
	vmulps	(%rax,%rbp,4), %ymm1, %ymm1
	vfmadd231ps	%ymm1, %ymm2, %ymm9     ## ymm9 = (ymm2 * ymm1) + ymm9
	vaddps	%ymm6, %ymm9, %ymm1
	movq	-24(%rsp), %rax                 ## 8-byte Reload
	vmovups	(%rax,%rbp,4), %ymm2
	vmaxps	%ymm15, %ymm2, %ymm2
	vfmadd231ps	%ymm2, %ymm5, %ymm1     ## ymm1 = (ymm5 * ymm2) + ymm1
	vbroadcastss	LCPI11_3(%rip), %ymm2   ## ymm2 = [-1.0E+0,-1.0E+0,-1.0E+0,-1.0E+0,-1.0E+0,-1.0E+0,-1.0E+0,-1.0E+0]
	vaddps	%ymm2, %ymm0, %ymm0
	movq	416(%rsp), %rax                 ## 8-byte Reload
	vmovups	(%rax,%rbp,4), %ymm2
	vmaxps	%ymm15, %ymm2, %ymm2
	vandps	%ymm2, %ymm4, %ymm2
	vmovups	(%r13,%rbp,4), %ymm3
	vfmadd213ps	%ymm1, %ymm3, %ymm2     ## ymm2 = (ymm3 * ymm2) + ymm1
	vmulps	%ymm3, %ymm0, %ymm0
	movq	168(%rsp), %rax                 ## 8-byte Reload
	vmovups	(%rax,%rbp,4), %ymm1
	vmaxps	%ymm15, %ymm1, %ymm1
	vfmadd231ps	%ymm0, %ymm1, %ymm2     ## ymm2 = (ymm1 * ymm0) + ymm2
	movq	32(%rsp), %rax                  ## 8-byte Reload
	vmovups	(%rax,%rbp,4), %ymm0
	vmaxps	%ymm15, %ymm0, %ymm0
	movq	352(%rsp), %rax                 ## 8-byte Reload
	vfmadd231ps	(%rax,%rbp,4), %ymm0, %ymm2 ## ymm2 = (ymm0 * mem) + ymm2
	vbroadcastss	LCPI11_4(%rip), %ymm0   ## ymm0 = [9.99999971E-10,9.99999971E-10,9.99999971E-10,9.99999971E-10,9.99999971E-10,9.99999971E-10,9.99999971E-10,9.99999971E-10]
	vfmadd213ps	(%rdx,%rbp,4), %ymm2, %ymm0 ## ymm0 = (ymm2 * ymm0) + mem
	vmovups	%ymm0, (%rdx,%rbp,4)
	addq	$8, %rbp
	cmpq	%rbp, 272(%rsp)                 ## 8-byte Folded Reload
	jne	LBB11_11
## %bb.12:                              ## %middle.block
                                        ##   in Loop: Header=BB11_8 Depth=1
	movq	272(%rsp), %rax                 ## 8-byte Reload
	movq	%rax, %r12
	cmpq	192(%rsp), %rax                 ## 8-byte Folded Reload
	movq	184(%rsp), %rbx                 ## 8-byte Reload
	movq	320(%rsp), %rcx                 ## 8-byte Reload
	movq	224(%rsp), %rsi                 ## 8-byte Reload
	movq	288(%rsp), %rdi                 ## 8-byte Reload
	movq	352(%rsp), %rax                 ## 8-byte Reload
	movq	280(%rsp), %r11                 ## 8-byte Reload
	jne	LBB11_13
	jmp	LBB11_19
	.p2align	4, 0x90
LBB11_18:                               ## %"for f1.s1.n.ni.us"
                                        ##   in Loop: Header=BB11_13 Depth=2
	movq	320(%rsp), %rcx                 ## 8-byte Reload
	movq	224(%rsp), %rsi                 ## 8-byte Reload
	movq	288(%rsp), %rdi                 ## 8-byte Reload
	movq	352(%rsp), %rax                 ## 8-byte Reload
	vfmadd213ss	%xmm0, %xmm5, %xmm3     ## xmm3 = (xmm5 * xmm3) + xmm0
	vfmadd213ss	%xmm3, %xmm4, %xmm2     ## xmm2 = (xmm4 * xmm2) + xmm3
	vfmadd132ss	(%rax,%r12,4), %xmm2, %xmm1 ## xmm1 = (xmm1 * mem) + xmm2
	vmovss	LCPI11_4(%rip), %xmm0           ## xmm0 = mem[0],zero,zero,zero
	vfmadd213ss	(%rdx,%r12,4), %xmm0, %xmm1 ## xmm1 = (xmm0 * xmm1) + mem
	vmovss	%xmm1, (%rdx,%r12,4)
	incq	%r12
	cmpq	%r12, 192(%rsp)                 ## 8-byte Folded Reload
	je	LBB11_19
LBB11_13:                               ## %"for f1.s1.n.ni.us"
                                        ##   Parent Loop BB11_8 Depth=1
                                        ## =>  This Inner Loop Header: Depth=2
	cmpq	$0, 384(%rsp)                   ## 8-byte Folded Reload
	movq	80(%rsp), %rax                  ## 8-byte Reload
	leaq	(%rax,%r12,4), %rax
	leaq	(%rdi,%r12,4), %rbp
	cmoveq	%rax, %rbp
	leaq	(%rsi,%r12,4), %rax
	leaq	(%rcx,%r12,4), %r13
	cmoveq	%rax, %r13
	vmovss	(%r14,%r12,4), %xmm13           ## xmm13 = mem[0],zero,zero,zero
	vmovss	(%r8,%r12,4), %xmm14            ## xmm14 = mem[0],zero,zero,zero
	vmovss	(%r9,%r12,4), %xmm12            ## xmm12 = mem[0],zero,zero,zero
	vmulss	(%r11,%r12,4), %xmm12, %xmm11
	vmovss	LCPI11_0(%rip), %xmm7           ## xmm7 = mem[0],zero,zero,zero
	vmaxss	%xmm7, %xmm11, %xmm9
	vdivss	448(%rsp), %xmm9, %xmm2         ## 16-byte Folded Reload
	movq	608(%rsp), %rax                 ## 8-byte Reload
	vmovss	(%rax,%r12,4), %xmm0            ## xmm0 = mem[0],zero,zero,zero
	vmulss	(%r10,%r12,4), %xmm13, %xmm4
	vucomiss	%xmm12, %xmm7
	vmaxss	%xmm7, %xmm0, %xmm8
	vxorps	%xmm3, %xmm3, %xmm3
	movq	-24(%rsp), %rax                 ## 8-byte Reload
	vmovss	(%rax,%r12,4), %xmm0            ## xmm0 = mem[0],zero,zero,zero
	movq	48(%rsp), %rax                  ## 8-byte Reload
	vmovss	(%rax,%r12,4), %xmm5            ## xmm5 = mem[0],zero,zero,zero
	vmaxss	%xmm3, %xmm5, %xmm5
	movq	56(%rsp), %rax                  ## 8-byte Reload
	vmovss	(%rax,%r12,4), %xmm6            ## xmm6 = mem[0],zero,zero,zero
	vmaxss	%xmm3, %xmm6, %xmm6
	vmulss	%xmm6, %xmm14, %xmm6
	vfmadd231ss	%xmm5, %xmm4, %xmm6     ## xmm6 = (xmm4 * xmm5) + xmm6
	vmovss	(%rbx,%r12,4), %xmm5            ## xmm5 = mem[0],zero,zero,zero
	vmaxss	%xmm3, %xmm5, %xmm5
	movq	64(%rsp), %rax                  ## 8-byte Reload
	vmovss	(%rax,%r12,4), %xmm1            ## xmm1 = mem[0],zero,zero,zero
	vmaxss	%xmm3, %xmm1, %xmm1
	vmulss	%xmm1, %xmm14, %xmm1
	vfmadd231ss	%xmm5, %xmm4, %xmm1     ## xmm1 = (xmm4 * xmm5) + xmm1
	movq	-104(%rsp), %rax                ## 8-byte Reload
	vcmpeqss	(%rax,%r12,4), %xmm3, %xmm4
	vblendvps	%xmm4, %xmm6, %xmm1, %xmm1
	vmaxss	%xmm3, %xmm0, %xmm0
	vroundss	$10, %xmm2, %xmm2, %xmm4
	vmaxss	%xmm7, %xmm2, %xmm2
	vmulss	%xmm1, %xmm4, %xmm1
	vdivss	%xmm2, %xmm1, %xmm2
	movq	72(%rsp), %rax                  ## 8-byte Reload
	vmovss	(%rax,%r12,4), %xmm1            ## xmm1 = mem[0],zero,zero,zero
	vmaxss	%xmm3, %xmm1, %xmm10
	vmovss	LCPI11_1(%rip), %xmm1           ## xmm1 = mem[0],zero,zero,zero
	vdivss	%xmm8, %xmm1, %xmm1
	vminss	%xmm12, %xmm1, %xmm1
	movq	-96(%rsp), %rax                 ## 8-byte Reload
	vmulss	(%rax,%r12,4), %xmm1, %xmm1
	vmulss	%xmm1, %xmm11, %xmm7
	jb	LBB11_14
## %bb.15:                              ## %"for f1.s1.n.ni.us"
                                        ##   in Loop: Header=BB11_13 Depth=2
	vxorps	%xmm11, %xmm11, %xmm11
	jmp	LBB11_16
	.p2align	4, 0x90
LBB11_14:                               ##   in Loop: Header=BB11_13 Depth=2
	movq	480(%rsp), %rax                 ## 8-byte Reload
	vmovss	(%rax,%r12,4), %xmm1            ## xmm1 = mem[0],zero,zero,zero
	vmaxss	%xmm3, %xmm1, %xmm1
	vaddss	%xmm14, %xmm13, %xmm4
	vmulss	%xmm1, %xmm4, %xmm1
	vdivss	%xmm8, %xmm1, %xmm11
LBB11_16:                               ## %"for f1.s1.n.ni.us"
                                        ##   in Loop: Header=BB11_13 Depth=2
	movq	40(%rsp), %rcx                  ## 8-byte Reload
	movq	32(%rsp), %rsi                  ## 8-byte Reload
	movq	-112(%rsp), %rdi                ## 8-byte Reload
	movq	512(%rsp), %rax                 ## 8-byte Reload
	leaq	(%rax,%r12,4), %rax
	cmovbq	%rax, %rbp
	leaq	(%rcx,%r12,4), %rax
	cmovbq	%rax, %r13
	vmovss	(%rbp), %xmm1                   ## xmm1 = mem[0],zero,zero,zero
	vmaxss	%xmm3, %xmm1, %xmm1
	movq	-88(%rsp), %rax                 ## 8-byte Reload
	vmulss	(%rax,%r12,4), %xmm12, %xmm4
	vmulss	%xmm1, %xmm4, %xmm1
	vmovss	(%r13), %xmm4                   ## xmm4 = mem[0],zero,zero,zero
	vdivss	%xmm8, %xmm1, %xmm1
	vmovss	(%r15,%r12,4), %xmm8            ## xmm8 = mem[0],zero,zero,zero
	vmaxss	%xmm3, %xmm4, %xmm4
	movq	-80(%rsp), %rax                 ## 8-byte Reload
	vfmadd132ss	(%rax,%r12,4), %xmm1, %xmm4 ## xmm4 = (xmm4 * mem) + xmm1
	movq	24(%rsp), %rax                  ## 8-byte Reload
	vmovss	(%rax,%r12,4), %xmm5            ## xmm5 = mem[0],zero,zero,zero
	vfmadd213ss	%xmm11, %xmm8, %xmm4    ## xmm4 = (xmm8 * xmm4) + xmm11
	vfmadd213ss	%xmm4, %xmm10, %xmm7    ## xmm7 = (xmm10 * xmm7) + xmm4
	movq	152(%rsp), %rax                 ## 8-byte Reload
	vmovss	(%rax,%r12,4), %xmm1            ## xmm1 = mem[0],zero,zero,zero
	vmaxss	%xmm3, %xmm1, %xmm1
	movq	160(%rsp), %rax                 ## 8-byte Reload
	vmovss	(%rax,%r12,4), %xmm4            ## xmm4 = mem[0],zero,zero,zero
	vmaxss	%xmm3, %xmm4, %xmm4
	vmulss	(%rdi,%r12,4), %xmm8, %xmm6
	vmulss	%xmm6, %xmm4, %xmm4
	movq	-32(%rsp), %rax                 ## 8-byte Reload
	vmulss	(%rax,%r12,4), %xmm8, %xmm6
	vfmadd231ss	%xmm6, %xmm1, %xmm4     ## xmm4 = (xmm1 * xmm6) + xmm4
	movq	144(%rsp), %rax                 ## 8-byte Reload
	vmovss	(%rax,%r12,4), %xmm1            ## xmm1 = mem[0],zero,zero,zero
	vmaxss	%xmm3, %xmm1, %xmm1
	movq	-40(%rsp), %rax                 ## 8-byte Reload
	vmulss	(%rax,%r12,4), %xmm13, %xmm6
	vfmadd231ss	%xmm6, %xmm1, %xmm4     ## xmm4 = (xmm1 * xmm6) + xmm4
	movq	544(%rsp), %rax                 ## 8-byte Reload
	vmovss	(%rax,%r12,4), %xmm1            ## xmm1 = mem[0],zero,zero,zero
	movq	-48(%rsp), %rax                 ## 8-byte Reload
	vmulss	(%rax,%r12,4), %xmm14, %xmm6
	vmaxss	%xmm3, %xmm1, %xmm1
	vfmadd231ss	%xmm6, %xmm1, %xmm4     ## xmm4 = (xmm1 * xmm6) + xmm4
	movq	136(%rsp), %rax                 ## 8-byte Reload
	vmovss	(%rax,%r12,4), %xmm1            ## xmm1 = mem[0],zero,zero,zero
	vmaxss	%xmm3, %xmm1, %xmm1
	movq	-56(%rsp), %rax                 ## 8-byte Reload
	vmulss	(%rax,%r12,4), %xmm13, %xmm6
	vfmadd231ss	%xmm6, %xmm1, %xmm4     ## xmm4 = (xmm1 * xmm6) + xmm4
	movq	128(%rsp), %rax                 ## 8-byte Reload
	vmovss	(%rax,%r12,4), %xmm1            ## xmm1 = mem[0],zero,zero,zero
	vmaxss	%xmm3, %xmm1, %xmm1
	vmulss	%xmm5, %xmm14, %xmm6
	vfmadd231ss	%xmm6, %xmm1, %xmm4     ## xmm4 = (xmm1 * xmm6) + xmm4
	movq	120(%rsp), %rax                 ## 8-byte Reload
	vmovss	(%rax,%r12,4), %xmm1            ## xmm1 = mem[0],zero,zero,zero
	vmaxss	%xmm3, %xmm1, %xmm1
	vmulss	%xmm5, %xmm13, %xmm5
	vfmadd231ss	%xmm5, %xmm1, %xmm4     ## xmm4 = (xmm1 * xmm5) + xmm4
	movq	200(%rsp), %rax                 ## 8-byte Reload
	vmovss	(%rax,%r12,4), %xmm1            ## xmm1 = mem[0],zero,zero,zero
	movq	112(%rsp), %rax                 ## 8-byte Reload
	vmovss	(%rax,%r12,4), %xmm5            ## xmm5 = mem[0],zero,zero,zero
	vmaxss	%xmm3, %xmm5, %xmm5
	vmulss	%xmm1, %xmm14, %xmm6
	vfmadd231ss	%xmm6, %xmm5, %xmm4     ## xmm4 = (xmm5 * xmm6) + xmm4
	vmulss	%xmm1, %xmm13, %xmm1
	movq	104(%rsp), %rax                 ## 8-byte Reload
	vmovss	(%rax,%r12,4), %xmm5            ## xmm5 = mem[0],zero,zero,zero
	vmaxss	%xmm3, %xmm5, %xmm5
	vfmadd231ss	%xmm1, %xmm5, %xmm4     ## xmm4 = (xmm5 * xmm1) + xmm4
	movq	96(%rsp), %rax                  ## 8-byte Reload
	vmovss	(%rax,%r12,4), %xmm1            ## xmm1 = mem[0],zero,zero,zero
	vmaxss	%xmm3, %xmm1, %xmm1
	movq	-64(%rsp), %rax                 ## 8-byte Reload
	vmulss	(%rax,%r12,4), %xmm9, %xmm5
	vfmadd231ss	%xmm5, %xmm1, %xmm4     ## xmm4 = (xmm1 * xmm5) + xmm4
	movq	88(%rsp), %rax                  ## 8-byte Reload
	vmovss	(%rax,%r12,4), %xmm1            ## xmm1 = mem[0],zero,zero,zero
	movq	-72(%rsp), %rax                 ## 8-byte Reload
	vmulss	(%rax,%r12,4), %xmm9, %xmm5
	vmaxss	%xmm3, %xmm1, %xmm1
	vfmadd231ss	%xmm5, %xmm1, %xmm4     ## xmm4 = (xmm1 * xmm5) + xmm4
	vfmadd231ss	LCPI11_2(%rip), %xmm7, %xmm2 ## xmm2 = (xmm7 * mem) + xmm2
	vaddss	%xmm4, %xmm2, %xmm1
	movq	576(%rsp), %rax                 ## 8-byte Reload
	vmovss	(%rax,%r12,4), %xmm5            ## xmm5 = mem[0],zero,zero,zero
	vfmadd213ss	%xmm1, %xmm8, %xmm0     ## xmm0 = (xmm8 * xmm0) + xmm1
	vmovss	(%rsi,%r12,4), %xmm1            ## xmm1 = mem[0],zero,zero,zero
	vmaxss	%xmm3, %xmm1, %xmm1
	movq	168(%rsp), %rax                 ## 8-byte Reload
	vmovss	(%rax,%r12,4), %xmm2            ## xmm2 = mem[0],zero,zero,zero
	vmaxss	%xmm3, %xmm2, %xmm2
	vaddss	LCPI11_3(%rip), %xmm12, %xmm4
	vmulss	%xmm5, %xmm4, %xmm4
	jae	LBB11_18
## %bb.17:                              ##   in Loop: Header=BB11_13 Depth=2
	movq	416(%rsp), %rax                 ## 8-byte Reload
	vmovss	(%rax,%r12,4), %xmm6            ## xmm6 = mem[0],zero,zero,zero
	vmaxss	%xmm3, %xmm6, %xmm3
	jmp	LBB11_18
                                        ## -- End function
	.section	__TEXT,__literal4,4byte_literals
	.p2align	2                               ## -- Begin function train_cost_model.par_for.f0_0_d_def__.s0.n.n.n
LCPI12_0:
	.long	0x3f800000                      ## float 1
LCPI12_1:
	.long	0x2edbe6ff                      ## float 1.00000001E-10
LCPI12_3:
	.long	0xc0000000                      ## float -2
LCPI12_4:
	.long	8                               ## 0x8
	.section	__TEXT,__const
	.p2align	5
LCPI12_2:
	.long	0                               ## 0x0
	.long	1                               ## 0x1
	.long	2                               ## 0x2
	.long	3                               ## 0x3
	.long	4                               ## 0x4
	.long	5                               ## 0x5
	.long	6                               ## 0x6
	.long	7                               ## 0x7
	.section	__TEXT,__text,regular,pure_instructions
	.p2align	4, 0x90
_train_cost_model.par_for.f0_0_d_def__.s0.n.n.n: ## @train_cost_model.par_for.f0_0_d_def__.s0.n.n.n
	.cfi_startproc
## %bb.0:                               ## %entry
	pushq	%rbp
	.cfi_def_cfa_offset 16
	pushq	%r15
	.cfi_def_cfa_offset 24
	pushq	%r14
	.cfi_def_cfa_offset 32
	pushq	%r13
	.cfi_def_cfa_offset 40
	pushq	%r12
	.cfi_def_cfa_offset 48
	pushq	%rbx
	.cfi_def_cfa_offset 56
	subq	$424, %rsp                      ## imm = 0x1A8
	.cfi_def_cfa_offset 480
	.cfi_offset %rbx, -56
	.cfi_offset %r12, -48
	.cfi_offset %r13, -40
	.cfi_offset %r14, -32
	.cfi_offset %r15, -24
	.cfi_offset %rbp, -16
	movq	%rdx, %rcx
	movl	%esi, %r8d
	movl	(%rdx), %r9d
	movl	16(%rdx), %ebx
	movl	%esi, %r11d
	sarl	$31, %r11d
	xorl	%r10d, %r10d
	testl	%ebx, %ebx
	sete	%r10b
	movl	%r10d, %ebp
	negl	%ebp
	subl	%r11d, %r8d
	orl	%ebx, %ebp
	movl	%r8d, %eax
	cltd
	idivl	%ebp
	movl	%ebx, %r12d
	sarl	$31, %r12d
	movl	%r12d, %esi
	xorl	%ebx, %esi
	movl	%r12d, %ebp
	notl	%ebp
	addl	%ebp, %esi
	movl	%r11d, 8(%rsp)                  ## 4-byte Spill
	andl	%r11d, %esi
	addl	%edx, %esi
	addl	%r10d, %ebx
	movl	%r8d, %eax
	cltd
	idivl	%ebx
	movl	%eax, %r13d
	leal	-1(%r10), %ebx
	andl	%ebx, %esi
	leal	(,%rsi,8), %edx
	movq	%rsi, 56(%rsp)                  ## 8-byte Spill
	leal	8(,%rsi,8), %eax
	cmpl	%eax, %r9d
	movq	%r9, 152(%rsp)                  ## 8-byte Spill
                                        ## kill: def $r9d killed $r9d killed $r9 def $r9
	movl	%eax, 108(%rsp)                 ## 4-byte Spill
	cmovgl	%eax, %r9d
	movq	%r9, 144(%rsp)                  ## 8-byte Spill
	movl	%r9d, %eax
	movq	%rdx, 136(%rsp)                 ## 8-byte Spill
	subl	%edx, %eax
	movl	%eax, %edx
	sarl	$31, %edx
	andnl	%eax, %edx, %eax
	leaq	(,%rax,4), %rdx
	cmpl	$536870912, %eax                ## imm = 0x20000000
	jae	LBB12_62
## %bb.1:                               ## %"assert succeeded"
	movl	%ebx, 24(%rsp)                  ## 4-byte Spill
	movl	12(%rcx), %eax
	movl	%eax, 104(%rsp)                 ## 4-byte Spill
	movslq	4(%rcx), %rax
	movq	%rax, 48(%rsp)                  ## 8-byte Spill
	movslq	8(%rcx), %rax
	movq	%rax, 240(%rsp)                 ## 8-byte Spill
	movslq	20(%rcx), %rax
	movq	%rax, 96(%rsp)                  ## 8-byte Spill
	movslq	24(%rcx), %rax
	movq	%rax, 88(%rsp)                  ## 8-byte Spill
	movslq	28(%rcx), %rax
	movq	%rax, 80(%rsp)                  ## 8-byte Spill
	movq	32(%rcx), %rax
	movq	%rax, 40(%rsp)                  ## 8-byte Spill
	movq	48(%rcx), %r15
	movq	64(%rcx), %rax
	movq	%rax, 72(%rsp)                  ## 8-byte Spill
	movq	80(%rcx), %r14
	addq	$12, %rdx
	movq	%rdi, 120(%rsp)                 ## 8-byte Spill
	movq	%rdx, %rsi
	callq	_halide_malloc
	testq	%rax, %rax
	je	LBB12_63
## %bb.2:                               ## %"assert succeeded2"
	subl	%r12d, %ebp
	andl	8(%rsp), %ebp                   ## 4-byte Folded Reload
	movq	%r13, %r9
	addl	%ebp, %r9d
	movq	144(%rsp), %r11                 ## 8-byte Reload
	movl	%r11d, %ecx
	sarl	$3, %ecx
	movq	%rcx, %r13
	movl	%ecx, %r8d
	subl	56(%rsp), %r8d                  ## 4-byte Folded Reload
	movl	%r8d, %edx
	sarl	$31, %edx
	andnl	%r8d, %edx, %ecx
	movq	152(%rsp), %r12                 ## 8-byte Reload
	vmovd	%r12d, %xmm10
	movq	%rcx, 64(%rsp)                  ## 8-byte Spill
	movl	%ecx, %ecx
	movq	%rcx, 160(%rsp)                 ## 8-byte Spill
	testl	%r8d, %r8d
	jle	LBB12_5
## %bb.3:                               ## %"for f1_1_d_def__.s0.n.n.preheader"
	movslq	56(%rsp), %rcx                  ## 4-byte Folded Reload
	vmovss	LCPI12_0(%rip), %xmm0           ## xmm0 = mem[0],zero,zero,zero
	movq	96(%rsp), %rdx                  ## 8-byte Reload
	vdivss	(%r14,%rdx,4), %xmm0, %xmm0
	vbroadcastss	%xmm0, %ymm0
	vpbroadcastd	%xmm10, %ymm2
	movq	72(%rsp), %rdx                  ## 8-byte Reload
	vpbroadcastd	(%rdx), %ymm3
	movq	136(%rsp), %rdx                 ## 8-byte Reload
	leaq	1(%rdx), %r10
	shlq	$3, %rcx
	movq	%rcx, %rsi
	subq	80(%rsp), %rsi                  ## 8-byte Folded Reload
	leaq	(%r14,%rsi,4), %rsi
	subq	88(%rsp), %rcx                  ## 8-byte Folded Reload
	leaq	(%r15,%rcx,4), %rdx
	xorl	%ebp, %ebp
	vbroadcastss	LCPI12_1(%rip), %ymm4   ## ymm4 = [1.00000001E-10,1.00000001E-10,1.00000001E-10,1.00000001E-10,1.00000001E-10,1.00000001E-10,1.00000001E-10,1.00000001E-10]
	vmovdqa	LCPI12_2(%rip), %ymm5           ## ymm5 = [0,1,2,3,4,5,6,7]
	vbroadcastss	LCPI12_0(%rip), %ymm6   ## ymm6 = [1.0E+0,1.0E+0,1.0E+0,1.0E+0,1.0E+0,1.0E+0,1.0E+0,1.0E+0]
	vbroadcastss	LCPI12_3(%rip), %ymm7   ## ymm7 = [-2.0E+0,-2.0E+0,-2.0E+0,-2.0E+0,-2.0E+0,-2.0E+0,-2.0E+0,-2.0E+0]
	movq	160(%rsp), %rbx                 ## 8-byte Reload
	.p2align	4, 0x90
LBB12_4:                                ## %"for f1_1_d_def__.s0.n.n"
                                        ## =>This Inner Loop Header: Depth=1
	vmulps	(%rdx,%rbp,4), %ymm0, %ymm8
	vmaxps	%ymm4, %ymm8, %ymm9
	leal	(%r10,%rbp), %ecx
	vmovd	%ecx, %xmm1
	vpbroadcastd	%xmm1, %ymm1
	vdivps	%ymm9, %ymm6, %ymm11
	vpaddd	%ymm5, %ymm1, %ymm1
	vpcmpgtd	%ymm2, %ymm1, %ymm1
	vmulps	(%rsi,%rbp,4), %ymm0, %ymm12
	vpandn	%ymm3, %ymm1, %ymm1
	vdivps	%ymm12, %ymm6, %ymm12
	vmulps	%ymm7, %ymm1, %ymm1
	vsubps	%ymm12, %ymm11, %ymm11
	vmulps	%ymm1, %ymm11, %ymm1
	vmulps	%ymm9, %ymm9, %ymm9
	vmulps	%ymm1, %ymm0, %ymm1
	vdivps	%ymm9, %ymm1, %ymm1
	vcmpleps	%ymm8, %ymm4, %ymm8
	vandps	%ymm1, %ymm8, %ymm1
	vmovaps	%ymm1, (%rax,%rbp,4)
	addq	$8, %rbp
	decq	%rbx
	jne	LBB12_4
LBB12_5:                                ## %"end for f1_1_d_def__.s0.n.n"
	movq	%rax, 16(%rsp)                  ## 8-byte Spill
	andl	24(%rsp), %r9d                  ## 4-byte Folded Reload
	movq	%r9, 128(%rsp)                  ## 8-byte Spill
	leal	7(%r11), %ecx
	sarl	$3, %ecx
	movq	56(%rsp), %r9                   ## 8-byte Reload
	movl	%r9d, %edx
	subl	%r13d, %edx
	cmovgl	%r9d, %r13d
	movq	%r13, 168(%rsp)                 ## 8-byte Spill
	movl	%edx, %esi
	sarl	$31, %esi
	andl	%edx, %esi
	subl	%r9d, %ecx
	addl	%esi, %ecx
	addl	64(%rsp), %ecx                  ## 4-byte Folded Reload
	leal	-1(%r11), %edx
	sarl	$3, %edx
	subl	%r9d, %edx
	incl	%edx
	cmpl	%edx, %ecx
	cmovlel	%ecx, %edx
	cmpl	%r8d, %edx
	cmovlel	%r8d, %edx
	movl	%edx, %ecx
	sarl	$31, %ecx
	andnl	%edx, %ecx, %ecx
	addl	%esi, %ecx
	movq	88(%rsp), %rsi                  ## 8-byte Reload
	movq	80(%rsp), %rbx                  ## 8-byte Reload
	movq	72(%rsp), %r13                  ## 8-byte Reload
	jle	LBB12_10
## %bb.6:                               ## %"for f1_1_d_def__.s0.n.n.rebased.preheader"
	leal	-1(%r12), %edx
	vmovd	%edx, %xmm0
	vpbroadcastd	%xmm0, %ymm12
	vmovd	%esi, %xmm0
	vpbroadcastd	%xmm0, %ymm14
	vpbroadcastd	%xmm10, %ymm11
	vmovd	%ebx, %xmm0
	vpbroadcastd	%xmm0, %ymm13
	movl	%ecx, %eax
	movq	%rax, 248(%rsp)                 ## 8-byte Spill
	movq	168(%rsp), %rax                 ## 8-byte Reload
	leal	(,%rax,8), %eax
	movl	%r11d, %ecx
	movq	%rax, 24(%rsp)                  ## 8-byte Spill
	subl	%eax, %ecx
	vmovq	%rsi, %xmm0
	vpbroadcastq	%xmm0, %ymm0
	vmovdqu	%ymm0, 320(%rsp)                ## 32-byte Spill
	vmovq	%rbx, %xmm0
	vpbroadcastq	%xmm0, %ymm15
	movq	64(%rsp), %rax                  ## 8-byte Reload
	leal	(,%rax,8), %ebp
	movl	%ebp, 36(%rsp)                  ## 4-byte Spill
	addl	%r9d, %eax
	shll	$3, %eax
	movq	%rax, 8(%rsp)                   ## 8-byte Spill
	xorl	%edi, %edi
	vmovss	LCPI12_0(%rip), %xmm10          ## xmm10 = mem[0],zero,zero,zero
	vmovss	LCPI12_1(%rip), %xmm5           ## xmm5 = mem[0],zero,zero,zero
	vmovss	LCPI12_3(%rip), %xmm8           ## xmm8 = mem[0],zero,zero,zero
	vxorps	%xmm9, %xmm9, %xmm9
	movl	%ecx, 112(%rsp)                 ## 4-byte Spill
	movl	%edx, 32(%rsp)                  ## 4-byte Spill
	vmovdqu	%ymm14, 384(%rsp)               ## 32-byte Spill
	vmovdqu	%ymm13, 352(%rsp)               ## 32-byte Spill
	vmovdqu	%ymm11, 192(%rsp)               ## 32-byte Spill
	jmp	LBB12_7
	.p2align	4, 0x90
LBB12_8:                                ## %then_bb
                                        ##   in Loop: Header=BB12_7 Depth=1
	vmovd	%ecx, %xmm0
	vpbroadcastd	%xmm0, %ymm0
	vmovdqa	LCPI12_2(%rip), %ymm5           ## ymm5 = [0,1,2,3,4,5,6,7]
	vpor	%ymm5, %ymm0, %ymm0
	vpminsd	%ymm0, %ymm12, %ymm0
	vpmaxsd	%ymm9, %ymm0, %ymm8
	vpsubd	%ymm14, %ymm8, %ymm0
	vextracti128	$1, %ymm0, %xmm1
	vmovd	%xmm1, %ecx
	movslq	%ecx, %rcx
	vpextrd	$1, %xmm1, %edx
	movslq	%edx, %rdx
	vmovss	(%r15,%rcx,4), %xmm2            ## xmm2 = mem[0],zero,zero,zero
	vpextrd	$2, %xmm1, %ecx
	movslq	%ecx, %rcx
	vinsertps	$16, (%r15,%rdx,4), %xmm2, %xmm2 ## xmm2 = xmm2[0],mem[0],xmm2[2,3]
	vinsertps	$32, (%r15,%rcx,4), %xmm2, %xmm2 ## xmm2 = xmm2[0,1],mem[0],xmm2[3]
	vmovd	%xmm0, %ecx
	vpextrd	$3, %xmm1, %edx
	movslq	%edx, %rdx
	vinsertps	$48, (%r15,%rdx,4), %xmm2, %xmm1 ## xmm1 = xmm2[0,1,2],mem[0]
	vpextrd	$1, %xmm0, %edx
	movslq	%ecx, %rcx
	vmovss	(%r15,%rcx,4), %xmm2            ## xmm2 = mem[0],zero,zero,zero
	vpextrd	$2, %xmm0, %ecx
	movslq	%edx, %rdx
	movslq	%ecx, %rcx
	vinsertps	$16, (%r15,%rdx,4), %xmm2, %xmm2 ## xmm2 = xmm2[0],mem[0],xmm2[2,3]
	vpextrd	$3, %xmm0, %edx
	vinsertps	$32, (%r15,%rcx,4), %xmm2, %xmm0 ## xmm0 = xmm2[0,1],mem[0],xmm2[3]
	movslq	%edx, %rcx
	vinsertps	$48, (%r15,%rcx,4), %xmm0, %xmm3 ## xmm3 = xmm0[0,1,2],mem[0]
	vpsubd	%ymm13, %ymm8, %ymm0
	vextracti128	$1, %ymm0, %xmm2
	vmovd	%xmm2, %ecx
	vpextrd	$1, %xmm2, %edx
	movslq	%ecx, %rcx
	vmovss	(%r14,%rcx,4), %xmm7            ## xmm7 = mem[0],zero,zero,zero
	vpextrd	$2, %xmm2, %ecx
	movslq	%edx, %rdx
	movslq	%ecx, %rcx
	vinsertps	$16, (%r14,%rdx,4), %xmm7, %xmm7 ## xmm7 = xmm7[0],mem[0],xmm7[2,3]
	vpextrd	$3, %xmm2, %edx
	vinsertps	$32, (%r14,%rcx,4), %xmm7, %xmm2 ## xmm2 = xmm7[0,1],mem[0],xmm7[3]
	vmovd	%xmm0, %ecx
	movslq	%edx, %rdx
	vinsertps	$48, (%r14,%rdx,4), %xmm2, %xmm2 ## xmm2 = xmm2[0,1,2],mem[0]
	movslq	%ecx, %rcx
	vpextrd	$1, %xmm0, %edx
	movslq	%edx, %rdx
	vmovss	(%r14,%rcx,4), %xmm7            ## xmm7 = mem[0],zero,zero,zero
	vpextrd	$2, %xmm0, %ecx
	movslq	%ecx, %rcx
	vinsertps	$16, (%r14,%rdx,4), %xmm7, %xmm7 ## xmm7 = xmm7[0],mem[0],xmm7[2,3]
	vinsertps	$32, (%r14,%rcx,4), %xmm7, %xmm7 ## xmm7 = xmm7[0,1],mem[0],xmm7[3]
	movq	160(%rsp), %rax                 ## 8-byte Reload
	leaq	(%rdi,%rax), %rcx
	vpextrd	$3, %xmm0, %edx
	movslq	%edx, %rdx
	vinsertps	$48, (%r14,%rdx,4), %xmm7, %xmm0 ## xmm0 = xmm7[0,1,2],mem[0]
	leal	(%r9,%rcx), %edx
	leal	1(,%rdx,8), %edx
	vmovd	%edx, %xmm7
	movl	32(%rsp), %edx                  ## 4-byte Reload
	vinsertf128	$1, %xmm1, %ymm3, %ymm1
	movq	96(%rsp), %rax                  ## 8-byte Reload
	vdivss	(%r14,%rax,4), %xmm10, %xmm3
	vbroadcastss	%xmm3, %ymm3
	vmulps	%ymm1, %ymm3, %ymm1
	vbroadcastss	LCPI12_1(%rip), %ymm8   ## ymm8 = [1.00000001E-10,1.00000001E-10,1.00000001E-10,1.00000001E-10,1.00000001E-10,1.00000001E-10,1.00000001E-10,1.00000001E-10]
	vmaxps	%ymm8, %ymm1, %ymm9
	vpbroadcastd	%xmm7, %ymm7
	vpaddd	%ymm5, %ymm7, %ymm7
	vpbroadcastd	(%r13), %ymm10
	vmovdqa	%ymm11, %ymm4
	vbroadcastss	LCPI12_0(%rip), %ymm11  ## ymm11 = [1.0E+0,1.0E+0,1.0E+0,1.0E+0,1.0E+0,1.0E+0,1.0E+0,1.0E+0]
	vinsertf128	$1, %xmm2, %ymm0, %ymm0
	vdivps	%ymm9, %ymm11, %ymm2
	vmulps	%ymm0, %ymm3, %ymm0
	vdivps	%ymm0, %ymm11, %ymm0
	vmovdqa	%ymm4, %ymm11
	vpcmpgtd	%ymm4, %ymm7, %ymm7
	vpandn	%ymm10, %ymm7, %ymm7
	vsubps	%ymm0, %ymm2, %ymm0
	vbroadcastss	LCPI12_3(%rip), %ymm2   ## ymm2 = [-2.0E+0,-2.0E+0,-2.0E+0,-2.0E+0,-2.0E+0,-2.0E+0,-2.0E+0,-2.0E+0]
	vmulps	%ymm2, %ymm7, %ymm2
	vmulps	%ymm9, %ymm9, %ymm7
	vxorps	%xmm9, %xmm9, %xmm9
	vmovss	LCPI12_0(%rip), %xmm10          ## xmm10 = mem[0],zero,zero,zero
	vmulps	%ymm3, %ymm2, %ymm2
	vmulps	%ymm2, %ymm0, %ymm0
	vdivps	%ymm7, %ymm0, %ymm0
	vcmpleps	%ymm1, %ymm8, %ymm1
	vmovss	LCPI12_3(%rip), %xmm8           ## xmm8 = mem[0],zero,zero,zero
	vmovss	LCPI12_1(%rip), %xmm5           ## xmm5 = mem[0],zero,zero,zero
	vandps	%ymm0, %ymm1, %ymm0
	shlq	$5, %rcx
	movq	16(%rsp), %rax                  ## 8-byte Reload
	vmovaps	%ymm0, (%rax,%rcx)
LBB12_9:                                ## %after_bb
                                        ##   in Loop: Header=BB12_7 Depth=1
	movq	184(%rsp), %rdi                 ## 8-byte Reload
	incq	%rdi
	movl	116(%rsp), %ecx                 ## 4-byte Reload
	addl	$-8, %ecx
	addl	$8, 36(%rsp)                    ## 4-byte Folded Spill
	addq	$8, 8(%rsp)                     ## 8-byte Folded Spill
	addq	$8, 24(%rsp)                    ## 8-byte Folded Spill
	cmpq	248(%rsp), %rdi                 ## 8-byte Folded Reload
	je	LBB12_10
LBB12_7:                                ## %"for f1_1_d_def__.s0.n.n.rebased"
                                        ## =>This Loop Header: Depth=1
                                        ##     Child Loop BB12_20 Depth 2
                                        ##     Child Loop BB12_22 Depth 2
	cmpl	$8, %ecx
	movl	$8, %r8d
	movl	%ecx, 116(%rsp)                 ## 4-byte Spill
	cmovll	%ecx, %r8d
	leal	(,%rdi,8), %ecx
	movl	112(%rsp), %eax                 ## 4-byte Reload
	movl	%eax, %ebp
	subl	%ecx, %ebp
	cmpl	$8, %ebp
	movl	$8, %eax
	cmovgel	%eax, %ebp
	movq	%rbp, 176(%rsp)                 ## 8-byte Spill
	movq	168(%rsp), %rax                 ## 8-byte Reload
	leal	(%rax,%rdi), %ecx
	leal	8(,%rcx,8), %ebp
                                        ## kill: def $ecx killed $ecx killed $rcx
	shll	$3, %ecx
	cmpl	%r11d, %ebp
	movq	%rdi, 184(%rsp)                 ## 8-byte Spill
	jle	LBB12_8
## %bb.16:                              ## %next_bb
                                        ##   in Loop: Header=BB12_7 Depth=1
	cmpl	%ecx, %r11d
	jle	LBB12_9
## %bb.17:                              ## %"for f1_1_d_def__.s0.n.ni.preheader"
                                        ##   in Loop: Header=BB12_7 Depth=1
	movl	36(%rsp), %ebp                  ## 4-byte Reload
	movq	16(%rsp), %rax                  ## 8-byte Reload
	leaq	(%rax,%rbp,4), %r10
	movq	96(%rsp), %rax                  ## 8-byte Reload
	vdivss	(%r14,%rax,4), %xmm10, %xmm3
	vmovd	(%r13), %xmm4                   ## xmm4 = mem[0],zero,zero,zero
	movq	176(%rsp), %rbp                 ## 8-byte Reload
	cmpl	$8, %ebp
	jae	LBB12_19
## %bb.18:                              ##   in Loop: Header=BB12_7 Depth=1
	xorl	%ebp, %ebp
	jmp	LBB12_22
	.p2align	4, 0x90
LBB12_19:                               ## %vector.ph
                                        ##   in Loop: Header=BB12_7 Depth=1
	movl	%r8d, %r11d
	andl	$-8, %r11d
	movq	64(%rsp), %rax                  ## 8-byte Reload
	movq	184(%rsp), %rdx                 ## 8-byte Reload
	leal	(%rax,%rdx), %r12d
	addl	%r9d, %r12d
	shll	$3, %r12d
                                        ## kill: def $ebp killed $ebp killed $rbp def $rbp
	andl	$-8, %ebp
	vmovd	%ecx, %xmm0
	vpbroadcastd	%xmm0, %ymm0
	vmovdqu	%ymm0, 256(%rsp)                ## 32-byte Spill
	vmovdqu	320(%rsp), %ymm10               ## 32-byte Reload
	vbroadcastss	%xmm3, %ymm14
	vmovd	%r12d, %xmm0
	vpbroadcastd	%xmm0, %ymm5
	vmovdqa	%xmm4, 304(%rsp)                ## 16-byte Spill
	vpbroadcastd	%xmm4, %ymm8
	xorl	%r12d, %r12d
	vpxor	%xmm13, %xmm13, %xmm13
	vmovdqa	LCPI12_2(%rip), %ymm9           ## ymm9 = [0,1,2,3,4,5,6,7]
	.p2align	4, 0x90
LBB12_20:                               ## %vector.body
                                        ##   Parent Loop BB12_7 Depth=1
                                        ## =>  This Inner Loop Header: Depth=2
	vpaddd	256(%rsp), %ymm9, %ymm0         ## 32-byte Folded Reload
	vpminsd	%ymm0, %ymm12, %ymm0
	vextracti128	$1, %ymm0, %xmm1
	vpmovsxdq	%xmm1, %ymm1
	vpmovsxdq	%xmm0, %ymm0
	vpcmpgtq	%ymm13, %ymm0, %ymm2
	vmovdqa	%ymm12, %ymm4
	vpand	%ymm0, %ymm2, %ymm12
	vpcmpgtq	%ymm13, %ymm1, %ymm0
	vpand	%ymm1, %ymm0, %ymm0
	vpsubq	%ymm10, %ymm0, %ymm1
	vpsubq	%ymm10, %ymm12, %ymm2
	vmovq	%xmm2, %r13
	vpextrq	$1, %xmm2, %r9
	vextracti128	$1, %ymm2, %xmm2
	vmovq	%xmm2, %rdx
	vpextrq	$1, %xmm2, %rcx
	vpextrq	$1, %xmm1, %rax
	vextracti128	$1, %ymm1, %xmm2
	vmovq	%xmm1, %rsi
	vmovss	(%r15,%r13,4), %xmm1            ## xmm1 = mem[0],zero,zero,zero
	vinsertps	$16, (%r15,%r9,4), %xmm1, %xmm1 ## xmm1 = xmm1[0],mem[0],xmm1[2,3]
	vmovq	%xmm2, %rdi
	vbroadcastss	LCPI12_1(%rip), %ymm11  ## ymm11 = [1.00000001E-10,1.00000001E-10,1.00000001E-10,1.00000001E-10,1.00000001E-10,1.00000001E-10,1.00000001E-10,1.00000001E-10]
	vpaddd	%ymm5, %ymm9, %ymm7
	vpextrq	$1, %xmm2, %rbx
	vmovdqu	192(%rsp), %ymm2                ## 32-byte Reload
	vpcmpgtd	%ymm7, %ymm2, %ymm2
	vpand	%ymm2, %ymm8, %ymm2
	vpsubq	%ymm15, %ymm0, %ymm0
	vmovss	(%r15,%rsi,4), %xmm7            ## xmm7 = mem[0],zero,zero,zero
	vpsubq	%ymm15, %ymm12, %ymm12
	vmovq	%xmm12, %rsi
	vinsertps	$32, (%r15,%rdx,4), %xmm1, %xmm1 ## xmm1 = xmm1[0,1],mem[0],xmm1[3]
	vpextrq	$1, %xmm12, %rdx
	vinsertps	$16, (%r15,%rax,4), %xmm7, %xmm7 ## xmm7 = xmm7[0],mem[0],xmm7[2,3]
	vextracti128	$1, %ymm12, %xmm6
	vmovdqa	%ymm4, %ymm12
	vmovq	%xmm6, %rax
	vinsertps	$48, (%r15,%rcx,4), %xmm1, %xmm1 ## xmm1 = xmm1[0,1,2],mem[0]
	vpextrq	$1, %xmm6, %rcx
	vinsertps	$32, (%r15,%rdi,4), %xmm7, %xmm6 ## xmm6 = xmm7[0,1],mem[0],xmm7[3]
	vpextrq	$1, %xmm0, %rdi
	vinsertps	$48, (%r15,%rbx,4), %xmm6, %xmm6 ## xmm6 = xmm6[0,1,2],mem[0]
	vmovq	%xmm0, %rbx
	vextracti128	$1, %ymm0, %xmm0
	vmovss	(%r14,%rbx,4), %xmm7            ## xmm7 = mem[0],zero,zero,zero
	vmovq	%xmm0, %rbx
	vinsertps	$16, (%r14,%rdi,4), %xmm7, %xmm7 ## xmm7 = xmm7[0],mem[0],xmm7[2,3]
	vpextrq	$1, %xmm0, %rdi
	vinsertps	$32, (%r14,%rbx,4), %xmm7, %xmm0 ## xmm0 = xmm7[0,1],mem[0],xmm7[3]
	vinsertps	$48, (%r14,%rdi,4), %xmm0, %xmm0 ## xmm0 = xmm0[0,1,2],mem[0]
	vmovss	(%r14,%rsi,4), %xmm7            ## xmm7 = mem[0],zero,zero,zero
	vinsertps	$16, (%r14,%rdx,4), %xmm7, %xmm7 ## xmm7 = xmm7[0],mem[0],xmm7[2,3]
	vinsertf128	$1, %xmm6, %ymm1, %ymm1
	vbroadcastss	LCPI12_0(%rip), %ymm6   ## ymm6 = [1.0E+0,1.0E+0,1.0E+0,1.0E+0,1.0E+0,1.0E+0,1.0E+0,1.0E+0]
	vinsertps	$32, (%r14,%rax,4), %xmm7, %xmm7 ## xmm7 = xmm7[0,1],mem[0],xmm7[3]
	vmulps	%ymm1, %ymm14, %ymm1
	vinsertps	$48, (%r14,%rcx,4), %xmm7, %xmm7 ## xmm7 = xmm7[0,1,2],mem[0]
	vinsertf128	$1, %xmm0, %ymm7, %ymm0
	vmaxps	%ymm11, %ymm1, %ymm7
	vmulps	%ymm0, %ymm14, %ymm0
	vdivps	%ymm0, %ymm6, %ymm0
	vdivps	%ymm7, %ymm6, %ymm6
	vsubps	%ymm0, %ymm6, %ymm0
	vbroadcastss	LCPI12_3(%rip), %ymm6   ## ymm6 = [-2.0E+0,-2.0E+0,-2.0E+0,-2.0E+0,-2.0E+0,-2.0E+0,-2.0E+0,-2.0E+0]
	vmulps	%ymm6, %ymm2, %ymm2
	vmulps	%ymm2, %ymm0, %ymm0
	vmulps	%ymm7, %ymm7, %ymm2
	vmulps	%ymm0, %ymm14, %ymm0
	vdivps	%ymm2, %ymm0, %ymm0
	vcmpleps	%ymm1, %ymm11, %ymm1
	vandps	%ymm0, %ymm1, %ymm0
	vmovups	%ymm0, (%r10,%r12,4)
	vpbroadcastd	LCPI12_4(%rip), %ymm0   ## ymm0 = [8,8,8,8,8,8,8,8]
	vpaddd	%ymm0, %ymm9, %ymm9
	addq	$8, %r12
	cmpq	%r12, %r11
	jne	LBB12_20
## %bb.21:                              ## %middle.block
                                        ##   in Loop: Header=BB12_7 Depth=1
	cmpq	176(%rsp), %rbp                 ## 8-byte Folded Reload
	movq	152(%rsp), %r12                 ## 8-byte Reload
	movq	56(%rsp), %r9                   ## 8-byte Reload
	movq	144(%rsp), %r11                 ## 8-byte Reload
	movq	88(%rsp), %rsi                  ## 8-byte Reload
	movq	80(%rsp), %rbx                  ## 8-byte Reload
	movq	72(%rsp), %r13                  ## 8-byte Reload
	movl	32(%rsp), %edx                  ## 4-byte Reload
	vmovdqu	384(%rsp), %ymm14               ## 32-byte Reload
	vmovdqu	192(%rsp), %ymm11               ## 32-byte Reload
	vmovdqu	352(%rsp), %ymm13               ## 32-byte Reload
	vmovss	LCPI12_0(%rip), %xmm10          ## xmm10 = mem[0],zero,zero,zero
	vmovss	LCPI12_1(%rip), %xmm5           ## xmm5 = mem[0],zero,zero,zero
	vmovss	LCPI12_3(%rip), %xmm8           ## xmm8 = mem[0],zero,zero,zero
	vpxor	%xmm9, %xmm9, %xmm9
	vmovdqa	304(%rsp), %xmm4                ## 16-byte Reload
	jne	LBB12_22
	jmp	LBB12_9
	.p2align	4, 0x90
LBB12_26:                               ## %select.end
                                        ##   in Loop: Header=BB12_22 Depth=2
	vmovss	%xmm2, (%r10,%rbp,4)
	incq	%rbp
	cmpq	%rbp, %r8
	je	LBB12_9
LBB12_22:                               ## %"for f1_1_d_def__.s0.n.ni"
                                        ##   Parent Loop BB12_7 Depth=1
                                        ## =>  This Inner Loop Header: Depth=2
	movq	24(%rsp), %rax                  ## 8-byte Reload
	addl	%ebp, %eax
	cmpl	%eax, %edx
	cmovlel	%edx, %eax
	cltq
	movq	%rax, %rcx
	sarq	$63, %rcx
	andnq	%rax, %rcx, %rcx
	movq	%rcx, %rax
	subq	%rsi, %rax
	vmulss	(%r15,%rax,4), %xmm3, %xmm7
	vmaxss	%xmm5, %xmm7, %xmm0
	movq	8(%rsp), %rax                   ## 8-byte Reload
	addl	%ebp, %eax
	vpxor	%xmm2, %xmm2, %xmm2
	vmovdqa	%xmm4, %xmm1
	cmpl	%r12d, %eax
	jl	LBB12_24
## %bb.23:                              ## %"for f1_1_d_def__.s0.n.ni"
                                        ##   in Loop: Header=BB12_22 Depth=2
	vpxor	%xmm1, %xmm1, %xmm1
LBB12_24:                               ## %"for f1_1_d_def__.s0.n.ni"
                                        ##   in Loop: Header=BB12_22 Depth=2
	subq	%rbx, %rcx
	vucomiss	%xmm5, %xmm7
	vdivss	%xmm0, %xmm10, %xmm6
	vmulss	(%r14,%rcx,4), %xmm3, %xmm7
	vdivss	%xmm7, %xmm10, %xmm7
	vsubss	%xmm7, %xmm6, %xmm6
	vmulss	%xmm6, %xmm1, %xmm1
	vmulss	%xmm1, %xmm8, %xmm1
	jb	LBB12_26
## %bb.25:                              ## %select.false.sink
                                        ##   in Loop: Header=BB12_22 Depth=2
	vmulss	%xmm0, %xmm0, %xmm0
	vmulss	%xmm1, %xmm3, %xmm1
	vdivss	%xmm0, %xmm1, %xmm2
	jmp	LBB12_26
LBB12_10:                               ## %"consume f1_1_d_def__"
	movq	128(%rsp), %rax                 ## 8-byte Reload
	leal	(%rax,%rax), %r8d
	movl	104(%rsp), %edx                 ## 4-byte Reload
	subl	%r8d, %edx
	cmpl	$2, %edx
	movl	$2, %ecx
	cmovll	%edx, %ecx
	xorl	%r13d, %r13d
	testl	%edx, %edx
	cmovlel	%r13d, %ecx
	jle	LBB12_61
## %bb.11:                              ## %"for f0_0_d_def__.s0.w.wi.preheader"
	movl	%r12d, %edx
	movq	136(%rsp), %rsi                 ## 8-byte Reload
	subl	%esi, %edx
	cltq
	subq	240(%rsp), %rax                 ## 8-byte Folded Reload
	addq	%rax, %rax
	movq	%rax, 8(%rsp)                   ## 8-byte Spill
	cmpl	$8, %edx
	movl	$8, %eax
	cmovll	%edx, %eax
	testl	%edx, %edx
	cmovgl	%eax, %r13d
	movslq	%esi, %rax
	movq	%rax, 192(%rsp)                 ## 8-byte Spill
	cmpl	%r12d, 108(%rsp)                ## 4-byte Folded Reload
	jle	LBB12_44
## %bb.12:                              ## %"for f0_0_d_def__.s0.w.wi.preheader.split.us"
	testl	%edx, %edx
	jle	LBB12_61
## %bb.13:                              ## %"for f0_0_d_def__.s0.w.wi.us.us.preheader"
	movl	%r13d, %edi
	movslq	%r8d, %r14
	movl	%ecx, %r12d
	movl	%edi, %r10d
	andl	$2147483616, %r10d              ## imm = 0x7FFFFFE0
	leaq	-32(%r10), %r11
	movq	%r11, 24(%rsp)                  ## 8-byte Spill
	shrq	$5, %r11
	incq	%r11
	movq	48(%rsp), %rdx                  ## 8-byte Reload
	movq	%rdx, %rax
	imulq	8(%rsp), %rax                   ## 8-byte Folded Reload
	addq	192(%rsp), %rax                 ## 8-byte Folded Reload
	movq	40(%rsp), %rsi                  ## 8-byte Reload
	leaq	(%rsi,%rax,4), %rcx
	addq	$224, %rcx
	leaq	(,%rdx,4), %rbp
	movq	%r11, %rdx
	andq	$-2, %rdx
	negq	%rdx
	movq	%rdx, 256(%rsp)                 ## 8-byte Spill
	leaq	(%rsi,%rax,4), %rbx
	xorl	%esi, %esi
	jmp	LBB12_14
	.p2align	4, 0x90
LBB12_43:                               ## %after_bb3.loopexit.us.us
                                        ##   in Loop: Header=BB12_14 Depth=1
	incq	%rsi
	addq	%rbp, %rcx
	addq	%rbp, %rbx
	cmpq	%r12, %rsi
	je	LBB12_61
LBB12_14:                               ## %"for f0_0_d_def__.s0.w.wi.us.us"
                                        ## =>This Loop Header: Depth=1
                                        ##     Child Loop BB12_30 Depth 2
                                        ##     Child Loop BB12_40 Depth 2
	leaq	(%rsi,%r14), %r8
	cmpl	$32, %r13d
	jae	LBB12_27
## %bb.15:                              ##   in Loop: Header=BB12_14 Depth=1
	xorl	%edx, %edx
	movq	16(%rsp), %rax                  ## 8-byte Reload
	jmp	LBB12_40
	.p2align	4, 0x90
LBB12_27:                               ## %vector.ph56
                                        ##   in Loop: Header=BB12_14 Depth=1
	cmpq	$0, 24(%rsp)                    ## 8-byte Folded Reload
	je	LBB12_28
## %bb.29:                              ## %vector.body54.preheader
                                        ##   in Loop: Header=BB12_14 Depth=1
	movq	256(%rsp), %r15                 ## 8-byte Reload
	xorl	%r9d, %r9d
	movq	16(%rsp), %rax                  ## 8-byte Reload
	jmp	LBB12_30
	.p2align	4, 0x90
LBB12_34:                               ## %vector.body54
                                        ##   in Loop: Header=BB12_30 Depth=2
	vmovups	%ymm0, -96(%rcx,%r9,4)
	vmovups	%ymm1, -64(%rcx,%r9,4)
	vmovdqu	%ymm2, -32(%rcx,%r9,4)
	vmovdqu	%ymm3, (%rcx,%r9,4)
	addq	$64, %r9
	addq	$2, %r15
	je	LBB12_35
LBB12_30:                               ## %vector.body54
                                        ##   Parent Loop BB12_14 Depth=1
                                        ## =>  This Inner Loop Header: Depth=2
	vxorps	%xmm0, %xmm0, %xmm0
	vxorps	%xmm1, %xmm1, %xmm1
	vpxor	%xmm2, %xmm2, %xmm2
	vpxor	%xmm3, %xmm3, %xmm3
	vxorps	%xmm4, %xmm4, %xmm4
	testq	%r8, %r8
	js	LBB12_32
## %bb.31:                              ## %vector.body54
                                        ##   in Loop: Header=BB12_30 Depth=2
	vmovups	(%rax,%r9,4), %ymm1
	vmovdqu	32(%rax,%r9,4), %ymm2
	vmovdqu	64(%rax,%r9,4), %ymm3
	vmovups	96(%rax,%r9,4), %ymm4
LBB12_32:                               ## %vector.body54
                                        ##   in Loop: Header=BB12_30 Depth=2
	vmovups	%ymm1, -224(%rcx,%r9,4)
	vmovdqu	%ymm2, -192(%rcx,%r9,4)
	vmovdqu	%ymm3, -160(%rcx,%r9,4)
	vmovups	%ymm4, -128(%rcx,%r9,4)
	vxorps	%xmm1, %xmm1, %xmm1
	vpxor	%xmm2, %xmm2, %xmm2
	vpxor	%xmm3, %xmm3, %xmm3
	js	LBB12_34
## %bb.33:                              ## %vector.body54
                                        ##   in Loop: Header=BB12_30 Depth=2
	vmovups	128(%rax,%r9,4), %ymm0
	vmovups	160(%rax,%r9,4), %ymm1
	vmovdqu	192(%rax,%r9,4), %ymm2
	vmovdqu	224(%rax,%r9,4), %ymm3
	jmp	LBB12_34
LBB12_28:                               ##   in Loop: Header=BB12_14 Depth=1
	xorl	%r9d, %r9d
	movq	16(%rsp), %rax                  ## 8-byte Reload
LBB12_35:                               ## %middle.block52.unr-lcssa
                                        ##   in Loop: Header=BB12_14 Depth=1
	testb	$1, %r11b
	je	LBB12_39
## %bb.36:                              ## %vector.body54.epil
                                        ##   in Loop: Header=BB12_14 Depth=1
	movq	8(%rsp), %rdx                   ## 8-byte Reload
	addq	%rsi, %rdx
	imulq	48(%rsp), %rdx                  ## 8-byte Folded Reload
	addq	192(%rsp), %rdx                 ## 8-byte Folded Reload
	vxorps	%xmm0, %xmm0, %xmm0
	vxorps	%xmm1, %xmm1, %xmm1
	vpxor	%xmm2, %xmm2, %xmm2
	vpxor	%xmm3, %xmm3, %xmm3
	testq	%r8, %r8
	js	LBB12_38
## %bb.37:                              ## %vector.body54.epil
                                        ##   in Loop: Header=BB12_14 Depth=1
	vmovups	(%rax,%r9,4), %ymm0
	vmovups	32(%rax,%r9,4), %ymm1
	vmovdqu	64(%rax,%r9,4), %ymm2
	vmovdqu	96(%rax,%r9,4), %ymm3
LBB12_38:                               ## %vector.body54.epil
                                        ##   in Loop: Header=BB12_14 Depth=1
	addq	%rdx, %r9
	movq	40(%rsp), %rdx                  ## 8-byte Reload
	vmovups	%ymm0, (%rdx,%r9,4)
	vmovups	%ymm1, 32(%rdx,%r9,4)
	vmovdqu	%ymm2, 64(%rdx,%r9,4)
	vmovdqu	%ymm3, 96(%rdx,%r9,4)
LBB12_39:                               ## %middle.block52
                                        ##   in Loop: Header=BB12_14 Depth=1
	movq	%r10, %rdx
	cmpq	%rdi, %r10
	jne	LBB12_40
	jmp	LBB12_43
	.p2align	4, 0x90
LBB12_42:                               ## %"for f0_0_d_def__.s0.n.ni.us.us"
                                        ##   in Loop: Header=BB12_40 Depth=2
	vmovss	%xmm0, (%rbx,%rdx,4)
	incq	%rdx
	cmpq	%rdx, %rdi
	je	LBB12_43
LBB12_40:                               ## %"for f0_0_d_def__.s0.n.ni.us.us"
                                        ##   Parent Loop BB12_14 Depth=1
                                        ## =>  This Inner Loop Header: Depth=2
	vxorps	%xmm0, %xmm0, %xmm0
	testq	%r8, %r8
	js	LBB12_42
## %bb.41:                              ## %"for f0_0_d_def__.s0.n.ni.us.us"
                                        ##   in Loop: Header=BB12_40 Depth=2
	vmovss	(%rax,%rdx,4), %xmm0            ## xmm0 = mem[0],zero,zero,zero
	jmp	LBB12_42
LBB12_44:                               ## %"for f0_0_d_def__.s0.w.wi.preheader.split"
	movq	16(%rsp), %rax                  ## 8-byte Reload
	vmovdqa	(%rax), %ymm0
	movl	%ecx, %r10d
	leaq	-1(%r10), %rax
	movl	%r10d, %ecx
	andl	$3, %ecx
	cmpq	$3, %rax
	jae	LBB12_46
## %bb.45:
	xorl	%edi, %edi
LBB12_56:                               ## %call_destructor.exit16.loopexit66.unr-lcssa
	testq	%rcx, %rcx
	movq	128(%rsp), %rsi                 ## 8-byte Reload
	je	LBB12_61
## %bb.57:                              ## %"for f0_0_d_def__.s0.w.wi.epil.preheader"
	movq	8(%rsp), %rbp                   ## 8-byte Reload
	addq	%rdi, %rbp
	movq	48(%rsp), %rax                  ## 8-byte Reload
	imulq	%rax, %rbp
	addq	192(%rsp), %rbp                 ## 8-byte Folded Reload
	movq	40(%rsp), %rdx                  ## 8-byte Reload
	leaq	(%rdx,%rbp,4), %rdx
	shlq	$2, %rax
	leal	(%rdi,%rsi,2), %esi
	jmp	LBB12_58
	.p2align	4, 0x90
LBB12_60:                               ## %"for f0_0_d_def__.s0.w.wi.epil"
                                        ##   in Loop: Header=BB12_58 Depth=1
	vmovups	%ymm1, (%rdx)
	addq	%rax, %rdx
	incl	%esi
	decq	%rcx
	je	LBB12_61
LBB12_58:                               ## %"for f0_0_d_def__.s0.w.wi.epil"
                                        ## =>This Inner Loop Header: Depth=1
	vxorps	%xmm1, %xmm1, %xmm1
	testl	%esi, %esi
	js	LBB12_60
## %bb.59:                              ## %"for f0_0_d_def__.s0.w.wi.epil"
                                        ##   in Loop: Header=BB12_58 Depth=1
	vmovdqa	%ymm0, %ymm1
	jmp	LBB12_60
LBB12_61:                               ## %call_destructor.exit16
	movq	120(%rsp), %rdi                 ## 8-byte Reload
	movq	16(%rsp), %rsi                  ## 8-byte Reload
	vzeroupper
	callq	_halide_free
	xorl	%eax, %eax
	addq	$424, %rsp                      ## imm = 0x1A8
	popq	%rbx
	popq	%r12
	popq	%r13
	popq	%r14
	popq	%r15
	popq	%rbp
	retq
LBB12_46:                               ## %"for f0_0_d_def__.s0.w.wi.preheader.split.new"
	andl	$2147483644, %r10d              ## imm = 0x7FFFFFFC
	movq	8(%rsp), %rbp                   ## 8-byte Reload
	leaq	3(%rbp), %rax
	movq	48(%rsp), %rdx                  ## 8-byte Reload
	imulq	%rdx, %rax
	movq	40(%rsp), %rdi                  ## 8-byte Reload
	leaq	(%rdi,%rax,4), %r9
	movq	192(%rsp), %rax                 ## 8-byte Reload
	leaq	(,%rax,4), %rbx
	movq	%rdx, %r14
	shlq	$4, %r14
	leaq	2(%rbp), %rax
	imulq	%rdx, %rax
	leaq	(%rdi,%rax,4), %r11
	leaq	1(%rbp), %rax
	imulq	%rdx, %rax
	leaq	(%rdi,%rax,4), %rsi
	movq	%rdx, %rax
	imulq	%rbp, %rax
	leaq	(%rdi,%rax,4), %rbp
	xorl	%edi, %edi
	jmp	LBB12_47
	.p2align	4, 0x90
LBB12_55:                               ## %"for f0_0_d_def__.s0.w.wi"
                                        ##   in Loop: Header=BB12_47 Depth=1
	vmovups	%ymm1, (%r9,%rbx)
	addq	$4, %rdi
	addq	%r14, %rbx
	cmpq	%rdi, %r10
	je	LBB12_56
LBB12_47:                               ## %"for f0_0_d_def__.s0.w.wi"
                                        ## =>This Inner Loop Header: Depth=1
	movl	%r8d, %edx
	addl	%edi, %edx
	vxorps	%xmm1, %xmm1, %xmm1
	vpxor	%xmm2, %xmm2, %xmm2
	js	LBB12_49
## %bb.48:                              ## %"for f0_0_d_def__.s0.w.wi"
                                        ##   in Loop: Header=BB12_47 Depth=1
	vmovdqa	%ymm0, %ymm2
LBB12_49:                               ## %"for f0_0_d_def__.s0.w.wi"
                                        ##   in Loop: Header=BB12_47 Depth=1
	vmovdqu	%ymm2, (%rbp,%rbx)
	movl	%edx, %eax
	addl	$1, %eax
	vpxor	%xmm2, %xmm2, %xmm2
	js	LBB12_51
## %bb.50:                              ## %"for f0_0_d_def__.s0.w.wi"
                                        ##   in Loop: Header=BB12_47 Depth=1
	vmovdqa	%ymm0, %ymm2
LBB12_51:                               ## %"for f0_0_d_def__.s0.w.wi"
                                        ##   in Loop: Header=BB12_47 Depth=1
	vmovdqu	%ymm2, (%rsi,%rbx)
	movl	%edx, %eax
	addl	$2, %eax
	vpxor	%xmm2, %xmm2, %xmm2
	js	LBB12_53
## %bb.52:                              ## %"for f0_0_d_def__.s0.w.wi"
                                        ##   in Loop: Header=BB12_47 Depth=1
	vmovdqa	%ymm0, %ymm2
LBB12_53:                               ## %"for f0_0_d_def__.s0.w.wi"
                                        ##   in Loop: Header=BB12_47 Depth=1
	vmovdqu	%ymm2, (%r11,%rbx)
	addl	$3, %edx
	js	LBB12_55
## %bb.54:                              ## %"for f0_0_d_def__.s0.w.wi"
                                        ##   in Loop: Header=BB12_47 Depth=1
	vmovdqa	%ymm0, %ymm1
	jmp	LBB12_55
LBB12_62:                               ## %"assert failed"
	leaq	l_str.120(%rip), %rsi
	movl	$2147483647, %ecx               ## imm = 0x7FFFFFFF
	addq	$424, %rsp                      ## imm = 0x1A8
	popq	%rbx
	popq	%r12
	popq	%r13
	popq	%r14
	popq	%r15
	popq	%rbp
	jmp	_halide_error_buffer_allocation_too_large ## TAILCALL
LBB12_63:                               ## %"assert failed1"
	movq	120(%rsp), %rdi                 ## 8-byte Reload
	addq	$424, %rsp                      ## imm = 0x1A8
	popq	%rbx
	popq	%r12
	popq	%r13
	popq	%r14
	popq	%r15
	popq	%rbp
	jmp	_halide_error_out_of_memory     ## TAILCALL
	.cfi_endproc
                                        ## -- End function
	.p2align	4, 0x90                         ## -- Begin function train_cost_model.par_for.relu1_0_d_def__.s0.n.n.n
_train_cost_model.par_for.relu1_0_d_def__.s0.n.n.n: ## @train_cost_model.par_for.relu1_0_d_def__.s0.n.n.n
## %bb.0:                               ## %entry
	pushq	%rbp
	pushq	%r15
	pushq	%r14
	pushq	%r13
	pushq	%r12
	pushq	%rbx
	subq	$248, %rsp
	movq	%rdx, %rbx
	movl	4(%rdx), %r14d
	movl	12(%rdx), %r10d
	movl	%esi, %r9d
	sarl	$31, %r9d
	xorl	%ecx, %ecx
	testl	%r10d, %r10d
	sete	%cl
	movl	%ecx, %ebp
	negl	%ebp
	movl	%r10d, %edi
	sarl	$31, %edi
	subl	%r9d, %esi
	orl	%r10d, %ebp
	movl	%esi, %eax
	cltd
	idivl	%ebp
	movl	%edx, %r8d
	movl	%edi, %r11d
	leal	(%r10,%rcx), %ebp
	movl	%esi, %eax
	cltd
	idivl	%ebp
	notl	%r11d
	decl	%ecx
	movl	%r11d, %ebp
	subl	%edi, %ebp
	andl	%r9d, %ebp
	addl	%eax, %ebp
	andl	%ecx, %ebp
	leal	(%rbp,%rbp), %eax
	subl	%eax, %r14d
	cmpl	$2, %r14d
	movl	$2, %edx
	cmovll	%r14d, %edx
	xorl	%r12d, %r12d
	testl	%r14d, %r14d
	cmovgl	%edx, %r12d
	jle	LBB13_7
## %bb.1:                               ## %"for relu1_0_d_def__.s0.w.wi.preheader"
	movl	(%rbx), %r14d
	movslq	8(%rbx), %rdx
	movl	16(%rbx), %r15d
	movq	24(%rbx), %rbx
	xorl	%r10d, %edi
	addl	%r11d, %edi
	andl	%r9d, %edi
	addl	%r8d, %edi
	andl	%ecx, %edi
	shll	$3, %edi
	movl	%r14d, %esi
	subl	%edi, %esi
	leal	8(%rdi), %r8d
	cmpl	$8, %esi
	movl	$8, %r10d
	cmovll	%esi, %r10d
	movl	%edx, %ecx
	shll	$5, %ecx
	movl	%ecx, 12(%rsp)                  ## 4-byte Spill
	cmpl	%r14d, %r8d
	jle	LBB13_2
## %bb.4:                               ## %"for relu1_0_d_def__.s0.w.wi.preheader.split.us"
	testl	%esi, %esi
	movl	12(%rsp), %r9d                  ## 4-byte Reload
	jle	LBB13_7
## %bb.5:                               ## %"for relu1_0_d_def__.s0.w.wi.us.us.preheader"
	movq	%rdx, %r14
	subl	%r15d, %ebp
	imull	%r14d, %ebp
	shll	$6, %ebp
	decl	%r10d
	leaq	4(,%r10,4), %r15
	movq	%rdi, (%rsp)                    ## 8-byte Spill
	movl	%r12d, %r13d
	leaq	(%rdx,%rdx), %rax
	leaq	(%rdx,%rdx,2), %r10
	leaq	(,%rdx,4), %r8
	leaq	(%rdx,%rdx,4), %rsi
	leaq	(%rax,%rax,2), %rdi
	movq	%rdi, 144(%rsp)                 ## 8-byte Spill
	leaq	(,%rdx,8), %r11
	movq	%r11, %rdx
	subq	%r14, %rdx
	movq	%rdx, 128(%rsp)                 ## 8-byte Spill
	leaq	(%r14,%r14,8), %rdx
	movq	%r14, %rcx
	shlq	$4, %rcx
	movq	%rcx, %rdi
	subq	%r14, %rdi
	subq	%r14, %rdi
	movq	%rdi, 104(%rsp)                 ## 8-byte Spill
	leaq	(,%r10,8), %rdi
	subq	%r14, %rdi
	movq	%rdi, 96(%rsp)                  ## 8-byte Spill
	movq	%r14, %rdi
	shlq	$5, %rdi
	subq	%r14, %rdi
	addl	(%rsp), %ebp                    ## 4-byte Folded Reload
	movq	%rdi, 88(%rsp)                  ## 8-byte Spill
	subq	%r14, %rdi
	movq	%rdi, 80(%rsp)                  ## 8-byte Spill
	leaq	(%rax,%rax,4), %rdi
	movq	%rdi, 72(%rsp)                  ## 8-byte Spill
	leaq	(%r14,%rsi,2), %rdi
	movq	%rdi, 64(%rsp)                  ## 8-byte Spill
	leaq	(%r8,%r8,2), %rdi
	movq	%rdi, 56(%rsp)                  ## 8-byte Spill
	movq	%r10, 168(%rsp)                 ## 8-byte Spill
	leaq	(%r14,%r10,4), %rdi
	movq	%rdi, 48(%rsp)                  ## 8-byte Spill
	leaq	(%rsi,%rsi,2), %rdi
	movq	%rdi, 40(%rsp)                  ## 8-byte Spill
	movq	%rcx, 112(%rsp)                 ## 8-byte Spill
	leaq	(%r14,%rcx), %rcx
	movq	%rcx, 32(%rsp)                  ## 8-byte Spill
	movq	%rax, (%rsp)                    ## 8-byte Spill
	leaq	(%rax,%rax,8), %rax
	movq	%rax, 24(%rsp)                  ## 8-byte Spill
	leaq	(%r14,%rdx,2), %rax
	movq	%rax, 16(%rsp)                  ## 8-byte Spill
	movq	%r8, 160(%rsp)                  ## 8-byte Spill
	leaq	(%r8,%r8,4), %rax
	movq	%rax, 240(%rsp)                 ## 8-byte Spill
	leaq	(%r14,%rsi,4), %rax
	movq	%rax, 232(%rsp)                 ## 8-byte Spill
	leaq	(%r14,%rax), %rax
	movq	%rax, 224(%rsp)                 ## 8-byte Spill
	movq	%r11, 136(%rsp)                 ## 8-byte Spill
	leaq	(%r11,%r11,2), %rax
	movq	%rax, 216(%rsp)                 ## 8-byte Spill
	movq	%rsi, 152(%rsp)                 ## 8-byte Spill
	leaq	(%rsi,%rsi,4), %rax
	movq	%rax, 208(%rsp)                 ## 8-byte Spill
	addq	%r14, %rax
	movq	%rax, 200(%rsp)                 ## 8-byte Spill
	movq	%rdx, 120(%rsp)                 ## 8-byte Spill
	leaq	(%rdx,%rdx,2), %rax
	movq	%rax, 192(%rsp)                 ## 8-byte Spill
	addq	%r14, %rax
	movq	%rax, 184(%rsp)                 ## 8-byte Spill
	addq	%r14, %rax
	movq	%rax, 176(%rsp)                 ## 8-byte Spill
	.p2align	4, 0x90
LBB13_6:                                ## %"for relu1_0_d_def__.s0.w.wi.us.us"
                                        ## =>This Inner Loop Header: Depth=1
	movslq	%ebp, %rbp
	leaq	(%rbx,%rbp,4), %rdi
	xorl	%esi, %esi
	movq	%r15, %rdx
	movl	%r9d, %r12d
	callq	_memset
	leaq	(%r14,%rbp), %rax
	leaq	(%rbx,%rax,4), %rdi
	xorl	%esi, %esi
	movq	%r15, %rdx
	callq	_memset
	movq	(%rsp), %rax                    ## 8-byte Reload
	addq	%rbp, %rax
	leaq	(%rbx,%rax,4), %rdi
	xorl	%esi, %esi
	movq	%r15, %rdx
	callq	_memset
	movq	168(%rsp), %rax                 ## 8-byte Reload
	addq	%rbp, %rax
	leaq	(%rbx,%rax,4), %rdi
	xorl	%esi, %esi
	movq	%r15, %rdx
	callq	_memset
	movq	160(%rsp), %rax                 ## 8-byte Reload
	addq	%rbp, %rax
	leaq	(%rbx,%rax,4), %rdi
	xorl	%esi, %esi
	movq	%r15, %rdx
	callq	_memset
	movq	152(%rsp), %rax                 ## 8-byte Reload
	addq	%rbp, %rax
	leaq	(%rbx,%rax,4), %rdi
	xorl	%esi, %esi
	movq	%r15, %rdx
	callq	_memset
	movq	144(%rsp), %rax                 ## 8-byte Reload
	addq	%rbp, %rax
	leaq	(%rbx,%rax,4), %rdi
	xorl	%esi, %esi
	movq	%r15, %rdx
	callq	_memset
	movq	128(%rsp), %rax                 ## 8-byte Reload
	addq	%rbp, %rax
	leaq	(%rbx,%rax,4), %rdi
	xorl	%esi, %esi
	movq	%r15, %rdx
	callq	_memset
	movq	136(%rsp), %rax                 ## 8-byte Reload
	addq	%rbp, %rax
	leaq	(%rbx,%rax,4), %rdi
	xorl	%esi, %esi
	movq	%r15, %rdx
	callq	_memset
	movq	120(%rsp), %rax                 ## 8-byte Reload
	addq	%rbp, %rax
	leaq	(%rbx,%rax,4), %rdi
	xorl	%esi, %esi
	movq	%r15, %rdx
	callq	_memset
	movq	72(%rsp), %rax                  ## 8-byte Reload
	addq	%rbp, %rax
	leaq	(%rbx,%rax,4), %rdi
	xorl	%esi, %esi
	movq	%r15, %rdx
	callq	_memset
	movq	64(%rsp), %rax                  ## 8-byte Reload
	addq	%rbp, %rax
	leaq	(%rbx,%rax,4), %rdi
	xorl	%esi, %esi
	movq	%r15, %rdx
	callq	_memset
	movq	56(%rsp), %rax                  ## 8-byte Reload
	addq	%rbp, %rax
	leaq	(%rbx,%rax,4), %rdi
	xorl	%esi, %esi
	movq	%r15, %rdx
	callq	_memset
	movq	48(%rsp), %rax                  ## 8-byte Reload
	addq	%rbp, %rax
	leaq	(%rbx,%rax,4), %rdi
	xorl	%esi, %esi
	movq	%r15, %rdx
	callq	_memset
	movq	104(%rsp), %rax                 ## 8-byte Reload
	addq	%rbp, %rax
	leaq	(%rbx,%rax,4), %rdi
	xorl	%esi, %esi
	movq	%r15, %rdx
	callq	_memset
	movq	40(%rsp), %rax                  ## 8-byte Reload
	addq	%rbp, %rax
	leaq	(%rbx,%rax,4), %rdi
	xorl	%esi, %esi
	movq	%r15, %rdx
	callq	_memset
	movq	112(%rsp), %rax                 ## 8-byte Reload
	addq	%rbp, %rax
	leaq	(%rbx,%rax,4), %rdi
	xorl	%esi, %esi
	movq	%r15, %rdx
	callq	_memset
	movq	32(%rsp), %rax                  ## 8-byte Reload
	addq	%rbp, %rax
	leaq	(%rbx,%rax,4), %rdi
	xorl	%esi, %esi
	movq	%r15, %rdx
	callq	_memset
	movq	24(%rsp), %rax                  ## 8-byte Reload
	addq	%rbp, %rax
	leaq	(%rbx,%rax,4), %rdi
	xorl	%esi, %esi
	movq	%r15, %rdx
	callq	_memset
	movq	16(%rsp), %rax                  ## 8-byte Reload
	addq	%rbp, %rax
	leaq	(%rbx,%rax,4), %rdi
	xorl	%esi, %esi
	movq	%r15, %rdx
	callq	_memset
	movq	240(%rsp), %rax                 ## 8-byte Reload
	addq	%rbp, %rax
	leaq	(%rbx,%rax,4), %rdi
	xorl	%esi, %esi
	movq	%r15, %rdx
	callq	_memset
	movq	232(%rsp), %rax                 ## 8-byte Reload
	addq	%rbp, %rax
	leaq	(%rbx,%rax,4), %rdi
	xorl	%esi, %esi
	movq	%r15, %rdx
	callq	_memset
	movq	224(%rsp), %rax                 ## 8-byte Reload
	addq	%rbp, %rax
	leaq	(%rbx,%rax,4), %rdi
	xorl	%esi, %esi
	movq	%r15, %rdx
	callq	_memset
	movq	96(%rsp), %rax                  ## 8-byte Reload
	addq	%rbp, %rax
	leaq	(%rbx,%rax,4), %rdi
	xorl	%esi, %esi
	movq	%r15, %rdx
	callq	_memset
	movq	216(%rsp), %rax                 ## 8-byte Reload
	addq	%rbp, %rax
	leaq	(%rbx,%rax,4), %rdi
	xorl	%esi, %esi
	movq	%r15, %rdx
	callq	_memset
	movq	208(%rsp), %rax                 ## 8-byte Reload
	addq	%rbp, %rax
	leaq	(%rbx,%rax,4), %rdi
	xorl	%esi, %esi
	movq	%r15, %rdx
	callq	_memset
	movq	200(%rsp), %rax                 ## 8-byte Reload
	addq	%rbp, %rax
	leaq	(%rbx,%rax,4), %rdi
	xorl	%esi, %esi
	movq	%r15, %rdx
	callq	_memset
	movq	192(%rsp), %rax                 ## 8-byte Reload
	addq	%rbp, %rax
	leaq	(%rbx,%rax,4), %rdi
	xorl	%esi, %esi
	movq	%r15, %rdx
	callq	_memset
	movq	184(%rsp), %rax                 ## 8-byte Reload
	addq	%rbp, %rax
	leaq	(%rbx,%rax,4), %rdi
	xorl	%esi, %esi
	movq	%r15, %rdx
	callq	_memset
	movq	176(%rsp), %rax                 ## 8-byte Reload
	addq	%rbp, %rax
	leaq	(%rbx,%rax,4), %rdi
	xorl	%esi, %esi
	movq	%r15, %rdx
	callq	_memset
	movq	80(%rsp), %rax                  ## 8-byte Reload
	addq	%rbp, %rax
	leaq	(%rbx,%rax,4), %rdi
	xorl	%esi, %esi
	movq	%r15, %rdx
	callq	_memset
	movq	88(%rsp), %rax                  ## 8-byte Reload
	addq	%rbp, %rax
	leaq	(%rbx,%rax,4), %rdi
	xorl	%esi, %esi
	movq	%r15, %rdx
	callq	_memset
	movl	%r12d, %r9d
	addl	%r12d, %ebp
	decq	%r13
	jne	LBB13_6
	jmp	LBB13_7
LBB13_2:                                ## %"for relu1_0_d_def__.s0.w.wi.preheader11"
	movl	%r12d, %ecx
	leaq	(%rdx,%rdx), %r14
	leaq	(%rdx,%rdx,2), %r11
	leaq	(,%rdx,4), %rsi
	movq	%rsi, (%rsp)                    ## 8-byte Spill
	leaq	(%rdx,%rdx,4), %r12
	leaq	(%r14,%r14,2), %rsi
	movq	%rsi, 152(%rsp)                 ## 8-byte Spill
	leaq	(,%rdx,8), %r10
	movq	%r10, %rbp
	subq	%rdx, %rbp
	movq	%rbp, 136(%rsp)                 ## 8-byte Spill
	leaq	(%rdx,%rdx,8), %r9
	leaq	(%r14,%r14,4), %rsi
	movq	%rsi, 128(%rsp)                 ## 8-byte Spill
	movq	%rdx, %rsi
	shlq	$4, %rsi
	movq	%rsi, %rbp
	subq	%rdx, %rbp
	subq	%rdx, %rbp
	movq	%rbp, 112(%rsp)                 ## 8-byte Spill
	leaq	(,%r11,8), %rbp
	subq	%rdx, %rbp
	movq	%rbp, 104(%rsp)                 ## 8-byte Spill
	addl	%r15d, %r15d
	subl	%r15d, %eax
	movq	%rdx, %r8
	shlq	$5, %r8
	subq	%rdx, %r8
	imull	%edx, %eax
	shll	$5, %eax
	addl	%edi, %eax
	movq	%r8, %r13
	subq	%rdx, %r13
	vxorps	%xmm0, %xmm0, %xmm0
	leaq	(%rdx,%r12,2), %rdi
	movq	%rdi, 96(%rsp)                  ## 8-byte Spill
	movq	(%rsp), %rbp                    ## 8-byte Reload
	leaq	(%rbp,%rbp,2), %rdi
	movq	%rdi, 88(%rsp)                  ## 8-byte Spill
	movq	%r11, 160(%rsp)                 ## 8-byte Spill
	leaq	(%rdx,%r11,4), %rdi
	movq	%rdi, 80(%rsp)                  ## 8-byte Spill
	leaq	(%r12,%r12,2), %rdi
	movq	%rdi, 72(%rsp)                  ## 8-byte Spill
	movq	%rsi, 120(%rsp)                 ## 8-byte Spill
	leaq	(%rdx,%rsi), %rsi
	movq	%rsi, 64(%rsp)                  ## 8-byte Spill
	movq	%r14, 168(%rsp)                 ## 8-byte Spill
	leaq	(%r14,%r14,8), %rsi
	movq	%rsi, 56(%rsp)                  ## 8-byte Spill
	leaq	(%rdx,%r9,2), %rsi
	movq	%rsi, 48(%rsp)                  ## 8-byte Spill
	leaq	(%rbp,%rbp,4), %rsi
	movq	%rsi, 40(%rsp)                  ## 8-byte Spill
	leaq	(%rdx,%r12,4), %r14
	leaq	(%rdx,%r14), %rsi
	movq	%rsi, 32(%rsp)                  ## 8-byte Spill
	movq	%r10, 144(%rsp)                 ## 8-byte Spill
	leaq	(%r10,%r10,2), %rsi
	movq	%rsi, 24(%rsp)                  ## 8-byte Spill
	movq	%r12, %r10
	leaq	(%r12,%r12,4), %r11
	leaq	(%r11,%rdx), %rsi
	movq	%rsi, 16(%rsp)                  ## 8-byte Spill
	movq	%r9, %r15
	leaq	(%r9,%r9,2), %rbp
	leaq	(%rdx,%rbp), %rdi
	leaq	(%rdx,%rdi), %r12
	movl	12(%rsp), %r9d                  ## 4-byte Reload
	.p2align	4, 0x90
LBB13_3:                                ## %"for relu1_0_d_def__.s0.w.wi"
                                        ## =>This Inner Loop Header: Depth=1
	cltq
	vmovups	%ymm0, (%rbx,%rax,4)
	leaq	(%rdx,%rax), %rsi
	vmovups	%ymm0, (%rbx,%rsi,4)
	movq	168(%rsp), %rsi                 ## 8-byte Reload
	addq	%rax, %rsi
	vmovups	%ymm0, (%rbx,%rsi,4)
	movq	160(%rsp), %rsi                 ## 8-byte Reload
	leaq	(%rsi,%rax), %rsi
	vmovups	%ymm0, (%rbx,%rsi,4)
	movq	(%rsp), %rsi                    ## 8-byte Reload
	leaq	(%rsi,%rax), %rsi
	vmovups	%ymm0, (%rbx,%rsi,4)
	leaq	(%r10,%rax), %rsi
	vmovups	%ymm0, (%rbx,%rsi,4)
	movq	152(%rsp), %rsi                 ## 8-byte Reload
	leaq	(%rsi,%rax), %rsi
	vmovups	%ymm0, (%rbx,%rsi,4)
	movq	136(%rsp), %rsi                 ## 8-byte Reload
	leaq	(%rsi,%rax), %rsi
	vmovups	%ymm0, (%rbx,%rsi,4)
	movq	144(%rsp), %rsi                 ## 8-byte Reload
	leaq	(%rsi,%rax), %rsi
	vmovups	%ymm0, (%rbx,%rsi,4)
	leaq	(%r15,%rax), %rsi
	vmovups	%ymm0, (%rbx,%rsi,4)
	movq	128(%rsp), %rsi                 ## 8-byte Reload
	leaq	(%rsi,%rax), %rsi
	vmovups	%ymm0, (%rbx,%rsi,4)
	movq	96(%rsp), %rsi                  ## 8-byte Reload
	leaq	(%rsi,%rax), %rsi
	vmovups	%ymm0, (%rbx,%rsi,4)
	movq	88(%rsp), %rsi                  ## 8-byte Reload
	leaq	(%rsi,%rax), %rsi
	vmovups	%ymm0, (%rbx,%rsi,4)
	movq	80(%rsp), %rsi                  ## 8-byte Reload
	leaq	(%rsi,%rax), %rsi
	vmovups	%ymm0, (%rbx,%rsi,4)
	movq	112(%rsp), %rsi                 ## 8-byte Reload
	leaq	(%rsi,%rax), %rsi
	vmovups	%ymm0, (%rbx,%rsi,4)
	movq	72(%rsp), %rsi                  ## 8-byte Reload
	leaq	(%rsi,%rax), %rsi
	vmovups	%ymm0, (%rbx,%rsi,4)
	movq	120(%rsp), %rsi                 ## 8-byte Reload
	leaq	(%rsi,%rax), %rsi
	vmovups	%ymm0, (%rbx,%rsi,4)
	movq	64(%rsp), %rsi                  ## 8-byte Reload
	leaq	(%rsi,%rax), %rsi
	vmovups	%ymm0, (%rbx,%rsi,4)
	movq	56(%rsp), %rsi                  ## 8-byte Reload
	leaq	(%rsi,%rax), %rsi
	vmovups	%ymm0, (%rbx,%rsi,4)
	movq	48(%rsp), %rsi                  ## 8-byte Reload
	leaq	(%rsi,%rax), %rsi
	vmovups	%ymm0, (%rbx,%rsi,4)
	movq	40(%rsp), %rsi                  ## 8-byte Reload
	leaq	(%rsi,%rax), %rsi
	vmovups	%ymm0, (%rbx,%rsi,4)
	leaq	(%r14,%rax), %rsi
	vmovups	%ymm0, (%rbx,%rsi,4)
	movq	32(%rsp), %rsi                  ## 8-byte Reload
	leaq	(%rsi,%rax), %rsi
	vmovups	%ymm0, (%rbx,%rsi,4)
	movq	104(%rsp), %rsi                 ## 8-byte Reload
	leaq	(%rsi,%rax), %rsi
	vmovups	%ymm0, (%rbx,%rsi,4)
	movq	24(%rsp), %rsi                  ## 8-byte Reload
	leaq	(%rsi,%rax), %rsi
	vmovups	%ymm0, (%rbx,%rsi,4)
	leaq	(%r11,%rax), %rsi
	vmovups	%ymm0, (%rbx,%rsi,4)
	movq	16(%rsp), %rsi                  ## 8-byte Reload
	leaq	(%rsi,%rax), %rsi
	vmovups	%ymm0, (%rbx,%rsi,4)
	leaq	(%rbp,%rax), %rsi
	vmovups	%ymm0, (%rbx,%rsi,4)
	leaq	(%rdi,%rax), %rsi
	vmovups	%ymm0, (%rbx,%rsi,4)
	leaq	(%r12,%rax), %rsi
	vmovups	%ymm0, (%rbx,%rsi,4)
	leaq	(%r13,%rax), %rsi
	vmovups	%ymm0, (%rbx,%rsi,4)
	leaq	(%r8,%rax), %rsi
	vmovups	%ymm0, (%rbx,%rsi,4)
	addl	%r9d, %eax
	decq	%rcx
	jne	LBB13_3
LBB13_7:                                ## %destructor_block
	xorl	%eax, %eax
	addq	$248, %rsp
	popq	%rbx
	popq	%r12
	popq	%r13
	popq	%r14
	popq	%r15
	popq	%rbp
	vzeroupper
	retq
                                        ## -- End function
	.section	__TEXT,__const
	.p2align	5                               ## -- Begin function train_cost_model.par_for.relu1_0_d_def__.s6.n.n.n
LCPI14_0:
	.long	0                               ## 0x0
	.long	1                               ## 0x1
	.long	2                               ## 0x2
	.long	3                               ## 0x3
	.long	4                               ## 0x4
	.long	5                               ## 0x5
	.long	6                               ## 0x6
	.long	7                               ## 0x7
	.section	__TEXT,__literal4,4byte_literals
	.p2align	2
LCPI14_1:
	.long	0x3089705f                      ## float 9.99999971E-10
	.section	__TEXT,__text,regular,pure_instructions
	.p2align	4, 0x90
_train_cost_model.par_for.relu1_0_d_def__.s6.n.n.n: ## @train_cost_model.par_for.relu1_0_d_def__.s6.n.n.n
## %bb.0:                               ## %entry
	pushq	%rbp
	pushq	%r15
	pushq	%r14
	pushq	%r13
	pushq	%r12
	pushq	%rbx
	subq	$160, %rsp
	movq	%rdx, %rcx
	movl	%esi, %r10d
	movl	(%rdx), %eax
	movq	%rax, -56(%rsp)                 ## 8-byte Spill
	movl	12(%rdx), %r9d
	movslq	4(%rdx), %rax
	movq	%rax, -32(%rsp)                 ## 8-byte Spill
	movslq	8(%rdx), %r8
	movl	16(%rdx), %eax
	movq	%rax, -8(%rsp)                  ## 8-byte Spill
	movslq	20(%rdx), %r14
	movl	24(%rdx), %ebp
	movl	28(%rdx), %eax
	movq	%rax, -40(%rsp)                 ## 8-byte Spill
	movl	%esi, %edi
	sarl	$31, %edi
	xorl	%r11d, %r11d
	testl	%ebp, %ebp
	sete	%r11b
	movl	%r11d, %esi
	negl	%esi
	subl	%edi, %r10d
	orl	%ebp, %esi
	movl	%r10d, %eax
	cltd
	movl	36(%rcx), %ebx
	movl	%ebx, -96(%rsp)                 ## 4-byte Spill
	movq	40(%rcx), %rbx
	movq	%rbx, -88(%rsp)                 ## 8-byte Spill
	movq	56(%rcx), %rbx
	movq	%rbx, -112(%rsp)                ## 8-byte Spill
	movq	%rcx, 40(%rsp)                  ## 8-byte Spill
	movq	72(%rcx), %rbx
	movq	%rbx, -104(%rsp)                ## 8-byte Spill
	idivl	%esi
	movl	%ebp, %eax
	sarl	$31, %eax
	movl	%eax, %ebx
	xorl	%ebp, %ebx
	movl	%eax, %esi
	notl	%esi
	addl	%esi, %ebx
	andl	%edi, %ebx
	addl	%edx, %ebx
	subl	%eax, %esi
	andl	%edi, %esi
	addl	%r11d, %ebp
	movl	%r10d, %eax
	cltd
	idivl	%ebp
	addl	%eax, %esi
	leal	-1(%r11), %eax
	andl	%eax, %ebx
	andl	%eax, %esi
	leal	(%rsi,%rsi), %ecx
	subl	%ecx, %r9d
	cmpl	$2, %r9d
	movl	$2, %edi
	cmovll	%r9d, %edi
	movl	%esi, %eax
	sarl	$31, %eax
	andl	%esi, %eax
	movl	$-1, %edx
	cmovnsl	%eax, %edx
	addl	%edx, %edx
	movl	%edx, 56(%rsp)                  ## 4-byte Spill
	movl	%edx, %eax
	negl	%eax
	cmpl	%eax, %edi
	cmovlel	%eax, %edi
	movl	%edi, 48(%rsp)                  ## 4-byte Spill
	cmpl	%eax, %r9d
	cmovll	%r9d, %eax
	movq	%rsi, -24(%rsp)                 ## 8-byte Spill
	movslq	%esi, %rdx
	subq	%r8, %rdx
	addq	%rdx, %rdx
	movq	%rdx, -16(%rsp)                 ## 8-byte Spill
	leal	(,%rbx,8), %edx
	vmovd	%edx, %xmm0
	movq	%rbx, -48(%rsp)                 ## 8-byte Spill
	leal	8(,%rbx,8), %esi
	movl	%esi, 8(%rsp)                   ## 4-byte Spill
	movq	%rdx, 16(%rsp)                  ## 8-byte Spill
	movslq	%edx, %rdx
	movq	%rdx, -120(%rsp)                ## 8-byte Spill
	movq	%rcx, 24(%rsp)                  ## 8-byte Spill
	movslq	%ecx, %rcx
	movq	%rcx, 112(%rsp)                 ## 8-byte Spill
	testl	%eax, %eax
	movq	%r14, (%rsp)                    ## 8-byte Spill
	jle	LBB14_19
## %bb.1:                               ## %"for relu1_0_d_def__.s6.w.wi.preheader"
	movq	%r14, %rbx
	movl	%eax, %ecx
	sarl	$31, %ecx
	andnl	%eax, %ecx, %ecx
	movq	-8(%rsp), %rbp                  ## 8-byte Reload
	movl	%ebp, %eax
	shll	$5, %eax
	movl	%eax, -80(%rsp)                 ## 4-byte Spill
	movq	-56(%rsp), %rsi                 ## 8-byte Reload
	movl	%esi, %eax
	subl	16(%rsp), %eax                  ## 4-byte Folded Reload
	cmpl	$8, %eax
	movl	$8, %edi
	cmovll	%eax, %edi
	xorl	%edx, %edx
	testl	%eax, %eax
	cmovlel	%edx, %edi
	movl	%edi, -124(%rsp)                ## 4-byte Spill
	movl	%ecx, %r8d
	cmpl	%esi, 8(%rsp)                   ## 4-byte Folded Reload
	jle	LBB14_2
## %bb.4:                               ## %"for relu1_0_d_def__.s6.w.wi.preheader.split.us"
	testl	%eax, %eax
	jle	LBB14_19
## %bb.5:                               ## %"for relu1_0_d_def__.s6.w.wi.us.us.preheader"
	movl	-124(%rsp), %edi                ## 4-byte Reload
	movslq	24(%rsp), %rdx                  ## 4-byte Folded Reload
	movl	%edi, %eax
	andl	$2147483640, %eax               ## imm = 0x7FFFFFF8
	movq	%rax, 104(%rsp)                 ## 8-byte Spill
	addq	$-8, %rax
	movq	%rax, 96(%rsp)                  ## 8-byte Spill
	movq	%rax, %rbp
	shrq	$3, %rbp
	incq	%rbp
	movl	%ebp, %r13d
	andl	$3, %r13d
	movq	-32(%rsp), %rax                 ## 8-byte Reload
	movq	%rax, %r14
	imulq	-16(%rsp), %r14                 ## 8-byte Folded Reload
	addq	-120(%rsp), %r14                ## 8-byte Folded Reload
	movq	-88(%rsp), %rcx                 ## 8-byte Reload
	leaq	(%rcx,%r14,4), %r11
	addq	$96, %r11
	leaq	(,%rax,4), %rax
	movq	%rax, 32(%rsp)                  ## 8-byte Spill
	movq	-104(%rsp), %rax                ## 8-byte Reload
	leaq	96(%rax), %rax
	movq	%rax, 72(%rsp)                  ## 8-byte Spill
	movq	-40(%rsp), %rax                 ## 8-byte Reload
	movq	%rbx, %r9
	movq	-48(%rsp), %rbx                 ## 8-byte Reload
	leal	(%rax,%rbx,8), %eax
	movl	%eax, -60(%rsp)                 ## 4-byte Spill
	movq	-112(%rsp), %rax                ## 8-byte Reload
	addq	$96, %rax
	movq	%rax, 64(%rsp)                  ## 8-byte Spill
	movq	-24(%rsp), %rax                 ## 8-byte Reload
                                        ## kill: def $eax killed $eax killed $rax def $rax
	shll	$6, %eax
	movl	-96(%rsp), %esi                 ## 4-byte Reload
	shll	$6, %esi
	subl	%esi, %eax
	orl	$27, %eax
	imull	-8(%rsp), %eax                  ## 4-byte Folded Reload
	leal	(%rax,%rbx,8), %r15d
	movq	%r9, %rbx
	movl	-80(%rsp), %r12d                ## 4-byte Reload
	andq	$-4, %rbp
	negq	%rbp
	movq	%rbp, 80(%rsp)                  ## 8-byte Spill
	movq	%r13, 88(%rsp)                  ## 8-byte Spill
	shlq	$5, %r13
	leaq	(%rcx,%r14,4), %r9
	vmovss	LCPI14_1(%rip), %xmm1           ## xmm1 = mem[0],zero,zero,zero
	vbroadcastss	LCPI14_1(%rip), %ymm2   ## ymm2 = [9.99999971E-10,9.99999971E-10,9.99999971E-10,9.99999971E-10,9.99999971E-10,9.99999971E-10,9.99999971E-10,9.99999971E-10]
	xorl	%r10d, %r10d
	jmp	LBB14_6
	.p2align	4, 0x90
LBB14_18:                               ## %after_bb.loopexit.us.us
                                        ##   in Loop: Header=BB14_6 Depth=1
	incq	%r10
	movq	32(%rsp), %rax                  ## 8-byte Reload
	addq	%rax, %r11
	incq	%rdx
	addl	%r12d, %r15d
	addq	-32(%rsp), %r14                 ## 8-byte Folded Reload
	addq	%rax, %r9
	cmpq	%r8, %r10
	je	LBB14_19
LBB14_6:                                ## %"for relu1_0_d_def__.s6.w.wi.us.us"
                                        ## =>This Loop Header: Depth=1
                                        ##     Child Loop BB14_11 Depth 2
                                        ##     Child Loop BB14_14 Depth 2
                                        ##     Child Loop BB14_17 Depth 2
	testq	%rdx, %rdx
	movl	$0, %eax
	cmovgl	%edx, %eax
	imull	%ebx, %eax
	addl	-60(%rsp), %eax                 ## 4-byte Folded Reload
	movslq	%eax, %rcx
	movslq	%r15d, %rax
	movq	%rax, -72(%rsp)                 ## 8-byte Spill
	cmpl	$8, -124(%rsp)                  ## 4-byte Folded Reload
	jae	LBB14_8
## %bb.7:                               ##   in Loop: Header=BB14_6 Depth=1
	xorl	%esi, %esi
	jmp	LBB14_16
	.p2align	4, 0x90
LBB14_8:                                ## %vector.ph
                                        ##   in Loop: Header=BB14_6 Depth=1
	cmpq	$24, 96(%rsp)                   ## 8-byte Folded Reload
	jae	LBB14_10
## %bb.9:                               ##   in Loop: Header=BB14_6 Depth=1
	xorl	%ebp, %ebp
	jmp	LBB14_12
	.p2align	4, 0x90
LBB14_10:                               ## %vector.body.preheader
                                        ##   in Loop: Header=BB14_6 Depth=1
	movq	72(%rsp), %rax                  ## 8-byte Reload
	leaq	(%rax,%rcx,4), %r12
	movq	64(%rsp), %rax                  ## 8-byte Reload
	movq	-72(%rsp), %rsi                 ## 8-byte Reload
	leaq	(%rax,%rsi,4), %rbx
	movq	80(%rsp), %rsi                  ## 8-byte Reload
	xorl	%ebp, %ebp
	.p2align	4, 0x90
LBB14_11:                               ## %vector.body
                                        ##   Parent Loop BB14_6 Depth=1
                                        ## =>  This Inner Loop Header: Depth=2
	vmovups	-96(%r12,%rbp,4), %ymm3
	vmulps	-96(%r11,%rbp,4), %ymm3, %ymm3
	vfmadd213ps	-96(%rbx,%rbp,4), %ymm2, %ymm3 ## ymm3 = (ymm2 * ymm3) + mem
	vmovups	%ymm3, -96(%rbx,%rbp,4)
	vmovups	-64(%r12,%rbp,4), %ymm3
	vmulps	-64(%r11,%rbp,4), %ymm3, %ymm3
	vfmadd213ps	-64(%rbx,%rbp,4), %ymm2, %ymm3 ## ymm3 = (ymm2 * ymm3) + mem
	vmovups	%ymm3, -64(%rbx,%rbp,4)
	vmovups	-32(%r12,%rbp,4), %ymm3
	vmulps	-32(%r11,%rbp,4), %ymm3, %ymm3
	vfmadd213ps	-32(%rbx,%rbp,4), %ymm2, %ymm3 ## ymm3 = (ymm2 * ymm3) + mem
	vmovups	%ymm3, -32(%rbx,%rbp,4)
	vmovups	(%r12,%rbp,4), %ymm3
	vmulps	(%r11,%rbp,4), %ymm3, %ymm3
	vfmadd213ps	(%rbx,%rbp,4), %ymm2, %ymm3 ## ymm3 = (ymm2 * ymm3) + mem
	vmovups	%ymm3, (%rbx,%rbp,4)
	addq	$32, %rbp
	addq	$4, %rsi
	jne	LBB14_11
LBB14_12:                               ## %middle.block.unr-lcssa
                                        ##   in Loop: Header=BB14_6 Depth=1
	cmpq	$0, 88(%rsp)                    ## 8-byte Folded Reload
	je	LBB14_15
## %bb.13:                              ## %vector.body.epil.preheader
                                        ##   in Loop: Header=BB14_6 Depth=1
	leaq	(%r14,%rbp), %rax
	movq	-88(%rsp), %rsi                 ## 8-byte Reload
	leaq	(%rsi,%rax,4), %rsi
	leaq	(%rcx,%rbp), %rax
	movq	-104(%rsp), %rbx                ## 8-byte Reload
	leaq	(%rbx,%rax,4), %rbx
	addq	-72(%rsp), %rbp                 ## 8-byte Folded Reload
	movq	-112(%rsp), %rax                ## 8-byte Reload
	leaq	(%rax,%rbp,4), %rbp
	xorl	%eax, %eax
	.p2align	4, 0x90
LBB14_14:                               ## %vector.body.epil
                                        ##   Parent Loop BB14_6 Depth=1
                                        ## =>  This Inner Loop Header: Depth=2
	vmovups	(%rbx,%rax), %ymm3
	vmulps	(%rsi,%rax), %ymm3, %ymm3
	vfmadd213ps	(%rbp,%rax), %ymm2, %ymm3 ## ymm3 = (ymm2 * ymm3) + mem
	vmovups	%ymm3, (%rbp,%rax)
	addq	$32, %rax
	cmpq	%rax, %r13
	jne	LBB14_14
LBB14_15:                               ## %middle.block
                                        ##   in Loop: Header=BB14_6 Depth=1
	movq	104(%rsp), %rax                 ## 8-byte Reload
	movq	%rax, %rsi
	cmpq	%rdi, %rax
	movq	(%rsp), %rbx                    ## 8-byte Reload
	movl	-80(%rsp), %r12d                ## 4-byte Reload
	je	LBB14_18
LBB14_16:                               ## %"for relu1_0_d_def__.s6.n.ni.us.us.preheader"
                                        ##   in Loop: Header=BB14_6 Depth=1
	movq	-104(%rsp), %rax                ## 8-byte Reload
	leaq	(%rax,%rcx,4), %rbp
	movq	-112(%rsp), %rax                ## 8-byte Reload
	movq	%rbx, %rcx
	movq	-72(%rsp), %rbx                 ## 8-byte Reload
	leaq	(%rax,%rbx,4), %rax
	movq	%rcx, %rbx
	.p2align	4, 0x90
LBB14_17:                               ## %"for relu1_0_d_def__.s6.n.ni.us.us"
                                        ##   Parent Loop BB14_6 Depth=1
                                        ## =>  This Inner Loop Header: Depth=2
	vmovss	(%rbp,%rsi,4), %xmm3            ## xmm3 = mem[0],zero,zero,zero
	vmulss	(%r9,%rsi,4), %xmm3, %xmm3
	vfmadd213ss	(%rax,%rsi,4), %xmm1, %xmm3 ## xmm3 = (xmm1 * xmm3) + mem
	vmovss	%xmm3, (%rax,%rsi,4)
	incq	%rsi
	cmpq	%rsi, %rdi
	jne	LBB14_17
	jmp	LBB14_18
LBB14_2:                                ## %"for relu1_0_d_def__.s6.w.wi.preheader26"
	vpbroadcastd	%xmm0, %ymm1
	vpor	LCPI14_0(%rip), %ymm1, %ymm1
	leal	-1(%rsi), %eax
	vmovd	%eax, %xmm2
	vpbroadcastd	%xmm2, %ymm2
	vpminsd	%ymm1, %ymm2, %ymm1
	negq	%r8
	movq	-32(%rsp), %rdx                 ## 8-byte Reload
	movq	%rdx, %rax
	imulq	-16(%rsp), %rax                 ## 8-byte Folded Reload
	addq	-120(%rsp), %rax                ## 8-byte Folded Reload
	movq	-88(%rsp), %rsi                 ## 8-byte Reload
	leaq	(%rsi,%rax,4), %rax
	leaq	(,%rdx,4), %r9
	movq	-24(%rsp), %rsi                 ## 8-byte Reload
                                        ## kill: def $esi killed $esi killed $rsi def $rsi
	shll	$6, %esi
	movl	-96(%rsp), %edi                 ## 4-byte Reload
	shll	$6, %edi
	subl	%edi, %esi
	orl	$27, %esi
	imull	%ebp, %esi
	movq	-48(%rsp), %rdx                 ## 8-byte Reload
	leal	(%rsi,%rdx,8), %edi
	vbroadcastss	LCPI14_1(%rip), %ymm2   ## ymm2 = [9.99999971E-10,9.99999971E-10,9.99999971E-10,9.99999971E-10,9.99999971E-10,9.99999971E-10,9.99999971E-10,9.99999971E-10]
	movq	112(%rsp), %rbp                 ## 8-byte Reload
	movq	-104(%rsp), %rdx                ## 8-byte Reload
	.p2align	4, 0x90
LBB14_3:                                ## %"for relu1_0_d_def__.s6.w.wi"
                                        ## =>This Inner Loop Header: Depth=1
	testq	%rbp, %rbp
	movl	$0, %esi
	cmovgl	%ebp, %esi
	imull	%ebx, %esi
	addl	-40(%rsp), %esi                 ## 4-byte Folded Reload
	vmovd	%esi, %xmm3
	vpbroadcastd	%xmm3, %ymm3
	vpaddd	%ymm1, %ymm3, %ymm3
	vextracti128	$1, %ymm3, %xmm4
	vmovd	%xmm4, %esi
	movslq	%esi, %rsi
	movq	%rbx, %rcx
	vpextrd	$1, %xmm4, %ebx
	movslq	%ebx, %rbx
	vmovss	(%rdx,%rsi,4), %xmm5            ## xmm5 = mem[0],zero,zero,zero
	vpextrd	$2, %xmm4, %esi
	vinsertps	$16, (%rdx,%rbx,4), %xmm5, %xmm5 ## xmm5 = xmm5[0],mem[0],xmm5[2,3]
	vmovd	%xmm3, %ebx
	movslq	%esi, %rsi
	vinsertps	$32, (%rdx,%rsi,4), %xmm5, %xmm5 ## xmm5 = xmm5[0,1],mem[0],xmm5[3]
	movslq	%ebx, %rsi
	vpextrd	$3, %xmm4, %ebx
	movslq	%ebx, %rbx
	vinsertps	$48, (%rdx,%rbx,4), %xmm5, %xmm4 ## xmm4 = xmm5[0,1,2],mem[0]
	vpextrd	$1, %xmm3, %ebx
	vmovss	(%rdx,%rsi,4), %xmm5            ## xmm5 = mem[0],zero,zero,zero
	movslq	%ebx, %rsi
	movq	%rcx, %rbx
	vinsertps	$16, (%rdx,%rsi,4), %xmm5, %xmm5 ## xmm5 = xmm5[0],mem[0],xmm5[2,3]
	vpextrd	$2, %xmm3, %esi
	movslq	%esi, %rsi
	vinsertps	$32, (%rdx,%rsi,4), %xmm5, %xmm5 ## xmm5 = xmm5[0,1],mem[0],xmm5[3]
	vpextrd	$3, %xmm3, %esi
	movslq	%esi, %rsi
	vinsertps	$48, (%rdx,%rsi,4), %xmm5, %xmm3 ## xmm3 = xmm5[0,1,2],mem[0]
	movslq	%edi, %rdi
	vinsertf128	$1, %xmm4, %ymm3, %ymm3
	vmulps	(%rax), %ymm3, %ymm3
	movq	-112(%rsp), %rsi                ## 8-byte Reload
	vfmadd213ps	(%rsi,%rdi,4), %ymm2, %ymm3 ## ymm3 = (ymm2 * ymm3) + mem
	vmovups	%ymm3, (%rsi,%rdi,4)
	addq	%r9, %rax
	incq	%rbp
	addl	-80(%rsp), %edi                 ## 4-byte Folded Reload
	incq	%r8
	jne	LBB14_3
LBB14_19:                               ## %"end for relu1_0_d_def__.s6.w.wi"
	movl	56(%rsp), %r15d                 ## 4-byte Reload
	movl	48(%rsp), %r10d                 ## 4-byte Reload
	addl	%r15d, %r10d
	movq	-32(%rsp), %r14                 ## 8-byte Reload
	movq	24(%rsp), %r9                   ## 8-byte Reload
	movq	-40(%rsp), %rbp                 ## 8-byte Reload
	jle	LBB14_23
## %bb.20:                              ## %"for relu1_0_d_def__.s6.w.wi.rebased.preheader"
	movq	-8(%rsp), %r11                  ## 8-byte Reload
	movl	%r11d, %eax
	shll	$5, %eax
	movl	%eax, -124(%rsp)                ## 4-byte Spill
	movq	-56(%rsp), %rdx                 ## 8-byte Reload
	movl	%edx, %ecx
	movq	16(%rsp), %rsi                  ## 8-byte Reload
	subl	%esi, %ecx
	cmpl	$8, %ecx
	movl	$8, %eax
	cmovll	%ecx, %eax
	xorl	%edi, %edi
	movl	%ecx, -60(%rsp)                 ## 4-byte Spill
	testl	%ecx, %ecx
	cmovgl	%eax, %edi
	movl	%esi, %eax
	subl	%edx, %eax
	movl	%eax, %ecx
	sarl	$31, %ecx
	andl	%eax, %ecx
	cmpl	$-8, %ecx
	movl	$-8, %eax
	cmovgl	%ecx, %eax
	movslq	-16(%rsp), %r12                 ## 4-byte Folded Reload
	cmpl	%edx, 8(%rsp)                   ## 4-byte Folded Reload
	jle	LBB14_21
## %bb.24:                              ## %"for relu1_0_d_def__.s6.w.wi.rebased.us.preheader"
	leal	(%r11,%r11,8), %edx
	movq	-24(%rsp), %rcx                 ## 8-byte Reload
	movl	-96(%rsp), %r13d                ## 4-byte Reload
	subl	%r13d, %ecx
	movq	40(%rsp), %rbx                  ## 8-byte Reload
	movslq	32(%rbx), %rbx
	movq	%rbx, 64(%rsp)                  ## 8-byte Spill
	addl	%ecx, %ecx
	movq	%rcx, -24(%rsp)                 ## 8-byte Spill
	leal	(%rdx,%rdx,2), %ecx
	movl	%ecx, 124(%rsp)                 ## 4-byte Spill
	addl	%edi, %eax
	leal	(%rdi,%rsi), %ecx
	addl	%ebp, %esi
	movq	%rdi, 72(%rsp)                  ## 8-byte Spill
	movl	%edi, %r8d
	movslq	%ecx, %rcx
	movq	%rcx, -56(%rsp)                 ## 8-byte Spill
	movl	%eax, 104(%rsp)                 ## 4-byte Spill
	movq	%r11, %rdi
	movl	%eax, %r11d
	movslq	%r15d, %rdx
	movslq	%esi, %rax
	movl	%r10d, %ecx
	movq	%rcx, 96(%rsp)                  ## 8-byte Spill
	movl	%r8d, %ecx
	andl	$2147483640, %ecx               ## imm = 0x7FFFFFF8
	movq	%rcx, 16(%rsp)                  ## 8-byte Spill
	leaq	-8(%rcx), %rsi
	movq	112(%rsp), %rcx                 ## 8-byte Reload
	subq	%rdx, %rcx
	movq	(%rsp), %rbp                    ## 8-byte Reload
	imulq	%rbp, %rcx
	addq	%rax, %rcx
	movq	%rsi, -16(%rsp)                 ## 8-byte Spill
	shrq	$3, %rsi
	incq	%rsi
	addl	%r13d, %r13d
	subl	%r13d, %r9d
	movl	%r11d, %eax
	andl	$-16, %eax
	subl	%r15d, %r9d
	movq	%rax, 8(%rsp)                   ## 8-byte Spill
	addq	$-16, %rax
	shll	$5, %r9d
	orl	$27, %r9d
	imull	%edi, %r9d
	movq	%rax, 56(%rsp)                  ## 8-byte Spill
	movq	%rax, %rdi
	shrq	$4, %rdi
	incq	%rdi
	movl	%esi, %r15d
	andl	$3, %r15d
	movq	%r12, 144(%rsp)                 ## 8-byte Spill
	movq	%r12, %rax
	movq	%rdx, -8(%rsp)                  ## 8-byte Spill
	subq	%rdx, %rax
	movq	%r14, %rbx
	imulq	%r14, %rax
	movq	-120(%rsp), %r14                ## 8-byte Reload
	addq	%rax, %r14
	movq	-48(%rsp), %rdx                 ## 8-byte Reload
	leal	(%r9,%rdx,8), %edx
	movl	%edx, -40(%rsp)                 ## 4-byte Spill
	andq	$-4, %rsi
	negq	%rsi
	movq	%rsi, 136(%rsp)                 ## 8-byte Spill
	movq	%r15, 40(%rsp)                  ## 8-byte Spill
	shlq	$5, %r15
	movq	%r15, 128(%rsp)                 ## 8-byte Spill
	movq	-56(%rsp), %r15                 ## 8-byte Reload
	addq	%r15, %rax
	movq	%rdi, %rdx
	movq	%rdi, 48(%rsp)                  ## 8-byte Spill
	andq	$-2, %rdi
	negq	%rdi
	movq	%rdi, 152(%rsp)                 ## 8-byte Spill
	movq	%r9, %rdi
	movq	-88(%rsp), %rsi                 ## 8-byte Reload
	leaq	(%rsi,%rax,4), %r13
	addq	$96, %r13
	leaq	(%rsi,%rax,4), %r12
	vmovss	LCPI14_1(%rip), %xmm0           ## xmm0 = mem[0],zero,zero,zero
	vbroadcastss	LCPI14_1(%rip), %ymm1   ## ymm1 = [9.99999971E-10,9.99999971E-10,9.99999971E-10,9.99999971E-10,9.99999971E-10,9.99999971E-10,9.99999971E-10,9.99999971E-10]
	leaq	96(%rsi,%r14,4), %r9
	leaq	(,%rbx,4), %rax
	movq	%rax, 88(%rsp)                  ## 8-byte Spill
	movq	-104(%rsp), %rdx                ## 8-byte Reload
	leaq	96(%rdx,%rcx,4), %r10
	leaq	(,%rbp,4), %rax
	movq	%rax, 80(%rsp)                  ## 8-byte Spill
	movq	-112(%rsp), %rax                ## 8-byte Reload
	leaq	96(%rax), %rbp
	movq	%rbp, -96(%rsp)                 ## 8-byte Spill
	movq	%r14, -120(%rsp)                ## 8-byte Spill
	leaq	(%rsi,%r14,4), %rbp
	movq	%rdi, %r14
	movq	%rcx, -80(%rsp)                 ## 8-byte Spill
	leaq	(%rdx,%rcx,4), %rdi
	movq	%r15, %rcx
	leaq	(%rax,%r15,4), %rax
	movq	%rax, -48(%rsp)                 ## 8-byte Spill
	xorl	%eax, %eax
	movq	%rax, -72(%rsp)                 ## 8-byte Spill
	movl	-124(%rsp), %edx                ## 4-byte Reload
	jmp	LBB14_25
	.p2align	4, 0x90
LBB14_33:                               ## %after_bb2.us
                                        ##   in Loop: Header=BB14_25 Depth=1
	movq	-72(%rsp), %rsi                 ## 8-byte Reload
	incq	%rsi
	movq	88(%rsp), %rcx                  ## 8-byte Reload
	addq	%rcx, %r9
	movq	%r14, %r15
	movq	80(%rsp), %r14                  ## 8-byte Reload
	addq	%r14, %r10
	addl	%edx, -40(%rsp)                 ## 4-byte Folded Spill
	movq	-32(%rsp), %rbx                 ## 8-byte Reload
	addq	%rbx, -120(%rsp)                ## 8-byte Folded Spill
	movq	-80(%rsp), %rax                 ## 8-byte Reload
	addq	(%rsp), %rax                    ## 8-byte Folded Reload
	movq	%rax, -80(%rsp)                 ## 8-byte Spill
	addq	%rcx, %rbp
	addq	%r14, %rdi
	movq	%r15, %r14
	addq	%rcx, %r13
	addl	%edx, %r14d
	addq	%rcx, %r12
	movq	%rsi, %rax
	movq	%rsi, -72(%rsp)                 ## 8-byte Spill
	cmpq	96(%rsp), %rsi                  ## 8-byte Folded Reload
	je	LBB14_23
LBB14_25:                               ## %"for relu1_0_d_def__.s6.w.wi.rebased.us"
                                        ## =>This Loop Header: Depth=1
                                        ##     Child Loop BB14_45 Depth 2
                                        ##     Child Loop BB14_48 Depth 2
                                        ##     Child Loop BB14_28 Depth 2
                                        ##     Child Loop BB14_37 Depth 2
                                        ##     Child Loop BB14_32 Depth 2
	cmpl	$0, -60(%rsp)                   ## 4-byte Folded Reload
	movl	104(%rsp), %r15d                ## 4-byte Reload
	jle	LBB14_29
## %bb.26:                              ## %"for relu1_0_d_def__.s6.n.ni5.preheader.us"
                                        ##   in Loop: Header=BB14_25 Depth=1
	movslq	-40(%rsp), %rsi                 ## 4-byte Folded Reload
	cmpl	$8, 72(%rsp)                    ## 4-byte Folded Reload
	jae	LBB14_42
## %bb.27:                              ##   in Loop: Header=BB14_25 Depth=1
	xorl	%ecx, %ecx
	jmp	LBB14_50
	.p2align	4, 0x90
LBB14_42:                               ## %vector.ph79
                                        ##   in Loop: Header=BB14_25 Depth=1
	cmpq	$24, -16(%rsp)                  ## 8-byte Folded Reload
	movq	%r14, 24(%rsp)                  ## 8-byte Spill
	movq	%rsi, 32(%rsp)                  ## 8-byte Spill
	jae	LBB14_44
## %bb.43:                              ##   in Loop: Header=BB14_25 Depth=1
	movq	-120(%rsp), %rax                ## 8-byte Reload
	xorl	%edx, %edx
	movq	128(%rsp), %rbx                 ## 8-byte Reload
	jmp	LBB14_46
	.p2align	4, 0x90
LBB14_44:                               ## %vector.body77.preheader
                                        ##   in Loop: Header=BB14_25 Depth=1
	movq	-120(%rsp), %rax                ## 8-byte Reload
	movq	-96(%rsp), %rcx                 ## 8-byte Reload
	leaq	(%rcx,%rsi,4), %r14
	movq	136(%rsp), %rsi                 ## 8-byte Reload
	xorl	%edx, %edx
	movq	128(%rsp), %rbx                 ## 8-byte Reload
	.p2align	4, 0x90
LBB14_45:                               ## %vector.body77
                                        ##   Parent Loop BB14_25 Depth=1
                                        ## =>  This Inner Loop Header: Depth=2
	vmovups	-96(%r10,%rdx,4), %ymm2
	vmulps	-96(%r9,%rdx,4), %ymm2, %ymm2
	vfmadd213ps	-96(%r14,%rdx,4), %ymm1, %ymm2 ## ymm2 = (ymm1 * ymm2) + mem
	vmovups	%ymm2, -96(%r14,%rdx,4)
	vmovups	-64(%r10,%rdx,4), %ymm2
	vmulps	-64(%r9,%rdx,4), %ymm2, %ymm2
	vfmadd213ps	-64(%r14,%rdx,4), %ymm1, %ymm2 ## ymm2 = (ymm1 * ymm2) + mem
	vmovups	%ymm2, -64(%r14,%rdx,4)
	vmovups	-32(%r10,%rdx,4), %ymm2
	vmulps	-32(%r9,%rdx,4), %ymm2, %ymm2
	vfmadd213ps	-32(%r14,%rdx,4), %ymm1, %ymm2 ## ymm2 = (ymm1 * ymm2) + mem
	vmovups	%ymm2, -32(%r14,%rdx,4)
	vmovups	(%r10,%rdx,4), %ymm2
	vmulps	(%r9,%rdx,4), %ymm2, %ymm2
	vfmadd213ps	(%r14,%rdx,4), %ymm1, %ymm2 ## ymm2 = (ymm1 * ymm2) + mem
	vmovups	%ymm2, (%r14,%rdx,4)
	addq	$32, %rdx
	addq	$4, %rsi
	jne	LBB14_45
LBB14_46:                               ## %middle.block75.unr-lcssa
                                        ##   in Loop: Header=BB14_25 Depth=1
	cmpq	$0, 40(%rsp)                    ## 8-byte Folded Reload
	movq	%rax, %r14
	je	LBB14_49
## %bb.47:                              ## %vector.body77.epil.preheader
                                        ##   in Loop: Header=BB14_25 Depth=1
	leaq	(%rdx,%r14), %rax
	movq	-88(%rsp), %rcx                 ## 8-byte Reload
	leaq	(%rcx,%rax,4), %rcx
	movq	-80(%rsp), %rax                 ## 8-byte Reload
	addq	%rdx, %rax
	movq	-104(%rsp), %rsi                ## 8-byte Reload
	leaq	(%rsi,%rax,4), %rsi
	addq	32(%rsp), %rdx                  ## 8-byte Folded Reload
	movq	-112(%rsp), %rax                ## 8-byte Reload
	leaq	(%rax,%rdx,4), %rdx
	xorl	%eax, %eax
	.p2align	4, 0x90
LBB14_48:                               ## %vector.body77.epil
                                        ##   Parent Loop BB14_25 Depth=1
                                        ## =>  This Inner Loop Header: Depth=2
	vmovups	(%rsi,%rax), %ymm2
	vmulps	(%rcx,%rax), %ymm2, %ymm2
	vfmadd213ps	(%rdx,%rax), %ymm1, %ymm2 ## ymm2 = (ymm1 * ymm2) + mem
	vmovups	%ymm2, (%rdx,%rax)
	addq	$32, %rax
	cmpq	%rax, %rbx
	jne	LBB14_48
LBB14_49:                               ## %middle.block75
                                        ##   in Loop: Header=BB14_25 Depth=1
	movq	%r14, -120(%rsp)                ## 8-byte Spill
	movq	16(%rsp), %rax                  ## 8-byte Reload
	movq	%rax, %rcx
	cmpq	%r8, %rax
	movq	24(%rsp), %r14                  ## 8-byte Reload
	movl	-124(%rsp), %edx                ## 4-byte Reload
	movq	32(%rsp), %rsi                  ## 8-byte Reload
	je	LBB14_29
LBB14_50:                               ## %"for relu1_0_d_def__.s6.n.ni5.us.preheader"
                                        ##   in Loop: Header=BB14_25 Depth=1
	movq	-112(%rsp), %rax                ## 8-byte Reload
	leaq	(%rax,%rsi,4), %rax
	.p2align	4, 0x90
LBB14_28:                               ## %"for relu1_0_d_def__.s6.n.ni5.us"
                                        ##   Parent Loop BB14_25 Depth=1
                                        ## =>  This Inner Loop Header: Depth=2
	vmovss	(%rdi,%rcx,4), %xmm2            ## xmm2 = mem[0],zero,zero,zero
	vmulss	(%rbp,%rcx,4), %xmm2, %xmm2
	vfmadd213ss	(%rax,%rcx,4), %xmm0, %xmm2 ## xmm2 = (xmm0 * xmm2) + mem
	vmovss	%xmm2, (%rax,%rcx,4)
	incq	%rcx
	cmpq	%rcx, %r8
	jne	LBB14_28
LBB14_29:                               ## %"end for relu1_0_d_def__.s6.n.ni6.us"
                                        ##   in Loop: Header=BB14_25 Depth=1
	testl	%r15d, %r15d
	jle	LBB14_33
## %bb.30:                              ## %"for relu1_0_d_def__.s6.n.ni.rebased.preheader.us"
                                        ##   in Loop: Header=BB14_25 Depth=1
	movq	-72(%rsp), %rdx                 ## 8-byte Reload
	subq	-8(%rsp), %rdx                  ## 8-byte Folded Reload
	movslq	%r14d, %rbx
	movq	112(%rsp), %rax                 ## 8-byte Reload
	addq	%rdx, %rax
	imulq	(%rsp), %rax                    ## 8-byte Folded Reload
	addq	64(%rsp), %rax                  ## 8-byte Folded Reload
	movq	-104(%rsp), %rcx                ## 8-byte Reload
	vmovss	-4(%rcx,%rax,4), %xmm2          ## xmm2 = mem[0],zero,zero,zero
	cmpl	$16, %r15d
	jae	LBB14_34
## %bb.31:                              ##   in Loop: Header=BB14_25 Depth=1
	xorl	%eax, %eax
	movl	-124(%rsp), %edx                ## 4-byte Reload
	jmp	LBB14_41
	.p2align	4, 0x90
LBB14_34:                               ## %vector.ph62
                                        ##   in Loop: Header=BB14_25 Depth=1
	vbroadcastss	%xmm2, %ymm3
	cmpq	$0, 56(%rsp)                    ## 8-byte Folded Reload
	je	LBB14_35
## %bb.36:                              ## %vector.body60.preheader
                                        ##   in Loop: Header=BB14_25 Depth=1
	movq	-56(%rsp), %rax                 ## 8-byte Reload
	addq	%rbx, %rax
	movq	-96(%rsp), %rcx                 ## 8-byte Reload
	leaq	(%rcx,%rax,4), %rsi
	movq	152(%rsp), %rcx                 ## 8-byte Reload
	xorl	%eax, %eax
	.p2align	4, 0x90
LBB14_37:                               ## %vector.body60
                                        ##   Parent Loop BB14_25 Depth=1
                                        ## =>  This Inner Loop Header: Depth=2
	vmulps	-96(%r13,%rax,4), %ymm3, %ymm4
	vmulps	-64(%r13,%rax,4), %ymm3, %ymm5
	vfmadd213ps	-96(%rsi,%rax,4), %ymm1, %ymm4 ## ymm4 = (ymm1 * ymm4) + mem
	vfmadd213ps	-64(%rsi,%rax,4), %ymm1, %ymm5 ## ymm5 = (ymm1 * ymm5) + mem
	vmovups	%ymm4, -96(%rsi,%rax,4)
	vmovups	%ymm5, -64(%rsi,%rax,4)
	vmulps	-32(%r13,%rax,4), %ymm3, %ymm4
	vmulps	(%r13,%rax,4), %ymm3, %ymm5
	vfmadd213ps	-32(%rsi,%rax,4), %ymm1, %ymm4 ## ymm4 = (ymm1 * ymm4) + mem
	vfmadd213ps	(%rsi,%rax,4), %ymm1, %ymm5 ## ymm5 = (ymm1 * ymm5) + mem
	vmovups	%ymm4, -32(%rsi,%rax,4)
	vmovups	%ymm5, (%rsi,%rax,4)
	addq	$32, %rax
	addq	$2, %rcx
	jne	LBB14_37
## %bb.38:                              ## %middle.block58.unr-lcssa
                                        ##   in Loop: Header=BB14_25 Depth=1
	testb	$1, 48(%rsp)                    ## 1-byte Folded Reload
	je	LBB14_40
LBB14_39:                               ## %vector.body60.epil
                                        ##   in Loop: Header=BB14_25 Depth=1
	movq	-24(%rsp), %rcx                 ## 8-byte Reload
	addl	%edx, %ecx
	imull	-124(%rsp), %ecx                ## 4-byte Folded Reload
	addl	124(%rsp), %ecx                 ## 4-byte Folded Reload
	addq	144(%rsp), %rdx                 ## 8-byte Folded Reload
	imulq	-32(%rsp), %rdx                 ## 8-byte Folded Reload
	movslq	%ecx, %rcx
	addq	-56(%rsp), %rax                 ## 8-byte Folded Reload
	addq	%rax, %rcx
	addq	%rdx, %rax
	movq	-88(%rsp), %rdx                 ## 8-byte Reload
	vmulps	(%rdx,%rax,4), %ymm3, %ymm4
	vmulps	32(%rdx,%rax,4), %ymm3, %ymm3
	movq	-112(%rsp), %rax                ## 8-byte Reload
	vfmadd213ps	(%rax,%rcx,4), %ymm1, %ymm4 ## ymm4 = (ymm1 * ymm4) + mem
	vfmadd213ps	32(%rax,%rcx,4), %ymm1, %ymm3 ## ymm3 = (ymm1 * ymm3) + mem
	vmovups	%ymm4, (%rax,%rcx,4)
	vmovups	%ymm3, 32(%rax,%rcx,4)
LBB14_40:                               ## %middle.block58
                                        ##   in Loop: Header=BB14_25 Depth=1
	movq	8(%rsp), %rcx                   ## 8-byte Reload
	movq	%rcx, %rax
	cmpq	%r11, %rcx
	movl	-124(%rsp), %edx                ## 4-byte Reload
	je	LBB14_33
LBB14_41:                               ## %"for relu1_0_d_def__.s6.n.ni.rebased.us.preheader"
                                        ##   in Loop: Header=BB14_25 Depth=1
	movq	-48(%rsp), %rcx                 ## 8-byte Reload
	leaq	(%rcx,%rbx,4), %rcx
	.p2align	4, 0x90
LBB14_32:                               ## %"for relu1_0_d_def__.s6.n.ni.rebased.us"
                                        ##   Parent Loop BB14_25 Depth=1
                                        ## =>  This Inner Loop Header: Depth=2
	vmulss	(%r12,%rax,4), %xmm2, %xmm3
	vfmadd213ss	(%rcx,%rax,4), %xmm0, %xmm3 ## xmm3 = (xmm0 * xmm3) + mem
	vmovss	%xmm3, (%rcx,%rax,4)
	incq	%rax
	cmpq	%rax, %r11
	jne	LBB14_32
	jmp	LBB14_33
LBB14_35:                               ##   in Loop: Header=BB14_25 Depth=1
	xorl	%eax, %eax
	testb	$1, 48(%rsp)                    ## 1-byte Folded Reload
	jne	LBB14_39
	jmp	LBB14_40
LBB14_21:                               ## %"for relu1_0_d_def__.s6.w.wi.rebased.preheader23"
	decl	%edx
	vpbroadcastd	%xmm0, %ymm0
	vpor	LCPI14_0(%rip), %ymm0, %ymm0
	vmovd	%edx, %xmm1
	vpbroadcastd	%xmm1, %ymm1
	vpminsd	%ymm0, %ymm1, %ymm0
	movslq	%r15d, %rax
	movl	%r10d, %ecx
	negq	%rcx
	subq	%rax, %r12
	imulq	%r14, %r12
	addq	-120(%rsp), %r12                ## 8-byte Folded Reload
	movq	-88(%rsp), %rax                 ## 8-byte Reload
	leaq	(%rax,%r12,4), %rax
	shlq	$2, %r14
	movl	%r9d, %edx
	subl	%r15d, %edx
	movq	(%rsp), %r8                     ## 8-byte Reload
	imull	%r8d, %edx
	addl	%edx, %ebp
	movl	-96(%rsp), %edx                 ## 4-byte Reload
	addl	%edx, %edx
	subl	%edx, %r9d
	subl	%r15d, %r9d
	shll	$5, %r9d
	orl	$27, %r9d
	imull	%r9d, %r11d
	movq	-48(%rsp), %rdx                 ## 8-byte Reload
	leal	(%r11,%rdx,8), %edx
	vbroadcastss	LCPI14_1(%rip), %ymm1   ## ymm1 = [9.99999971E-10,9.99999971E-10,9.99999971E-10,9.99999971E-10,9.99999971E-10,9.99999971E-10,9.99999971E-10,9.99999971E-10]
	movl	-124(%rsp), %r9d                ## 4-byte Reload
	movq	-104(%rsp), %rbx                ## 8-byte Reload
	.p2align	4, 0x90
LBB14_22:                               ## %"for relu1_0_d_def__.s6.w.wi.rebased"
                                        ## =>This Inner Loop Header: Depth=1
	vmovd	%ebp, %xmm2
	vpbroadcastd	%xmm2, %ymm2
	vpaddd	%ymm0, %ymm2, %ymm2
	vextracti128	$1, %ymm2, %xmm3
	vmovd	%xmm3, %esi
	movslq	%esi, %rsi
	vpextrd	$1, %xmm3, %edi
	movslq	%edi, %rdi
	vmovss	(%rbx,%rsi,4), %xmm4            ## xmm4 = mem[0],zero,zero,zero
	vpextrd	$2, %xmm3, %esi
	vinsertps	$16, (%rbx,%rdi,4), %xmm4, %xmm4 ## xmm4 = xmm4[0],mem[0],xmm4[2,3]
	vmovd	%xmm2, %edi
	movslq	%esi, %rsi
	vinsertps	$32, (%rbx,%rsi,4), %xmm4, %xmm4 ## xmm4 = xmm4[0,1],mem[0],xmm4[3]
	movslq	%edi, %rsi
	vpextrd	$3, %xmm3, %edi
	movslq	%edi, %rdi
	vinsertps	$48, (%rbx,%rdi,4), %xmm4, %xmm3 ## xmm3 = xmm4[0,1,2],mem[0]
	vpextrd	$1, %xmm2, %edi
	vmovss	(%rbx,%rsi,4), %xmm4            ## xmm4 = mem[0],zero,zero,zero
	movslq	%edi, %rsi
	vinsertps	$16, (%rbx,%rsi,4), %xmm4, %xmm4 ## xmm4 = xmm4[0],mem[0],xmm4[2,3]
	vpextrd	$2, %xmm2, %esi
	movslq	%esi, %rsi
	vinsertps	$32, (%rbx,%rsi,4), %xmm4, %xmm4 ## xmm4 = xmm4[0,1],mem[0],xmm4[3]
	vpextrd	$3, %xmm2, %esi
	movslq	%esi, %rsi
	vinsertps	$48, (%rbx,%rsi,4), %xmm4, %xmm2 ## xmm2 = xmm4[0,1,2],mem[0]
	movslq	%edx, %rdx
	vinsertf128	$1, %xmm3, %ymm2, %ymm2
	vmulps	(%rax), %ymm2, %ymm2
	movq	-112(%rsp), %rsi                ## 8-byte Reload
	vfmadd213ps	(%rsi,%rdx,4), %ymm1, %ymm2 ## ymm2 = (ymm1 * ymm2) + mem
	vmovups	%ymm2, (%rsi,%rdx,4)
	addq	%r14, %rax
	addl	%r8d, %ebp
	addl	%r9d, %edx
	incq	%rcx
	jne	LBB14_22
LBB14_23:                               ## %destructor_block
	xorl	%eax, %eax
	addq	$160, %rsp
	popq	%rbx
	popq	%r12
	popq	%r13
	popq	%r14
	popq	%r15
	popq	%rbp
	vzeroupper
	retq
                                        ## -- End function
	.section	__TEXT,__const
	.p2align	5                               ## -- Begin function train_cost_model.par_for.relu1_0_d_def__.s7.n.n.n
LCPI15_0:
	.long	0                               ## 0x0
	.long	1                               ## 0x1
	.long	2                               ## 0x2
	.long	3                               ## 0x3
	.long	4                               ## 0x4
	.long	5                               ## 0x5
	.long	6                               ## 0x6
	.long	7                               ## 0x7
	.section	__TEXT,__literal4,4byte_literals
	.p2align	2
LCPI15_1:
	.long	0x3f800000                      ## float 1
LCPI15_2:
	.long	0x3089705f                      ## float 9.99999971E-10
	.section	__TEXT,__text,regular,pure_instructions
	.p2align	4, 0x90
_train_cost_model.par_for.relu1_0_d_def__.s7.n.n.n: ## @train_cost_model.par_for.relu1_0_d_def__.s7.n.n.n
## %bb.0:                               ## %entry
	pushq	%rbp
	pushq	%r15
	pushq	%r14
	pushq	%r13
	pushq	%r12
	pushq	%rbx
	subq	$112, %rsp
	movq	%rdx, %rdi
	movl	12(%rdx), %r8d
	movl	28(%rdx), %r15d
	movl	%esi, %r9d
	sarl	$31, %r9d
	xorl	%ecx, %ecx
	testl	%r15d, %r15d
	sete	%cl
	movl	%ecx, %ebp
	negl	%ebp
	movl	%r15d, %ebx
	sarl	$31, %ebx
	subl	%r9d, %esi
	orl	%r15d, %ebp
	movl	%esi, %eax
	cltd
	idivl	%ebp
	movl	%edx, %r10d
	movl	%ebx, %r12d
	leal	(%r15,%rcx), %ebp
	movl	%esi, %eax
	cltd
	idivl	%ebp
	notl	%r12d
	decl	%ecx
	movl	%r12d, %r14d
	subl	%ebx, %r14d
	andl	%r9d, %r14d
	addl	%eax, %r14d
	andl	%ecx, %r14d
	leal	(%r14,%r14), %eax
	movl	%eax, -96(%rsp)                 ## 4-byte Spill
	subl	%eax, %r8d
	cmpl	$2, %r8d
	movl	$2, %edx
	cmovll	%r8d, %edx
	xorl	%eax, %eax
	testl	%r8d, %r8d
	cmovlel	%eax, %edx
	movl	%edx, -104(%rsp)                ## 4-byte Spill
	jle	LBB15_70
## %bb.1:                               ## %"for relu1_0_d_def__.s7.w.wi.preheader"
	movl	(%rdi), %edx
	movl	16(%rdi), %esi
	movl	20(%rdi), %r8d
	movl	36(%rdi), %r13d
	movl	40(%rdi), %ebp
	xorl	%r15d, %ebx
	addl	%r12d, %ebx
	andl	%r9d, %ebx
	addl	%ebx, %r10d
	andl	%ecx, %r10d
	movq	%r10, -80(%rsp)                 ## 8-byte Spill
	leal	(,%r10,8), %eax
	movl	%r8d, %r15d
	subl	%r13d, %r15d
	movl	%ebp, -120(%rsp)                ## 4-byte Spill
	movl	%ebp, %r9d
	subl	%r13d, %r9d
	movslq	8(%rdi), %rcx
	movslq	%r14d, %rbx
	subq	%rcx, %rbx
	addq	%rbx, %rbx
	movq	%rsi, -112(%rsp)                ## 8-byte Spill
	movl	%esi, %ecx
	shll	$5, %ecx
	movl	%ecx, -84(%rsp)                 ## 4-byte Spill
	movl	%edx, %ecx
	subl	%eax, %ecx
	cmpl	$8, %ecx
	movl	$8, %esi
	cmovll	%ecx, %esi
	testl	%ecx, %ecx
	movl	$0, %r11d
	cmovgl	%esi, %r11d
	movl	24(%rdi), %esi
	movl	%esi, -124(%rsp)                ## 4-byte Spill
	movl	%eax, %ebp
	subl	%edx, %ebp
	movl	%ebp, %esi
	sarl	$31, %esi
	andl	%ebp, %esi
	cmpl	$-8, %esi
	movl	$-8, %r10d
	cmovgl	%esi, %r10d
	movl	32(%rdi), %ebp
	movq	48(%rdi), %rsi
	movq	%rsi, -64(%rsp)                 ## 8-byte Spill
	movq	64(%rdi), %rsi
	movq	%rsi, -72(%rsp)                 ## 8-byte Spill
	movq	80(%rdi), %rsi
	movq	%rsi, -32(%rsp)                 ## 8-byte Spill
	movslq	4(%rdi), %rdi
	leal	8(%rax), %esi
	movslq	%eax, %r12
	cmpl	%edx, %esi
	jle	LBB15_2
## %bb.4:                               ## %"for relu1_0_d_def__.s7.w.wi.preheader.split.us"
	movq	%rdi, -8(%rsp)                  ## 8-byte Spill
	movl	%ebp, -48(%rsp)                 ## 4-byte Spill
	movq	%rbx, %rsi
	movq	-112(%rsp), %rbx                ## 8-byte Reload
	addl	%edx, %r8d
	subl	%r13d, %r8d
	subl	%r13d, %edx
	addl	-120(%rsp), %edx                ## 4-byte Folded Reload
	addl	%r11d, %eax
	addl	%r11d, %r10d
	movslq	%edx, %rdx
	movq	%rdx, 40(%rsp)                  ## 8-byte Spill
	movslq	%r8d, %rdx
	movq	%rdx, 32(%rsp)                  ## 8-byte Spill
	testl	%ecx, %ecx
	movl	%r10d, -88(%rsp)                ## 4-byte Spill
	jle	LBB15_41
## %bb.5:                               ## %"for relu1_0_d_def__.s7.w.wi.us.us.preheader"
	addl	%r12d, %r9d
	addl	%r12d, %r15d
	movslq	%r9d, %rcx
	movq	%rcx, 16(%rsp)                  ## 8-byte Spill
	movslq	%r15d, %rcx
	movq	%rcx, 8(%rsp)                   ## 8-byte Spill
	movl	%r11d, -16(%rsp)                ## 4-byte Spill
	movl	%r11d, %r11d
	movslq	%eax, %rdi
	movl	%r10d, %r13d
	movslq	-96(%rsp), %rax                 ## 4-byte Folded Reload
	movq	%rax, -40(%rsp)                 ## 8-byte Spill
	movl	-104(%rsp), %eax                ## 4-byte Reload
	movq	%rax, -24(%rsp)                 ## 8-byte Spill
	movl	%r13d, %eax
	andl	$-8, %eax
	movq	%rax, 80(%rsp)                  ## 8-byte Spill
	addq	$-8, %rax
	movq	%rax, 72(%rsp)                  ## 8-byte Spill
	shrq	$3, %rax
	incq	%rax
	movl	%r11d, %r8d
	andl	$2147483616, %r8d               ## imm = 0x7FFFFFE0
	shll	$6, %r14d
	movl	-48(%rsp), %ecx                 ## 4-byte Reload
	shll	$6, %ecx
	subl	%ecx, %r14d
	movl	%eax, %ecx
	andl	$3, %ecx
	movq	-8(%rsp), %r10                  ## 8-byte Reload
	imulq	%r10, %rsi
	addq	%rsi, %r12
	orl	$25, %r14d
	imull	%r14d, %ebx
	movq	-64(%rsp), %rdx                 ## 8-byte Reload
	leaq	(%rdx,%r12,4), %r15
	addq	$96, %r15
	movq	%rbx, -112(%rsp)                ## 8-byte Spill
	movq	-80(%rsp), %rbp                 ## 8-byte Reload
	leal	(%rbx,%rbp,8), %r9d
	leaq	(%rdx,%r12,4), %rbp
	addq	%rdi, %rsi
	andq	$-4, %rax
	negq	%rax
	movq	%rax, 48(%rsp)                  ## 8-byte Spill
	movq	%rcx, 64(%rsp)                  ## 8-byte Spill
	shlq	$5, %rcx
	movq	%rcx, (%rsp)                    ## 8-byte Spill
	vmovss	LCPI15_2(%rip), %xmm0           ## xmm0 = mem[0],zero,zero,zero
	vmovss	LCPI15_1(%rip), %xmm1           ## xmm1 = mem[0],zero,zero,zero
	vbroadcastss	LCPI15_2(%rip), %ymm2   ## ymm2 = [9.99999971E-10,9.99999971E-10,9.99999971E-10,9.99999971E-10,9.99999971E-10,9.99999971E-10,9.99999971E-10,9.99999971E-10]
	vbroadcastss	LCPI15_1(%rip), %ymm3   ## ymm3 = [1.0E+0,1.0E+0,1.0E+0,1.0E+0,1.0E+0,1.0E+0,1.0E+0,1.0E+0]
	leaq	(,%r10,4), %rax
	movq	%rax, -48(%rsp)                 ## 8-byte Spill
	movq	-32(%rsp), %rcx                 ## 8-byte Reload
	leaq	96(%rcx), %rax
	movq	%rax, 56(%rsp)                  ## 8-byte Spill
	movq	-72(%rsp), %rax                 ## 8-byte Reload
	leaq	96(%rax), %rbx
	movq	%rbx, -56(%rsp)                 ## 8-byte Spill
	movq	8(%rsp), %rbx                   ## 8-byte Reload
	leaq	(%rcx,%rbx,4), %rbx
	movq	%rbx, 104(%rsp)                 ## 8-byte Spill
	movq	16(%rsp), %rbx                  ## 8-byte Reload
	leaq	(%rcx,%rbx,4), %rcx
	movq	%rcx, 96(%rsp)                  ## 8-byte Spill
	movl	%r9d, %ecx
	leaq	96(%rdx,%rsi,4), %r14
	movq	%rsi, -120(%rsp)                ## 8-byte Spill
	leaq	(%rdx,%rsi,4), %r9
	movq	%rdi, %rdx
	movq	%rdi, 24(%rsp)                  ## 8-byte Spill
	leaq	(%rax,%rdi,4), %rax
	movq	%rax, 88(%rsp)                  ## 8-byte Spill
	movq	-40(%rsp), %rdx                 ## 8-byte Reload
	xorl	%esi, %esi
	movl	-124(%rsp), %ebx                ## 4-byte Reload
	jmp	LBB15_6
	.p2align	4, 0x90
LBB15_17:                               ## %after_bb.us.us
                                        ##   in Loop: Header=BB15_6 Depth=1
	movq	-104(%rsp), %rsi                ## 8-byte Reload
	incq	%rsi
	movq	-48(%rsp), %rax                 ## 8-byte Reload
	addq	%rax, %r15
	movq	-96(%rsp), %rdx                 ## 8-byte Reload
	incq	%rdx
	movl	-80(%rsp), %ecx                 ## 4-byte Reload
	movl	-84(%rsp), %edi                 ## 4-byte Reload
	addl	%edi, %ecx
	addq	%rax, %rbp
	addq	%rax, %r14
	addl	%edi, %r10d
	movq	%r10, -112(%rsp)                ## 8-byte Spill
	movq	-8(%rsp), %rdi                  ## 8-byte Reload
	addq	%rdi, -120(%rsp)                ## 8-byte Folded Spill
	addq	%rax, %r9
	cmpq	-24(%rsp), %rsi                 ## 8-byte Folded Reload
	movl	%r12d, %ebx
	je	LBB15_70
LBB15_6:                                ## %"for relu1_0_d_def__.s7.w.wi.us.us"
                                        ## =>This Loop Header: Depth=1
                                        ##     Child Loop BB15_9 Depth 2
                                        ##     Child Loop BB15_12 Depth 2
                                        ##     Child Loop BB15_21 Depth 2
                                        ##     Child Loop BB15_35 Depth 2
                                        ##     Child Loop BB15_16 Depth 2
	testq	%rdx, %rdx
	movl	$0, %eax
	movq	%rdx, -96(%rsp)                 ## 8-byte Spill
	cmovgl	%edx, %eax
	imull	%ebx, %eax
	movslq	%eax, %rdx
	movq	-40(%rsp), %rax                 ## 8-byte Reload
	movq	%rsi, -104(%rsp)                ## 8-byte Spill
	addq	%rsi, %rax
	testq	%rax, %rax
	movl	$0, %esi
	cmovlel	%esi, %eax
	movl	%ecx, -80(%rsp)                 ## 4-byte Spill
	movslq	%ecx, %r12
	cmpl	$32, -16(%rsp)                  ## 4-byte Folded Reload
	jae	LBB15_8
## %bb.7:                               ##   in Loop: Header=BB15_6 Depth=1
	xorl	%ecx, %ecx
	jmp	LBB15_11
	.p2align	4, 0x90
LBB15_8:                                ## %vector.body63.preheader
                                        ##   in Loop: Header=BB15_6 Depth=1
	movq	8(%rsp), %rcx                   ## 8-byte Reload
	addq	%rdx, %rcx
	movq	16(%rsp), %rsi                  ## 8-byte Reload
	addq	%rdx, %rsi
	movq	56(%rsp), %rdi                  ## 8-byte Reload
	leaq	(%rdi,%rcx,4), %rcx
	leaq	(%rdi,%rsi,4), %rsi
	movq	-56(%rsp), %rdi                 ## 8-byte Reload
	leaq	(%rdi,%r12,4), %rdi
	xorl	%r10d, %r10d
	.p2align	4, 0x90
LBB15_9:                                ## %vector.body63
                                        ##   Parent Loop BB15_6 Depth=1
                                        ## =>  This Inner Loop Header: Depth=2
	vcmpltps	-96(%rsi,%r10,4), %ymm3, %ymm4
	vcmpltps	-64(%rsi,%r10,4), %ymm3, %ymm5
	vcmpltps	-32(%rsi,%r10,4), %ymm3, %ymm6
	vcmpltps	(%rsi,%r10,4), %ymm3, %ymm7
	vmovups	-96(%rcx,%r10,4), %ymm8
	vmovups	-64(%rcx,%r10,4), %ymm9
	vmovups	-32(%rcx,%r10,4), %ymm10
	vmovups	(%rcx,%r10,4), %ymm11
	vmulps	-96(%r15,%r10,4), %ymm8, %ymm8
	vmulps	-64(%r15,%r10,4), %ymm9, %ymm9
	vmulps	-32(%r15,%r10,4), %ymm10, %ymm10
	vmulps	(%r15,%r10,4), %ymm11, %ymm11
	vmulps	%ymm2, %ymm8, %ymm8
	vandps	%ymm4, %ymm8, %ymm4
	vmulps	%ymm2, %ymm9, %ymm8
	vandps	%ymm5, %ymm8, %ymm5
	vmulps	%ymm2, %ymm10, %ymm8
	vandps	%ymm6, %ymm8, %ymm6
	vmulps	%ymm2, %ymm11, %ymm8
	vandps	%ymm7, %ymm8, %ymm7
	vaddps	-96(%rdi,%r10,4), %ymm4, %ymm4
	vaddps	-64(%rdi,%r10,4), %ymm5, %ymm5
	vaddps	-32(%rdi,%r10,4), %ymm6, %ymm6
	vaddps	(%rdi,%r10,4), %ymm7, %ymm7
	vmovups	%ymm4, -96(%rdi,%r10,4)
	vmovups	%ymm5, -64(%rdi,%r10,4)
	vmovups	%ymm6, -32(%rdi,%r10,4)
	vmovups	%ymm7, (%rdi,%r10,4)
	addq	$32, %r10
	cmpq	%r10, %r8
	jne	LBB15_9
## %bb.10:                              ## %middle.block61
                                        ##   in Loop: Header=BB15_6 Depth=1
	movq	%r8, %rcx
	cmpq	%r11, %r8
	je	LBB15_13
LBB15_11:                               ## %"for relu1_0_d_def__.s7.n.ni.us.us.preheader"
                                        ##   in Loop: Header=BB15_6 Depth=1
	movq	104(%rsp), %rsi                 ## 8-byte Reload
	leaq	(%rsi,%rdx,4), %rsi
	movq	96(%rsp), %rdi                  ## 8-byte Reload
	leaq	(%rdi,%rdx,4), %rdx
	movq	-72(%rsp), %rdi                 ## 8-byte Reload
	leaq	(%rdi,%r12,4), %rdi
	.p2align	4, 0x90
LBB15_12:                               ## %"for relu1_0_d_def__.s7.n.ni.us.us"
                                        ##   Parent Loop BB15_6 Depth=1
                                        ## =>  This Inner Loop Header: Depth=2
	vmovss	(%rsi,%rcx,4), %xmm4            ## xmm4 = mem[0],zero,zero,zero
	vmulss	(%rbp,%rcx,4), %xmm4, %xmm4
	vcmpltss	(%rdx,%rcx,4), %xmm1, %xmm5
	vmulss	%xmm0, %xmm4, %xmm4
	vandps	%xmm4, %xmm5, %xmm4
	vaddss	(%rdi,%rcx,4), %xmm4, %xmm4
	vmovss	%xmm4, (%rdi,%rcx,4)
	incq	%rcx
	cmpq	%rcx, %r11
	jne	LBB15_12
LBB15_13:                               ## %"end for relu1_0_d_def__.s7.n.ni.loopexit.us.us"
                                        ##   in Loop: Header=BB15_6 Depth=1
	movl	-88(%rsp), %esi                 ## 4-byte Reload
	testl	%esi, %esi
	movq	-112(%rsp), %r10                ## 8-byte Reload
	movl	-124(%rsp), %r12d               ## 4-byte Reload
	jle	LBB15_17
## %bb.14:                              ## %"for relu1_0_d_def__.s7.n.ni.rebased.preheader.us.us"
                                        ##   in Loop: Header=BB15_6 Depth=1
	movslq	%r10d, %rdi
	imull	%r12d, %eax
	cltq
	movq	40(%rsp), %rcx                  ## 8-byte Reload
	addq	%rax, %rcx
	addq	32(%rsp), %rax                  ## 8-byte Folded Reload
	movq	-32(%rsp), %rdx                 ## 8-byte Reload
	vmovss	-4(%rdx,%rcx,4), %xmm4          ## xmm4 = mem[0],zero,zero,zero
	vmovss	-4(%rdx,%rax,4), %xmm5          ## xmm5 = mem[0],zero,zero,zero
	cmpl	$8, %esi
	jae	LBB15_18
## %bb.15:                              ##   in Loop: Header=BB15_6 Depth=1
	xorl	%eax, %eax
	jmp	LBB15_40
	.p2align	4, 0x90
LBB15_18:                               ## %vector.ph50
                                        ##   in Loop: Header=BB15_6 Depth=1
	vbroadcastss	%xmm5, %ymm6
	cmpq	$24, 72(%rsp)                   ## 8-byte Folded Reload
	jae	LBB15_20
## %bb.19:                              ##   in Loop: Header=BB15_6 Depth=1
	xorl	%eax, %eax
	movq	(%rsp), %rsi                    ## 8-byte Reload
LBB15_33:                               ## %middle.block46.unr-lcssa
                                        ##   in Loop: Header=BB15_6 Depth=1
	cmpq	$0, 64(%rsp)                    ## 8-byte Folded Reload
	movq	-112(%rsp), %r10                ## 8-byte Reload
	movq	-64(%rsp), %rdx                 ## 8-byte Reload
	movl	-124(%rsp), %r12d               ## 4-byte Reload
	je	LBB15_39
## %bb.34:                              ## %vector.body48.epil.preheader
                                        ##   in Loop: Header=BB15_6 Depth=1
	movq	-120(%rsp), %rcx                ## 8-byte Reload
	addq	%rax, %rcx
	leaq	(%rdx,%rcx,4), %rcx
	addq	24(%rsp), %rax                  ## 8-byte Folded Reload
	addq	%rdi, %rax
	movq	-72(%rsp), %rdx                 ## 8-byte Reload
	leaq	(%rdx,%rax,4), %rax
	xorl	%edx, %edx
	jmp	LBB15_35
	.p2align	4, 0x90
LBB15_37:                               ## %vector.body48.epil
                                        ##   in Loop: Header=BB15_35 Depth=2
	vxorps	%xmm7, %xmm7, %xmm7
LBB15_38:                               ## %vector.body48.epil
                                        ##   in Loop: Header=BB15_35 Depth=2
	vaddps	(%rax,%rdx), %ymm7, %ymm7
	vmovups	%ymm7, (%rax,%rdx)
	addq	$32, %rdx
	cmpq	%rdx, %rsi
	je	LBB15_39
LBB15_35:                               ## %vector.body48.epil
                                        ##   Parent Loop BB15_6 Depth=1
                                        ## =>  This Inner Loop Header: Depth=2
	vucomiss	%xmm1, %xmm4
	jbe	LBB15_37
## %bb.36:                              ##   in Loop: Header=BB15_35 Depth=2
	vmulps	(%rcx,%rdx), %ymm6, %ymm7
	vmulps	%ymm2, %ymm7, %ymm7
	jmp	LBB15_38
	.p2align	4, 0x90
LBB15_39:                               ## %middle.block46
                                        ##   in Loop: Header=BB15_6 Depth=1
	movq	80(%rsp), %rcx                  ## 8-byte Reload
	movq	%rcx, %rax
	cmpq	%r13, %rcx
	je	LBB15_17
LBB15_40:                               ## %"for relu1_0_d_def__.s7.n.ni.rebased.us.us.preheader"
                                        ##   in Loop: Header=BB15_6 Depth=1
	movq	88(%rsp), %rcx                  ## 8-byte Reload
	leaq	(%rcx,%rdi,4), %rcx
	.p2align	4, 0x90
LBB15_16:                               ## %"for relu1_0_d_def__.s7.n.ni.rebased.us.us"
                                        ##   Parent Loop BB15_6 Depth=1
                                        ## =>  This Inner Loop Header: Depth=2
	vmulss	(%r9,%rax,4), %xmm5, %xmm6
	vmulss	%xmm0, %xmm6, %xmm6
	vcmpltss	%xmm4, %xmm1, %xmm7
	vandps	%xmm6, %xmm7, %xmm6
	vaddss	(%rcx,%rax,4), %xmm6, %xmm6
	vmovss	%xmm6, (%rcx,%rax,4)
	incq	%rax
	cmpq	%rax, %r13
	jne	LBB15_16
	jmp	LBB15_17
	.p2align	4, 0x90
LBB15_20:                               ## %vector.body48.preheader
                                        ##   in Loop: Header=BB15_6 Depth=1
	movq	24(%rsp), %rax                  ## 8-byte Reload
	addq	%rdi, %rax
	movq	-56(%rsp), %rcx                 ## 8-byte Reload
	leaq	(%rcx,%rax,4), %r12
	movq	48(%rsp), %rdx                  ## 8-byte Reload
	xorl	%eax, %eax
	movq	(%rsp), %rsi                    ## 8-byte Reload
	jmp	LBB15_21
	.p2align	4, 0x90
LBB15_32:                               ## %vector.body48
                                        ##   in Loop: Header=BB15_21 Depth=2
	vaddps	(%r12,%rax,4), %ymm7, %ymm7
	vmovups	%ymm7, (%r12,%rax,4)
	addq	$32, %rax
	addq	$4, %rdx
	je	LBB15_33
LBB15_21:                               ## %vector.body48
                                        ##   Parent Loop BB15_6 Depth=1
                                        ## =>  This Inner Loop Header: Depth=2
	vucomiss	%xmm1, %xmm4
	vxorps	%xmm7, %xmm7, %xmm7
	ja	LBB15_22
## %bb.23:                              ## %vector.body48
                                        ##   in Loop: Header=BB15_21 Depth=2
	vxorps	%xmm8, %xmm8, %xmm8
	vaddps	-96(%r12,%rax,4), %ymm8, %ymm8
	vmovups	%ymm8, -96(%r12,%rax,4)
	jbe	LBB15_26
LBB15_25:                               ##   in Loop: Header=BB15_21 Depth=2
	vmulps	-64(%r14,%rax,4), %ymm6, %ymm8
	vmulps	%ymm2, %ymm8, %ymm8
	vaddps	-64(%r12,%rax,4), %ymm8, %ymm8
	vmovups	%ymm8, -64(%r12,%rax,4)
	jbe	LBB15_29
LBB15_28:                               ##   in Loop: Header=BB15_21 Depth=2
	vmulps	-32(%r14,%rax,4), %ymm6, %ymm8
	vmulps	%ymm2, %ymm8, %ymm8
	vaddps	-32(%r12,%rax,4), %ymm8, %ymm8
	vmovups	%ymm8, -32(%r12,%rax,4)
	jbe	LBB15_32
	jmp	LBB15_31
	.p2align	4, 0x90
LBB15_22:                               ##   in Loop: Header=BB15_21 Depth=2
	vmulps	-96(%r14,%rax,4), %ymm6, %ymm8
	vmulps	%ymm2, %ymm8, %ymm8
	vaddps	-96(%r12,%rax,4), %ymm8, %ymm8
	vmovups	%ymm8, -96(%r12,%rax,4)
	ja	LBB15_25
LBB15_26:                               ## %vector.body48
                                        ##   in Loop: Header=BB15_21 Depth=2
	vxorps	%xmm8, %xmm8, %xmm8
	vaddps	-64(%r12,%rax,4), %ymm8, %ymm8
	vmovups	%ymm8, -64(%r12,%rax,4)
	ja	LBB15_28
LBB15_29:                               ## %vector.body48
                                        ##   in Loop: Header=BB15_21 Depth=2
	vxorps	%xmm8, %xmm8, %xmm8
	vaddps	-32(%r12,%rax,4), %ymm8, %ymm8
	vmovups	%ymm8, -32(%r12,%rax,4)
	jbe	LBB15_32
LBB15_31:                               ##   in Loop: Header=BB15_21 Depth=2
	vmulps	(%r14,%rax,4), %ymm6, %ymm7
	vmulps	%ymm2, %ymm7, %ymm7
	jmp	LBB15_32
LBB15_2:                                ## %"for relu1_0_d_def__.s7.w.wi.preheader20"
	decl	%edx
	vmovd	%r12d, %xmm0
	vpbroadcastd	%xmm0, %ymm0
	vpor	LCPI15_0(%rip), %ymm0, %ymm0
	vmovd	%edx, %xmm1
	vpbroadcastd	%xmm1, %ymm1
	vpminsd	%ymm0, %ymm1, %ymm0
	movslq	-96(%rsp), %rdx                 ## 4-byte Folded Reload
	movl	-104(%rsp), %r11d               ## 4-byte Reload
	imulq	%rdi, %rbx
	addq	%r12, %rbx
	movq	-64(%rsp), %rax                 ## 8-byte Reload
	leaq	(%rax,%rbx,4), %rax
	shlq	$2, %rdi
	shll	$6, %r14d
	shll	$6, %ebp
	subl	%ebp, %r14d
	orl	$25, %r14d
	movq	-112(%rsp), %rcx                ## 8-byte Reload
	imull	%r14d, %ecx
	movq	-80(%rsp), %rsi                 ## 8-byte Reload
	leal	(%rcx,%rsi,8), %ebp
	vbroadcastss	LCPI15_1(%rip), %ymm8   ## ymm8 = [1.0E+0,1.0E+0,1.0E+0,1.0E+0,1.0E+0,1.0E+0,1.0E+0,1.0E+0]
	vbroadcastss	LCPI15_2(%rip), %ymm9   ## ymm9 = [9.99999971E-10,9.99999971E-10,9.99999971E-10,9.99999971E-10,9.99999971E-10,9.99999971E-10,9.99999971E-10,9.99999971E-10]
	movl	-84(%rsp), %r8d                 ## 4-byte Reload
	movl	-124(%rsp), %r10d               ## 4-byte Reload
	movq	%rdi, %r14
	movq	-32(%rsp), %rdi                 ## 8-byte Reload
	.p2align	4, 0x90
LBB15_3:                                ## %"for relu1_0_d_def__.s7.w.wi"
                                        ## =>This Inner Loop Header: Depth=1
	testq	%rdx, %rdx
	movl	$0, %esi
	cmovgl	%edx, %esi
	imull	%r10d, %esi
	leal	(%rsi,%r9), %ecx
	vmovd	%ecx, %xmm3
	vpbroadcastd	%xmm3, %ymm3
	vpaddd	%ymm0, %ymm3, %ymm3
	vmovd	%xmm3, %ecx
	vpextrd	$1, %xmm3, %ebx
	movslq	%ecx, %rcx
	vmovss	(%rdi,%rcx,4), %xmm4            ## xmm4 = mem[0],zero,zero,zero
	vpextrd	$2, %xmm3, %ecx
	movslq	%ebx, %rbx
	movslq	%ecx, %rcx
	vinsertps	$16, (%rdi,%rbx,4), %xmm4, %xmm4 ## xmm4 = xmm4[0],mem[0],xmm4[2,3]
	vpextrd	$3, %xmm3, %ebx
	vextracti128	$1, %ymm3, %xmm3
	movslq	%ebx, %rbx
	vinsertps	$32, (%rdi,%rcx,4), %xmm4, %xmm4 ## xmm4 = xmm4[0,1],mem[0],xmm4[3]
	vinsertps	$48, (%rdi,%rbx,4), %xmm4, %xmm4 ## xmm4 = xmm4[0,1,2],mem[0]
	vmovd	%xmm3, %ecx
	vpextrd	$1, %xmm3, %ebx
	movslq	%ecx, %rcx
	vmovss	(%rdi,%rcx,4), %xmm5            ## xmm5 = mem[0],zero,zero,zero
	movslq	%ebx, %rcx
	vinsertps	$16, (%rdi,%rcx,4), %xmm5, %xmm5 ## xmm5 = xmm5[0],mem[0],xmm5[2,3]
	addl	%r15d, %esi
	vmovd	%esi, %xmm6
	vpextrd	$2, %xmm3, %ecx
	movslq	%ecx, %rcx
	vpbroadcastd	%xmm6, %ymm6
	vinsertps	$32, (%rdi,%rcx,4), %xmm5, %xmm5 ## xmm5 = xmm5[0,1],mem[0],xmm5[3]
	vpaddd	%ymm0, %ymm6, %ymm6
	vextracti128	$1, %ymm6, %xmm7
	vmovd	%xmm7, %ecx
	movslq	%ecx, %rcx
	vmovss	(%rdi,%rcx,4), %xmm1            ## xmm1 = mem[0],zero,zero,zero
	vpextrd	$1, %xmm7, %ecx
	movslq	%ecx, %rcx
	vinsertps	$16, (%rdi,%rcx,4), %xmm1, %xmm1 ## xmm1 = xmm1[0],mem[0],xmm1[2,3]
	vmovd	%xmm6, %ecx
	movslq	%ecx, %rcx
	vmovss	(%rdi,%rcx,4), %xmm2            ## xmm2 = mem[0],zero,zero,zero
	vpextrd	$1, %xmm6, %ecx
	movslq	%ecx, %rcx
	vinsertps	$16, (%rdi,%rcx,4), %xmm2, %xmm2 ## xmm2 = xmm2[0],mem[0],xmm2[2,3]
	vpextrd	$2, %xmm7, %ecx
	movslq	%ecx, %rcx
	vinsertps	$32, (%rdi,%rcx,4), %xmm1, %xmm1 ## xmm1 = xmm1[0,1],mem[0],xmm1[3]
	vpextrd	$2, %xmm6, %ecx
	movslq	%ecx, %rcx
	vinsertps	$32, (%rdi,%rcx,4), %xmm2, %xmm2 ## xmm2 = xmm2[0,1],mem[0],xmm2[3]
	vpextrd	$3, %xmm7, %ecx
	movslq	%ecx, %rcx
	vinsertps	$48, (%rdi,%rcx,4), %xmm1, %xmm1 ## xmm1 = xmm1[0,1,2],mem[0]
	vpextrd	$3, %xmm6, %ecx
	movslq	%ecx, %rcx
	vinsertps	$48, (%rdi,%rcx,4), %xmm2, %xmm2 ## xmm2 = xmm2[0,1,2],mem[0]
	vpextrd	$3, %xmm3, %ecx
	movslq	%ecx, %rcx
	vinsertps	$48, (%rdi,%rcx,4), %xmm5, %xmm3 ## xmm3 = xmm5[0,1,2],mem[0]
	vinsertf128	$1, %xmm3, %ymm4, %ymm3
	movslq	%ebp, %rbp
	vinsertf128	$1, %xmm1, %ymm2, %ymm1
	vcmpltps	%ymm3, %ymm8, %ymm2
	vmulps	(%rax), %ymm1, %ymm1
	vmulps	%ymm1, %ymm9, %ymm1
	vandps	%ymm1, %ymm2, %ymm1
	movq	-72(%rsp), %rcx                 ## 8-byte Reload
	vaddps	(%rcx,%rbp,4), %ymm1, %ymm1
	vmovups	%ymm1, (%rcx,%rbp,4)
	addq	%r14, %rax
	incq	%rdx
	addl	%r8d, %ebp
	decq	%r11
	jne	LBB15_3
LBB15_70:                               ## %destructor_block
	xorl	%eax, %eax
	addq	$112, %rsp
	popq	%rbx
	popq	%r12
	popq	%r13
	popq	%r14
	popq	%r15
	popq	%rbp
	vzeroupper
	retq
LBB15_41:                               ## %"for relu1_0_d_def__.s7.w.wi.preheader.split.us.split"
	testl	%r10d, %r10d
	movl	-124(%rsp), %ecx                ## 4-byte Reload
	movq	-8(%rsp), %r9                   ## 8-byte Reload
	jle	LBB15_70
## %bb.42:                              ## %"for relu1_0_d_def__.s7.w.wi.us.us4.preheader"
	movslq	%eax, %rdx
	movq	%rdx, -120(%rsp)                ## 8-byte Spill
	movl	-88(%rsp), %edi                 ## 4-byte Reload
	movslq	-96(%rsp), %rax                 ## 4-byte Folded Reload
	movq	%rax, -80(%rsp)                 ## 8-byte Spill
	movl	-104(%rsp), %eax                ## 4-byte Reload
	movq	%rax, -96(%rsp)                 ## 8-byte Spill
	movl	%edi, %eax
	andl	$-8, %eax
	movq	%rax, -104(%rsp)                ## 8-byte Spill
	addq	$-8, %rax
	movq	%rax, -40(%rsp)                 ## 8-byte Spill
	shrq	$3, %rax
	incq	%rax
	movl	%eax, %ebp
	andl	$3, %ebp
	imulq	%r9, %rsi
	addq	%rdx, %rsi
	movq	-64(%rsp), %r10                 ## 8-byte Reload
	leaq	(%r10,%rsi,4), %r15
	addq	$96, %r15
	leaq	(,%r9,4), %r11
	movq	-72(%rsp), %r8                  ## 8-byte Reload
	leaq	96(%r8), %rdx
	movq	%rdx, -56(%rsp)                 ## 8-byte Spill
	shll	$6, %r14d
	movl	-48(%rsp), %edx                 ## 4-byte Reload
	shll	$6, %edx
	subl	%edx, %r14d
	orl	$25, %r14d
	imull	%r14d, %ebx
	andq	$-4, %rax
	negq	%rax
	movq	%rax, -24(%rsp)                 ## 8-byte Spill
	movq	%rbp, -16(%rsp)                 ## 8-byte Spill
	shlq	$5, %rbp
	movq	%rsi, %r12
	leaq	(%r10,%rsi,4), %r13
	movq	-120(%rsp), %rax                ## 8-byte Reload
	leaq	(%r8,%rax,4), %rax
	movq	%rax, -112(%rsp)                ## 8-byte Spill
	vmovss	LCPI15_2(%rip), %xmm0           ## xmm0 = mem[0],zero,zero,zero
	vmovss	LCPI15_1(%rip), %xmm1           ## xmm1 = mem[0],zero,zero,zero
	vbroadcastss	LCPI15_2(%rip), %ymm2   ## ymm2 = [9.99999971E-10,9.99999971E-10,9.99999971E-10,9.99999971E-10,9.99999971E-10,9.99999971E-10,9.99999971E-10,9.99999971E-10]
	xorl	%r10d, %r10d
	jmp	LBB15_43
LBB15_69:                               ## %after_bb.loopexit.us.us17
                                        ##   in Loop: Header=BB15_43 Depth=1
	incq	%r10
	addq	%r11, %r15
	addl	-84(%rsp), %ebx                 ## 4-byte Folded Reload
	addq	%r9, %r12
	addq	%r11, %r13
	cmpq	-96(%rsp), %r10                 ## 8-byte Folded Reload
	movl	-124(%rsp), %ecx                ## 4-byte Reload
	je	LBB15_70
LBB15_43:                               ## %"for relu1_0_d_def__.s7.w.wi.us.us4"
                                        ## =>This Loop Header: Depth=1
                                        ##     Child Loop BB15_48 Depth 2
                                        ##     Child Loop BB15_62 Depth 2
                                        ##     Child Loop BB15_68 Depth 2
	movq	-80(%rsp), %rax                 ## 8-byte Reload
	addq	%r10, %rax
	testq	%rax, %rax
	movl	$0, %edx
	cmovlel	%edx, %eax
	movslq	%ebx, %rdx
	imull	%ecx, %eax
	cltq
	movq	40(%rsp), %rcx                  ## 8-byte Reload
	addq	%rax, %rcx
	addq	32(%rsp), %rax                  ## 8-byte Folded Reload
	movq	-32(%rsp), %rsi                 ## 8-byte Reload
	vmovss	-4(%rsi,%rcx,4), %xmm3          ## xmm3 = mem[0],zero,zero,zero
	vmovss	-4(%rsi,%rax,4), %xmm4          ## xmm4 = mem[0],zero,zero,zero
	cmpl	$8, -88(%rsp)                   ## 4-byte Folded Reload
	jae	LBB15_45
## %bb.44:                              ##   in Loop: Header=BB15_43 Depth=1
	xorl	%eax, %eax
	jmp	LBB15_67
LBB15_45:                               ## %vector.ph
                                        ##   in Loop: Header=BB15_43 Depth=1
	vbroadcastss	%xmm4, %ymm5
	cmpq	$24, -40(%rsp)                  ## 8-byte Folded Reload
	jae	LBB15_47
## %bb.46:                              ##   in Loop: Header=BB15_43 Depth=1
	xorl	%eax, %eax
LBB15_60:                               ## %middle.block.unr-lcssa
                                        ##   in Loop: Header=BB15_43 Depth=1
	cmpq	$0, -16(%rsp)                   ## 8-byte Folded Reload
	je	LBB15_66
## %bb.61:                              ## %vector.body.epil.preheader
                                        ##   in Loop: Header=BB15_43 Depth=1
	leaq	(%rax,%r12), %rcx
	movq	-64(%rsp), %rsi                 ## 8-byte Reload
	leaq	(%rsi,%rcx,4), %rcx
	addq	-120(%rsp), %rax                ## 8-byte Folded Reload
	addq	%rdx, %rax
	movq	-72(%rsp), %rsi                 ## 8-byte Reload
	leaq	(%rsi,%rax,4), %rax
	xorl	%esi, %esi
	jmp	LBB15_62
LBB15_64:                               ## %vector.body.epil
                                        ##   in Loop: Header=BB15_62 Depth=2
	vxorps	%xmm6, %xmm6, %xmm6
LBB15_65:                               ## %vector.body.epil
                                        ##   in Loop: Header=BB15_62 Depth=2
	vaddps	(%rax,%rsi), %ymm6, %ymm6
	vmovups	%ymm6, (%rax,%rsi)
	addq	$32, %rsi
	cmpq	%rsi, %rbp
	je	LBB15_66
LBB15_62:                               ## %vector.body.epil
                                        ##   Parent Loop BB15_43 Depth=1
                                        ## =>  This Inner Loop Header: Depth=2
	vucomiss	%xmm1, %xmm3
	jbe	LBB15_64
## %bb.63:                              ##   in Loop: Header=BB15_62 Depth=2
	vmulps	(%rcx,%rsi), %ymm5, %ymm6
	vmulps	%ymm2, %ymm6, %ymm6
	jmp	LBB15_65
LBB15_66:                               ## %middle.block
                                        ##   in Loop: Header=BB15_43 Depth=1
	movq	-104(%rsp), %rcx                ## 8-byte Reload
	movq	%rcx, %rax
	cmpq	%rdi, %rcx
	je	LBB15_69
LBB15_67:                               ## %"for relu1_0_d_def__.s7.n.ni.rebased.us.us9.preheader"
                                        ##   in Loop: Header=BB15_43 Depth=1
	movq	-112(%rsp), %rcx                ## 8-byte Reload
	leaq	(%rcx,%rdx,4), %rcx
LBB15_68:                               ## %"for relu1_0_d_def__.s7.n.ni.rebased.us.us9"
                                        ##   Parent Loop BB15_43 Depth=1
                                        ## =>  This Inner Loop Header: Depth=2
	vmulss	(%r13,%rax,4), %xmm4, %xmm5
	vmulss	%xmm0, %xmm5, %xmm5
	vcmpltss	%xmm3, %xmm1, %xmm6
	vandps	%xmm5, %xmm6, %xmm5
	vaddss	(%rcx,%rax,4), %xmm5, %xmm5
	vmovss	%xmm5, (%rcx,%rax,4)
	incq	%rax
	cmpq	%rax, %rdi
	jne	LBB15_68
	jmp	LBB15_69
LBB15_47:                               ## %vector.body.preheader
                                        ##   in Loop: Header=BB15_43 Depth=1
	movq	-120(%rsp), %rax                ## 8-byte Reload
	addq	%rdx, %rax
	movq	-56(%rsp), %rcx                 ## 8-byte Reload
	leaq	(%rcx,%rax,4), %r8
	movq	-24(%rsp), %r14                 ## 8-byte Reload
	xorl	%eax, %eax
	jmp	LBB15_48
LBB15_59:                               ## %vector.body
                                        ##   in Loop: Header=BB15_48 Depth=2
	vaddps	(%r8,%rax,4), %ymm6, %ymm6
	vmovups	%ymm6, (%r8,%rax,4)
	addq	$32, %rax
	addq	$4, %r14
	je	LBB15_60
LBB15_48:                               ## %vector.body
                                        ##   Parent Loop BB15_43 Depth=1
                                        ## =>  This Inner Loop Header: Depth=2
	vucomiss	%xmm1, %xmm3
	vxorps	%xmm6, %xmm6, %xmm6
	ja	LBB15_49
## %bb.50:                              ## %vector.body
                                        ##   in Loop: Header=BB15_48 Depth=2
	vxorps	%xmm7, %xmm7, %xmm7
	jmp	LBB15_51
LBB15_49:                               ##   in Loop: Header=BB15_48 Depth=2
	vmulps	-96(%r15,%rax,4), %ymm5, %ymm7
	vmulps	%ymm2, %ymm7, %ymm7
LBB15_51:                               ## %vector.body
                                        ##   in Loop: Header=BB15_48 Depth=2
	vaddps	-96(%r8,%rax,4), %ymm7, %ymm7
	vmovups	%ymm7, -96(%r8,%rax,4)
	ja	LBB15_52
## %bb.53:                              ## %vector.body
                                        ##   in Loop: Header=BB15_48 Depth=2
	vxorps	%xmm7, %xmm7, %xmm7
	jmp	LBB15_54
LBB15_52:                               ##   in Loop: Header=BB15_48 Depth=2
	vmulps	-64(%r15,%rax,4), %ymm5, %ymm7
	vmulps	%ymm2, %ymm7, %ymm7
LBB15_54:                               ## %vector.body
                                        ##   in Loop: Header=BB15_48 Depth=2
	vaddps	-64(%r8,%rax,4), %ymm7, %ymm7
	vmovups	%ymm7, -64(%r8,%rax,4)
	ja	LBB15_55
## %bb.56:                              ## %vector.body
                                        ##   in Loop: Header=BB15_48 Depth=2
	vxorps	%xmm7, %xmm7, %xmm7
	jmp	LBB15_57
LBB15_55:                               ##   in Loop: Header=BB15_48 Depth=2
	vmulps	-32(%r15,%rax,4), %ymm5, %ymm7
	vmulps	%ymm2, %ymm7, %ymm7
LBB15_57:                               ## %vector.body
                                        ##   in Loop: Header=BB15_48 Depth=2
	vaddps	-32(%r8,%rax,4), %ymm7, %ymm7
	vmovups	%ymm7, -32(%r8,%rax,4)
	jbe	LBB15_59
## %bb.58:                              ##   in Loop: Header=BB15_48 Depth=2
	vmulps	(%r15,%rax,4), %ymm5, %ymm6
	vmulps	%ymm2, %ymm6, %ymm6
	jmp	LBB15_59
                                        ## -- End function
	.section	__TEXT,__const
	.p2align	5                               ## -- Begin function train_cost_model.par_for.relu1_0_d_def__.s8.n.n.n
LCPI16_0:
	.long	0                               ## 0x0
	.long	1                               ## 0x1
	.long	2                               ## 0x2
	.long	3                               ## 0x3
	.long	4                               ## 0x4
	.long	5                               ## 0x5
	.long	6                               ## 0x6
	.long	7                               ## 0x7
	.section	__TEXT,__literal4,4byte_literals
	.p2align	2
LCPI16_1:
	.long	0xbf800000                      ## float -1
LCPI16_2:
	.long	0x3089705f                      ## float 9.99999971E-10
	.section	__TEXT,__text,regular,pure_instructions
	.p2align	4, 0x90
_train_cost_model.par_for.relu1_0_d_def__.s8.n.n.n: ## @train_cost_model.par_for.relu1_0_d_def__.s8.n.n.n
## %bb.0:                               ## %entry
	pushq	%rbp
	pushq	%r15
	pushq	%r14
	pushq	%r13
	pushq	%r12
	pushq	%rbx
	subq	$120, %rsp
	movq	%rdx, %rdi
	movl	12(%rdx), %r10d
	movl	28(%rdx), %r12d
	movl	%esi, %r9d
	sarl	$31, %r9d
	xorl	%ecx, %ecx
	testl	%r12d, %r12d
	sete	%cl
	movl	%ecx, %ebx
	negl	%ebx
	movl	%r12d, %ebp
	sarl	$31, %ebp
	subl	%r9d, %esi
	orl	%r12d, %ebx
	movl	%esi, %eax
	cltd
	idivl	%ebx
	movl	%edx, %ebx
	movl	%ebp, %r13d
	leal	(%r12,%rcx), %r8d
	movl	%esi, %eax
	cltd
	idivl	%r8d
	notl	%r13d
	decl	%ecx
	movl	%r13d, %r8d
	subl	%ebp, %r8d
	andl	%r9d, %r8d
	addl	%eax, %r8d
	andl	%ecx, %r8d
	leal	(%r8,%r8), %eax
	movl	%eax, -120(%rsp)                ## 4-byte Spill
	subl	%eax, %r10d
	cmpl	$2, %r10d
	movl	$2, %edx
	cmovll	%r10d, %edx
	xorl	%eax, %eax
	testl	%r10d, %r10d
	cmovlel	%eax, %edx
	movl	%edx, -104(%rsp)                ## 4-byte Spill
	jle	LBB16_40
## %bb.1:                               ## %"for relu1_0_d_def__.s8.w.wi.preheader"
	movl	(%rdi), %eax
	movl	16(%rdi), %edx
	movl	20(%rdi), %r10d
	movl	36(%rdi), %esi
	movl	40(%rdi), %r14d
	xorl	%r12d, %ebp
	addl	%r13d, %ebp
	andl	%r9d, %ebp
	addl	%ebp, %ebx
	andl	%ecx, %ebx
	movq	%rbx, -80(%rsp)                 ## 8-byte Spill
	leal	(,%rbx,8), %r11d
	movl	%r10d, %r9d
	subl	%esi, %r9d
	movl	%r14d, -88(%rsp)                ## 4-byte Spill
	movl	%esi, -124(%rsp)                ## 4-byte Spill
	subl	%esi, %r14d
	movslq	8(%rdi), %rcx
	movslq	%r8d, %rbp
	subq	%rcx, %rbp
	addq	%rbp, %rbp
	movq	%rdx, -96(%rsp)                 ## 8-byte Spill
	movl	%edx, %ecx
	shll	$5, %ecx
	movl	%ecx, -108(%rsp)                ## 4-byte Spill
	movl	%eax, %edx
	subl	%r11d, %edx
	cmpl	$8, %edx
	movl	$8, %ecx
	cmovll	%edx, %ecx
	testl	%edx, %edx
	movl	$0, %r12d
	cmovgl	%ecx, %r12d
	movl	24(%rdi), %ecx
	movl	%ecx, -128(%rsp)                ## 4-byte Spill
	movl	%r11d, %ecx
	subl	%eax, %ecx
	movl	%ecx, %esi
	sarl	$31, %esi
	andl	%ecx, %esi
	cmpl	$-8, %esi
	movl	$-8, %r15d
	cmovgl	%esi, %r15d
	movl	32(%rdi), %esi
	movq	48(%rdi), %r13
	movq	64(%rdi), %rcx
	movq	%rcx, -72(%rsp)                 ## 8-byte Spill
	movq	80(%rdi), %rcx
	movq	%rcx, -40(%rsp)                 ## 8-byte Spill
	movslq	4(%rdi), %rbx
	leal	8(%r11), %ecx
	movslq	%r11d, %rdi
	cmpl	%eax, %ecx
	jle	LBB16_2
## %bb.4:                               ## %"for relu1_0_d_def__.s8.w.wi.preheader.split.us"
	movq	%rdi, -24(%rsp)                 ## 8-byte Spill
	movq	%rbx, -48(%rsp)                 ## 8-byte Spill
	movq	%r13, -56(%rsp)                 ## 8-byte Spill
	movq	-96(%rsp), %rdi                 ## 8-byte Reload
	leal	(%rdi,%rdi,4), %ecx
	leal	(%rcx,%rcx,4), %ecx
	addl	%edi, %ecx
	movl	%ecx, -60(%rsp)                 ## 4-byte Spill
	movl	%r8d, %ecx
	subl	%esi, %ecx
	addl	%ecx, %ecx
	movq	%rcx, -8(%rsp)                  ## 8-byte Spill
	addl	%eax, %r10d
	movl	-124(%rsp), %ecx                ## 4-byte Reload
	subl	%ecx, %r10d
	subl	%ecx, %eax
	addl	-88(%rsp), %eax                 ## 4-byte Folded Reload
	addl	%r12d, %r11d
	addl	%r12d, %r15d
	movslq	%r10d, %rcx
	movq	%rcx, 40(%rsp)                  ## 8-byte Spill
	cltq
	movq	%rax, 32(%rsp)                  ## 8-byte Spill
	testl	%edx, %edx
	movl	%r15d, -124(%rsp)               ## 4-byte Spill
	movq	%rbp, -32(%rsp)                 ## 8-byte Spill
	jle	LBB16_26
## %bb.5:                               ## %"for relu1_0_d_def__.s8.w.wi.us.us.preheader"
	movq	-24(%rsp), %rbx                 ## 8-byte Reload
	addl	%ebx, %r14d
	addl	%ebx, %r9d
	movslq	%r9d, %rax
	movq	%rax, 8(%rsp)                   ## 8-byte Spill
	movslq	%r14d, %rax
	movq	%rax, (%rsp)                    ## 8-byte Spill
	movl	%r12d, -12(%rsp)                ## 4-byte Spill
	movl	%r12d, %r10d
	movslq	%r11d, %r12
	movl	%r15d, %r14d
	movslq	-120(%rsp), %rax                ## 4-byte Folded Reload
	movq	%rax, -88(%rsp)                 ## 8-byte Spill
	movl	-104(%rsp), %eax                ## 4-byte Reload
	movq	%rax, 112(%rsp)                 ## 8-byte Spill
	movl	%r14d, %eax
	andl	$-16, %eax
	movq	%rax, 80(%rsp)                  ## 8-byte Spill
	addq	$-16, %rax
	movq	%rax, 72(%rsp)                  ## 8-byte Spill
	movq	%rax, %r15
	shrq	$4, %r15
	incq	%r15
	movl	%r10d, %r11d
	andl	$2147483616, %r11d              ## imm = 0x7FFFFFE0
	shll	$6, %r8d
	shll	$6, %esi
	subl	%esi, %r8d
	movq	-48(%rsp), %r9                  ## 8-byte Reload
	movq	%r9, %rax
	imulq	%rbp, %rax
	addq	%rax, %rbx
	orl	$26, %r8d
	imull	%r8d, %edi
	movq	-56(%rsp), %rcx                 ## 8-byte Reload
	movq	%rdi, %rdx
	leaq	(%rcx,%rbx,4), %rdi
	addq	$96, %rdi
	movq	%rdx, -96(%rsp)                 ## 8-byte Spill
	movq	-80(%rsp), %rsi                 ## 8-byte Reload
	leal	(%rdx,%rsi,8), %r13d
	leaq	(%rcx,%rbx,4), %rbp
	addq	%r12, %rax
	movq	%r15, %rsi
	movq	%r15, 64(%rsp)                  ## 8-byte Spill
	andq	$-2, %r15
	negq	%r15
	movq	%r15, 48(%rsp)                  ## 8-byte Spill
	leaq	(%rcx,%rax,4), %r8
	addq	$96, %r8
	leaq	(%rcx,%rax,4), %r15
	vmovss	LCPI16_1(%rip), %xmm0           ## xmm0 = mem[0],zero,zero,zero
	vmovss	LCPI16_2(%rip), %xmm1           ## xmm1 = mem[0],zero,zero,zero
	vbroadcastss	LCPI16_2(%rip), %ymm2   ## ymm2 = [9.99999971E-10,9.99999971E-10,9.99999971E-10,9.99999971E-10,9.99999971E-10,9.99999971E-10,9.99999971E-10,9.99999971E-10]
	vbroadcastss	LCPI16_1(%rip), %ymm3   ## ymm3 = [-1.0E+0,-1.0E+0,-1.0E+0,-1.0E+0,-1.0E+0,-1.0E+0,-1.0E+0,-1.0E+0]
	movq	-40(%rsp), %rcx                 ## 8-byte Reload
	leaq	96(%rcx), %rax
	movq	%rax, 56(%rsp)                  ## 8-byte Spill
	leaq	(,%r9,4), %rax
	movq	%rax, -24(%rsp)                 ## 8-byte Spill
	movq	-72(%rsp), %rax                 ## 8-byte Reload
	leaq	96(%rax), %rdx
	movq	%rdx, 24(%rsp)                  ## 8-byte Spill
	movq	(%rsp), %rdx                    ## 8-byte Reload
	leaq	(%rcx,%rdx,4), %rdx
	movq	%rdx, 104(%rsp)                 ## 8-byte Spill
	movq	8(%rsp), %rdx                   ## 8-byte Reload
	leaq	(%rcx,%rdx,4), %rcx
	movq	%rcx, 96(%rsp)                  ## 8-byte Spill
	movl	%r13d, %ecx
	movq	%r12, 16(%rsp)                  ## 8-byte Spill
	leaq	(%rax,%r12,4), %rax
	movq	%rax, 88(%rsp)                  ## 8-byte Spill
	movq	-88(%rsp), %rdx                 ## 8-byte Reload
	xorl	%ebx, %ebx
	movl	-128(%rsp), %esi                ## 4-byte Reload
	jmp	LBB16_6
	.p2align	4, 0x90
LBB16_17:                               ## %after_bb.us.us
                                        ##   in Loop: Header=BB16_6 Depth=1
	movq	-120(%rsp), %rbx                ## 8-byte Reload
	incq	%rbx
	movq	-80(%rsp), %rdx                 ## 8-byte Reload
	incq	%rdx
	movq	-24(%rsp), %rax                 ## 8-byte Reload
	addq	%rax, %rdi
	movl	-104(%rsp), %ecx                ## 4-byte Reload
	addl	%r12d, %ecx
	addq	%rax, %rbp
	addq	%rax, %r8
	addl	%r12d, %r13d
	movq	%r13, -96(%rsp)                 ## 8-byte Spill
	addq	%rax, %r15
	cmpq	112(%rsp), %rbx                 ## 8-byte Folded Reload
	movl	-128(%rsp), %esi                ## 4-byte Reload
	je	LBB16_40
LBB16_6:                                ## %"for relu1_0_d_def__.s8.w.wi.us.us"
                                        ## =>This Loop Header: Depth=1
                                        ##     Child Loop BB16_9 Depth 2
                                        ##     Child Loop BB16_12 Depth 2
                                        ##     Child Loop BB16_21 Depth 2
                                        ##     Child Loop BB16_16 Depth 2
	testq	%rdx, %rdx
	movl	$0, %eax
	movq	%rdx, -80(%rsp)                 ## 8-byte Spill
	cmovgl	%edx, %eax
	imull	%esi, %eax
	movslq	%eax, %rsi
	movq	-88(%rsp), %rax                 ## 8-byte Reload
	movq	%rbx, -120(%rsp)                ## 8-byte Spill
	leaq	(%rbx,%rax), %r9
	testq	%r9, %r9
	movl	$0, %edx
	cmovlel	%edx, %r9d
	movl	%ecx, -104(%rsp)                ## 4-byte Spill
	movslq	%ecx, %rax
	cmpl	$32, -12(%rsp)                  ## 4-byte Folded Reload
	jae	LBB16_8
## %bb.7:                               ##   in Loop: Header=BB16_6 Depth=1
	xorl	%ecx, %ecx
	jmp	LBB16_11
	.p2align	4, 0x90
LBB16_8:                                ## %vector.body71.preheader
                                        ##   in Loop: Header=BB16_6 Depth=1
	movq	(%rsp), %rcx                    ## 8-byte Reload
	addq	%rsi, %rcx
	movq	8(%rsp), %rdx                   ## 8-byte Reload
	addq	%rsi, %rdx
	movq	56(%rsp), %rbx                  ## 8-byte Reload
	leaq	(%rbx,%rcx,4), %r12
	leaq	(%rbx,%rdx,4), %r13
	movq	24(%rsp), %rcx                  ## 8-byte Reload
	movq	%rax, %rbx
	leaq	(%rcx,%rax,4), %rcx
	xorl	%edx, %edx
	.p2align	4, 0x90
LBB16_9:                                ## %vector.body71
                                        ##   Parent Loop BB16_6 Depth=1
                                        ## =>  This Inner Loop Header: Depth=2
	vaddps	-96(%r12,%rdx,4), %ymm3, %ymm4
	vaddps	-64(%r12,%rdx,4), %ymm3, %ymm5
	vaddps	-32(%r12,%rdx,4), %ymm3, %ymm6
	vaddps	(%r12,%rdx,4), %ymm3, %ymm7
	vmulps	-96(%r13,%rdx,4), %ymm4, %ymm4
	vmulps	-64(%r13,%rdx,4), %ymm5, %ymm5
	vmulps	-32(%r13,%rdx,4), %ymm6, %ymm6
	vmulps	(%r13,%rdx,4), %ymm7, %ymm7
	vmulps	-96(%rdi,%rdx,4), %ymm4, %ymm4
	vmulps	-64(%rdi,%rdx,4), %ymm5, %ymm5
	vmulps	-32(%rdi,%rdx,4), %ymm6, %ymm6
	vmulps	(%rdi,%rdx,4), %ymm7, %ymm7
	vfmadd213ps	-96(%rcx,%rdx,4), %ymm2, %ymm4 ## ymm4 = (ymm2 * ymm4) + mem
	vfmadd213ps	-64(%rcx,%rdx,4), %ymm2, %ymm5 ## ymm5 = (ymm2 * ymm5) + mem
	vfmadd213ps	-32(%rcx,%rdx,4), %ymm2, %ymm6 ## ymm6 = (ymm2 * ymm6) + mem
	vfmadd213ps	(%rcx,%rdx,4), %ymm2, %ymm7 ## ymm7 = (ymm2 * ymm7) + mem
	vmovups	%ymm4, -96(%rcx,%rdx,4)
	vmovups	%ymm5, -64(%rcx,%rdx,4)
	vmovups	%ymm6, -32(%rcx,%rdx,4)
	vmovups	%ymm7, (%rcx,%rdx,4)
	addq	$32, %rdx
	cmpq	%rdx, %r11
	jne	LBB16_9
## %bb.10:                              ## %middle.block69
                                        ##   in Loop: Header=BB16_6 Depth=1
	movq	%r11, %rcx
	cmpq	%r10, %r11
	movq	%rbx, %rax
	je	LBB16_13
LBB16_11:                               ## %"for relu1_0_d_def__.s8.n.ni.us.us.preheader"
                                        ##   in Loop: Header=BB16_6 Depth=1
	movq	104(%rsp), %rdx                 ## 8-byte Reload
	leaq	(%rdx,%rsi,4), %rdx
	movq	96(%rsp), %rbx                  ## 8-byte Reload
	leaq	(%rbx,%rsi,4), %rsi
	movq	-72(%rsp), %rbx                 ## 8-byte Reload
	leaq	(%rbx,%rax,4), %rbx
	.p2align	4, 0x90
LBB16_12:                               ## %"for relu1_0_d_def__.s8.n.ni.us.us"
                                        ##   Parent Loop BB16_6 Depth=1
                                        ## =>  This Inner Loop Header: Depth=2
	vaddss	(%rdx,%rcx,4), %xmm0, %xmm4
	vmulss	(%rsi,%rcx,4), %xmm4, %xmm4
	vmulss	(%rbp,%rcx,4), %xmm4, %xmm4
	vfmadd213ss	(%rbx,%rcx,4), %xmm1, %xmm4 ## xmm4 = (xmm1 * xmm4) + mem
	vmovss	%xmm4, (%rbx,%rcx,4)
	incq	%rcx
	cmpq	%rcx, %r10
	jne	LBB16_12
LBB16_13:                               ## %"end for relu1_0_d_def__.s8.n.ni.loopexit.us.us"
                                        ##   in Loop: Header=BB16_6 Depth=1
	movl	-124(%rsp), %esi                ## 4-byte Reload
	testl	%esi, %esi
	movl	-108(%rsp), %r12d               ## 4-byte Reload
	movq	-96(%rsp), %r13                 ## 8-byte Reload
	jle	LBB16_17
## %bb.14:                              ## %"for relu1_0_d_def__.s8.n.ni.rebased.preheader.us.us"
                                        ##   in Loop: Header=BB16_6 Depth=1
	imull	-128(%rsp), %r9d                ## 4-byte Folded Reload
	movslq	%r9d, %rax
	movq	40(%rsp), %rcx                  ## 8-byte Reload
	addq	%rax, %rcx
	addq	32(%rsp), %rax                  ## 8-byte Folded Reload
	movq	-40(%rsp), %rdx                 ## 8-byte Reload
	vaddss	-4(%rdx,%rax,4), %xmm0, %xmm4
	movslq	%r13d, %rax
	vmulss	-4(%rdx,%rcx,4), %xmm4, %xmm4
	cmpl	$16, %esi
	jae	LBB16_18
## %bb.15:                              ##   in Loop: Header=BB16_6 Depth=1
	xorl	%ecx, %ecx
	jmp	LBB16_25
	.p2align	4, 0x90
LBB16_18:                               ## %vector.ph54
                                        ##   in Loop: Header=BB16_6 Depth=1
	vbroadcastss	%xmm4, %ymm5
	cmpq	$0, 72(%rsp)                    ## 8-byte Folded Reload
	je	LBB16_19
## %bb.20:                              ## %vector.body52.preheader
                                        ##   in Loop: Header=BB16_6 Depth=1
	movq	16(%rsp), %rcx                  ## 8-byte Reload
	addq	%rax, %rcx
	movq	24(%rsp), %rdx                  ## 8-byte Reload
	leaq	(%rdx,%rcx,4), %rcx
	movq	48(%rsp), %rdx                  ## 8-byte Reload
	xorl	%esi, %esi
	movq	-120(%rsp), %r9                 ## 8-byte Reload
	.p2align	4, 0x90
LBB16_21:                               ## %vector.body52
                                        ##   Parent Loop BB16_6 Depth=1
                                        ## =>  This Inner Loop Header: Depth=2
	vmulps	-96(%r8,%rsi,4), %ymm5, %ymm6
	vmulps	-64(%r8,%rsi,4), %ymm5, %ymm7
	vfmadd213ps	-96(%rcx,%rsi,4), %ymm2, %ymm6 ## ymm6 = (ymm2 * ymm6) + mem
	vfmadd213ps	-64(%rcx,%rsi,4), %ymm2, %ymm7 ## ymm7 = (ymm2 * ymm7) + mem
	vmovups	%ymm6, -96(%rcx,%rsi,4)
	vmovups	%ymm7, -64(%rcx,%rsi,4)
	vmulps	-32(%r8,%rsi,4), %ymm5, %ymm6
	vmulps	(%r8,%rsi,4), %ymm5, %ymm7
	vfmadd213ps	-32(%rcx,%rsi,4), %ymm2, %ymm6 ## ymm6 = (ymm2 * ymm6) + mem
	vfmadd213ps	(%rcx,%rsi,4), %ymm2, %ymm7 ## ymm7 = (ymm2 * ymm7) + mem
	vmovups	%ymm6, -32(%rcx,%rsi,4)
	vmovups	%ymm7, (%rcx,%rsi,4)
	addq	$32, %rsi
	addq	$2, %rdx
	jne	LBB16_21
## %bb.22:                              ## %middle.block50.unr-lcssa
                                        ##   in Loop: Header=BB16_6 Depth=1
	testb	$1, 64(%rsp)                    ## 1-byte Folded Reload
	je	LBB16_24
LBB16_23:                               ## %vector.body52.epil
                                        ##   in Loop: Header=BB16_6 Depth=1
	movq	-8(%rsp), %rcx                  ## 8-byte Reload
	addl	%r9d, %ecx
	movq	-32(%rsp), %rdx                 ## 8-byte Reload
	addq	%r9, %rdx
	imull	%r12d, %ecx
	imulq	-48(%rsp), %rdx                 ## 8-byte Folded Reload
	addl	-60(%rsp), %ecx                 ## 4-byte Folded Reload
	movslq	%ecx, %rcx
	addq	16(%rsp), %rsi                  ## 8-byte Folded Reload
	addq	%rsi, %rcx
	addq	%rdx, %rsi
	movq	-56(%rsp), %rdx                 ## 8-byte Reload
	vmulps	(%rdx,%rsi,4), %ymm5, %ymm6
	vmulps	32(%rdx,%rsi,4), %ymm5, %ymm5
	movq	-72(%rsp), %rdx                 ## 8-byte Reload
	vfmadd213ps	(%rdx,%rcx,4), %ymm2, %ymm6 ## ymm6 = (ymm2 * ymm6) + mem
	vfmadd213ps	32(%rdx,%rcx,4), %ymm2, %ymm5 ## ymm5 = (ymm2 * ymm5) + mem
	vmovups	%ymm6, (%rdx,%rcx,4)
	vmovups	%ymm5, 32(%rdx,%rcx,4)
LBB16_24:                               ## %middle.block50
                                        ##   in Loop: Header=BB16_6 Depth=1
	movq	80(%rsp), %rdx                  ## 8-byte Reload
	movq	%rdx, %rcx
	cmpq	%r14, %rdx
	je	LBB16_17
LBB16_25:                               ## %"for relu1_0_d_def__.s8.n.ni.rebased.us.us.preheader"
                                        ##   in Loop: Header=BB16_6 Depth=1
	movq	88(%rsp), %rdx                  ## 8-byte Reload
	leaq	(%rdx,%rax,4), %rax
	.p2align	4, 0x90
LBB16_16:                               ## %"for relu1_0_d_def__.s8.n.ni.rebased.us.us"
                                        ##   Parent Loop BB16_6 Depth=1
                                        ## =>  This Inner Loop Header: Depth=2
	vmulss	(%r15,%rcx,4), %xmm4, %xmm5
	vfmadd213ss	(%rax,%rcx,4), %xmm1, %xmm5 ## xmm5 = (xmm1 * xmm5) + mem
	vmovss	%xmm5, (%rax,%rcx,4)
	incq	%rcx
	cmpq	%rcx, %r14
	jne	LBB16_16
	jmp	LBB16_17
LBB16_19:                               ##   in Loop: Header=BB16_6 Depth=1
	xorl	%esi, %esi
	movq	-120(%rsp), %r9                 ## 8-byte Reload
	testb	$1, 64(%rsp)                    ## 1-byte Folded Reload
	jne	LBB16_23
	jmp	LBB16_24
LBB16_2:                                ## %"for relu1_0_d_def__.s8.w.wi.preheader20"
	decl	%eax
	vmovd	%edi, %xmm0
	vpbroadcastd	%xmm0, %ymm0
	vpor	LCPI16_0(%rip), %ymm0, %ymm0
	vmovd	%eax, %xmm1
	vpbroadcastd	%xmm1, %ymm1
	vpminsd	%ymm0, %ymm1, %ymm0
	movq	%rdi, %rax
	movslq	-120(%rsp), %rdi                ## 4-byte Folded Reload
	movl	-104(%rsp), %r11d               ## 4-byte Reload
	imulq	%rbx, %rbp
	addq	%rax, %rbp
	movq	%rbx, %r15
	leaq	(,%rbp,4), %rbx
	addq	%r13, %rbx
	shlq	$2, %r15
	shll	$6, %r8d
	shll	$6, %esi
	subl	%esi, %r8d
	orl	$26, %r8d
	movq	-96(%rsp), %rax                 ## 8-byte Reload
	imull	%r8d, %eax
	movq	-80(%rsp), %rcx                 ## 8-byte Reload
	leal	(%rax,%rcx,8), %edx
	vbroadcastss	LCPI16_1(%rip), %ymm8   ## ymm8 = [-1.0E+0,-1.0E+0,-1.0E+0,-1.0E+0,-1.0E+0,-1.0E+0,-1.0E+0,-1.0E+0]
	vbroadcastss	LCPI16_2(%rip), %ymm9   ## ymm9 = [9.99999971E-10,9.99999971E-10,9.99999971E-10,9.99999971E-10,9.99999971E-10,9.99999971E-10,9.99999971E-10,9.99999971E-10]
	movl	-108(%rsp), %r8d                ## 4-byte Reload
	movl	-128(%rsp), %r10d               ## 4-byte Reload
	movq	-40(%rsp), %rax                 ## 8-byte Reload
	.p2align	4, 0x90
LBB16_3:                                ## %"for relu1_0_d_def__.s8.w.wi"
                                        ## =>This Inner Loop Header: Depth=1
	testq	%rdi, %rdi
	movl	$0, %esi
	cmovgl	%edi, %esi
	imull	%r10d, %esi
	leal	(%rsi,%r9), %ecx
	vmovd	%ecx, %xmm3
	vpbroadcastd	%xmm3, %ymm3
	vpaddd	%ymm0, %ymm3, %ymm3
	vmovd	%xmm3, %ecx
	vpextrd	$1, %xmm3, %ebp
	movslq	%ecx, %rcx
	vmovss	(%rax,%rcx,4), %xmm4            ## xmm4 = mem[0],zero,zero,zero
	vpextrd	$2, %xmm3, %ecx
	movslq	%ebp, %rbp
	movslq	%ecx, %rcx
	vinsertps	$16, (%rax,%rbp,4), %xmm4, %xmm4 ## xmm4 = xmm4[0],mem[0],xmm4[2,3]
	vpextrd	$3, %xmm3, %ebp
	vextracti128	$1, %ymm3, %xmm3
	movslq	%ebp, %rbp
	vinsertps	$32, (%rax,%rcx,4), %xmm4, %xmm4 ## xmm4 = xmm4[0,1],mem[0],xmm4[3]
	vinsertps	$48, (%rax,%rbp,4), %xmm4, %xmm4 ## xmm4 = xmm4[0,1,2],mem[0]
	vmovd	%xmm3, %ecx
	vpextrd	$1, %xmm3, %ebp
	movslq	%ecx, %rcx
	vmovss	(%rax,%rcx,4), %xmm5            ## xmm5 = mem[0],zero,zero,zero
	movslq	%ebp, %rcx
	vinsertps	$16, (%rax,%rcx,4), %xmm5, %xmm5 ## xmm5 = xmm5[0],mem[0],xmm5[2,3]
	addl	%r14d, %esi
	vmovd	%esi, %xmm6
	vpextrd	$2, %xmm3, %ecx
	movslq	%ecx, %rcx
	vpbroadcastd	%xmm6, %ymm6
	vinsertps	$32, (%rax,%rcx,4), %xmm5, %xmm5 ## xmm5 = xmm5[0,1],mem[0],xmm5[3]
	vpaddd	%ymm0, %ymm6, %ymm6
	vextracti128	$1, %ymm6, %xmm7
	vmovd	%xmm7, %ecx
	movslq	%ecx, %rcx
	vmovss	(%rax,%rcx,4), %xmm1            ## xmm1 = mem[0],zero,zero,zero
	vpextrd	$1, %xmm7, %ecx
	movslq	%ecx, %rcx
	vinsertps	$16, (%rax,%rcx,4), %xmm1, %xmm1 ## xmm1 = xmm1[0],mem[0],xmm1[2,3]
	vmovd	%xmm6, %ecx
	movslq	%ecx, %rcx
	vmovss	(%rax,%rcx,4), %xmm2            ## xmm2 = mem[0],zero,zero,zero
	vpextrd	$1, %xmm6, %ecx
	movslq	%ecx, %rcx
	vinsertps	$16, (%rax,%rcx,4), %xmm2, %xmm2 ## xmm2 = xmm2[0],mem[0],xmm2[2,3]
	vpextrd	$2, %xmm7, %ecx
	movslq	%ecx, %rcx
	vinsertps	$32, (%rax,%rcx,4), %xmm1, %xmm1 ## xmm1 = xmm1[0,1],mem[0],xmm1[3]
	vpextrd	$2, %xmm6, %ecx
	movslq	%ecx, %rcx
	vinsertps	$32, (%rax,%rcx,4), %xmm2, %xmm2 ## xmm2 = xmm2[0,1],mem[0],xmm2[3]
	vpextrd	$3, %xmm7, %ecx
	movslq	%ecx, %rcx
	vinsertps	$48, (%rax,%rcx,4), %xmm1, %xmm1 ## xmm1 = xmm1[0,1,2],mem[0]
	vpextrd	$3, %xmm6, %ecx
	movslq	%ecx, %rcx
	vinsertps	$48, (%rax,%rcx,4), %xmm2, %xmm2 ## xmm2 = xmm2[0,1,2],mem[0]
	vpextrd	$3, %xmm3, %ecx
	movslq	%ecx, %rcx
	vinsertps	$48, (%rax,%rcx,4), %xmm5, %xmm3 ## xmm3 = xmm5[0,1,2],mem[0]
	vinsertf128	$1, %xmm3, %ymm4, %ymm3
	vinsertf128	$1, %xmm1, %ymm2, %ymm1
	vaddps	%ymm1, %ymm8, %ymm1
	vmulps	%ymm1, %ymm3, %ymm1
	vmulps	(%rbx), %ymm1, %ymm1
	movslq	%edx, %rdx
	movq	-72(%rsp), %rcx                 ## 8-byte Reload
	vfmadd213ps	(%rcx,%rdx,4), %ymm9, %ymm1 ## ymm1 = (ymm9 * ymm1) + mem
	vmovups	%ymm1, (%rcx,%rdx,4)
	addq	%r15, %rbx
	incq	%rdi
	addl	%r8d, %edx
	decq	%r11
	jne	LBB16_3
LBB16_40:                               ## %destructor_block
	xorl	%eax, %eax
	addq	$120, %rsp
	popq	%rbx
	popq	%r12
	popq	%r13
	popq	%r14
	popq	%r15
	popq	%rbp
	vzeroupper
	retq
LBB16_26:                               ## %"for relu1_0_d_def__.s8.w.wi.preheader.split.us.split"
	movl	%esi, -88(%rsp)                 ## 4-byte Spill
	testl	%r15d, %r15d
	movl	-128(%rsp), %ecx                ## 4-byte Reload
	jle	LBB16_40
## %bb.27:                              ## %"for relu1_0_d_def__.s8.w.wi.us.us4.preheader"
	movq	%rdi, %r9
	movslq	%r11d, %r11
	movl	-124(%rsp), %ebp                ## 4-byte Reload
	movslq	-120(%rsp), %r10                ## 4-byte Folded Reload
	movl	-104(%rsp), %r15d               ## 4-byte Reload
	movl	%ebp, %r12d
	andl	$-16, %r12d
	leaq	-16(%r12), %r14
	movq	%r14, -104(%rsp)                ## 8-byte Spill
	shrq	$4, %r14
	incq	%r14
	movq	-48(%rsp), %rdx                 ## 8-byte Reload
	movq	%rdx, %rax
	imulq	-32(%rsp), %rax                 ## 8-byte Folded Reload
	addq	%r11, %rax
	movq	-56(%rsp), %r13                 ## 8-byte Reload
	leaq	96(,%rax,4), %rdi
	addq	%r13, %rdi
	leaq	(,%rdx,4), %rbx
	movq	-72(%rsp), %rdx                 ## 8-byte Reload
	leaq	96(%rdx), %rsi
	movq	%rsi, -80(%rsp)                 ## 8-byte Spill
	shll	$6, %r8d
	movl	-88(%rsp), %esi                 ## 4-byte Reload
	shll	$6, %esi
	subl	%esi, %r8d
	orl	$26, %r8d
	imull	%r8d, %r9d
	movq	%r14, %rsi
	andq	$-2, %rsi
	negq	%rsi
	movq	%rsi, -96(%rsp)                 ## 8-byte Spill
	leaq	(%r13,%rax,4), %r13
	movq	%r11, -120(%rsp)                ## 8-byte Spill
	leaq	(%rdx,%r11,4), %r8
	vmovss	LCPI16_1(%rip), %xmm0           ## xmm0 = mem[0],zero,zero,zero
	vmovss	LCPI16_2(%rip), %xmm1           ## xmm1 = mem[0],zero,zero,zero
	vbroadcastss	LCPI16_2(%rip), %ymm2   ## ymm2 = [9.99999971E-10,9.99999971E-10,9.99999971E-10,9.99999971E-10,9.99999971E-10,9.99999971E-10,9.99999971E-10,9.99999971E-10]
	xorl	%r11d, %r11d
	jmp	LBB16_28
LBB16_39:                               ## %after_bb.loopexit.us.us17
                                        ##   in Loop: Header=BB16_28 Depth=1
	incq	%r11
	addq	%rbx, %rdi
	addl	-108(%rsp), %r9d                ## 4-byte Folded Reload
	addq	%rbx, %r13
	cmpq	%r15, %r11
	movl	-128(%rsp), %ecx                ## 4-byte Reload
	je	LBB16_40
LBB16_28:                               ## %"for relu1_0_d_def__.s8.w.wi.us.us4"
                                        ## =>This Loop Header: Depth=1
                                        ##     Child Loop BB16_33 Depth 2
                                        ##     Child Loop BB16_38 Depth 2
	leaq	(%r11,%r10), %rax
	testq	%rax, %rax
	movl	$0, %edx
	cmovlel	%edx, %eax
	imull	%ecx, %eax
	cltq
	movq	40(%rsp), %rcx                  ## 8-byte Reload
	addq	%rax, %rcx
	addq	32(%rsp), %rax                  ## 8-byte Folded Reload
	movq	-40(%rsp), %rdx                 ## 8-byte Reload
	vaddss	-4(%rdx,%rax,4), %xmm0, %xmm3
	movslq	%r9d, %rax
	vmulss	-4(%rdx,%rcx,4), %xmm3, %xmm3
	cmpl	$16, -124(%rsp)                 ## 4-byte Folded Reload
	jae	LBB16_30
## %bb.29:                              ##   in Loop: Header=BB16_28 Depth=1
	xorl	%ecx, %ecx
	jmp	LBB16_37
LBB16_30:                               ## %vector.ph
                                        ##   in Loop: Header=BB16_28 Depth=1
	vbroadcastss	%xmm3, %ymm4
	cmpq	$0, -104(%rsp)                  ## 8-byte Folded Reload
	je	LBB16_31
## %bb.32:                              ## %vector.body.preheader
                                        ##   in Loop: Header=BB16_28 Depth=1
	movq	-120(%rsp), %rcx                ## 8-byte Reload
	addq	%rax, %rcx
	movq	-80(%rsp), %rdx                 ## 8-byte Reload
	leaq	(%rdx,%rcx,4), %rcx
	movq	-96(%rsp), %rdx                 ## 8-byte Reload
	xorl	%esi, %esi
LBB16_33:                               ## %vector.body
                                        ##   Parent Loop BB16_28 Depth=1
                                        ## =>  This Inner Loop Header: Depth=2
	vmulps	-96(%rdi,%rsi,4), %ymm4, %ymm5
	vmulps	-64(%rdi,%rsi,4), %ymm4, %ymm6
	vfmadd213ps	-96(%rcx,%rsi,4), %ymm2, %ymm5 ## ymm5 = (ymm2 * ymm5) + mem
	vfmadd213ps	-64(%rcx,%rsi,4), %ymm2, %ymm6 ## ymm6 = (ymm2 * ymm6) + mem
	vmovups	%ymm5, -96(%rcx,%rsi,4)
	vmovups	%ymm6, -64(%rcx,%rsi,4)
	vmulps	-32(%rdi,%rsi,4), %ymm4, %ymm5
	vmulps	(%rdi,%rsi,4), %ymm4, %ymm6
	vfmadd213ps	-32(%rcx,%rsi,4), %ymm2, %ymm5 ## ymm5 = (ymm2 * ymm5) + mem
	vfmadd213ps	(%rcx,%rsi,4), %ymm2, %ymm6 ## ymm6 = (ymm2 * ymm6) + mem
	vmovups	%ymm5, -32(%rcx,%rsi,4)
	vmovups	%ymm6, (%rcx,%rsi,4)
	addq	$32, %rsi
	addq	$2, %rdx
	jne	LBB16_33
	jmp	LBB16_34
LBB16_31:                               ##   in Loop: Header=BB16_28 Depth=1
	xorl	%esi, %esi
LBB16_34:                               ## %middle.block.unr-lcssa
                                        ##   in Loop: Header=BB16_28 Depth=1
	testb	$1, %r14b
	je	LBB16_36
## %bb.35:                              ## %vector.body.epil
                                        ##   in Loop: Header=BB16_28 Depth=1
	movq	-8(%rsp), %rcx                  ## 8-byte Reload
	addl	%r11d, %ecx
	imull	-108(%rsp), %ecx                ## 4-byte Folded Reload
	movq	-32(%rsp), %rdx                 ## 8-byte Reload
	addq	%r11, %rdx
	addl	-60(%rsp), %ecx                 ## 4-byte Folded Reload
	imulq	-48(%rsp), %rdx                 ## 8-byte Folded Reload
	movslq	%ecx, %rcx
	addq	-120(%rsp), %rsi                ## 8-byte Folded Reload
	addq	%rsi, %rcx
	addq	%rdx, %rsi
	movq	-56(%rsp), %rdx                 ## 8-byte Reload
	vmulps	(%rdx,%rsi,4), %ymm4, %ymm5
	vmulps	32(%rdx,%rsi,4), %ymm4, %ymm4
	movq	-72(%rsp), %rdx                 ## 8-byte Reload
	vfmadd213ps	(%rdx,%rcx,4), %ymm2, %ymm5 ## ymm5 = (ymm2 * ymm5) + mem
	vfmadd213ps	32(%rdx,%rcx,4), %ymm2, %ymm4 ## ymm4 = (ymm2 * ymm4) + mem
	vmovups	%ymm5, (%rdx,%rcx,4)
	vmovups	%ymm4, 32(%rdx,%rcx,4)
LBB16_36:                               ## %middle.block
                                        ##   in Loop: Header=BB16_28 Depth=1
	movq	%r12, %rcx
	cmpq	%rbp, %r12
	je	LBB16_39
LBB16_37:                               ## %"for relu1_0_d_def__.s8.n.ni.rebased.us.us9.preheader"
                                        ##   in Loop: Header=BB16_28 Depth=1
	leaq	(%r8,%rax,4), %rax
LBB16_38:                               ## %"for relu1_0_d_def__.s8.n.ni.rebased.us.us9"
                                        ##   Parent Loop BB16_28 Depth=1
                                        ## =>  This Inner Loop Header: Depth=2
	vmulss	(%r13,%rcx,4), %xmm3, %xmm4
	vfmadd213ss	(%rax,%rcx,4), %xmm1, %xmm4 ## xmm4 = (xmm1 * xmm4) + mem
	vmovss	%xmm4, (%rax,%rcx,4)
	incq	%rcx
	cmpq	%rcx, %rbp
	jne	LBB16_38
	jmp	LBB16_39
                                        ## -- End function
	.section	__TEXT,__const
	.p2align	5                               ## -- Begin function train_cost_model.par_for.relu1_0_d_def__.s9.n.n.n
LCPI17_0:
	.long	0                               ## 0x0
	.long	1                               ## 0x1
	.long	2                               ## 0x2
	.long	3                               ## 0x3
	.long	4                               ## 0x4
	.long	5                               ## 0x5
	.long	6                               ## 0x6
	.long	7                               ## 0x7
	.section	__TEXT,__literal4,4byte_literals
	.p2align	2
LCPI17_1:
	.long	0x3089705f                      ## float 9.99999971E-10
	.section	__TEXT,__text,regular,pure_instructions
	.p2align	4, 0x90
_train_cost_model.par_for.relu1_0_d_def__.s9.n.n.n: ## @train_cost_model.par_for.relu1_0_d_def__.s9.n.n.n
## %bb.0:                               ## %entry
	pushq	%rbp
	pushq	%r15
	pushq	%r14
	pushq	%r13
	pushq	%r12
	pushq	%rbx
	subq	$128, %rsp
	movq	%rdx, %rbx
	movl	%esi, %r10d
	movl	(%rdx), %eax
	movq	%rax, 24(%rsp)                  ## 8-byte Spill
	movl	12(%rdx), %r14d
	movslq	4(%rdx), %rax
	movq	%rax, -40(%rsp)                 ## 8-byte Spill
	movslq	8(%rdx), %r8
	movl	16(%rdx), %eax
	movq	%rax, -48(%rsp)                 ## 8-byte Spill
	movl	20(%rdx), %eax
	movl	%eax, -116(%rsp)                ## 4-byte Spill
	movslq	24(%rdx), %r9
	movl	28(%rdx), %ebp
	movl	%esi, %edi
	sarl	$31, %edi
	xorl	%r15d, %r15d
	testl	%ebp, %ebp
	sete	%r15b
	movl	%r15d, %esi
	negl	%esi
	subl	%edi, %r10d
	orl	%ebp, %esi
	movl	%r10d, %eax
	cltd
	movl	32(%rbx), %ecx
	movl	%ecx, -120(%rsp)                ## 4-byte Spill
	movl	36(%rbx), %ecx
	movl	%ecx, 8(%rsp)                   ## 4-byte Spill
	movl	44(%rbx), %ecx
	movl	%ecx, -56(%rsp)                 ## 4-byte Spill
	movq	48(%rbx), %rcx
	movq	%rcx, -88(%rsp)                 ## 8-byte Spill
	movq	64(%rbx), %rcx
	movq	%rcx, -112(%rsp)                ## 8-byte Spill
	movq	%rbx, 48(%rsp)                  ## 8-byte Spill
	movq	80(%rbx), %rcx
	movq	%rcx, -104(%rsp)                ## 8-byte Spill
	idivl	%esi
	movl	%ebp, %eax
	sarl	$31, %eax
	movl	%eax, %esi
	xorl	%ebp, %esi
	movl	%eax, %ecx
	notl	%ecx
	addl	%ecx, %esi
	andl	%edi, %esi
	addl	%edx, %esi
	subl	%eax, %ecx
	andl	%edi, %ecx
	addl	%r15d, %ebp
	movl	%r10d, %eax
	cltd
	idivl	%ebp
	addl	%eax, %ecx
	leal	-1(%r15), %eax
	andl	%eax, %esi
	andl	%eax, %ecx
	leal	(%rcx,%rcx), %edx
	subl	%edx, %r14d
	cmpl	$2, %r14d
	movl	$2, %ebp
	cmovll	%r14d, %ebp
	movl	%ecx, %eax
	sarl	$31, %eax
	andl	%ecx, %eax
	movl	$-1, %edi
	cmovnsl	%eax, %edi
	addl	%edi, %edi
	movl	%edi, 56(%rsp)                  ## 4-byte Spill
	movl	%edi, %eax
	negl	%eax
	cmpl	%eax, %ebp
	cmovlel	%eax, %ebp
	movl	%ebp, -68(%rsp)                 ## 4-byte Spill
	cmpl	%eax, %r14d
	cmovll	%r14d, %eax
	movq	%rcx, -24(%rsp)                 ## 8-byte Spill
	movslq	%ecx, %rcx
	subq	%r8, %rcx
	addq	%rcx, %rcx
	movq	%rcx, -16(%rsp)                 ## 8-byte Spill
	leal	(,%rsi,8), %ecx
	vmovd	%ecx, %xmm0
	movq	%rsi, -64(%rsp)                 ## 8-byte Spill
	leal	8(,%rsi,8), %esi
	movl	%esi, 16(%rsp)                  ## 4-byte Spill
	movl	%ecx, -8(%rsp)                  ## 4-byte Spill
	movslq	%ecx, %rcx
	movq	%rcx, -96(%rsp)                 ## 8-byte Spill
	movl	%edx, -32(%rsp)                 ## 4-byte Spill
	movslq	%edx, %rcx
	movq	%rcx, 120(%rsp)                 ## 8-byte Spill
	testl	%eax, %eax
	movq	%r9, (%rsp)                     ## 8-byte Spill
	jle	LBB17_19
## %bb.1:                               ## %"for relu1_0_d_def__.s9.w.wi.preheader"
	movq	%r9, %r8
	movl	%eax, %ecx
	sarl	$31, %ecx
	andnl	%eax, %ecx, %ecx
	movq	-48(%rsp), %rdi                 ## 8-byte Reload
	movl	%edi, %eax
	shll	$5, %eax
	movl	%eax, -124(%rsp)                ## 4-byte Spill
	movq	24(%rsp), %rsi                  ## 8-byte Reload
	movl	%esi, %eax
	subl	-8(%rsp), %eax                  ## 4-byte Folded Reload
	cmpl	$8, %eax
	movl	$8, %ebp
	cmovll	%eax, %ebp
	xorl	%edx, %edx
	testl	%eax, %eax
	cmovlel	%edx, %ebp
	movl	%ebp, -128(%rsp)                ## 4-byte Spill
	movl	%ecx, %ebp
	cmpl	%esi, 16(%rsp)                  ## 4-byte Folded Reload
	jle	LBB17_2
## %bb.4:                               ## %"for relu1_0_d_def__.s9.w.wi.preheader.split.us"
	testl	%eax, %eax
	jle	LBB17_19
## %bb.5:                               ## %"for relu1_0_d_def__.s9.w.wi.us.us.preheader"
	movl	-128(%rsp), %ebx                ## 4-byte Reload
	movslq	-32(%rsp), %rdx                 ## 4-byte Folded Reload
	movl	%ebx, %eax
	andl	$2147483640, %eax               ## imm = 0x7FFFFFF8
	movq	%rax, 88(%rsp)                  ## 8-byte Spill
	addq	$-8, %rax
	movq	%rax, 80(%rsp)                  ## 8-byte Spill
	movq	%rax, %rdi
	shrq	$3, %rdi
	incq	%rdi
	movl	%edi, %r14d
	andl	$3, %r14d
	movq	-40(%rsp), %rax                 ## 8-byte Reload
	movq	%rax, %r12
	imulq	-16(%rsp), %r12                 ## 8-byte Folded Reload
	addq	-96(%rsp), %r12                 ## 8-byte Folded Reload
	movq	-88(%rsp), %rsi                 ## 8-byte Reload
	leaq	(%rsi,%r12,4), %r13
	addq	$96, %r13
	leaq	(,%rax,4), %rax
	movq	%rax, 112(%rsp)                 ## 8-byte Spill
	movq	-104(%rsp), %rax                ## 8-byte Reload
	addq	$96, %rax
	movq	%rax, 40(%rsp)                  ## 8-byte Spill
	movl	-8(%rsp), %eax                  ## 4-byte Reload
	subl	8(%rsp), %eax                   ## 4-byte Folded Reload
	movl	%eax, 104(%rsp)                 ## 4-byte Spill
	movq	-112(%rsp), %rax                ## 8-byte Reload
	addq	$96, %rax
	movq	%rax, 32(%rsp)                  ## 8-byte Spill
	movq	-24(%rsp), %rax                 ## 8-byte Reload
                                        ## kill: def $eax killed $eax killed $rax def $rax
	shll	$6, %eax
	movl	-56(%rsp), %ecx                 ## 4-byte Reload
	shll	$6, %ecx
	subl	%ecx, %eax
	orl	$24, %eax
	imull	-48(%rsp), %eax                 ## 4-byte Folded Reload
	movq	-64(%rsp), %rcx                 ## 8-byte Reload
	leal	(%rax,%rcx,8), %r15d
	andq	$-4, %rdi
	negq	%rdi
	movq	%rdi, 64(%rsp)                  ## 8-byte Spill
	movq	%r14, 72(%rsp)                  ## 8-byte Spill
	shlq	$5, %r14
	leaq	(%rsi,%r12,4), %r9
	vmovss	LCPI17_1(%rip), %xmm1           ## xmm1 = mem[0],zero,zero,zero
	vbroadcastss	LCPI17_1(%rip), %ymm2   ## ymm2 = [9.99999971E-10,9.99999971E-10,9.99999971E-10,9.99999971E-10,9.99999971E-10,9.99999971E-10,9.99999971E-10,9.99999971E-10]
	xorl	%r10d, %r10d
	movq	%rbp, 96(%rsp)                  ## 8-byte Spill
	jmp	LBB17_6
	.p2align	4, 0x90
LBB17_18:                               ## %after_bb.loopexit.us.us
                                        ##   in Loop: Header=BB17_6 Depth=1
	incq	%r10
	movq	112(%rsp), %rax                 ## 8-byte Reload
	addq	%rax, %r13
	incq	%rdx
	addl	%edi, %r15d
	addq	-40(%rsp), %r12                 ## 8-byte Folded Reload
	addq	%rax, %r9
	cmpq	96(%rsp), %r10                  ## 8-byte Folded Reload
	je	LBB17_19
LBB17_6:                                ## %"for relu1_0_d_def__.s9.w.wi.us.us"
                                        ## =>This Loop Header: Depth=1
                                        ##     Child Loop BB17_11 Depth 2
                                        ##     Child Loop BB17_14 Depth 2
                                        ##     Child Loop BB17_17 Depth 2
	testq	%rdx, %rdx
	movl	$0, %eax
	cmovgl	%edx, %eax
	imull	%r8d, %eax
	addl	104(%rsp), %eax                 ## 4-byte Folded Reload
	movslq	%eax, %rbp
	movslq	%r15d, %rax
	movq	%rax, -80(%rsp)                 ## 8-byte Spill
	cmpl	$8, -128(%rsp)                  ## 4-byte Folded Reload
	jae	LBB17_8
## %bb.7:                               ##   in Loop: Header=BB17_6 Depth=1
	xorl	%eax, %eax
	movl	-124(%rsp), %edi                ## 4-byte Reload
	jmp	LBB17_16
	.p2align	4, 0x90
LBB17_8:                                ## %vector.ph
                                        ##   in Loop: Header=BB17_6 Depth=1
	cmpq	$24, 80(%rsp)                   ## 8-byte Folded Reload
	jae	LBB17_10
## %bb.9:                               ##   in Loop: Header=BB17_6 Depth=1
	movq	%rbp, %rdi
	xorl	%eax, %eax
	jmp	LBB17_12
	.p2align	4, 0x90
LBB17_10:                               ## %vector.body.preheader
                                        ##   in Loop: Header=BB17_6 Depth=1
	movq	40(%rsp), %rax                  ## 8-byte Reload
	movq	%rbp, %rdi
	leaq	(%rax,%rbp,4), %r11
	movq	32(%rsp), %rax                  ## 8-byte Reload
	movq	-80(%rsp), %rcx                 ## 8-byte Reload
	leaq	(%rax,%rcx,4), %r8
	movq	64(%rsp), %rsi                  ## 8-byte Reload
	xorl	%eax, %eax
	.p2align	4, 0x90
LBB17_11:                               ## %vector.body
                                        ##   Parent Loop BB17_6 Depth=1
                                        ## =>  This Inner Loop Header: Depth=2
	vmovups	-96(%r11,%rax,4), %ymm3
	vmulps	-96(%r13,%rax,4), %ymm3, %ymm3
	vfmadd213ps	-96(%r8,%rax,4), %ymm2, %ymm3 ## ymm3 = (ymm2 * ymm3) + mem
	vmovups	%ymm3, -96(%r8,%rax,4)
	vmovups	-64(%r11,%rax,4), %ymm3
	vmulps	-64(%r13,%rax,4), %ymm3, %ymm3
	vfmadd213ps	-64(%r8,%rax,4), %ymm2, %ymm3 ## ymm3 = (ymm2 * ymm3) + mem
	vmovups	%ymm3, -64(%r8,%rax,4)
	vmovups	-32(%r11,%rax,4), %ymm3
	vmulps	-32(%r13,%rax,4), %ymm3, %ymm3
	vfmadd213ps	-32(%r8,%rax,4), %ymm2, %ymm3 ## ymm3 = (ymm2 * ymm3) + mem
	vmovups	%ymm3, -32(%r8,%rax,4)
	vmovups	(%r11,%rax,4), %ymm3
	vmulps	(%r13,%rax,4), %ymm3, %ymm3
	vfmadd213ps	(%r8,%rax,4), %ymm2, %ymm3 ## ymm3 = (ymm2 * ymm3) + mem
	vmovups	%ymm3, (%r8,%rax,4)
	addq	$32, %rax
	addq	$4, %rsi
	jne	LBB17_11
LBB17_12:                               ## %middle.block.unr-lcssa
                                        ##   in Loop: Header=BB17_6 Depth=1
	cmpq	$0, 72(%rsp)                    ## 8-byte Folded Reload
	movq	%rdi, %rbp
	je	LBB17_15
## %bb.13:                              ## %vector.body.epil.preheader
                                        ##   in Loop: Header=BB17_6 Depth=1
	leaq	(%rax,%r12), %rcx
	movq	-88(%rsp), %rsi                 ## 8-byte Reload
	leaq	(%rsi,%rcx,4), %rsi
	leaq	(%rax,%rbp), %rcx
	movq	-104(%rsp), %rdi                ## 8-byte Reload
	leaq	(%rdi,%rcx,4), %rcx
	addq	-80(%rsp), %rax                 ## 8-byte Folded Reload
	movq	-112(%rsp), %rdi                ## 8-byte Reload
	leaq	(%rdi,%rax,4), %rax
	xorl	%edi, %edi
	.p2align	4, 0x90
LBB17_14:                               ## %vector.body.epil
                                        ##   Parent Loop BB17_6 Depth=1
                                        ## =>  This Inner Loop Header: Depth=2
	vmovups	(%rcx,%rdi), %ymm3
	vmulps	(%rsi,%rdi), %ymm3, %ymm3
	vfmadd213ps	(%rax,%rdi), %ymm2, %ymm3 ## ymm3 = (ymm2 * ymm3) + mem
	vmovups	%ymm3, (%rax,%rdi)
	addq	$32, %rdi
	cmpq	%rdi, %r14
	jne	LBB17_14
LBB17_15:                               ## %middle.block
                                        ##   in Loop: Header=BB17_6 Depth=1
	movq	88(%rsp), %rcx                  ## 8-byte Reload
	movq	%rcx, %rax
	cmpq	%rbx, %rcx
	movq	(%rsp), %r8                     ## 8-byte Reload
	movl	-124(%rsp), %edi                ## 4-byte Reload
	je	LBB17_18
LBB17_16:                               ## %"for relu1_0_d_def__.s9.n.ni.us.us.preheader"
                                        ##   in Loop: Header=BB17_6 Depth=1
	movq	-104(%rsp), %rcx                ## 8-byte Reload
	leaq	(%rcx,%rbp,4), %rcx
	movq	-112(%rsp), %rsi                ## 8-byte Reload
	movq	-80(%rsp), %r11                 ## 8-byte Reload
	leaq	(%rsi,%r11,4), %rsi
	.p2align	4, 0x90
LBB17_17:                               ## %"for relu1_0_d_def__.s9.n.ni.us.us"
                                        ##   Parent Loop BB17_6 Depth=1
                                        ## =>  This Inner Loop Header: Depth=2
	vmovss	(%rcx,%rax,4), %xmm3            ## xmm3 = mem[0],zero,zero,zero
	vmulss	(%r9,%rax,4), %xmm3, %xmm3
	vfmadd213ss	(%rsi,%rax,4), %xmm1, %xmm3 ## xmm3 = (xmm1 * xmm3) + mem
	vmovss	%xmm3, (%rsi,%rax,4)
	incq	%rax
	cmpq	%rax, %rbx
	jne	LBB17_17
	jmp	LBB17_18
LBB17_2:                                ## %"for relu1_0_d_def__.s9.w.wi.preheader26"
	vpbroadcastd	%xmm0, %ymm1
	vpor	LCPI17_0(%rip), %ymm1, %ymm1
	leal	-1(%rsi), %eax
	vmovd	%eax, %xmm2
	vpbroadcastd	%xmm2, %ymm2
	vpminsd	%ymm1, %ymm2, %ymm1
	negq	%rbp
	movq	-40(%rsp), %rdx                 ## 8-byte Reload
	movq	%rdx, %rax
	imulq	-16(%rsp), %rax                 ## 8-byte Folded Reload
	addq	-96(%rsp), %rax                 ## 8-byte Folded Reload
	movq	-88(%rsp), %rcx                 ## 8-byte Reload
	leaq	(%rcx,%rax,4), %rcx
	leaq	(,%rdx,4), %r9
	movq	-24(%rsp), %rax                 ## 8-byte Reload
                                        ## kill: def $eax killed $eax killed $rax def $rax
	shll	$6, %eax
	movl	-56(%rsp), %esi                 ## 4-byte Reload
	shll	$6, %esi
	subl	%esi, %eax
	orl	$24, %eax
	imull	%edi, %eax
	movq	-64(%rsp), %rdx                 ## 8-byte Reload
	leal	(%rax,%rdx,8), %edi
	vbroadcastss	LCPI17_1(%rip), %ymm2   ## ymm2 = [9.99999971E-10,9.99999971E-10,9.99999971E-10,9.99999971E-10,9.99999971E-10,9.99999971E-10,9.99999971E-10,9.99999971E-10]
	movq	120(%rsp), %rbx                 ## 8-byte Reload
	movl	-124(%rsp), %r10d               ## 4-byte Reload
	movq	-104(%rsp), %rdx                ## 8-byte Reload
	.p2align	4, 0x90
LBB17_3:                                ## %"for relu1_0_d_def__.s9.w.wi"
                                        ## =>This Inner Loop Header: Depth=1
	testq	%rbx, %rbx
	movl	$0, %eax
	cmovgl	%ebx, %eax
	subl	-116(%rsp), %eax                ## 4-byte Folded Reload
	imull	%r8d, %eax
	subl	-120(%rsp), %eax                ## 4-byte Folded Reload
	vmovd	%eax, %xmm3
	vpbroadcastd	%xmm3, %ymm3
	vpaddd	%ymm1, %ymm3, %ymm3
	vextracti128	$1, %ymm3, %xmm4
	vmovd	%xmm4, %eax
	cltq
	vpextrd	$1, %xmm4, %esi
	movslq	%esi, %rsi
	vmovss	(%rdx,%rax,4), %xmm5            ## xmm5 = mem[0],zero,zero,zero
	vpextrd	$2, %xmm4, %eax
	vinsertps	$16, (%rdx,%rsi,4), %xmm5, %xmm5 ## xmm5 = xmm5[0],mem[0],xmm5[2,3]
	vmovd	%xmm3, %esi
	cltq
	vinsertps	$32, (%rdx,%rax,4), %xmm5, %xmm5 ## xmm5 = xmm5[0,1],mem[0],xmm5[3]
	movslq	%esi, %rax
	vpextrd	$3, %xmm4, %esi
	movslq	%esi, %rsi
	vinsertps	$48, (%rdx,%rsi,4), %xmm5, %xmm4 ## xmm4 = xmm5[0,1,2],mem[0]
	vpextrd	$1, %xmm3, %esi
	vmovss	(%rdx,%rax,4), %xmm5            ## xmm5 = mem[0],zero,zero,zero
	movslq	%esi, %rax
	vinsertps	$16, (%rdx,%rax,4), %xmm5, %xmm5 ## xmm5 = xmm5[0],mem[0],xmm5[2,3]
	vpextrd	$2, %xmm3, %eax
	cltq
	vinsertps	$32, (%rdx,%rax,4), %xmm5, %xmm5 ## xmm5 = xmm5[0,1],mem[0],xmm5[3]
	vpextrd	$3, %xmm3, %eax
	cltq
	vinsertps	$48, (%rdx,%rax,4), %xmm5, %xmm3 ## xmm3 = xmm5[0,1,2],mem[0]
	movslq	%edi, %rdi
	vinsertf128	$1, %xmm4, %ymm3, %ymm3
	vmulps	(%rcx), %ymm3, %ymm3
	movq	-112(%rsp), %rax                ## 8-byte Reload
	vfmadd213ps	(%rax,%rdi,4), %ymm2, %ymm3 ## ymm3 = (ymm2 * ymm3) + mem
	vmovaps	%ymm3, (%rax,%rdi,4)
	addq	%r9, %rcx
	incq	%rbx
	addl	%r10d, %edi
	incq	%rbp
	jne	LBB17_3
LBB17_19:                               ## %"end for relu1_0_d_def__.s9.w.wi"
	movl	56(%rsp), %r9d                  ## 4-byte Reload
	movl	-68(%rsp), %r14d                ## 4-byte Reload
	addl	%r9d, %r14d
	movq	-48(%rsp), %rdi                 ## 8-byte Reload
	movq	-40(%rsp), %r11                 ## 8-byte Reload
	jle	LBB17_23
## %bb.20:                              ## %"for relu1_0_d_def__.s9.w.wi.rebased.preheader"
	movl	%edi, %eax
	shll	$5, %eax
	movl	%eax, -128(%rsp)                ## 4-byte Spill
	movq	24(%rsp), %rdx                  ## 8-byte Reload
	movl	%edx, %ecx
	movl	-8(%rsp), %esi                  ## 4-byte Reload
	subl	%esi, %ecx
	cmpl	$8, %ecx
	movl	$8, %eax
	cmovll	%ecx, %eax
	xorl	%ebx, %ebx
	movl	%ecx, -124(%rsp)                ## 4-byte Spill
	testl	%ecx, %ecx
	cmovgl	%eax, %ebx
	movl	%esi, %eax
	subl	%edx, %eax
	movl	%eax, %ecx
	sarl	$31, %ecx
	andl	%eax, %ecx
	cmpl	$-8, %ecx
	movl	$-8, %ebp
	cmovgl	%ecx, %ebp
	movslq	-16(%rsp), %r10                 ## 4-byte Folded Reload
	cmpl	%edx, 16(%rsp)                  ## 4-byte Folded Reload
	jle	LBB17_21
## %bb.24:                              ## %"for relu1_0_d_def__.s9.w.wi.rebased.us.preheader"
	movq	-24(%rsp), %rax                 ## 8-byte Reload
	movl	-56(%rsp), %edx                 ## 4-byte Reload
	subl	%edx, %eax
	movq	48(%rsp), %rcx                  ## 8-byte Reload
	movslq	40(%rcx), %rcx
	movq	%rcx, 88(%rsp)                  ## 8-byte Spill
	addl	%eax, %eax
	movq	%rax, -24(%rsp)                 ## 8-byte Spill
	addl	%ebx, %ebp
	leal	(,%rdi,8), %eax
	leal	(%rax,%rax,2), %eax
	movl	%eax, -68(%rsp)                 ## 4-byte Spill
	addl	%ebx, %esi
	movl	%ebx, 96(%rsp)                  ## 4-byte Spill
	movl	%ebx, %ebx
	movslq	%esi, %r13
	movl	%ebp, -116(%rsp)                ## 4-byte Spill
	movl	%ebp, %r8d
	movslq	%r9d, %r15
	movl	%r14d, %eax
	movq	%rax, 112(%rsp)                 ## 8-byte Spill
	movl	%ebx, %eax
	andl	$2147483640, %eax               ## imm = 0x7FFFFFF8
	movq	%rax, -8(%rsp)                  ## 8-byte Spill
	leaq	-8(%rax), %rbp
	movl	-32(%rsp), %esi                 ## 4-byte Reload
	movl	%esi, %eax
	subl	%r9d, %eax
	imull	(%rsp), %eax                    ## 4-byte Folded Reload
	movq	-64(%rsp), %r14                 ## 8-byte Reload
	leal	(%rax,%r14,8), %eax
	subl	8(%rsp), %eax                   ## 4-byte Folded Reload
	movl	%eax, -80(%rsp)                 ## 4-byte Spill
	movq	%rbp, 24(%rsp)                  ## 8-byte Spill
	shrq	$3, %rbp
	incq	%rbp
	addl	%edx, %edx
	subl	%edx, %esi
	movl	%r8d, %eax
	andl	$-16, %eax
	subl	%r9d, %esi
	movq	%rax, -56(%rsp)                 ## 8-byte Spill
	addq	$-16, %rax
	shll	$5, %esi
	orl	$24, %esi
	imull	%esi, %edi
	movq	%rax, -16(%rsp)                 ## 8-byte Spill
	movq	%rax, %rsi
	shrq	$4, %rsi
	incq	%rsi
	movl	%ebp, %r12d
	andl	$3, %r12d
	movq	%r10, 8(%rsp)                   ## 8-byte Spill
	movq	%r10, %rax
	movq	%r15, 80(%rsp)                  ## 8-byte Spill
	subq	%r15, %rax
	imulq	%r11, %rax
	movq	-96(%rsp), %r15                 ## 8-byte Reload
	addq	%rax, %r15
	leal	(%rdi,%r14,8), %edx
	movl	%edx, -120(%rsp)                ## 4-byte Spill
	andq	$-4, %rbp
	negq	%rbp
	movq	%rbp, 56(%rsp)                  ## 8-byte Spill
	movq	%r12, 16(%rsp)                  ## 8-byte Spill
	shlq	$5, %r12
	movq	%r13, %rdx
	addq	%r13, %rax
	movq	%rsi, %rcx
	movq	%rsi, -64(%rsp)                 ## 8-byte Spill
	andq	$-2, %rsi
	negq	%rsi
	movq	%rsi, -32(%rsp)                 ## 8-byte Spill
	movq	-88(%rsp), %rcx                 ## 8-byte Reload
	leaq	(%rcx,%rax,4), %r10
	addq	$96, %r10
	leaq	(%rcx,%rax,4), %r13
	vmovss	LCPI17_1(%rip), %xmm0           ## xmm0 = mem[0],zero,zero,zero
	vbroadcastss	LCPI17_1(%rip), %ymm1   ## ymm1 = [9.99999971E-10,9.99999971E-10,9.99999971E-10,9.99999971E-10,9.99999971E-10,9.99999971E-10,9.99999971E-10,9.99999971E-10]
	leaq	96(%rcx,%r15,4), %r9
	leaq	(,%r11,4), %rax
	movq	%rax, 104(%rsp)                 ## 8-byte Spill
	movq	-104(%rsp), %rax                ## 8-byte Reload
	leaq	96(%rax), %rax
	movq	%rax, 48(%rsp)                  ## 8-byte Spill
	movq	-112(%rsp), %rax                ## 8-byte Reload
	leaq	96(%rax), %rsi
	movq	%rsi, 32(%rsp)                  ## 8-byte Spill
	movq	%r15, -96(%rsp)                 ## 8-byte Spill
	leaq	(%rcx,%r15,4), %r14
	movq	%rdx, %rcx
	movq	%rdx, 40(%rsp)                  ## 8-byte Spill
	leaq	(%rax,%rdx,4), %rax
	movq	%rax, 64(%rsp)                  ## 8-byte Spill
	xorl	%r15d, %r15d
	jmp	LBB17_25
	.p2align	4, 0x90
LBB17_33:                               ## %after_bb2.us
                                        ##   in Loop: Header=BB17_25 Depth=1
	incq	%r15
	movq	104(%rsp), %rcx                 ## 8-byte Reload
	addq	%rcx, %r9
	movl	-80(%rsp), %eax                 ## 4-byte Reload
	addl	(%rsp), %eax                    ## 4-byte Folded Reload
	movl	%eax, -80(%rsp)                 ## 4-byte Spill
	movl	-128(%rsp), %eax                ## 4-byte Reload
	addl	%eax, -120(%rsp)                ## 4-byte Folded Spill
	movq	-96(%rsp), %rdx                 ## 8-byte Reload
	addq	-40(%rsp), %rdx                 ## 8-byte Folded Reload
	movq	%rdx, -96(%rsp)                 ## 8-byte Spill
	addq	%rcx, %r14
	addq	%rcx, %r10
	addl	%eax, %edi
	addq	%rcx, %r13
	cmpq	112(%rsp), %r15                 ## 8-byte Folded Reload
	je	LBB17_23
LBB17_25:                               ## %"for relu1_0_d_def__.s9.w.wi.rebased.us"
                                        ## =>This Loop Header: Depth=1
                                        ##     Child Loop BB17_45 Depth 2
                                        ##     Child Loop BB17_48 Depth 2
                                        ##     Child Loop BB17_28 Depth 2
                                        ##     Child Loop BB17_37 Depth 2
                                        ##     Child Loop BB17_32 Depth 2
	cmpl	$0, -124(%rsp)                  ## 4-byte Folded Reload
	movq	%rdi, -48(%rsp)                 ## 8-byte Spill
	jle	LBB17_29
## %bb.26:                              ## %"for relu1_0_d_def__.s9.n.ni5.preheader.us"
                                        ##   in Loop: Header=BB17_25 Depth=1
	movslq	-80(%rsp), %rdx                 ## 4-byte Folded Reload
	movslq	-120(%rsp), %rbp                ## 4-byte Folded Reload
	cmpl	$8, 96(%rsp)                    ## 4-byte Folded Reload
	jae	LBB17_42
## %bb.27:                              ##   in Loop: Header=BB17_25 Depth=1
	xorl	%eax, %eax
	jmp	LBB17_50
	.p2align	4, 0x90
LBB17_42:                               ## %vector.ph79
                                        ##   in Loop: Header=BB17_25 Depth=1
	cmpq	$24, 24(%rsp)                   ## 8-byte Folded Reload
	movq	%rdx, 72(%rsp)                  ## 8-byte Spill
	jae	LBB17_44
## %bb.43:                              ##   in Loop: Header=BB17_25 Depth=1
	xorl	%eax, %eax
	jmp	LBB17_46
	.p2align	4, 0x90
LBB17_44:                               ## %vector.body77.preheader
                                        ##   in Loop: Header=BB17_25 Depth=1
	movq	48(%rsp), %rax                  ## 8-byte Reload
	movq	%rbp, %rcx
	leaq	(%rax,%rdx,4), %r11
	movq	32(%rsp), %rax                  ## 8-byte Reload
	leaq	(%rax,%rbp,4), %rcx
	movq	56(%rsp), %rsi                  ## 8-byte Reload
	xorl	%eax, %eax
	.p2align	4, 0x90
LBB17_45:                               ## %vector.body77
                                        ##   Parent Loop BB17_25 Depth=1
                                        ## =>  This Inner Loop Header: Depth=2
	vmovups	-96(%r11,%rax,4), %ymm2
	vmulps	-96(%r9,%rax,4), %ymm2, %ymm2
	vfmadd213ps	-96(%rcx,%rax,4), %ymm1, %ymm2 ## ymm2 = (ymm1 * ymm2) + mem
	vmovups	%ymm2, -96(%rcx,%rax,4)
	vmovups	-64(%r11,%rax,4), %ymm2
	vmulps	-64(%r9,%rax,4), %ymm2, %ymm2
	vfmadd213ps	-64(%rcx,%rax,4), %ymm1, %ymm2 ## ymm2 = (ymm1 * ymm2) + mem
	vmovups	%ymm2, -64(%rcx,%rax,4)
	vmovups	-32(%r11,%rax,4), %ymm2
	vmulps	-32(%r9,%rax,4), %ymm2, %ymm2
	vfmadd213ps	-32(%rcx,%rax,4), %ymm1, %ymm2 ## ymm2 = (ymm1 * ymm2) + mem
	vmovups	%ymm2, -32(%rcx,%rax,4)
	vmovups	(%r11,%rax,4), %ymm2
	vmulps	(%r9,%rax,4), %ymm2, %ymm2
	vfmadd213ps	(%rcx,%rax,4), %ymm1, %ymm2 ## ymm2 = (ymm1 * ymm2) + mem
	vmovups	%ymm2, (%rcx,%rax,4)
	addq	$32, %rax
	addq	$4, %rsi
	jne	LBB17_45
LBB17_46:                               ## %middle.block75.unr-lcssa
                                        ##   in Loop: Header=BB17_25 Depth=1
	cmpq	$0, 16(%rsp)                    ## 8-byte Folded Reload
	movq	72(%rsp), %rdx                  ## 8-byte Reload
	je	LBB17_49
## %bb.47:                              ## %vector.body77.epil.preheader
                                        ##   in Loop: Header=BB17_25 Depth=1
	movq	-96(%rsp), %rcx                 ## 8-byte Reload
	addq	%rax, %rcx
	movq	-88(%rsp), %rsi                 ## 8-byte Reload
	leaq	(%rsi,%rcx,4), %rcx
	leaq	(%rax,%rdx), %rsi
	movq	-104(%rsp), %rdi                ## 8-byte Reload
	leaq	(%rdi,%rsi,4), %rsi
	addq	%rbp, %rax
	movq	-112(%rsp), %rdi                ## 8-byte Reload
	leaq	(%rdi,%rax,4), %rax
	xorl	%edi, %edi
	.p2align	4, 0x90
LBB17_48:                               ## %vector.body77.epil
                                        ##   Parent Loop BB17_25 Depth=1
                                        ## =>  This Inner Loop Header: Depth=2
	vmovups	(%rsi,%rdi), %ymm2
	vmulps	(%rcx,%rdi), %ymm2, %ymm2
	vfmadd213ps	(%rax,%rdi), %ymm1, %ymm2 ## ymm2 = (ymm1 * ymm2) + mem
	vmovups	%ymm2, (%rax,%rdi)
	addq	$32, %rdi
	cmpq	%rdi, %r12
	jne	LBB17_48
LBB17_49:                               ## %middle.block75
                                        ##   in Loop: Header=BB17_25 Depth=1
	movq	-8(%rsp), %rcx                  ## 8-byte Reload
	movq	%rcx, %rax
	cmpq	%rbx, %rcx
	movq	-48(%rsp), %rdi                 ## 8-byte Reload
	je	LBB17_29
LBB17_50:                               ## %"for relu1_0_d_def__.s9.n.ni5.us.preheader"
                                        ##   in Loop: Header=BB17_25 Depth=1
	movq	-104(%rsp), %rcx                ## 8-byte Reload
	leaq	(%rcx,%rdx,4), %rcx
	movq	-112(%rsp), %rsi                ## 8-byte Reload
	leaq	(%rsi,%rbp,4), %rsi
	.p2align	4, 0x90
LBB17_28:                               ## %"for relu1_0_d_def__.s9.n.ni5.us"
                                        ##   Parent Loop BB17_25 Depth=1
                                        ## =>  This Inner Loop Header: Depth=2
	vmovss	(%rcx,%rax,4), %xmm2            ## xmm2 = mem[0],zero,zero,zero
	vmulss	(%r14,%rax,4), %xmm2, %xmm2
	vfmadd213ss	(%rsi,%rax,4), %xmm0, %xmm2 ## xmm2 = (xmm0 * xmm2) + mem
	vmovss	%xmm2, (%rsi,%rax,4)
	incq	%rax
	cmpq	%rax, %rbx
	jne	LBB17_28
LBB17_29:                               ## %"end for relu1_0_d_def__.s9.n.ni6.us"
                                        ##   in Loop: Header=BB17_25 Depth=1
	cmpl	$0, -116(%rsp)                  ## 4-byte Folded Reload
	jle	LBB17_33
## %bb.30:                              ## %"for relu1_0_d_def__.s9.n.ni.rebased.preheader.us"
                                        ##   in Loop: Header=BB17_25 Depth=1
	movq	%r15, %rcx
	subq	80(%rsp), %rcx                  ## 8-byte Folded Reload
	movslq	%edi, %r11
	movq	120(%rsp), %rax                 ## 8-byte Reload
	addq	%rcx, %rax
	imulq	(%rsp), %rax                    ## 8-byte Folded Reload
	addq	88(%rsp), %rax                  ## 8-byte Folded Reload
	movq	-104(%rsp), %rsi                ## 8-byte Reload
	vmovss	-4(%rsi,%rax,4), %xmm2          ## xmm2 = mem[0],zero,zero,zero
	cmpl	$16, -116(%rsp)                 ## 4-byte Folded Reload
	jae	LBB17_34
## %bb.31:                              ##   in Loop: Header=BB17_25 Depth=1
	xorl	%eax, %eax
	jmp	LBB17_41
	.p2align	4, 0x90
LBB17_34:                               ## %vector.ph62
                                        ##   in Loop: Header=BB17_25 Depth=1
	vbroadcastss	%xmm2, %ymm3
	cmpq	$0, -16(%rsp)                   ## 8-byte Folded Reload
	je	LBB17_35
## %bb.36:                              ## %vector.body60.preheader
                                        ##   in Loop: Header=BB17_25 Depth=1
	movq	40(%rsp), %rax                  ## 8-byte Reload
	addq	%r11, %rax
	movq	32(%rsp), %rsi                  ## 8-byte Reload
	leaq	(%rsi,%rax,4), %rsi
	movq	-32(%rsp), %rdi                 ## 8-byte Reload
	xorl	%eax, %eax
	.p2align	4, 0x90
LBB17_37:                               ## %vector.body60
                                        ##   Parent Loop BB17_25 Depth=1
                                        ## =>  This Inner Loop Header: Depth=2
	vmulps	-96(%r10,%rax,4), %ymm3, %ymm4
	vmulps	-64(%r10,%rax,4), %ymm3, %ymm5
	vfmadd213ps	-96(%rsi,%rax,4), %ymm1, %ymm4 ## ymm4 = (ymm1 * ymm4) + mem
	vfmadd213ps	-64(%rsi,%rax,4), %ymm1, %ymm5 ## ymm5 = (ymm1 * ymm5) + mem
	vmovups	%ymm4, -96(%rsi,%rax,4)
	vmovups	%ymm5, -64(%rsi,%rax,4)
	vmulps	-32(%r10,%rax,4), %ymm3, %ymm4
	vmulps	(%r10,%rax,4), %ymm3, %ymm5
	vfmadd213ps	-32(%rsi,%rax,4), %ymm1, %ymm4 ## ymm4 = (ymm1 * ymm4) + mem
	vfmadd213ps	(%rsi,%rax,4), %ymm1, %ymm5 ## ymm5 = (ymm1 * ymm5) + mem
	vmovups	%ymm4, -32(%rsi,%rax,4)
	vmovups	%ymm5, (%rsi,%rax,4)
	addq	$32, %rax
	addq	$2, %rdi
	jne	LBB17_37
## %bb.38:                              ## %middle.block58.unr-lcssa
                                        ##   in Loop: Header=BB17_25 Depth=1
	testb	$1, -64(%rsp)                   ## 1-byte Folded Reload
	movq	-48(%rsp), %rdi                 ## 8-byte Reload
	je	LBB17_40
LBB17_39:                               ## %vector.body60.epil
                                        ##   in Loop: Header=BB17_25 Depth=1
	movq	-24(%rsp), %rsi                 ## 8-byte Reload
	addl	%ecx, %esi
	imull	-128(%rsp), %esi                ## 4-byte Folded Reload
	addl	-68(%rsp), %esi                 ## 4-byte Folded Reload
	addq	8(%rsp), %rcx                   ## 8-byte Folded Reload
	imulq	-40(%rsp), %rcx                 ## 8-byte Folded Reload
	movslq	%esi, %rsi
	addq	40(%rsp), %rax                  ## 8-byte Folded Reload
	addq	%rax, %rsi
	addq	%rcx, %rax
	movq	-88(%rsp), %rcx                 ## 8-byte Reload
	vmulps	(%rcx,%rax,4), %ymm3, %ymm4
	vmulps	32(%rcx,%rax,4), %ymm3, %ymm3
	movq	-112(%rsp), %rax                ## 8-byte Reload
	vfmadd213ps	(%rax,%rsi,4), %ymm1, %ymm4 ## ymm4 = (ymm1 * ymm4) + mem
	vfmadd213ps	32(%rax,%rsi,4), %ymm1, %ymm3 ## ymm3 = (ymm1 * ymm3) + mem
	vmovups	%ymm4, (%rax,%rsi,4)
	vmovups	%ymm3, 32(%rax,%rsi,4)
LBB17_40:                               ## %middle.block58
                                        ##   in Loop: Header=BB17_25 Depth=1
	movq	-56(%rsp), %rcx                 ## 8-byte Reload
	movq	%rcx, %rax
	cmpq	%r8, %rcx
	je	LBB17_33
LBB17_41:                               ## %"for relu1_0_d_def__.s9.n.ni.rebased.us.preheader"
                                        ##   in Loop: Header=BB17_25 Depth=1
	movq	64(%rsp), %rcx                  ## 8-byte Reload
	leaq	(%rcx,%r11,4), %rcx
	.p2align	4, 0x90
LBB17_32:                               ## %"for relu1_0_d_def__.s9.n.ni.rebased.us"
                                        ##   Parent Loop BB17_25 Depth=1
                                        ## =>  This Inner Loop Header: Depth=2
	vmulss	(%r13,%rax,4), %xmm2, %xmm3
	vfmadd213ss	(%rcx,%rax,4), %xmm0, %xmm3 ## xmm3 = (xmm0 * xmm3) + mem
	vmovss	%xmm3, (%rcx,%rax,4)
	incq	%rax
	cmpq	%rax, %r8
	jne	LBB17_32
	jmp	LBB17_33
LBB17_35:                               ##   in Loop: Header=BB17_25 Depth=1
	xorl	%eax, %eax
	testb	$1, -64(%rsp)                   ## 1-byte Folded Reload
	movq	-48(%rsp), %rdi                 ## 8-byte Reload
	jne	LBB17_39
	jmp	LBB17_40
LBB17_21:                               ## %"for relu1_0_d_def__.s9.w.wi.rebased.preheader23"
	vpbroadcastd	%xmm0, %ymm0
	vpor	LCPI17_0(%rip), %ymm0, %ymm0
	decl	%edx
	vmovd	%edx, %xmm1
	vpbroadcastd	%xmm1, %ymm1
	vpminsd	%ymm0, %ymm1, %ymm0
	movslq	%r9d, %rax
	movl	%r14d, %edx
	negq	%rdx
	subq	%rax, %r10
	imulq	%r11, %r10
	addq	-96(%rsp), %r10                 ## 8-byte Folded Reload
	movq	-88(%rsp), %rax                 ## 8-byte Reload
	leaq	(%rax,%r10,4), %rax
	shlq	$2, %r11
	movl	-32(%rsp), %ebp                 ## 4-byte Reload
	movl	%ebp, %ecx
	subl	-116(%rsp), %ecx                ## 4-byte Folded Reload
	subl	%r9d, %ecx
	movq	(%rsp), %r8                     ## 8-byte Reload
	imull	%r8d, %ecx
	subl	-120(%rsp), %ecx                ## 4-byte Folded Reload
	movl	-56(%rsp), %esi                 ## 4-byte Reload
	addl	%esi, %esi
	subl	%esi, %ebp
	subl	%r9d, %ebp
	shll	$5, %ebp
	orl	$24, %ebp
	imull	%ebp, %edi
	movq	-64(%rsp), %rsi                 ## 8-byte Reload
	leal	(%rdi,%rsi,8), %edi
	vbroadcastss	LCPI17_1(%rip), %ymm1   ## ymm1 = [9.99999971E-10,9.99999971E-10,9.99999971E-10,9.99999971E-10,9.99999971E-10,9.99999971E-10,9.99999971E-10,9.99999971E-10]
	movl	-128(%rsp), %r9d                ## 4-byte Reload
	movq	-104(%rsp), %rbx                ## 8-byte Reload
	.p2align	4, 0x90
LBB17_22:                               ## %"for relu1_0_d_def__.s9.w.wi.rebased"
                                        ## =>This Inner Loop Header: Depth=1
	vmovd	%ecx, %xmm2
	vpbroadcastd	%xmm2, %ymm2
	vpaddd	%ymm0, %ymm2, %ymm2
	vextracti128	$1, %ymm2, %xmm3
	vmovd	%xmm3, %esi
	movslq	%esi, %rsi
	vpextrd	$1, %xmm3, %ebp
	movslq	%ebp, %rbp
	vmovss	(%rbx,%rsi,4), %xmm4            ## xmm4 = mem[0],zero,zero,zero
	vpextrd	$2, %xmm3, %esi
	vinsertps	$16, (%rbx,%rbp,4), %xmm4, %xmm4 ## xmm4 = xmm4[0],mem[0],xmm4[2,3]
	vmovd	%xmm2, %ebp
	movslq	%esi, %rsi
	vinsertps	$32, (%rbx,%rsi,4), %xmm4, %xmm4 ## xmm4 = xmm4[0,1],mem[0],xmm4[3]
	movslq	%ebp, %rsi
	vpextrd	$3, %xmm3, %ebp
	movslq	%ebp, %rbp
	vinsertps	$48, (%rbx,%rbp,4), %xmm4, %xmm3 ## xmm3 = xmm4[0,1,2],mem[0]
	vpextrd	$1, %xmm2, %ebp
	vmovss	(%rbx,%rsi,4), %xmm4            ## xmm4 = mem[0],zero,zero,zero
	movslq	%ebp, %rsi
	vinsertps	$16, (%rbx,%rsi,4), %xmm4, %xmm4 ## xmm4 = xmm4[0],mem[0],xmm4[2,3]
	vpextrd	$2, %xmm2, %esi
	movslq	%esi, %rsi
	vinsertps	$32, (%rbx,%rsi,4), %xmm4, %xmm4 ## xmm4 = xmm4[0,1],mem[0],xmm4[3]
	vpextrd	$3, %xmm2, %esi
	movslq	%esi, %rsi
	vinsertps	$48, (%rbx,%rsi,4), %xmm4, %xmm2 ## xmm2 = xmm4[0,1,2],mem[0]
	movslq	%edi, %rdi
	vinsertf128	$1, %xmm3, %ymm2, %ymm2
	vmulps	(%rax), %ymm2, %ymm2
	movq	-112(%rsp), %rsi                ## 8-byte Reload
	vfmadd213ps	(%rsi,%rdi,4), %ymm1, %ymm2 ## ymm2 = (ymm1 * ymm2) + mem
	vmovaps	%ymm2, (%rsi,%rdi,4)
	addq	%r11, %rax
	addl	%r8d, %ecx
	addl	%r9d, %edi
	incq	%rdx
	jne	LBB17_22
LBB17_23:                               ## %destructor_block
	xorl	%eax, %eax
	addq	$128, %rsp
	popq	%rbx
	popq	%r12
	popq	%r13
	popq	%r14
	popq	%r15
	popq	%rbp
	vzeroupper
	retq
                                        ## -- End function
	.section	__TEXT,__const
	.p2align	5                               ## -- Begin function train_cost_model.par_for.relu1_0_d_def__.s10.n.n.n
LCPI18_0:
	.long	0                               ## 0x0
	.long	1                               ## 0x1
	.long	2                               ## 0x2
	.long	3                               ## 0x3
	.long	4                               ## 0x4
	.long	5                               ## 0x5
	.long	6                               ## 0x6
	.long	7                               ## 0x7
	.section	__TEXT,__literal4,4byte_literals
	.p2align	2
LCPI18_1:
	.long	0x3f800000                      ## float 1
LCPI18_2:
	.long	0x3089705f                      ## float 9.99999971E-10
	.section	__TEXT,__text,regular,pure_instructions
	.p2align	4, 0x90
_train_cost_model.par_for.relu1_0_d_def__.s10.n.n.n: ## @train_cost_model.par_for.relu1_0_d_def__.s10.n.n.n
## %bb.0:                               ## %entry
	pushq	%rbp
	pushq	%r15
	pushq	%r14
	pushq	%r13
	pushq	%r12
	pushq	%rbx
	subq	$248, %rsp
	movq	%rdx, %rdi
	movl	12(%rdx), %r11d
	movl	24(%rdx), %r9d
	movl	%esi, %r8d
	sarl	$31, %r8d
	xorl	%ecx, %ecx
	testl	%r9d, %r9d
	sete	%cl
	movl	%ecx, %ebx
	negl	%ebx
	movl	%r9d, %ebp
	sarl	$31, %ebp
	subl	%r8d, %esi
	orl	%r9d, %ebx
	movl	%esi, %eax
	cltd
	idivl	%ebx
	movl	%edx, %r13d
	movl	%ebp, %r14d
	leal	(%r9,%rcx), %ebx
	movl	%esi, %eax
	cltd
	idivl	%ebx
	notl	%r14d
	decl	%ecx
	movl	%r14d, %r10d
	subl	%ebp, %r10d
	andl	%r8d, %r10d
	addl	%eax, %r10d
	andl	%ecx, %r10d
	leal	(%r10,%r10), %eax
	movl	%eax, -88(%rsp)                 ## 4-byte Spill
	subl	%eax, %r11d
	cmpl	$2, %r11d
	movl	$2, %edx
	cmovll	%r11d, %edx
	xorl	%eax, %eax
	testl	%r11d, %r11d
	cmovlel	%eax, %edx
	movl	%edx, -96(%rsp)                 ## 4-byte Spill
	jle	LBB18_44
## %bb.1:                               ## %"for relu1_0_d_def__.s10.w.wi.preheader"
	movl	(%rdi), %edx
	movl	16(%rdi), %r11d
	movl	32(%rdi), %r12d
	movl	36(%rdi), %ebx
	movl	40(%rdi), %eax
	movl	44(%rdi), %r15d
	xorl	%r9d, %ebp
	addl	%r14d, %ebp
	andl	%r8d, %ebp
	addl	%ebp, %r13d
	andl	%ecx, %r13d
	movq	%r13, -104(%rsp)                ## 8-byte Spill
	leal	(,%r13,8), %esi
	movl	%ebx, -56(%rsp)                 ## 4-byte Spill
	movl	%ebx, %r8d
	subl	%r12d, %r8d
	movl	%eax, -32(%rsp)                 ## 4-byte Spill
	movl	%eax, %r9d
	subl	%r12d, %r9d
	movl	%r15d, -40(%rsp)                ## 4-byte Spill
	movl	%r15d, %r14d
	subl	%r12d, %r14d
	movslq	8(%rdi), %rcx
	movslq	%r10d, %rbp
	subq	%rcx, %rbp
	addq	%rbp, %rbp
	movq	%r11, -120(%rsp)                ## 8-byte Spill
	movl	%r11d, %eax
	shll	$5, %eax
	movl	%eax, -128(%rsp)                ## 4-byte Spill
	movl	%edx, %ebx
	subl	%esi, %ebx
	cmpl	$8, %ebx
	movl	$8, %ecx
	cmovll	%ebx, %ecx
	testl	%ebx, %ebx
	movl	$0, %r11d
	cmovgl	%ecx, %r11d
	movl	%esi, %eax
	subl	%edx, %eax
	movl	%eax, %ecx
	sarl	$31, %ecx
	andl	%eax, %ecx
	cmpl	$-8, %ecx
	movl	$-8, %r13d
	cmovgl	%ecx, %r13d
	movl	20(%rdi), %eax
	movl	%eax, -124(%rsp)                ## 4-byte Spill
	movl	28(%rdi), %eax
	movl	%eax, -112(%rsp)                ## 4-byte Spill
	movq	48(%rdi), %rcx
	movq	64(%rdi), %rax
	movq	%rax, -80(%rsp)                 ## 8-byte Spill
	movq	80(%rdi), %rax
	movq	%rax, -48(%rsp)                 ## 8-byte Spill
	movslq	4(%rdi), %r15
	leal	8(%rsi), %eax
	movslq	%esi, %rdi
	cmpl	%edx, %eax
	jle	LBB18_2
## %bb.4:                               ## %"for relu1_0_d_def__.s10.w.wi.preheader.split.us"
	movq	%r15, 24(%rsp)                  ## 8-byte Spill
	movq	%rdi, -24(%rsp)                 ## 8-byte Spill
	movq	%rcx, -64(%rsp)                 ## 8-byte Spill
	movq	-120(%rsp), %rax                ## 8-byte Reload
	leal	(%rax,%rax,4), %eax
	leal	(%rax,%rax,2), %eax
	movq	%rax, 16(%rsp)                  ## 8-byte Spill
	movl	%r10d, %eax
	subl	-112(%rsp), %eax                ## 4-byte Folded Reload
	addl	%eax, %eax
	movq	%rax, 80(%rsp)                  ## 8-byte Spill
	subl	%r12d, %edx
	movl	-56(%rsp), %ecx                 ## 4-byte Reload
	addl	%edx, %ecx
	movl	-32(%rsp), %edi                 ## 4-byte Reload
	addl	%edx, %edi
	addl	-40(%rsp), %edx                 ## 4-byte Folded Reload
	addl	%r11d, %esi
	addl	%r11d, %r13d
	movslq	%edx, %rax
	movq	%rax, 72(%rsp)                  ## 8-byte Spill
	movslq	%ecx, %rax
	movq	%rax, 64(%rsp)                  ## 8-byte Spill
	movslq	%edi, %rax
	movq	%rax, 56(%rsp)                  ## 8-byte Spill
	testl	%ebx, %ebx
	movl	%r13d, -68(%rsp)                ## 4-byte Spill
	movq	%rbp, 8(%rsp)                   ## 8-byte Spill
	jle	LBB18_30
## %bb.5:                               ## %"for relu1_0_d_def__.s10.w.wi.us.us.preheader"
	movq	-24(%rsp), %rax                 ## 8-byte Reload
	addl	%eax, %r14d
	addl	%eax, %r9d
	addl	%eax, %r8d
	movslq	%r14d, %rax
	movq	%rax, -8(%rsp)                  ## 8-byte Spill
	movslq	%r8d, %rax
	movq	%rax, -16(%rsp)                 ## 8-byte Spill
	movslq	%r9d, %r9
	movl	%r11d, 36(%rsp)                 ## 4-byte Spill
	movl	%r11d, %ecx
	movslq	%esi, %r8
	movl	%r13d, %r12d
	movslq	-88(%rsp), %rax                 ## 4-byte Folded Reload
	movq	%rax, 88(%rsp)                  ## 8-byte Spill
	movl	-96(%rsp), %eax                 ## 4-byte Reload
	movq	%rax, 240(%rsp)                 ## 8-byte Spill
	movq	%rcx, (%rsp)                    ## 8-byte Spill
	movl	%ecx, %eax
	andl	$2147483640, %eax               ## imm = 0x7FFFFFF8
	movq	%rax, 184(%rsp)                 ## 8-byte Spill
	addq	$-8, %rax
	movq	%rax, 176(%rsp)                 ## 8-byte Spill
	movq	%rax, %rdx
	shrq	$3, %rdx
	incq	%rdx
	movl	%r12d, %eax
	andl	$-16, %eax
	shll	$6, %r10d
	movl	-112(%rsp), %ecx                ## 4-byte Reload
	shll	$6, %ecx
	subl	%ecx, %r10d
	movq	%rax, 160(%rsp)                 ## 8-byte Spill
	addq	$-16, %rax
	orl	$15, %r10d
	movq	-120(%rsp), %rsi                ## 8-byte Reload
	imull	%r10d, %esi
	movq	%rax, 152(%rsp)                 ## 8-byte Spill
	movq	%rax, %rdi
	shrq	$4, %rdi
	incq	%rdi
	movq	24(%rsp), %r15                  ## 8-byte Reload
	movq	%r15, %rax
	imulq	%rbp, %rax
	movq	%rsi, -120(%rsp)                ## 8-byte Spill
	movq	-104(%rsp), %rcx                ## 8-byte Reload
	leal	(%rsi,%rcx,8), %ebx
	movq	%rdx, %rcx
	movq	%rdx, 168(%rsp)                 ## 8-byte Spill
	andq	$-2, %rdx
	negq	%rdx
	movq	%rdx, 136(%rsp)                 ## 8-byte Spill
	movq	-24(%rsp), %rdx                 ## 8-byte Reload
	leaq	(%rax,%rdx), %rcx
	movq	-64(%rsp), %rsi                 ## 8-byte Reload
	leaq	(%rsi,%rcx,4), %r14
	addq	$32, %r14
	leaq	(%rsi,%rcx,4), %r11
	addq	%r8, %rax
	movq	%rdi, %rcx
	movq	%rdi, 144(%rsp)                 ## 8-byte Spill
	andq	$-2, %rdi
	negq	%rdi
	movq	%rdi, 128(%rsp)                 ## 8-byte Spill
	movq	%r12, %rdi
	leaq	(%rsi,%rax,4), %r12
	addq	$96, %r12
	leaq	(%rsi,%rax,4), %r13
	vmovss	LCPI18_1(%rip), %xmm0           ## xmm0 = mem[0],zero,zero,zero
	vmovss	LCPI18_2(%rip), %xmm1           ## xmm1 = mem[0],zero,zero,zero
	vbroadcastss	LCPI18_2(%rip), %ymm2   ## ymm2 = [9.99999971E-10,9.99999971E-10,9.99999971E-10,9.99999971E-10,9.99999971E-10,9.99999971E-10,9.99999971E-10,9.99999971E-10]
	vbroadcastss	LCPI18_1(%rip), %ymm3   ## ymm3 = [1.0E+0,1.0E+0,1.0E+0,1.0E+0,1.0E+0,1.0E+0,1.0E+0,1.0E+0]
	movq	16(%rsp), %rax                  ## 8-byte Reload
	leal	(%rdx,%rax), %eax
	movq	%rax, 96(%rsp)                  ## 8-byte Spill
	movq	-48(%rsp), %rcx                 ## 8-byte Reload
	leaq	32(%rcx), %rax
	movq	%rax, 120(%rsp)                 ## 8-byte Spill
	leaq	(,%r15,4), %rax
	movq	%rax, 232(%rsp)                 ## 8-byte Spill
	movq	-80(%rsp), %r10                 ## 8-byte Reload
	leaq	32(%r10), %rax
	movq	%rax, 112(%rsp)                 ## 8-byte Spill
	movq	%r9, 48(%rsp)                   ## 8-byte Spill
	leaq	(%rcx,%r9,4), %rax
	movq	%rax, 216(%rsp)                 ## 8-byte Spill
	movq	-16(%rsp), %rax                 ## 8-byte Reload
	leaq	(%rcx,%rax,4), %rax
	movq	%rax, 208(%rsp)                 ## 8-byte Spill
	movq	-8(%rsp), %rax                  ## 8-byte Reload
	leaq	(%rcx,%rax,4), %rax
	movq	%rax, 200(%rsp)                 ## 8-byte Spill
	leaq	96(%r10), %rax
	movq	%rax, 104(%rsp)                 ## 8-byte Spill
	movq	%r8, 40(%rsp)                   ## 8-byte Spill
	leaq	(%r10,%r8,4), %rax
	movq	%rax, 192(%rsp)                 ## 8-byte Spill
	movq	88(%rsp), %rdx                  ## 8-byte Reload
	xorl	%ebp, %ebp
	movl	-128(%rsp), %r9d                ## 4-byte Reload
	movl	-124(%rsp), %ecx                ## 4-byte Reload
	jmp	LBB18_6
	.p2align	4, 0x90
LBB18_21:                               ## %after_bb.us.us
                                        ##   in Loop: Header=BB18_6 Depth=1
	movq	-96(%rsp), %rbp                 ## 8-byte Reload
	incq	%rbp
	movq	-88(%rsp), %rdx                 ## 8-byte Reload
	incq	%rdx
	movq	232(%rsp), %rax                 ## 8-byte Reload
	addq	%rax, %r14
	movl	-104(%rsp), %ebx                ## 4-byte Reload
	addl	%r9d, %ebx
	addq	%rax, %r11
	addq	%rax, %r12
	addl	%r9d, %r10d
	movq	%r10, -120(%rsp)                ## 8-byte Spill
	addq	%rax, %r13
	cmpq	240(%rsp), %rbp                 ## 8-byte Folded Reload
	movl	-124(%rsp), %ecx                ## 4-byte Reload
	movq	24(%rsp), %r15                  ## 8-byte Reload
	je	LBB18_44
LBB18_6:                                ## %"for relu1_0_d_def__.s10.w.wi.us.us"
                                        ## =>This Loop Header: Depth=1
                                        ##     Child Loop BB18_11 Depth 2
                                        ##     Child Loop BB18_16 Depth 2
                                        ##     Child Loop BB18_25 Depth 2
                                        ##     Child Loop BB18_20 Depth 2
	testq	%rdx, %rdx
	movl	$0, %eax
	cmovgl	%edx, %eax
	imull	%ecx, %eax
	movslq	%eax, %r8
	movq	88(%rsp), %rax                  ## 8-byte Reload
	addq	%rbp, %rax
	testq	%rax, %rax
	movl	$0, %esi
	cmovlel	%esi, %eax
	imull	%ecx, %eax
	movq	80(%rsp), %rcx                  ## 8-byte Reload
	addl	%ebp, %ecx
	imull	%r9d, %ecx
	movq	%rcx, -40(%rsp)                 ## 8-byte Spill
	movq	8(%rsp), %rcx                   ## 8-byte Reload
	addq	%rbp, %rcx
	imulq	%r15, %rcx
	movq	%rcx, -32(%rsp)                 ## 8-byte Spill
	movslq	%ebx, %rsi
	cltq
	movq	%rax, -112(%rsp)                ## 8-byte Spill
	cmpl	$8, 36(%rsp)                    ## 4-byte Folded Reload
	movl	%ebx, -104(%rsp)                ## 4-byte Spill
	movq	%rdx, -88(%rsp)                 ## 8-byte Spill
	movq	%rbp, -96(%rsp)                 ## 8-byte Spill
	movq	%rsi, -56(%rsp)                 ## 8-byte Spill
	jae	LBB18_8
## %bb.7:                               ##   in Loop: Header=BB18_6 Depth=1
	xorl	%eax, %eax
	movq	-120(%rsp), %r10                ## 8-byte Reload
	movq	-64(%rsp), %r15                 ## 8-byte Reload
	movq	(%rsp), %rbp                    ## 8-byte Reload
	jmp	LBB18_15
	.p2align	4, 0x90
LBB18_8:                                ## %vector.ph74
                                        ##   in Loop: Header=BB18_6 Depth=1
	cmpq	$0, 176(%rsp)                   ## 8-byte Folded Reload
	movq	%r8, 224(%rsp)                  ## 8-byte Spill
	je	LBB18_9
## %bb.10:                              ## %vector.body72.preheader
                                        ##   in Loop: Header=BB18_6 Depth=1
	movq	48(%rsp), %rax                  ## 8-byte Reload
	addq	%r8, %rax
	movq	-16(%rsp), %rcx                 ## 8-byte Reload
	leaq	(%rcx,%r8), %rcx
	movq	-8(%rsp), %rdx                  ## 8-byte Reload
	addq	%r8, %rdx
	movq	120(%rsp), %rbx                 ## 8-byte Reload
	leaq	(%rbx,%rax,4), %r8
	leaq	(%rbx,%rcx,4), %rax
	leaq	(%rbx,%rdx,4), %r10
	movq	112(%rsp), %rcx                 ## 8-byte Reload
	leaq	(%rcx,%rsi,4), %r15
	movq	136(%rsp), %rdx                 ## 8-byte Reload
	xorl	%r9d, %r9d
	movq	(%rsp), %rbp                    ## 8-byte Reload
	.p2align	4, 0x90
LBB18_11:                               ## %vector.body72
                                        ##   Parent Loop BB18_6 Depth=1
                                        ## =>  This Inner Loop Header: Depth=2
	vmovups	-32(%rax,%r9,4), %ymm4
	vmulps	-32(%r8,%r9,4), %ymm4, %ymm4
	vmaxps	%ymm3, %ymm4, %ymm4
	vmulps	-32(%r10,%r9,4), %ymm4, %ymm4
	vmulps	-32(%r14,%r9,4), %ymm4, %ymm4
	vfmadd213ps	-32(%r15,%r9,4), %ymm2, %ymm4 ## ymm4 = (ymm2 * ymm4) + mem
	vmovups	%ymm4, -32(%r15,%r9,4)
	vmovups	(%rax,%r9,4), %ymm4
	vmulps	(%r8,%r9,4), %ymm4, %ymm4
	vmaxps	%ymm3, %ymm4, %ymm4
	vmulps	(%r10,%r9,4), %ymm4, %ymm4
	vmulps	(%r14,%r9,4), %ymm4, %ymm4
	vfmadd213ps	(%r15,%r9,4), %ymm2, %ymm4 ## ymm4 = (ymm2 * ymm4) + mem
	vmovups	%ymm4, (%r15,%r9,4)
	addq	$16, %r9
	addq	$2, %rdx
	jne	LBB18_11
## %bb.12:                              ## %middle.block70.unr-lcssa
                                        ##   in Loop: Header=BB18_6 Depth=1
	testb	$1, 168(%rsp)                   ## 1-byte Folded Reload
	movq	-64(%rsp), %r15                 ## 8-byte Reload
	je	LBB18_14
LBB18_13:                               ## %vector.body72.epil
                                        ##   in Loop: Header=BB18_6 Depth=1
	movq	96(%rsp), %rax                  ## 8-byte Reload
	movq	-40(%rsp), %rcx                 ## 8-byte Reload
	addl	%ecx, %eax
	movq	-32(%rsp), %rcx                 ## 8-byte Reload
	movq	-24(%rsp), %rdx                 ## 8-byte Reload
	addq	%rdx, %rcx
	cltq
	addq	%r9, %rax
	addq	%r9, %rcx
	addq	-112(%rsp), %r9                 ## 8-byte Folded Reload
	movq	-16(%rsp), %rdx                 ## 8-byte Reload
	addq	%r9, %rdx
	movq	-48(%rsp), %rsi                 ## 8-byte Reload
	vmovups	(%rsi,%rdx,4), %ymm4
	movq	-8(%rsp), %rdx                  ## 8-byte Reload
	addq	%r9, %rdx
	addq	48(%rsp), %r9                   ## 8-byte Folded Reload
	vmulps	(%rsi,%r9,4), %ymm4, %ymm4
	vmaxps	%ymm3, %ymm4, %ymm4
	vmulps	(%rsi,%rdx,4), %ymm4, %ymm4
	vmulps	(%r15,%rcx,4), %ymm4, %ymm4
	movq	-80(%rsp), %rcx                 ## 8-byte Reload
	vfmadd213ps	(%rcx,%rax,4), %ymm2, %ymm4 ## ymm4 = (ymm2 * ymm4) + mem
	vmovups	%ymm4, (%rcx,%rax,4)
LBB18_14:                               ## %middle.block70
                                        ##   in Loop: Header=BB18_6 Depth=1
	movq	184(%rsp), %rcx                 ## 8-byte Reload
	movq	%rcx, %rax
	cmpq	%rbp, %rcx
	movl	-128(%rsp), %r9d                ## 4-byte Reload
	movq	-120(%rsp), %r10                ## 8-byte Reload
	movq	224(%rsp), %r8                  ## 8-byte Reload
	je	LBB18_17
LBB18_15:                               ## %"for relu1_0_d_def__.s10.n.ni.us.us.preheader"
                                        ##   in Loop: Header=BB18_6 Depth=1
	movq	216(%rsp), %rcx                 ## 8-byte Reload
	leaq	(%rcx,%r8,4), %rdx
	movq	208(%rsp), %rcx                 ## 8-byte Reload
	leaq	(%rcx,%r8,4), %rbx
	movq	200(%rsp), %rcx                 ## 8-byte Reload
	leaq	(%rcx,%r8,4), %rcx
	movq	-80(%rsp), %rsi                 ## 8-byte Reload
	movq	%rdi, %r8
	movq	-56(%rsp), %rdi                 ## 8-byte Reload
	leaq	(%rsi,%rdi,4), %rsi
	movq	%r8, %rdi
	.p2align	4, 0x90
LBB18_16:                               ## %"for relu1_0_d_def__.s10.n.ni.us.us"
                                        ##   Parent Loop BB18_6 Depth=1
                                        ## =>  This Inner Loop Header: Depth=2
	vmovss	(%rbx,%rax,4), %xmm4            ## xmm4 = mem[0],zero,zero,zero
	vmulss	(%rdx,%rax,4), %xmm4, %xmm4
	vmaxss	%xmm0, %xmm4, %xmm4
	vmulss	(%rcx,%rax,4), %xmm4, %xmm4
	vmulss	(%r11,%rax,4), %xmm4, %xmm4
	vfmadd213ss	(%rsi,%rax,4), %xmm1, %xmm4 ## xmm4 = (xmm1 * xmm4) + mem
	vmovss	%xmm4, (%rsi,%rax,4)
	incq	%rax
	cmpq	%rax, %rbp
	jne	LBB18_16
LBB18_17:                               ## %"end for relu1_0_d_def__.s10.n.ni.loopexit.us.us"
                                        ##   in Loop: Header=BB18_6 Depth=1
	movl	-68(%rsp), %esi                 ## 4-byte Reload
	testl	%esi, %esi
	jle	LBB18_21
## %bb.18:                              ## %"for relu1_0_d_def__.s10.n.ni.rebased.preheader.us.us"
                                        ##   in Loop: Header=BB18_6 Depth=1
	movslq	%r10d, %r8
	movq	72(%rsp), %rax                  ## 8-byte Reload
	movq	-112(%rsp), %rbx                ## 8-byte Reload
	addq	%rbx, %rax
	movq	64(%rsp), %rcx                  ## 8-byte Reload
	addq	%rbx, %rcx
	addq	56(%rsp), %rbx                  ## 8-byte Folded Reload
	movq	-48(%rsp), %rdx                 ## 8-byte Reload
	vmovss	-4(%rdx,%rcx,4), %xmm4          ## xmm4 = mem[0],zero,zero,zero
	vmulss	-4(%rdx,%rbx,4), %xmm4, %xmm4
	vmaxss	%xmm0, %xmm4, %xmm4
	vmulss	-4(%rdx,%rax,4), %xmm4, %xmm4
	cmpl	$16, %esi
	movq	16(%rsp), %rcx                  ## 8-byte Reload
	jae	LBB18_22
## %bb.19:                              ##   in Loop: Header=BB18_6 Depth=1
	xorl	%eax, %eax
	jmp	LBB18_29
	.p2align	4, 0x90
LBB18_22:                               ## %vector.ph55
                                        ##   in Loop: Header=BB18_6 Depth=1
	vbroadcastss	%xmm4, %ymm5
	cmpq	$0, 152(%rsp)                   ## 8-byte Folded Reload
	je	LBB18_23
## %bb.24:                              ## %vector.body53.preheader
                                        ##   in Loop: Header=BB18_6 Depth=1
	movq	40(%rsp), %rax                  ## 8-byte Reload
	addq	%r8, %rax
	movq	104(%rsp), %rdx                 ## 8-byte Reload
	leaq	(%rdx,%rax,4), %rax
	movq	128(%rsp), %rdx                 ## 8-byte Reload
	xorl	%esi, %esi
	.p2align	4, 0x90
LBB18_25:                               ## %vector.body53
                                        ##   Parent Loop BB18_6 Depth=1
                                        ## =>  This Inner Loop Header: Depth=2
	vmulps	-96(%r12,%rsi,4), %ymm5, %ymm6
	vmulps	-64(%r12,%rsi,4), %ymm5, %ymm7
	vfmadd213ps	-96(%rax,%rsi,4), %ymm2, %ymm6 ## ymm6 = (ymm2 * ymm6) + mem
	vfmadd213ps	-64(%rax,%rsi,4), %ymm2, %ymm7 ## ymm7 = (ymm2 * ymm7) + mem
	vmovups	%ymm6, -96(%rax,%rsi,4)
	vmovups	%ymm7, -64(%rax,%rsi,4)
	vmulps	-32(%r12,%rsi,4), %ymm5, %ymm6
	vmulps	(%r12,%rsi,4), %ymm5, %ymm7
	vfmadd213ps	-32(%rax,%rsi,4), %ymm2, %ymm6 ## ymm6 = (ymm2 * ymm6) + mem
	vfmadd213ps	(%rax,%rsi,4), %ymm2, %ymm7 ## ymm7 = (ymm2 * ymm7) + mem
	vmovups	%ymm6, -32(%rax,%rsi,4)
	vmovups	%ymm7, (%rax,%rsi,4)
	addq	$32, %rsi
	addq	$2, %rdx
	jne	LBB18_25
## %bb.26:                              ## %middle.block51.unr-lcssa
                                        ##   in Loop: Header=BB18_6 Depth=1
	testb	$1, 144(%rsp)                   ## 1-byte Folded Reload
	je	LBB18_28
LBB18_27:                               ## %vector.body53.epil
                                        ##   in Loop: Header=BB18_6 Depth=1
	movq	-40(%rsp), %rax                 ## 8-byte Reload
	addl	%ecx, %eax
	cltq
	addq	40(%rsp), %rsi                  ## 8-byte Folded Reload
	addq	%rsi, %rax
	addq	-32(%rsp), %rsi                 ## 8-byte Folded Reload
	vmulps	(%r15,%rsi,4), %ymm5, %ymm6
	vmulps	32(%r15,%rsi,4), %ymm5, %ymm5
	movq	-80(%rsp), %rcx                 ## 8-byte Reload
	vfmadd213ps	(%rcx,%rax,4), %ymm2, %ymm6 ## ymm6 = (ymm2 * ymm6) + mem
	vfmadd213ps	32(%rcx,%rax,4), %ymm2, %ymm5 ## ymm5 = (ymm2 * ymm5) + mem
	vmovups	%ymm6, (%rcx,%rax,4)
	vmovups	%ymm5, 32(%rcx,%rax,4)
LBB18_28:                               ## %middle.block51
                                        ##   in Loop: Header=BB18_6 Depth=1
	movq	160(%rsp), %rcx                 ## 8-byte Reload
	movq	%rcx, %rax
	cmpq	%rdi, %rcx
	je	LBB18_21
LBB18_29:                               ## %"for relu1_0_d_def__.s10.n.ni.rebased.us.us.preheader"
                                        ##   in Loop: Header=BB18_6 Depth=1
	movq	192(%rsp), %rcx                 ## 8-byte Reload
	leaq	(%rcx,%r8,4), %rcx
	.p2align	4, 0x90
LBB18_20:                               ## %"for relu1_0_d_def__.s10.n.ni.rebased.us.us"
                                        ##   Parent Loop BB18_6 Depth=1
                                        ## =>  This Inner Loop Header: Depth=2
	vmulss	(%r13,%rax,4), %xmm4, %xmm5
	vfmadd213ss	(%rcx,%rax,4), %xmm1, %xmm5 ## xmm5 = (xmm1 * xmm5) + mem
	vmovss	%xmm5, (%rcx,%rax,4)
	incq	%rax
	cmpq	%rax, %rdi
	jne	LBB18_20
	jmp	LBB18_21
LBB18_9:                                ##   in Loop: Header=BB18_6 Depth=1
	xorl	%r9d, %r9d
	movq	(%rsp), %rbp                    ## 8-byte Reload
	testb	$1, 168(%rsp)                   ## 1-byte Folded Reload
	movq	-64(%rsp), %r15                 ## 8-byte Reload
	jne	LBB18_13
	jmp	LBB18_14
LBB18_23:                               ##   in Loop: Header=BB18_6 Depth=1
	xorl	%esi, %esi
	testb	$1, 144(%rsp)                   ## 1-byte Folded Reload
	jne	LBB18_27
	jmp	LBB18_28
LBB18_2:                                ## %"for relu1_0_d_def__.s10.w.wi.preheader21"
	decl	%edx
	vmovd	%edi, %xmm0
	vpbroadcastd	%xmm0, %ymm0
	vpor	LCPI18_0(%rip), %ymm0, %ymm0
	vmovd	%edx, %xmm1
	vpbroadcastd	%xmm1, %ymm1
	vpminsd	%ymm0, %ymm1, %ymm10
	movslq	-88(%rsp), %rax                 ## 4-byte Folded Reload
	movl	-96(%rsp), %r11d                ## 4-byte Reload
	imulq	%r15, %rbp
	addq	%rdi, %rbp
	leaq	(%rcx,%rbp,4), %r12
	shlq	$2, %r15
	shll	$6, %r10d
	movl	-112(%rsp), %ecx                ## 4-byte Reload
	shll	$6, %ecx
	subl	%ecx, %r10d
	orl	$15, %r10d
	movq	-120(%rsp), %rcx                ## 8-byte Reload
	imull	%r10d, %ecx
	movq	-104(%rsp), %rdx                ## 8-byte Reload
	leal	(%rcx,%rdx,8), %edx
	vbroadcastss	LCPI18_1(%rip), %ymm8   ## ymm8 = [1.0E+0,1.0E+0,1.0E+0,1.0E+0,1.0E+0,1.0E+0,1.0E+0,1.0E+0]
	vbroadcastss	LCPI18_2(%rip), %ymm9   ## ymm9 = [9.99999971E-10,9.99999971E-10,9.99999971E-10,9.99999971E-10,9.99999971E-10,9.99999971E-10,9.99999971E-10,9.99999971E-10]
	movl	-128(%rsp), %r10d               ## 4-byte Reload
	movq	%r15, %r13
	movl	-124(%rsp), %r15d               ## 4-byte Reload
	movq	-48(%rsp), %rbx                 ## 8-byte Reload
	.p2align	4, 0x90
LBB18_3:                                ## %"for relu1_0_d_def__.s10.w.wi"
                                        ## =>This Inner Loop Header: Depth=1
	testq	%rax, %rax
	movl	$0, %esi
	cmovgl	%eax, %esi
	imull	%r15d, %esi
	leal	(%rsi,%r14), %ecx
	vmovd	%ecx, %xmm3
	vpbroadcastd	%xmm3, %ymm3
	vpaddd	%ymm3, %ymm10, %ymm3
	vmovd	%xmm3, %ecx
	vpextrd	$1, %xmm3, %ebp
	movslq	%ecx, %rcx
	movslq	%ebp, %rbp
	vpextrd	$2, %xmm3, %edi
	movslq	%edi, %rdi
	vmovss	(%rbx,%rcx,4), %xmm5            ## xmm5 = mem[0],zero,zero,zero
	vpextrd	$3, %xmm3, %ecx
	movslq	%ecx, %rcx
	vextracti128	$1, %ymm3, %xmm4
	vinsertps	$16, (%rbx,%rbp,4), %xmm5, %xmm3 ## xmm3 = xmm5[0],mem[0],xmm5[2,3]
	vmovd	%xmm4, %ebp
	vinsertps	$32, (%rbx,%rdi,4), %xmm3, %xmm3 ## xmm3 = xmm3[0,1],mem[0],xmm3[3]
	leal	(%rsi,%r8), %edi
	movslq	%ebp, %rbp
	vmovd	%edi, %xmm5
	vpbroadcastd	%xmm5, %ymm5
	vpaddd	%ymm5, %ymm10, %ymm5
	vmovss	(%rbx,%rbp,4), %xmm6            ## xmm6 = mem[0],zero,zero,zero
	vmovd	%xmm5, %edi
	movslq	%edi, %rdi
	vinsertps	$48, (%rbx,%rcx,4), %xmm3, %xmm3 ## xmm3 = xmm3[0,1,2],mem[0]
	vpextrd	$1, %xmm5, %ecx
	vpextrd	$1, %xmm4, %ebp
	vmovss	(%rbx,%rdi,4), %xmm7            ## xmm7 = mem[0],zero,zero,zero
	movslq	%ecx, %rcx
	vextracti128	$1, %ymm5, %xmm1
	movslq	%ebp, %rdi
	vinsertps	$16, (%rbx,%rcx,4), %xmm7, %xmm7 ## xmm7 = xmm7[0],mem[0],xmm7[2,3]
	vinsertps	$16, (%rbx,%rdi,4), %xmm6, %xmm6 ## xmm6 = xmm6[0],mem[0],xmm6[2,3]
	vmovd	%xmm1, %ecx
	vpextrd	$1, %xmm1, %edi
	movslq	%ecx, %rcx
	vmovss	(%rbx,%rcx,4), %xmm2            ## xmm2 = mem[0],zero,zero,zero
	movslq	%edi, %rcx
	vinsertps	$16, (%rbx,%rcx,4), %xmm2, %xmm2 ## xmm2 = xmm2[0],mem[0],xmm2[2,3]
	vpextrd	$2, %xmm5, %ecx
	movslq	%ecx, %rcx
	vinsertps	$32, (%rbx,%rcx,4), %xmm7, %xmm7 ## xmm7 = xmm7[0,1],mem[0],xmm7[3]
	vpextrd	$2, %xmm4, %ecx
	movslq	%ecx, %rcx
	vinsertps	$32, (%rbx,%rcx,4), %xmm6, %xmm6 ## xmm6 = xmm6[0,1],mem[0],xmm6[3]
	vpextrd	$3, %xmm5, %ecx
	movslq	%ecx, %rcx
	vinsertps	$48, (%rbx,%rcx,4), %xmm7, %xmm5 ## xmm5 = xmm7[0,1,2],mem[0]
	vpextrd	$2, %xmm1, %ecx
	movslq	%ecx, %rcx
	vinsertps	$32, (%rbx,%rcx,4), %xmm2, %xmm2 ## xmm2 = xmm2[0,1],mem[0],xmm2[3]
	addl	%r9d, %esi
	vmovd	%esi, %xmm7
	vpextrd	$3, %xmm4, %ecx
	movslq	%ecx, %rcx
	vpbroadcastd	%xmm7, %ymm4
	vinsertps	$48, (%rbx,%rcx,4), %xmm6, %xmm6 ## xmm6 = xmm6[0,1,2],mem[0]
	vpaddd	%ymm4, %ymm10, %ymm4
	vextracti128	$1, %ymm4, %xmm7
	vmovd	%xmm7, %ecx
	movslq	%ecx, %rcx
	vmovss	(%rbx,%rcx,4), %xmm0            ## xmm0 = mem[0],zero,zero,zero
	vpextrd	$1, %xmm7, %ecx
	movslq	%ecx, %rcx
	vinsertps	$16, (%rbx,%rcx,4), %xmm0, %xmm0 ## xmm0 = xmm0[0],mem[0],xmm0[2,3]
	vpextrd	$3, %xmm1, %ecx
	movslq	%ecx, %rcx
	vinsertps	$48, (%rbx,%rcx,4), %xmm2, %xmm1 ## xmm1 = xmm2[0,1,2],mem[0]
	vpextrd	$2, %xmm7, %ecx
	movslq	%ecx, %rcx
	vinsertps	$32, (%rbx,%rcx,4), %xmm0, %xmm0 ## xmm0 = xmm0[0,1],mem[0],xmm0[3]
	vmovd	%xmm4, %ecx
	movslq	%ecx, %rcx
	vmovss	(%rbx,%rcx,4), %xmm2            ## xmm2 = mem[0],zero,zero,zero
	vpextrd	$1, %xmm4, %ecx
	movslq	%ecx, %rcx
	vinsertps	$16, (%rbx,%rcx,4), %xmm2, %xmm2 ## xmm2 = xmm2[0],mem[0],xmm2[2,3]
	vpextrd	$3, %xmm7, %ecx
	movslq	%ecx, %rcx
	vinsertps	$48, (%rbx,%rcx,4), %xmm0, %xmm0 ## xmm0 = xmm0[0,1,2],mem[0]
	vpextrd	$2, %xmm4, %ecx
	movslq	%ecx, %rcx
	vinsertps	$32, (%rbx,%rcx,4), %xmm2, %xmm2 ## xmm2 = xmm2[0,1],mem[0],xmm2[3]
	vpextrd	$3, %xmm4, %ecx
	movslq	%ecx, %rcx
	vinsertps	$48, (%rbx,%rcx,4), %xmm2, %xmm2 ## xmm2 = xmm2[0,1,2],mem[0]
	vinsertf128	$1, %xmm6, %ymm3, %ymm3
	vinsertf128	$1, %xmm1, %ymm5, %ymm1
	vinsertf128	$1, %xmm0, %ymm2, %ymm0
	vmulps	%ymm0, %ymm1, %ymm0
	vmaxps	%ymm8, %ymm0, %ymm0
	vmulps	%ymm0, %ymm3, %ymm0
	vmulps	(%r12), %ymm0, %ymm0
	movslq	%edx, %rdx
	movq	-80(%rsp), %rcx                 ## 8-byte Reload
	vfmadd213ps	(%rcx,%rdx,4), %ymm9, %ymm0 ## ymm0 = (ymm9 * ymm0) + mem
	vmovups	%ymm0, (%rcx,%rdx,4)
	addq	%r13, %r12
	incq	%rax
	addl	%r10d, %edx
	decq	%r11
	jne	LBB18_3
LBB18_44:                               ## %destructor_block
	xorl	%eax, %eax
	addq	$248, %rsp
	popq	%rbx
	popq	%r12
	popq	%r13
	popq	%r14
	popq	%r15
	popq	%rbp
	vzeroupper
	retq
LBB18_30:                               ## %"for relu1_0_d_def__.s10.w.wi.preheader.split.us.split"
	movq	-120(%rsp), %r12                ## 8-byte Reload
	testl	%r13d, %r13d
	movl	-124(%rsp), %ebx                ## 4-byte Reload
	movq	24(%rsp), %rdx                  ## 8-byte Reload
	jle	LBB18_44
## %bb.31:                              ## %"for relu1_0_d_def__.s10.w.wi.us.us4.preheader"
	movslq	%esi, %rcx
	movq	%rcx, -104(%rsp)                ## 8-byte Spill
	movl	-68(%rsp), %ebp                 ## 4-byte Reload
	movslq	-88(%rsp), %r8                  ## 4-byte Folded Reload
	movl	-96(%rsp), %r15d                ## 4-byte Reload
	movl	%ebp, %r13d
	andl	$-16, %r13d
	leaq	-16(%r13), %rax
	movq	%rax, -88(%rsp)                 ## 8-byte Spill
	movq	%rax, %rsi
	shrq	$4, %rsi
	incq	%rsi
	movq	%rdx, %rdi
	imulq	8(%rsp), %rdi                   ## 8-byte Folded Reload
	addq	%rcx, %rdi
	movq	-64(%rsp), %r14                 ## 8-byte Reload
	leaq	(%r14,%rdi,4), %rax
	addq	$96, %rax
	leaq	(,%rdx,4), %r11
	movq	-80(%rsp), %r9                  ## 8-byte Reload
	leaq	96(%r9), %rcx
	movq	%rcx, -96(%rsp)                 ## 8-byte Spill
	shll	$6, %r10d
	movl	-112(%rsp), %ecx                ## 4-byte Reload
	shll	$6, %ecx
	subl	%ecx, %r10d
	orl	$15, %r10d
	imull	%r10d, %r12d
	movq	%rsi, %rcx
	movq	%rsi, -120(%rsp)                ## 8-byte Spill
	andq	$-2, %rsi
	negq	%rsi
	movq	%rsi, -56(%rsp)                 ## 8-byte Spill
	leaq	(%r14,%rdi,4), %rdi
	movq	-104(%rsp), %rcx                ## 8-byte Reload
	leaq	(%r9,%rcx,4), %rcx
	movq	%rcx, -112(%rsp)                ## 8-byte Spill
	vmovss	LCPI18_1(%rip), %xmm0           ## xmm0 = mem[0],zero,zero,zero
	vmovss	LCPI18_2(%rip), %xmm1           ## xmm1 = mem[0],zero,zero,zero
	vbroadcastss	LCPI18_2(%rip), %ymm2   ## ymm2 = [9.99999971E-10,9.99999971E-10,9.99999971E-10,9.99999971E-10,9.99999971E-10,9.99999971E-10,9.99999971E-10,9.99999971E-10]
	xorl	%r9d, %r9d
	jmp	LBB18_32
LBB18_43:                               ## %after_bb.loopexit.us.us18
                                        ##   in Loop: Header=BB18_32 Depth=1
	incq	%r9
	addq	%r11, %rax
	addl	-128(%rsp), %r12d               ## 4-byte Folded Reload
	addq	%r11, %rdi
	cmpq	%r15, %r9
	movl	-124(%rsp), %ebx                ## 4-byte Reload
	movq	%r10, %rdx
	je	LBB18_44
LBB18_32:                               ## %"for relu1_0_d_def__.s10.w.wi.us.us4"
                                        ## =>This Loop Header: Depth=1
                                        ##     Child Loop BB18_37 Depth 2
                                        ##     Child Loop BB18_42 Depth 2
	movq	%rdx, %r10
	movslq	%r12d, %r14
	leaq	(%r9,%r8), %rcx
	testq	%rcx, %rcx
	movl	$0, %edx
	cmovlel	%edx, %ecx
	imull	%ebx, %ecx
	movslq	%ecx, %rcx
	movq	72(%rsp), %rdx                  ## 8-byte Reload
	addq	%rcx, %rdx
	movq	64(%rsp), %rsi                  ## 8-byte Reload
	addq	%rcx, %rsi
	addq	56(%rsp), %rcx                  ## 8-byte Folded Reload
	movq	-48(%rsp), %rbx                 ## 8-byte Reload
	vmovss	-4(%rbx,%rsi,4), %xmm3          ## xmm3 = mem[0],zero,zero,zero
	vmulss	-4(%rbx,%rcx,4), %xmm3, %xmm3
	vmaxss	%xmm0, %xmm3, %xmm3
	vmulss	-4(%rbx,%rdx,4), %xmm3, %xmm3
	cmpl	$16, -68(%rsp)                  ## 4-byte Folded Reload
	jae	LBB18_34
## %bb.33:                              ##   in Loop: Header=BB18_32 Depth=1
	xorl	%edx, %edx
	jmp	LBB18_41
LBB18_34:                               ## %vector.ph
                                        ##   in Loop: Header=BB18_32 Depth=1
	vbroadcastss	%xmm3, %ymm4
	cmpq	$0, -88(%rsp)                   ## 8-byte Folded Reload
	je	LBB18_35
## %bb.36:                              ## %vector.body.preheader
                                        ##   in Loop: Header=BB18_32 Depth=1
	movq	-104(%rsp), %rcx                ## 8-byte Reload
	addq	%r14, %rcx
	movq	-96(%rsp), %rdx                 ## 8-byte Reload
	leaq	(%rdx,%rcx,4), %rbx
	movq	-56(%rsp), %rdx                 ## 8-byte Reload
	xorl	%esi, %esi
LBB18_37:                               ## %vector.body
                                        ##   Parent Loop BB18_32 Depth=1
                                        ## =>  This Inner Loop Header: Depth=2
	vmulps	-96(%rax,%rsi,4), %ymm4, %ymm5
	vmulps	-64(%rax,%rsi,4), %ymm4, %ymm6
	vfmadd213ps	-96(%rbx,%rsi,4), %ymm2, %ymm5 ## ymm5 = (ymm2 * ymm5) + mem
	vfmadd213ps	-64(%rbx,%rsi,4), %ymm2, %ymm6 ## ymm6 = (ymm2 * ymm6) + mem
	vmovups	%ymm5, -96(%rbx,%rsi,4)
	vmovups	%ymm6, -64(%rbx,%rsi,4)
	vmulps	-32(%rax,%rsi,4), %ymm4, %ymm5
	vmulps	(%rax,%rsi,4), %ymm4, %ymm6
	vfmadd213ps	-32(%rbx,%rsi,4), %ymm2, %ymm5 ## ymm5 = (ymm2 * ymm5) + mem
	vfmadd213ps	(%rbx,%rsi,4), %ymm2, %ymm6 ## ymm6 = (ymm2 * ymm6) + mem
	vmovups	%ymm5, -32(%rbx,%rsi,4)
	vmovups	%ymm6, (%rbx,%rsi,4)
	addq	$32, %rsi
	addq	$2, %rdx
	jne	LBB18_37
	jmp	LBB18_38
LBB18_35:                               ##   in Loop: Header=BB18_32 Depth=1
	xorl	%esi, %esi
LBB18_38:                               ## %middle.block.unr-lcssa
                                        ##   in Loop: Header=BB18_32 Depth=1
	testb	$1, -120(%rsp)                  ## 1-byte Folded Reload
	je	LBB18_40
## %bb.39:                              ## %vector.body.epil
                                        ##   in Loop: Header=BB18_32 Depth=1
	movq	80(%rsp), %rcx                  ## 8-byte Reload
	addl	%r9d, %ecx
	imull	-128(%rsp), %ecx                ## 4-byte Folded Reload
	movq	8(%rsp), %rdx                   ## 8-byte Reload
	addq	%r9, %rdx
	addl	16(%rsp), %ecx                  ## 4-byte Folded Reload
	imulq	%r10, %rdx
	movslq	%ecx, %rcx
	addq	-104(%rsp), %rsi                ## 8-byte Folded Reload
	addq	%rsi, %rcx
	addq	%rdx, %rsi
	movq	-64(%rsp), %rdx                 ## 8-byte Reload
	vmulps	(%rdx,%rsi,4), %ymm4, %ymm5
	vmulps	32(%rdx,%rsi,4), %ymm4, %ymm4
	movq	-80(%rsp), %rdx                 ## 8-byte Reload
	vfmadd213ps	(%rdx,%rcx,4), %ymm2, %ymm5 ## ymm5 = (ymm2 * ymm5) + mem
	vfmadd213ps	32(%rdx,%rcx,4), %ymm2, %ymm4 ## ymm4 = (ymm2 * ymm4) + mem
	vmovups	%ymm5, (%rdx,%rcx,4)
	vmovups	%ymm4, 32(%rdx,%rcx,4)
LBB18_40:                               ## %middle.block
                                        ##   in Loop: Header=BB18_32 Depth=1
	movq	%r13, %rdx
	cmpq	%rbp, %r13
	je	LBB18_43
LBB18_41:                               ## %"for relu1_0_d_def__.s10.n.ni.rebased.us.us9.preheader"
                                        ##   in Loop: Header=BB18_32 Depth=1
	movq	-112(%rsp), %rcx                ## 8-byte Reload
	leaq	(%rcx,%r14,4), %rcx
LBB18_42:                               ## %"for relu1_0_d_def__.s10.n.ni.rebased.us.us9"
                                        ##   Parent Loop BB18_32 Depth=1
                                        ## =>  This Inner Loop Header: Depth=2
	vmulss	(%rdi,%rdx,4), %xmm3, %xmm4
	vfmadd213ss	(%rcx,%rdx,4), %xmm1, %xmm4 ## xmm4 = (xmm1 * xmm4) + mem
	vmovss	%xmm4, (%rcx,%rdx,4)
	incq	%rdx
	cmpq	%rdx, %rbp
	jne	LBB18_42
	jmp	LBB18_43
                                        ## -- End function
	.section	__TEXT,__const
	.p2align	5                               ## -- Begin function train_cost_model.par_for.relu1_0_d_def__.s11.n.n.n
LCPI19_0:
	.long	0                               ## 0x0
	.long	1                               ## 0x1
	.long	2                               ## 0x2
	.long	3                               ## 0x3
	.long	4                               ## 0x4
	.long	5                               ## 0x5
	.long	6                               ## 0x6
	.long	7                               ## 0x7
	.section	__TEXT,__literal4,4byte_literals
	.p2align	2
LCPI19_1:
	.long	0x3f800000                      ## float 1
LCPI19_2:
	.long	0x3089705f                      ## float 9.99999971E-10
	.section	__TEXT,__text,regular,pure_instructions
	.p2align	4, 0x90
_train_cost_model.par_for.relu1_0_d_def__.s11.n.n.n: ## @train_cost_model.par_for.relu1_0_d_def__.s11.n.n.n
## %bb.0:                               ## %entry
	pushq	%rbp
	pushq	%r15
	pushq	%r14
	pushq	%r13
	pushq	%r12
	pushq	%rbx
	subq	$248, %rsp
	movq	%rdx, %rdi
	movl	12(%rdx), %r11d
	movl	24(%rdx), %r9d
	movl	%esi, %r8d
	sarl	$31, %r8d
	xorl	%ecx, %ecx
	testl	%r9d, %r9d
	sete	%cl
	movl	%ecx, %ebx
	negl	%ebx
	movl	%r9d, %ebp
	sarl	$31, %ebp
	subl	%r8d, %esi
	orl	%r9d, %ebx
	movl	%esi, %eax
	cltd
	idivl	%ebx
	movl	%edx, %r12d
	movl	%ebp, %r14d
	leal	(%r9,%rcx), %ebx
	movl	%esi, %eax
	cltd
	idivl	%ebx
	notl	%r14d
	decl	%ecx
	movl	%r14d, %r10d
	subl	%ebp, %r10d
	andl	%r8d, %r10d
	addl	%eax, %r10d
	andl	%ecx, %r10d
	leal	(%r10,%r10), %eax
	movl	%eax, -88(%rsp)                 ## 4-byte Spill
	subl	%eax, %r11d
	cmpl	$2, %r11d
	movl	$2, %edx
	cmovll	%r11d, %edx
	xorl	%eax, %eax
	testl	%r11d, %r11d
	cmovlel	%eax, %edx
	movl	%edx, -96(%rsp)                 ## 4-byte Spill
	jle	LBB19_44
## %bb.1:                               ## %"for relu1_0_d_def__.s11.w.wi.preheader"
	movl	(%rdi), %edx
	movl	16(%rdi), %r11d
	movl	32(%rdi), %esi
	movl	36(%rdi), %ebx
	movl	40(%rdi), %eax
	movl	44(%rdi), %r15d
	xorl	%r9d, %ebp
	addl	%r14d, %ebp
	andl	%r8d, %ebp
	addl	%ebp, %r12d
	andl	%ecx, %r12d
	movq	%r12, -64(%rsp)                 ## 8-byte Spill
	leal	(,%r12,8), %r12d
	movl	%ebx, -40(%rsp)                 ## 4-byte Spill
	movl	%ebx, %r8d
	subl	%esi, %r8d
	movl	%eax, -48(%rsp)                 ## 4-byte Spill
	movl	%eax, %r9d
	subl	%esi, %r9d
	movl	%r15d, -72(%rsp)                ## 4-byte Spill
	movl	%r15d, %r14d
	subl	%esi, %r14d
	movslq	8(%rdi), %rcx
	movslq	%r10d, %r15
	subq	%rcx, %r15
	addq	%r15, %r15
	movq	%r11, -120(%rsp)                ## 8-byte Spill
	movl	%r11d, %eax
	shll	$5, %eax
	movl	%eax, -128(%rsp)                ## 4-byte Spill
	movl	%edx, %ebx
	subl	%r12d, %ebx
	cmpl	$8, %ebx
	movl	$8, %ecx
	cmovll	%ebx, %ecx
	testl	%ebx, %ebx
	movl	$0, %r11d
	cmovgl	%ecx, %r11d
	movl	%r12d, %eax
	subl	%edx, %eax
	movl	%eax, %ecx
	sarl	$31, %ecx
	andl	%eax, %ecx
	cmpl	$-8, %ecx
	movl	$-8, %r13d
	cmovgl	%ecx, %r13d
	movl	20(%rdi), %eax
	movl	%eax, -124(%rsp)                ## 4-byte Spill
	movl	28(%rdi), %eax
	movl	%eax, -112(%rsp)                ## 4-byte Spill
	movq	48(%rdi), %rcx
	movq	64(%rdi), %rax
	movq	%rax, -80(%rsp)                 ## 8-byte Spill
	movq	80(%rdi), %rax
	movq	%rax, -32(%rsp)                 ## 8-byte Spill
	movslq	4(%rdi), %rbp
	leal	8(%r12), %eax
	movslq	%r12d, %rdi
	cmpl	%edx, %eax
	jle	LBB19_2
## %bb.4:                               ## %"for relu1_0_d_def__.s11.w.wi.preheader.split.us"
	movq	%rbp, 24(%rsp)                  ## 8-byte Spill
	movq	%rdi, 40(%rsp)                  ## 8-byte Spill
	movq	%rcx, -56(%rsp)                 ## 8-byte Spill
	movq	-120(%rsp), %rax                ## 8-byte Reload
	movl	%eax, %ecx
	shll	$4, %ecx
	subl	%eax, %ecx
	subl	%eax, %ecx
	movl	%r10d, %eax
	subl	-112(%rsp), %eax                ## 4-byte Folded Reload
	addl	%eax, %eax
	movq	%rax, 80(%rsp)                  ## 8-byte Spill
	subl	%esi, %edx
	movl	-40(%rsp), %edi                 ## 4-byte Reload
	addl	%edx, %edi
	movl	-48(%rsp), %ebp                 ## 4-byte Reload
	addl	%edx, %ebp
	addl	-72(%rsp), %edx                 ## 4-byte Folded Reload
	addl	%r11d, %r12d
	addl	%r11d, %r13d
	movslq	%edx, %rax
	movq	%rax, 72(%rsp)                  ## 8-byte Spill
	movslq	%edi, %rax
	movq	%rax, 64(%rsp)                  ## 8-byte Spill
	movslq	%ebp, %rax
	movq	%rax, 56(%rsp)                  ## 8-byte Spill
	testl	%ebx, %ebx
	movl	%r13d, -100(%rsp)               ## 4-byte Spill
	movq	%rcx, %rax
	movq	%rcx, -24(%rsp)                 ## 8-byte Spill
	movq	%r15, 16(%rsp)                  ## 8-byte Spill
	jle	LBB19_30
## %bb.5:                               ## %"for relu1_0_d_def__.s11.w.wi.us.us.preheader"
	movq	40(%rsp), %rbx                  ## 8-byte Reload
	addl	%ebx, %r14d
	addl	%ebx, %r9d
	addl	%ebx, %r8d
	movslq	%r14d, %rax
	movq	%rax, (%rsp)                    ## 8-byte Spill
	movslq	%r8d, %rax
	movq	%rax, -8(%rsp)                  ## 8-byte Spill
	movslq	%r9d, %rax
	movq	%rax, -16(%rsp)                 ## 8-byte Spill
	movl	%r11d, 36(%rsp)                 ## 4-byte Spill
	movl	%r11d, %ecx
	movslq	%r12d, %r9
	movl	%r13d, %r12d
	movslq	-88(%rsp), %rax                 ## 4-byte Folded Reload
	movq	%rax, 88(%rsp)                  ## 8-byte Spill
	movl	-96(%rsp), %eax                 ## 4-byte Reload
	movq	%rax, 240(%rsp)                 ## 8-byte Spill
	movq	%rcx, 8(%rsp)                   ## 8-byte Spill
	movl	%ecx, %eax
	andl	$2147483640, %eax               ## imm = 0x7FFFFFF8
	movq	%rax, 192(%rsp)                 ## 8-byte Spill
	addq	$-8, %rax
	movq	%rax, 184(%rsp)                 ## 8-byte Spill
	movq	%rax, %rdx
	shrq	$3, %rdx
	incq	%rdx
	movl	%r12d, %eax
	andl	$-16, %eax
	shll	$6, %r10d
	movl	-112(%rsp), %ecx                ## 4-byte Reload
	shll	$6, %ecx
	subl	%ecx, %r10d
	movq	%rax, 168(%rsp)                 ## 8-byte Spill
	addq	$-16, %rax
	orl	$14, %r10d
	movq	-120(%rsp), %r8                 ## 8-byte Reload
	imull	%r10d, %r8d
	movq	%rax, 160(%rsp)                 ## 8-byte Spill
	movq	%rax, %rdi
	shrq	$4, %rdi
	incq	%rdi
	movq	24(%rsp), %rbp                  ## 8-byte Reload
	movq	%rbp, %rax
	imulq	%r15, %rax
	movq	-64(%rsp), %rcx                 ## 8-byte Reload
	leal	(%r8,%rcx,8), %esi
	movq	%rdx, %rcx
	movq	%rdx, 176(%rsp)                 ## 8-byte Spill
	andq	$-2, %rdx
	negq	%rdx
	movq	%rdx, 144(%rsp)                 ## 8-byte Spill
	leaq	(%rax,%rbx), %rcx
	movq	-56(%rsp), %r10                 ## 8-byte Reload
	leaq	(%r10,%rcx,4), %r14
	addq	$32, %r14
	leaq	(%r10,%rcx,4), %r11
	addq	%r9, %rax
	movq	%rdi, %rcx
	movq	%rdi, 152(%rsp)                 ## 8-byte Spill
	andq	$-2, %rdi
	negq	%rdi
	movq	%rdi, 136(%rsp)                 ## 8-byte Spill
	movq	%r12, %rdi
	movq	%rbp, %r15
	leaq	(%r10,%rax,4), %r12
	addq	$96, %r12
	leaq	(%r10,%rax,4), %r13
	vmovss	LCPI19_1(%rip), %xmm0           ## xmm0 = mem[0],zero,zero,zero
	vmovss	LCPI19_2(%rip), %xmm1           ## xmm1 = mem[0],zero,zero,zero
	vbroadcastss	LCPI19_2(%rip), %ymm2   ## ymm2 = [9.99999971E-10,9.99999971E-10,9.99999971E-10,9.99999971E-10,9.99999971E-10,9.99999971E-10,9.99999971E-10,9.99999971E-10]
	vbroadcastss	LCPI19_1(%rip), %ymm3   ## ymm3 = [1.0E+0,1.0E+0,1.0E+0,1.0E+0,1.0E+0,1.0E+0,1.0E+0,1.0E+0]
	movq	-24(%rsp), %rax                 ## 8-byte Reload
	leal	(%rbx,%rax), %eax
	movq	%rax, 104(%rsp)                 ## 8-byte Spill
	movq	-32(%rsp), %rcx                 ## 8-byte Reload
	leaq	32(%rcx), %rax
	movq	%rax, 128(%rsp)                 ## 8-byte Spill
	leaq	(,%rbp,4), %rax
	movq	%rax, 232(%rsp)                 ## 8-byte Spill
	movq	-80(%rsp), %rax                 ## 8-byte Reload
	leaq	32(%rax), %rbx
	movq	%rbx, 120(%rsp)                 ## 8-byte Spill
	movq	-16(%rsp), %rdx                 ## 8-byte Reload
	leaq	(%rcx,%rdx,4), %rbx
	movq	%rbx, 224(%rsp)                 ## 8-byte Spill
	movq	-8(%rsp), %rdx                  ## 8-byte Reload
	leaq	(%rcx,%rdx,4), %rbx
	movq	%rbx, 216(%rsp)                 ## 8-byte Spill
	movq	(%rsp), %rdx                    ## 8-byte Reload
	leaq	(%rcx,%rdx,4), %rcx
	movq	%rcx, 208(%rsp)                 ## 8-byte Spill
	leaq	96(%rax), %rcx
	movq	%rcx, 112(%rsp)                 ## 8-byte Spill
	movq	%r9, 48(%rsp)                   ## 8-byte Spill
	leaq	(%rax,%r9,4), %rax
	movq	%rax, 200(%rsp)                 ## 8-byte Spill
	movq	88(%rsp), %rdx                  ## 8-byte Reload
	xorl	%ebp, %ebp
	movl	-128(%rsp), %r9d                ## 4-byte Reload
	movl	-124(%rsp), %ecx                ## 4-byte Reload
	movq	%r8, %r10
	jmp	LBB19_6
	.p2align	4, 0x90
LBB19_21:                               ## %after_bb.us.us
                                        ##   in Loop: Header=BB19_6 Depth=1
	movq	-96(%rsp), %rbp                 ## 8-byte Reload
	incq	%rbp
	movq	-88(%rsp), %rdx                 ## 8-byte Reload
	incq	%rdx
	movq	232(%rsp), %rax                 ## 8-byte Reload
	addq	%rax, %r14
	movl	-64(%rsp), %esi                 ## 4-byte Reload
	addl	%r9d, %esi
	addq	%rax, %r11
	addq	%rax, %r12
	addl	%r9d, %r10d
	addq	%rax, %r13
	cmpq	240(%rsp), %rbp                 ## 8-byte Folded Reload
	movl	-124(%rsp), %ecx                ## 4-byte Reload
	movq	24(%rsp), %r15                  ## 8-byte Reload
	je	LBB19_44
LBB19_6:                                ## %"for relu1_0_d_def__.s11.w.wi.us.us"
                                        ## =>This Loop Header: Depth=1
                                        ##     Child Loop BB19_11 Depth 2
                                        ##     Child Loop BB19_16 Depth 2
                                        ##     Child Loop BB19_25 Depth 2
                                        ##     Child Loop BB19_20 Depth 2
	testq	%rdx, %rdx
	movl	$0, %eax
	cmovgl	%edx, %eax
	imull	%ecx, %eax
	movslq	%eax, %r8
	movq	88(%rsp), %rax                  ## 8-byte Reload
	addq	%rbp, %rax
	testq	%rax, %rax
	movl	$0, %ebx
	cmovlel	%ebx, %eax
	imull	%ecx, %eax
	movq	80(%rsp), %rcx                  ## 8-byte Reload
	addl	%ebp, %ecx
	imull	%r9d, %ecx
	movq	%rcx, -72(%rsp)                 ## 8-byte Spill
	movq	16(%rsp), %rcx                  ## 8-byte Reload
	addq	%rbp, %rcx
	imulq	%r15, %rcx
	movslq	%esi, %rbx
	cltq
	movq	%rax, -112(%rsp)                ## 8-byte Spill
	cmpl	$8, 36(%rsp)                    ## 4-byte Folded Reload
	movq	%r10, -120(%rsp)                ## 8-byte Spill
	movl	%esi, -64(%rsp)                 ## 4-byte Spill
	movq	%rdx, -88(%rsp)                 ## 8-byte Spill
	movq	%rbp, -96(%rsp)                 ## 8-byte Spill
	movq	%r8, -40(%rsp)                  ## 8-byte Spill
	movq	%rbx, -48(%rsp)                 ## 8-byte Spill
	jae	LBB19_8
## %bb.7:                               ##   in Loop: Header=BB19_6 Depth=1
	movq	%rcx, %r8
	xorl	%eax, %eax
	movl	-100(%rsp), %r15d               ## 4-byte Reload
	movq	8(%rsp), %rbp                   ## 8-byte Reload
	jmp	LBB19_15
	.p2align	4, 0x90
LBB19_8:                                ## %vector.ph74
                                        ##   in Loop: Header=BB19_6 Depth=1
	movq	%rcx, 96(%rsp)                  ## 8-byte Spill
	cmpq	$0, 184(%rsp)                   ## 8-byte Folded Reload
	je	LBB19_9
## %bb.10:                              ## %vector.body72.preheader
                                        ##   in Loop: Header=BB19_6 Depth=1
	movq	-16(%rsp), %rax                 ## 8-byte Reload
	addq	%r8, %rax
	movq	-8(%rsp), %rcx                  ## 8-byte Reload
	leaq	(%rcx,%r8), %rcx
	movq	(%rsp), %rdx                    ## 8-byte Reload
	addq	%r8, %rdx
	movq	128(%rsp), %rsi                 ## 8-byte Reload
	leaq	(%rsi,%rax,4), %r8
	leaq	(%rsi,%rcx,4), %rax
	leaq	(%rsi,%rdx,4), %r10
	movq	120(%rsp), %rcx                 ## 8-byte Reload
	leaq	(%rcx,%rbx,4), %r15
	movq	144(%rsp), %rdx                 ## 8-byte Reload
	xorl	%r9d, %r9d
	movq	8(%rsp), %rbp                   ## 8-byte Reload
	.p2align	4, 0x90
LBB19_11:                               ## %vector.body72
                                        ##   Parent Loop BB19_6 Depth=1
                                        ## =>  This Inner Loop Header: Depth=2
	vmovups	-32(%rax,%r9,4), %ymm4
	vmulps	-32(%r8,%r9,4), %ymm4, %ymm4
	vmaxps	%ymm3, %ymm4, %ymm4
	vmulps	-32(%r10,%r9,4), %ymm4, %ymm4
	vmulps	-32(%r14,%r9,4), %ymm4, %ymm4
	vfmadd213ps	-32(%r15,%r9,4), %ymm2, %ymm4 ## ymm4 = (ymm2 * ymm4) + mem
	vmovups	%ymm4, -32(%r15,%r9,4)
	vmovups	(%rax,%r9,4), %ymm4
	vmulps	(%r8,%r9,4), %ymm4, %ymm4
	vmaxps	%ymm3, %ymm4, %ymm4
	vmulps	(%r10,%r9,4), %ymm4, %ymm4
	vmulps	(%r14,%r9,4), %ymm4, %ymm4
	vfmadd213ps	(%r15,%r9,4), %ymm2, %ymm4 ## ymm4 = (ymm2 * ymm4) + mem
	vmovups	%ymm4, (%r15,%r9,4)
	addq	$16, %r9
	addq	$2, %rdx
	jne	LBB19_11
## %bb.12:                              ## %middle.block70.unr-lcssa
                                        ##   in Loop: Header=BB19_6 Depth=1
	testb	$1, 176(%rsp)                   ## 1-byte Folded Reload
	movq	-56(%rsp), %r8                  ## 8-byte Reload
	je	LBB19_14
LBB19_13:                               ## %vector.body72.epil
                                        ##   in Loop: Header=BB19_6 Depth=1
	movq	104(%rsp), %rax                 ## 8-byte Reload
	movq	-72(%rsp), %rcx                 ## 8-byte Reload
	addl	%ecx, %eax
	movq	40(%rsp), %rcx                  ## 8-byte Reload
	movq	96(%rsp), %rdx                  ## 8-byte Reload
	addq	%rdx, %rcx
	cltq
	addq	%r9, %rax
	addq	%r9, %rcx
	addq	-112(%rsp), %r9                 ## 8-byte Folded Reload
	movq	-8(%rsp), %rdx                  ## 8-byte Reload
	addq	%r9, %rdx
	movq	-32(%rsp), %rsi                 ## 8-byte Reload
	vmovups	(%rsi,%rdx,4), %ymm4
	movq	(%rsp), %rdx                    ## 8-byte Reload
	addq	%r9, %rdx
	addq	-16(%rsp), %r9                  ## 8-byte Folded Reload
	vmulps	(%rsi,%r9,4), %ymm4, %ymm4
	vmaxps	%ymm3, %ymm4, %ymm4
	vmulps	(%rsi,%rdx,4), %ymm4, %ymm4
	vmulps	(%r8,%rcx,4), %ymm4, %ymm4
	movq	-80(%rsp), %rcx                 ## 8-byte Reload
	vfmadd213ps	(%rcx,%rax,4), %ymm2, %ymm4 ## ymm4 = (ymm2 * ymm4) + mem
	vmovups	%ymm4, (%rcx,%rax,4)
LBB19_14:                               ## %middle.block70
                                        ##   in Loop: Header=BB19_6 Depth=1
	movq	96(%rsp), %r8                   ## 8-byte Reload
	movq	192(%rsp), %rcx                 ## 8-byte Reload
	movq	%rcx, %rax
	cmpq	%rbp, %rcx
	movl	-128(%rsp), %r9d                ## 4-byte Reload
	movl	-100(%rsp), %r15d               ## 4-byte Reload
	je	LBB19_17
LBB19_15:                               ## %"for relu1_0_d_def__.s11.n.ni.us.us.preheader"
                                        ##   in Loop: Header=BB19_6 Depth=1
	movq	224(%rsp), %rcx                 ## 8-byte Reload
	movq	-40(%rsp), %rsi                 ## 8-byte Reload
	leaq	(%rcx,%rsi,4), %rdx
	movq	216(%rsp), %rcx                 ## 8-byte Reload
	leaq	(%rcx,%rsi,4), %rbx
	movq	208(%rsp), %rcx                 ## 8-byte Reload
	leaq	(%rcx,%rsi,4), %rcx
	movq	-80(%rsp), %rsi                 ## 8-byte Reload
	movq	%rdi, %r10
	movq	-48(%rsp), %rdi                 ## 8-byte Reload
	leaq	(%rsi,%rdi,4), %rsi
	movq	%r10, %rdi
	.p2align	4, 0x90
LBB19_16:                               ## %"for relu1_0_d_def__.s11.n.ni.us.us"
                                        ##   Parent Loop BB19_6 Depth=1
                                        ## =>  This Inner Loop Header: Depth=2
	vmovss	(%rbx,%rax,4), %xmm4            ## xmm4 = mem[0],zero,zero,zero
	vmulss	(%rdx,%rax,4), %xmm4, %xmm4
	vmaxss	%xmm0, %xmm4, %xmm4
	vmulss	(%rcx,%rax,4), %xmm4, %xmm4
	vmulss	(%r11,%rax,4), %xmm4, %xmm4
	vfmadd213ss	(%rsi,%rax,4), %xmm1, %xmm4 ## xmm4 = (xmm1 * xmm4) + mem
	vmovss	%xmm4, (%rsi,%rax,4)
	incq	%rax
	cmpq	%rax, %rbp
	jne	LBB19_16
LBB19_17:                               ## %"end for relu1_0_d_def__.s11.n.ni.loopexit.us.us"
                                        ##   in Loop: Header=BB19_6 Depth=1
	testl	%r15d, %r15d
	movq	-120(%rsp), %r10                ## 8-byte Reload
	jle	LBB19_21
## %bb.18:                              ## %"for relu1_0_d_def__.s11.n.ni.rebased.preheader.us.us"
                                        ##   in Loop: Header=BB19_6 Depth=1
	movslq	%r10d, %rbp
	movq	72(%rsp), %rax                  ## 8-byte Reload
	movq	-112(%rsp), %rsi                ## 8-byte Reload
	addq	%rsi, %rax
	movq	64(%rsp), %rcx                  ## 8-byte Reload
	addq	%rsi, %rcx
	addq	56(%rsp), %rsi                  ## 8-byte Folded Reload
	movq	-32(%rsp), %rdx                 ## 8-byte Reload
	vmovss	-4(%rdx,%rcx,4), %xmm4          ## xmm4 = mem[0],zero,zero,zero
	vmulss	-4(%rdx,%rsi,4), %xmm4, %xmm4
	vmaxss	%xmm0, %xmm4, %xmm4
	vmulss	-4(%rdx,%rax,4), %xmm4, %xmm4
	cmpl	$16, %r15d
	jae	LBB19_22
## %bb.19:                              ##   in Loop: Header=BB19_6 Depth=1
	xorl	%eax, %eax
	jmp	LBB19_29
	.p2align	4, 0x90
LBB19_22:                               ## %vector.ph55
                                        ##   in Loop: Header=BB19_6 Depth=1
	vbroadcastss	%xmm4, %ymm5
	cmpq	$0, 160(%rsp)                   ## 8-byte Folded Reload
	je	LBB19_23
## %bb.24:                              ## %vector.body53.preheader
                                        ##   in Loop: Header=BB19_6 Depth=1
	movq	48(%rsp), %rax                  ## 8-byte Reload
	addq	%rbp, %rax
	movq	112(%rsp), %rcx                 ## 8-byte Reload
	leaq	(%rcx,%rax,4), %rax
	movq	136(%rsp), %rdx                 ## 8-byte Reload
	xorl	%esi, %esi
	movq	-24(%rsp), %rcx                 ## 8-byte Reload
	.p2align	4, 0x90
LBB19_25:                               ## %vector.body53
                                        ##   Parent Loop BB19_6 Depth=1
                                        ## =>  This Inner Loop Header: Depth=2
	vmulps	-96(%r12,%rsi,4), %ymm5, %ymm6
	vmulps	-64(%r12,%rsi,4), %ymm5, %ymm7
	vfmadd213ps	-96(%rax,%rsi,4), %ymm2, %ymm6 ## ymm6 = (ymm2 * ymm6) + mem
	vfmadd213ps	-64(%rax,%rsi,4), %ymm2, %ymm7 ## ymm7 = (ymm2 * ymm7) + mem
	vmovups	%ymm6, -96(%rax,%rsi,4)
	vmovups	%ymm7, -64(%rax,%rsi,4)
	vmulps	-32(%r12,%rsi,4), %ymm5, %ymm6
	vmulps	(%r12,%rsi,4), %ymm5, %ymm7
	vfmadd213ps	-32(%rax,%rsi,4), %ymm2, %ymm6 ## ymm6 = (ymm2 * ymm6) + mem
	vfmadd213ps	(%rax,%rsi,4), %ymm2, %ymm7 ## ymm7 = (ymm2 * ymm7) + mem
	vmovups	%ymm6, -32(%rax,%rsi,4)
	vmovups	%ymm7, (%rax,%rsi,4)
	addq	$32, %rsi
	addq	$2, %rdx
	jne	LBB19_25
## %bb.26:                              ## %middle.block51.unr-lcssa
                                        ##   in Loop: Header=BB19_6 Depth=1
	testb	$1, 152(%rsp)                   ## 1-byte Folded Reload
	je	LBB19_28
LBB19_27:                               ## %vector.body53.epil
                                        ##   in Loop: Header=BB19_6 Depth=1
	movq	-72(%rsp), %rax                 ## 8-byte Reload
	addl	%ecx, %eax
	cltq
	addq	48(%rsp), %rsi                  ## 8-byte Folded Reload
	addq	%rsi, %rax
	addq	%r8, %rsi
	movq	-56(%rsp), %rcx                 ## 8-byte Reload
	vmulps	(%rcx,%rsi,4), %ymm5, %ymm6
	vmulps	32(%rcx,%rsi,4), %ymm5, %ymm5
	movq	-80(%rsp), %rcx                 ## 8-byte Reload
	vfmadd213ps	(%rcx,%rax,4), %ymm2, %ymm6 ## ymm6 = (ymm2 * ymm6) + mem
	vfmadd213ps	32(%rcx,%rax,4), %ymm2, %ymm5 ## ymm5 = (ymm2 * ymm5) + mem
	vmovups	%ymm6, (%rcx,%rax,4)
	vmovups	%ymm5, 32(%rcx,%rax,4)
LBB19_28:                               ## %middle.block51
                                        ##   in Loop: Header=BB19_6 Depth=1
	movq	168(%rsp), %rcx                 ## 8-byte Reload
	movq	%rcx, %rax
	cmpq	%rdi, %rcx
	je	LBB19_21
LBB19_29:                               ## %"for relu1_0_d_def__.s11.n.ni.rebased.us.us.preheader"
                                        ##   in Loop: Header=BB19_6 Depth=1
	movq	200(%rsp), %rcx                 ## 8-byte Reload
	leaq	(%rcx,%rbp,4), %rcx
	.p2align	4, 0x90
LBB19_20:                               ## %"for relu1_0_d_def__.s11.n.ni.rebased.us.us"
                                        ##   Parent Loop BB19_6 Depth=1
                                        ## =>  This Inner Loop Header: Depth=2
	vmulss	(%r13,%rax,4), %xmm4, %xmm5
	vfmadd213ss	(%rcx,%rax,4), %xmm1, %xmm5 ## xmm5 = (xmm1 * xmm5) + mem
	vmovss	%xmm5, (%rcx,%rax,4)
	incq	%rax
	cmpq	%rax, %rdi
	jne	LBB19_20
	jmp	LBB19_21
LBB19_9:                                ##   in Loop: Header=BB19_6 Depth=1
	xorl	%r9d, %r9d
	movq	8(%rsp), %rbp                   ## 8-byte Reload
	testb	$1, 176(%rsp)                   ## 1-byte Folded Reload
	movq	-56(%rsp), %r8                  ## 8-byte Reload
	jne	LBB19_13
	jmp	LBB19_14
LBB19_23:                               ##   in Loop: Header=BB19_6 Depth=1
	xorl	%esi, %esi
	movq	-24(%rsp), %rcx                 ## 8-byte Reload
	testb	$1, 152(%rsp)                   ## 1-byte Folded Reload
	jne	LBB19_27
	jmp	LBB19_28
LBB19_2:                                ## %"for relu1_0_d_def__.s11.w.wi.preheader21"
	decl	%edx
	vmovd	%edi, %xmm0
	vpbroadcastd	%xmm0, %ymm0
	vpor	LCPI19_0(%rip), %ymm0, %ymm0
	vmovd	%edx, %xmm1
	vpbroadcastd	%xmm1, %ymm1
	vpminsd	%ymm0, %ymm1, %ymm10
	movslq	-88(%rsp), %rax                 ## 4-byte Folded Reload
	movl	-96(%rsp), %r11d                ## 4-byte Reload
	imulq	%rbp, %r15
	addq	%rdi, %r15
	leaq	(%rcx,%r15,4), %r12
	shlq	$2, %rbp
	shll	$6, %r10d
	movl	-112(%rsp), %ecx                ## 4-byte Reload
	shll	$6, %ecx
	subl	%ecx, %r10d
	orl	$14, %r10d
	movq	-120(%rsp), %rcx                ## 8-byte Reload
	imull	%r10d, %ecx
	movq	-64(%rsp), %rdx                 ## 8-byte Reload
	leal	(%rcx,%rdx,8), %edx
	vbroadcastss	LCPI19_1(%rip), %ymm8   ## ymm8 = [1.0E+0,1.0E+0,1.0E+0,1.0E+0,1.0E+0,1.0E+0,1.0E+0,1.0E+0]
	vbroadcastss	LCPI19_2(%rip), %ymm9   ## ymm9 = [9.99999971E-10,9.99999971E-10,9.99999971E-10,9.99999971E-10,9.99999971E-10,9.99999971E-10,9.99999971E-10,9.99999971E-10]
	movl	-128(%rsp), %r10d               ## 4-byte Reload
	movl	-124(%rsp), %r15d               ## 4-byte Reload
	movq	%rbp, %r13
	movq	-32(%rsp), %rbx                 ## 8-byte Reload
	.p2align	4, 0x90
LBB19_3:                                ## %"for relu1_0_d_def__.s11.w.wi"
                                        ## =>This Inner Loop Header: Depth=1
	testq	%rax, %rax
	movl	$0, %esi
	cmovgl	%eax, %esi
	imull	%r15d, %esi
	leal	(%rsi,%r14), %ecx
	vmovd	%ecx, %xmm3
	vpbroadcastd	%xmm3, %ymm3
	vpaddd	%ymm3, %ymm10, %ymm3
	vmovd	%xmm3, %ecx
	vpextrd	$1, %xmm3, %ebp
	movslq	%ecx, %rcx
	movslq	%ebp, %rbp
	vpextrd	$2, %xmm3, %edi
	movslq	%edi, %rdi
	vmovss	(%rbx,%rcx,4), %xmm5            ## xmm5 = mem[0],zero,zero,zero
	vpextrd	$3, %xmm3, %ecx
	movslq	%ecx, %rcx
	vextracti128	$1, %ymm3, %xmm4
	vinsertps	$16, (%rbx,%rbp,4), %xmm5, %xmm3 ## xmm3 = xmm5[0],mem[0],xmm5[2,3]
	vmovd	%xmm4, %ebp
	vinsertps	$32, (%rbx,%rdi,4), %xmm3, %xmm3 ## xmm3 = xmm3[0,1],mem[0],xmm3[3]
	leal	(%rsi,%r8), %edi
	movslq	%ebp, %rbp
	vmovd	%edi, %xmm5
	vpbroadcastd	%xmm5, %ymm5
	vpaddd	%ymm5, %ymm10, %ymm5
	vmovss	(%rbx,%rbp,4), %xmm6            ## xmm6 = mem[0],zero,zero,zero
	vmovd	%xmm5, %edi
	movslq	%edi, %rdi
	vinsertps	$48, (%rbx,%rcx,4), %xmm3, %xmm3 ## xmm3 = xmm3[0,1,2],mem[0]
	vpextrd	$1, %xmm5, %ecx
	vpextrd	$1, %xmm4, %ebp
	vmovss	(%rbx,%rdi,4), %xmm7            ## xmm7 = mem[0],zero,zero,zero
	movslq	%ecx, %rcx
	vextracti128	$1, %ymm5, %xmm1
	movslq	%ebp, %rdi
	vinsertps	$16, (%rbx,%rcx,4), %xmm7, %xmm7 ## xmm7 = xmm7[0],mem[0],xmm7[2,3]
	vinsertps	$16, (%rbx,%rdi,4), %xmm6, %xmm6 ## xmm6 = xmm6[0],mem[0],xmm6[2,3]
	vmovd	%xmm1, %ecx
	vpextrd	$1, %xmm1, %edi
	movslq	%ecx, %rcx
	vmovss	(%rbx,%rcx,4), %xmm2            ## xmm2 = mem[0],zero,zero,zero
	movslq	%edi, %rcx
	vinsertps	$16, (%rbx,%rcx,4), %xmm2, %xmm2 ## xmm2 = xmm2[0],mem[0],xmm2[2,3]
	vpextrd	$2, %xmm5, %ecx
	movslq	%ecx, %rcx
	vinsertps	$32, (%rbx,%rcx,4), %xmm7, %xmm7 ## xmm7 = xmm7[0,1],mem[0],xmm7[3]
	vpextrd	$2, %xmm4, %ecx
	movslq	%ecx, %rcx
	vinsertps	$32, (%rbx,%rcx,4), %xmm6, %xmm6 ## xmm6 = xmm6[0,1],mem[0],xmm6[3]
	vpextrd	$3, %xmm5, %ecx
	movslq	%ecx, %rcx
	vinsertps	$48, (%rbx,%rcx,4), %xmm7, %xmm5 ## xmm5 = xmm7[0,1,2],mem[0]
	vpextrd	$2, %xmm1, %ecx
	movslq	%ecx, %rcx
	vinsertps	$32, (%rbx,%rcx,4), %xmm2, %xmm2 ## xmm2 = xmm2[0,1],mem[0],xmm2[3]
	addl	%r9d, %esi
	vmovd	%esi, %xmm7
	vpextrd	$3, %xmm4, %ecx
	movslq	%ecx, %rcx
	vpbroadcastd	%xmm7, %ymm4
	vinsertps	$48, (%rbx,%rcx,4), %xmm6, %xmm6 ## xmm6 = xmm6[0,1,2],mem[0]
	vpaddd	%ymm4, %ymm10, %ymm4
	vextracti128	$1, %ymm4, %xmm7
	vmovd	%xmm7, %ecx
	movslq	%ecx, %rcx
	vmovss	(%rbx,%rcx,4), %xmm0            ## xmm0 = mem[0],zero,zero,zero
	vpextrd	$1, %xmm7, %ecx
	movslq	%ecx, %rcx
	vinsertps	$16, (%rbx,%rcx,4), %xmm0, %xmm0 ## xmm0 = xmm0[0],mem[0],xmm0[2,3]
	vpextrd	$3, %xmm1, %ecx
	movslq	%ecx, %rcx
	vinsertps	$48, (%rbx,%rcx,4), %xmm2, %xmm1 ## xmm1 = xmm2[0,1,2],mem[0]
	vpextrd	$2, %xmm7, %ecx
	movslq	%ecx, %rcx
	vinsertps	$32, (%rbx,%rcx,4), %xmm0, %xmm0 ## xmm0 = xmm0[0,1],mem[0],xmm0[3]
	vmovd	%xmm4, %ecx
	movslq	%ecx, %rcx
	vmovss	(%rbx,%rcx,4), %xmm2            ## xmm2 = mem[0],zero,zero,zero
	vpextrd	$1, %xmm4, %ecx
	movslq	%ecx, %rcx
	vinsertps	$16, (%rbx,%rcx,4), %xmm2, %xmm2 ## xmm2 = xmm2[0],mem[0],xmm2[2,3]
	vpextrd	$3, %xmm7, %ecx
	movslq	%ecx, %rcx
	vinsertps	$48, (%rbx,%rcx,4), %xmm0, %xmm0 ## xmm0 = xmm0[0,1,2],mem[0]
	vpextrd	$2, %xmm4, %ecx
	movslq	%ecx, %rcx
	vinsertps	$32, (%rbx,%rcx,4), %xmm2, %xmm2 ## xmm2 = xmm2[0,1],mem[0],xmm2[3]
	vpextrd	$3, %xmm4, %ecx
	movslq	%ecx, %rcx
	vinsertps	$48, (%rbx,%rcx,4), %xmm2, %xmm2 ## xmm2 = xmm2[0,1,2],mem[0]
	vinsertf128	$1, %xmm6, %ymm3, %ymm3
	vinsertf128	$1, %xmm1, %ymm5, %ymm1
	vinsertf128	$1, %xmm0, %ymm2, %ymm0
	vmulps	%ymm0, %ymm1, %ymm0
	vmaxps	%ymm8, %ymm0, %ymm0
	vmulps	%ymm0, %ymm3, %ymm0
	vmulps	(%r12), %ymm0, %ymm0
	movslq	%edx, %rdx
	movq	-80(%rsp), %rcx                 ## 8-byte Reload
	vfmadd213ps	(%rcx,%rdx,4), %ymm9, %ymm0 ## ymm0 = (ymm9 * ymm0) + mem
	vmovups	%ymm0, (%rcx,%rdx,4)
	addq	%r13, %r12
	incq	%rax
	addl	%r10d, %edx
	decq	%r11
	jne	LBB19_3
LBB19_44:                               ## %destructor_block
	xorl	%eax, %eax
	addq	$248, %rsp
	popq	%rbx
	popq	%r12
	popq	%r13
	popq	%r14
	popq	%r15
	popq	%rbp
	vzeroupper
	retq
LBB19_30:                               ## %"for relu1_0_d_def__.s11.w.wi.preheader.split.us.split"
	testl	%r13d, %r13d
	movl	-124(%rsp), %r11d               ## 4-byte Reload
	movq	24(%rsp), %r9                   ## 8-byte Reload
	movq	-120(%rsp), %rbx                ## 8-byte Reload
	jle	LBB19_44
## %bb.31:                              ## %"for relu1_0_d_def__.s11.w.wi.us.us4.preheader"
	movslq	%r12d, %r14
	movl	-100(%rsp), %ebp                ## 4-byte Reload
	movslq	-88(%rsp), %rax                 ## 4-byte Folded Reload
	movq	%rax, -64(%rsp)                 ## 8-byte Spill
	movl	-96(%rsp), %eax                 ## 4-byte Reload
	movq	%rax, -88(%rsp)                 ## 8-byte Spill
	movl	%ebp, %r13d
	andl	$-16, %r13d
	leaq	-16(%r13), %rax
	movq	%rax, -96(%rsp)                 ## 8-byte Spill
	movq	%rax, %rcx
	shrq	$4, %rcx
	incq	%rcx
	movq	%r9, %rdx
	imulq	16(%rsp), %rdx                  ## 8-byte Folded Reload
	addq	%r14, %rdx
	movq	-56(%rsp), %r15                 ## 8-byte Reload
	leaq	(%r15,%rdx,4), %rax
	addq	$96, %rax
	leaq	(,%r9,4), %r8
	movq	-80(%rsp), %rsi                 ## 8-byte Reload
	leaq	96(%rsi), %rdi
	movq	%rdi, -48(%rsp)                 ## 8-byte Spill
	shll	$6, %r10d
	movl	-112(%rsp), %r12d               ## 4-byte Reload
	shll	$6, %r12d
	subl	%r12d, %r10d
	orl	$14, %r10d
	imull	%r10d, %ebx
	movq	%rcx, %rdi
	movq	%rcx, -40(%rsp)                 ## 8-byte Spill
	andq	$-2, %rcx
	negq	%rcx
	movq	%rcx, -72(%rsp)                 ## 8-byte Spill
	leaq	(%r15,%rdx,4), %rdi
	movq	%r14, -120(%rsp)                ## 8-byte Spill
	leaq	(%rsi,%r14,4), %rcx
	movq	%rcx, -112(%rsp)                ## 8-byte Spill
	vmovss	LCPI19_1(%rip), %xmm0           ## xmm0 = mem[0],zero,zero,zero
	vmovss	LCPI19_2(%rip), %xmm1           ## xmm1 = mem[0],zero,zero,zero
	vbroadcastss	LCPI19_2(%rip), %ymm2   ## ymm2 = [9.99999971E-10,9.99999971E-10,9.99999971E-10,9.99999971E-10,9.99999971E-10,9.99999971E-10,9.99999971E-10,9.99999971E-10]
	xorl	%r12d, %r12d
	jmp	LBB19_32
LBB19_43:                               ## %after_bb.loopexit.us.us18
                                        ##   in Loop: Header=BB19_32 Depth=1
	incq	%r12
	addq	%r8, %rax
	movq	%r15, %rbx
	addl	-128(%rsp), %ebx                ## 4-byte Folded Reload
	addq	%r8, %rdi
	cmpq	-88(%rsp), %r12                 ## 8-byte Folded Reload
	movl	-124(%rsp), %r11d               ## 4-byte Reload
	je	LBB19_44
LBB19_32:                               ## %"for relu1_0_d_def__.s11.w.wi.us.us4"
                                        ## =>This Loop Header: Depth=1
                                        ##     Child Loop BB19_37 Depth 2
                                        ##     Child Loop BB19_42 Depth 2
	movq	%rbx, %r15
	movslq	%ebx, %r14
	movq	-64(%rsp), %rcx                 ## 8-byte Reload
	addq	%r12, %rcx
	testq	%rcx, %rcx
	movl	$0, %edx
	cmovlel	%edx, %ecx
	imull	%r11d, %ecx
	movslq	%ecx, %rcx
	movq	72(%rsp), %rdx                  ## 8-byte Reload
	addq	%rcx, %rdx
	movq	64(%rsp), %rsi                  ## 8-byte Reload
	addq	%rcx, %rsi
	addq	56(%rsp), %rcx                  ## 8-byte Folded Reload
	movq	-32(%rsp), %rbx                 ## 8-byte Reload
	vmovss	-4(%rbx,%rsi,4), %xmm3          ## xmm3 = mem[0],zero,zero,zero
	vmulss	-4(%rbx,%rcx,4), %xmm3, %xmm3
	vmaxss	%xmm0, %xmm3, %xmm3
	vmulss	-4(%rbx,%rdx,4), %xmm3, %xmm3
	cmpl	$16, -100(%rsp)                 ## 4-byte Folded Reload
	jae	LBB19_34
## %bb.33:                              ##   in Loop: Header=BB19_32 Depth=1
	xorl	%edx, %edx
	jmp	LBB19_41
LBB19_34:                               ## %vector.ph
                                        ##   in Loop: Header=BB19_32 Depth=1
	vbroadcastss	%xmm3, %ymm4
	cmpq	$0, -96(%rsp)                   ## 8-byte Folded Reload
	je	LBB19_35
## %bb.36:                              ## %vector.body.preheader
                                        ##   in Loop: Header=BB19_32 Depth=1
	movq	-120(%rsp), %rcx                ## 8-byte Reload
	addq	%r14, %rcx
	movq	-48(%rsp), %rdx                 ## 8-byte Reload
	leaq	(%rdx,%rcx,4), %rdx
	movq	-72(%rsp), %r10                 ## 8-byte Reload
	xorl	%esi, %esi
LBB19_37:                               ## %vector.body
                                        ##   Parent Loop BB19_32 Depth=1
                                        ## =>  This Inner Loop Header: Depth=2
	vmulps	-96(%rax,%rsi,4), %ymm4, %ymm5
	vmulps	-64(%rax,%rsi,4), %ymm4, %ymm6
	vfmadd213ps	-96(%rdx,%rsi,4), %ymm2, %ymm5 ## ymm5 = (ymm2 * ymm5) + mem
	vfmadd213ps	-64(%rdx,%rsi,4), %ymm2, %ymm6 ## ymm6 = (ymm2 * ymm6) + mem
	vmovups	%ymm5, -96(%rdx,%rsi,4)
	vmovups	%ymm6, -64(%rdx,%rsi,4)
	vmulps	-32(%rax,%rsi,4), %ymm4, %ymm5
	vmulps	(%rax,%rsi,4), %ymm4, %ymm6
	vfmadd213ps	-32(%rdx,%rsi,4), %ymm2, %ymm5 ## ymm5 = (ymm2 * ymm5) + mem
	vfmadd213ps	(%rdx,%rsi,4), %ymm2, %ymm6 ## ymm6 = (ymm2 * ymm6) + mem
	vmovups	%ymm5, -32(%rdx,%rsi,4)
	vmovups	%ymm6, (%rdx,%rsi,4)
	addq	$32, %rsi
	addq	$2, %r10
	jne	LBB19_37
	jmp	LBB19_38
LBB19_35:                               ##   in Loop: Header=BB19_32 Depth=1
	xorl	%esi, %esi
LBB19_38:                               ## %middle.block.unr-lcssa
                                        ##   in Loop: Header=BB19_32 Depth=1
	testb	$1, -40(%rsp)                   ## 1-byte Folded Reload
	je	LBB19_40
## %bb.39:                              ## %vector.body.epil
                                        ##   in Loop: Header=BB19_32 Depth=1
	movq	80(%rsp), %rcx                  ## 8-byte Reload
	addl	%r12d, %ecx
	imull	-128(%rsp), %ecx                ## 4-byte Folded Reload
	movq	16(%rsp), %rdx                  ## 8-byte Reload
	addq	%r12, %rdx
	addl	-24(%rsp), %ecx                 ## 4-byte Folded Reload
	imulq	%r9, %rdx
	movslq	%ecx, %rcx
	addq	-120(%rsp), %rsi                ## 8-byte Folded Reload
	addq	%rsi, %rcx
	addq	%rdx, %rsi
	movq	-56(%rsp), %rdx                 ## 8-byte Reload
	vmulps	(%rdx,%rsi,4), %ymm4, %ymm5
	vmulps	32(%rdx,%rsi,4), %ymm4, %ymm4
	movq	-80(%rsp), %rdx                 ## 8-byte Reload
	vfmadd213ps	(%rdx,%rcx,4), %ymm2, %ymm5 ## ymm5 = (ymm2 * ymm5) + mem
	vfmadd213ps	32(%rdx,%rcx,4), %ymm2, %ymm4 ## ymm4 = (ymm2 * ymm4) + mem
	vmovups	%ymm5, (%rdx,%rcx,4)
	vmovups	%ymm4, 32(%rdx,%rcx,4)
LBB19_40:                               ## %middle.block
                                        ##   in Loop: Header=BB19_32 Depth=1
	movq	%r13, %rdx
	cmpq	%rbp, %r13
	je	LBB19_43
LBB19_41:                               ## %"for relu1_0_d_def__.s11.n.ni.rebased.us.us9.preheader"
                                        ##   in Loop: Header=BB19_32 Depth=1
	movq	-112(%rsp), %rcx                ## 8-byte Reload
	leaq	(%rcx,%r14,4), %rcx
LBB19_42:                               ## %"for relu1_0_d_def__.s11.n.ni.rebased.us.us9"
                                        ##   Parent Loop BB19_32 Depth=1
                                        ## =>  This Inner Loop Header: Depth=2
	vmulss	(%rdi,%rdx,4), %xmm3, %xmm4
	vfmadd213ss	(%rcx,%rdx,4), %xmm1, %xmm4 ## xmm4 = (xmm1 * xmm4) + mem
	vmovss	%xmm4, (%rcx,%rdx,4)
	incq	%rdx
	cmpq	%rdx, %rbp
	jne	LBB19_42
	jmp	LBB19_43
                                        ## -- End function
	.section	__TEXT,__const
	.p2align	5                               ## -- Begin function train_cost_model.par_for.relu1_0_d_def__.s12.n.n.n
LCPI20_0:
	.long	0                               ## 0x0
	.long	1                               ## 0x1
	.long	2                               ## 0x2
	.long	3                               ## 0x3
	.long	4                               ## 0x4
	.long	5                               ## 0x5
	.long	6                               ## 0x6
	.long	7                               ## 0x7
	.section	__TEXT,__literal4,4byte_literals
	.p2align	2
LCPI20_1:
	.long	0x3089705f                      ## float 9.99999971E-10
	.section	__TEXT,__text,regular,pure_instructions
	.p2align	4, 0x90
_train_cost_model.par_for.relu1_0_d_def__.s12.n.n.n: ## @train_cost_model.par_for.relu1_0_d_def__.s12.n.n.n
## %bb.0:                               ## %entry
	pushq	%rbp
	pushq	%r15
	pushq	%r14
	pushq	%r13
	pushq	%r12
	pushq	%rbx
	subq	$112, %rsp
	movq	%rdx, %rdi
	movl	12(%rdx), %r10d
	movl	24(%rdx), %r13d
	movl	%esi, %r9d
	sarl	$31, %r9d
	xorl	%ecx, %ecx
	testl	%r13d, %r13d
	sete	%cl
	movl	%ecx, %ebx
	negl	%ebx
	movl	%r13d, %ebp
	sarl	$31, %ebp
	subl	%r9d, %esi
	orl	%r13d, %ebx
	movl	%esi, %eax
	cltd
	idivl	%ebx
	movl	%edx, %r14d
	movl	%ebp, %r15d
	leal	(%rcx,%r13), %ebx
	movl	%esi, %eax
	cltd
	idivl	%ebx
	notl	%r15d
	decl	%ecx
	movl	%r15d, %r8d
	subl	%ebp, %r8d
	andl	%r9d, %r8d
	addl	%eax, %r8d
	andl	%ecx, %r8d
	leal	(%r8,%r8), %eax
	movl	%eax, -96(%rsp)                 ## 4-byte Spill
	subl	%eax, %r10d
	cmpl	$2, %r10d
	movl	$2, %eax
	cmovll	%r10d, %eax
	xorl	%ebx, %ebx
	testl	%r10d, %r10d
	cmovlel	%ebx, %eax
	movl	%eax, -104(%rsp)                ## 4-byte Spill
	jle	LBB20_40
## %bb.1:                               ## %"for relu1_0_d_def__.s12.w.wi.preheader"
	movl	(%rdi), %edx
	movl	16(%rdi), %r12d
	movl	32(%rdi), %r10d
	movl	36(%rdi), %eax
	movl	40(%rdi), %r11d
	xorl	%r13d, %ebp
	addl	%r15d, %ebp
	andl	%r9d, %ebp
	addl	%ebp, %r14d
	andl	%ecx, %r14d
	movq	%r14, -120(%rsp)                ## 8-byte Spill
	leal	(,%r14,8), %esi
	movl	%eax, -128(%rsp)                ## 4-byte Spill
	movl	%eax, %r9d
	subl	%r10d, %r9d
	movl	%r11d, %r15d
	subl	%r10d, %r11d
	movslq	8(%rdi), %rax
	movslq	%r8d, %r13
	subq	%rax, %r13
	addq	%r13, %r13
	movq	%r12, -88(%rsp)                 ## 8-byte Spill
	movl	%r12d, %eax
	shll	$5, %eax
	movl	%eax, -112(%rsp)                ## 4-byte Spill
	movl	%edx, %eax
	subl	%esi, %eax
	cmpl	$8, %eax
	movl	$8, %ecx
	cmovll	%eax, %ecx
	testl	%eax, %eax
	cmovgl	%ecx, %ebx
	movl	%ebx, -108(%rsp)                ## 4-byte Spill
	movl	20(%rdi), %ecx
	movl	%ecx, -124(%rsp)                ## 4-byte Spill
	movl	%esi, %ebx
	subl	%edx, %ebx
	movl	%ebx, %ecx
	sarl	$31, %ecx
	andl	%ebx, %ecx
	cmpl	$-8, %ecx
	movl	$-8, %r14d
	cmovgl	%ecx, %r14d
	movl	28(%rdi), %ebx
	movq	48(%rdi), %rbp
	movq	64(%rdi), %rcx
	movq	%rcx, -80(%rsp)                 ## 8-byte Spill
	movq	80(%rdi), %rcx
	movq	%rcx, -40(%rsp)                 ## 8-byte Spill
	movslq	4(%rdi), %rcx
	movq	%rcx, -72(%rsp)                 ## 8-byte Spill
	leal	8(%rsi), %ecx
	movslq	%esi, %r12
	cmpl	%edx, %ecx
	jle	LBB20_2
## %bb.4:                               ## %"for relu1_0_d_def__.s12.w.wi.preheader.split.us"
	movq	%rbp, -56(%rsp)                 ## 8-byte Spill
	movq	%r13, -48(%rsp)                 ## 8-byte Spill
	movq	-88(%rsp), %rbp                 ## 8-byte Reload
	leal	(%rbp,%rbp,2), %ecx
	leal	(%rbp,%rcx,4), %ecx
	movl	%ecx, -60(%rsp)                 ## 4-byte Spill
	movl	%r8d, %ecx
	subl	%ebx, %ecx
	addl	%ecx, %ecx
	movq	%rcx, -32(%rsp)                 ## 8-byte Spill
	subl	%r10d, %edx
	movl	-128(%rsp), %edi                ## 4-byte Reload
	addl	%edx, %edi
	addl	%r15d, %edx
	movl	-108(%rsp), %ecx                ## 4-byte Reload
	addl	%ecx, %esi
	addl	%ecx, %r14d
	movslq	%edi, %rcx
	movq	%rcx, 8(%rsp)                   ## 8-byte Spill
	movslq	%edx, %rcx
	movq	%rcx, (%rsp)                    ## 8-byte Spill
	testl	%eax, %eax
	movl	%r14d, -128(%rsp)               ## 4-byte Spill
	movl	%ebx, %ecx
	jle	LBB20_26
## %bb.5:                               ## %"for relu1_0_d_def__.s12.w.wi.us.us.preheader"
	addl	%r12d, %r11d
	addl	%r12d, %r9d
	movslq	%r9d, %rax
	movq	%rax, -24(%rsp)                 ## 8-byte Spill
	movslq	%r11d, %r9
	movl	-108(%rsp), %r10d               ## 4-byte Reload
	movslq	%esi, %rsi
	movl	%r14d, %r14d
	movslq	-96(%rsp), %rax                 ## 4-byte Folded Reload
	movq	%rax, 16(%rsp)                  ## 8-byte Spill
	movl	-104(%rsp), %eax                ## 4-byte Reload
	movq	%rax, 104(%rsp)                 ## 8-byte Spill
	movl	%r14d, %eax
	andl	$-16, %eax
	movq	%rax, 56(%rsp)                  ## 8-byte Spill
	addq	$-16, %rax
	movq	%rax, 48(%rsp)                  ## 8-byte Spill
	movq	%rax, %rbx
	shrq	$4, %rbx
	incq	%rbx
	movl	%r10d, %r11d
	andl	$2147483616, %r11d              ## imm = 0x7FFFFFE0
	shll	$6, %r8d
	shll	$6, %ecx
	subl	%ecx, %r8d
	movq	-72(%rsp), %rdx                 ## 8-byte Reload
	movq	%rdx, %rax
	imulq	-48(%rsp), %rax                 ## 8-byte Folded Reload
	addq	%rax, %r12
	orl	$13, %r8d
	imull	%r8d, %ebp
	movq	-56(%rsp), %rcx                 ## 8-byte Reload
	movq	%rdx, %r8
	leaq	(%rcx,%r12,4), %rdi
	addq	$96, %rdi
	movq	%rbp, -88(%rsp)                 ## 8-byte Spill
	movq	-120(%rsp), %rdx                ## 8-byte Reload
	leal	(%rbp,%rdx,8), %r13d
	leaq	(%rcx,%r12,4), %rbp
	addq	%rsi, %rax
	movq	%rbx, %rdx
	movq	%rbx, 40(%rsp)                  ## 8-byte Spill
	andq	$-2, %rbx
	negq	%rbx
	movq	%rbx, 24(%rsp)                  ## 8-byte Spill
	leaq	(%rcx,%rax,4), %r12
	addq	$96, %r12
	leaq	(%rcx,%rax,4), %r15
	vmovss	LCPI20_1(%rip), %xmm0           ## xmm0 = mem[0],zero,zero,zero
	vbroadcastss	LCPI20_1(%rip), %ymm1   ## ymm1 = [9.99999971E-10,9.99999971E-10,9.99999971E-10,9.99999971E-10,9.99999971E-10,9.99999971E-10,9.99999971E-10,9.99999971E-10]
	movq	-40(%rsp), %rcx                 ## 8-byte Reload
	leaq	96(%rcx), %rax
	movq	%rax, 32(%rsp)                  ## 8-byte Spill
	leaq	(,%r8,4), %rax
	movq	%rax, 96(%rsp)                  ## 8-byte Spill
	movq	-80(%rsp), %rax                 ## 8-byte Reload
	leaq	96(%rax), %rdx
	movq	%rdx, -8(%rsp)                  ## 8-byte Spill
	movq	%r9, 64(%rsp)                   ## 8-byte Spill
	leaq	(%rcx,%r9,4), %rdx
	movq	%rdx, 88(%rsp)                  ## 8-byte Spill
	movq	-24(%rsp), %rdx                 ## 8-byte Reload
	leaq	(%rcx,%rdx,4), %rcx
	movq	%rcx, 80(%rsp)                  ## 8-byte Spill
	movq	%rsi, -16(%rsp)                 ## 8-byte Spill
	leaq	(%rax,%rsi,4), %rax
	movq	%rax, 72(%rsp)                  ## 8-byte Spill
	movq	16(%rsp), %rdx                  ## 8-byte Reload
	xorl	%ebx, %ebx
	movl	-124(%rsp), %esi                ## 4-byte Reload
	movl	-108(%rsp), %ecx                ## 4-byte Reload
	jmp	LBB20_6
	.p2align	4, 0x90
LBB20_17:                               ## %after_bb.us.us
                                        ##   in Loop: Header=BB20_6 Depth=1
	movl	%ebx, %ecx
	movq	-120(%rsp), %rbx                ## 8-byte Reload
	incq	%rbx
	movq	-104(%rsp), %rdx                ## 8-byte Reload
	incq	%rdx
	movq	96(%rsp), %rax                  ## 8-byte Reload
	addq	%rax, %rdi
	movl	-96(%rsp), %r13d                ## 4-byte Reload
	addl	%ecx, %r13d
	addq	%rax, %rbp
	addq	%rax, %r12
	addl	%ecx, %r8d
	movq	%r8, -88(%rsp)                  ## 8-byte Spill
	addq	%rax, %r15
	cmpq	104(%rsp), %rbx                 ## 8-byte Folded Reload
	movl	-124(%rsp), %esi                ## 4-byte Reload
	movl	-108(%rsp), %ecx                ## 4-byte Reload
	je	LBB20_40
LBB20_6:                                ## %"for relu1_0_d_def__.s12.w.wi.us.us"
                                        ## =>This Loop Header: Depth=1
                                        ##     Child Loop BB20_9 Depth 2
                                        ##     Child Loop BB20_12 Depth 2
                                        ##     Child Loop BB20_21 Depth 2
                                        ##     Child Loop BB20_16 Depth 2
	testq	%rdx, %rdx
	movl	$0, %eax
	movq	%rdx, -104(%rsp)                ## 8-byte Spill
	cmovgl	%edx, %eax
	imull	%esi, %eax
	cltq
	movq	16(%rsp), %rdx                  ## 8-byte Reload
	movq	%rbx, -120(%rsp)                ## 8-byte Spill
	leaq	(%rbx,%rdx), %r9
	testq	%r9, %r9
	movl	$0, %edx
	cmovlel	%edx, %r9d
	movslq	%r13d, %rsi
	cmpl	$32, %ecx
	movl	%r13d, -96(%rsp)                ## 4-byte Spill
	jae	LBB20_8
## %bb.7:                               ##   in Loop: Header=BB20_6 Depth=1
	xorl	%ecx, %ecx
	jmp	LBB20_11
	.p2align	4, 0x90
LBB20_8:                                ## %vector.body71.preheader
                                        ##   in Loop: Header=BB20_6 Depth=1
	movq	64(%rsp), %rcx                  ## 8-byte Reload
	addq	%rax, %rcx
	movq	-24(%rsp), %rdx                 ## 8-byte Reload
	addq	%rax, %rdx
	movq	32(%rsp), %rbx                  ## 8-byte Reload
	leaq	(%rbx,%rcx,4), %r13
	leaq	(%rbx,%rdx,4), %r8
	movq	-8(%rsp), %rcx                  ## 8-byte Reload
	movq	%rsi, %rbx
	leaq	(%rcx,%rsi,4), %rcx
	xorl	%edx, %edx
	.p2align	4, 0x90
LBB20_9:                                ## %vector.body71
                                        ##   Parent Loop BB20_6 Depth=1
                                        ## =>  This Inner Loop Header: Depth=2
	vmovups	-96(%r8,%rdx,4), %ymm2
	vmovups	-64(%r8,%rdx,4), %ymm3
	vmovups	-32(%r8,%rdx,4), %ymm4
	vmovups	(%r8,%rdx,4), %ymm5
	vmulps	-96(%r13,%rdx,4), %ymm2, %ymm2
	vmulps	-64(%r13,%rdx,4), %ymm3, %ymm3
	vmulps	-32(%r13,%rdx,4), %ymm4, %ymm4
	vmulps	(%r13,%rdx,4), %ymm5, %ymm5
	vmulps	-96(%rdi,%rdx,4), %ymm2, %ymm2
	vmulps	-64(%rdi,%rdx,4), %ymm3, %ymm3
	vmulps	-32(%rdi,%rdx,4), %ymm4, %ymm4
	vmulps	(%rdi,%rdx,4), %ymm5, %ymm5
	vfmadd213ps	-96(%rcx,%rdx,4), %ymm1, %ymm2 ## ymm2 = (ymm1 * ymm2) + mem
	vfmadd213ps	-64(%rcx,%rdx,4), %ymm1, %ymm3 ## ymm3 = (ymm1 * ymm3) + mem
	vfmadd213ps	-32(%rcx,%rdx,4), %ymm1, %ymm4 ## ymm4 = (ymm1 * ymm4) + mem
	vfmadd213ps	(%rcx,%rdx,4), %ymm1, %ymm5 ## ymm5 = (ymm1 * ymm5) + mem
	vmovups	%ymm2, -96(%rcx,%rdx,4)
	vmovups	%ymm3, -64(%rcx,%rdx,4)
	vmovups	%ymm4, -32(%rcx,%rdx,4)
	vmovups	%ymm5, (%rcx,%rdx,4)
	addq	$32, %rdx
	cmpq	%rdx, %r11
	jne	LBB20_9
## %bb.10:                              ## %middle.block69
                                        ##   in Loop: Header=BB20_6 Depth=1
	movq	%r11, %rcx
	cmpq	%r10, %r11
	movq	%rbx, %rsi
	je	LBB20_13
LBB20_11:                               ## %"for relu1_0_d_def__.s12.n.ni.us.us.preheader"
                                        ##   in Loop: Header=BB20_6 Depth=1
	movq	88(%rsp), %rdx                  ## 8-byte Reload
	leaq	(%rdx,%rax,4), %rdx
	movq	80(%rsp), %rbx                  ## 8-byte Reload
	leaq	(%rbx,%rax,4), %rax
	movq	-80(%rsp), %rbx                 ## 8-byte Reload
	leaq	(%rbx,%rsi,4), %rbx
	.p2align	4, 0x90
LBB20_12:                               ## %"for relu1_0_d_def__.s12.n.ni.us.us"
                                        ##   Parent Loop BB20_6 Depth=1
                                        ## =>  This Inner Loop Header: Depth=2
	vmovss	(%rax,%rcx,4), %xmm2            ## xmm2 = mem[0],zero,zero,zero
	vmulss	(%rdx,%rcx,4), %xmm2, %xmm2
	vmulss	(%rbp,%rcx,4), %xmm2, %xmm2
	vfmadd213ss	(%rbx,%rcx,4), %xmm0, %xmm2 ## xmm2 = (xmm0 * xmm2) + mem
	vmovss	%xmm2, (%rbx,%rcx,4)
	incq	%rcx
	cmpq	%rcx, %r10
	jne	LBB20_12
LBB20_13:                               ## %"end for relu1_0_d_def__.s12.n.ni.loopexit.us.us"
                                        ##   in Loop: Header=BB20_6 Depth=1
	cmpl	$0, -128(%rsp)                  ## 4-byte Folded Reload
	movl	-112(%rsp), %ebx                ## 4-byte Reload
	movq	-88(%rsp), %r8                  ## 8-byte Reload
	jle	LBB20_17
## %bb.14:                              ## %"for relu1_0_d_def__.s12.n.ni.rebased.preheader.us.us"
                                        ##   in Loop: Header=BB20_6 Depth=1
	movslq	%r8d, %rax
	imull	-124(%rsp), %r9d                ## 4-byte Folded Reload
	movslq	%r9d, %rcx
	movq	8(%rsp), %rdx                   ## 8-byte Reload
	addq	%rcx, %rdx
	addq	(%rsp), %rcx                    ## 8-byte Folded Reload
	movq	-40(%rsp), %rsi                 ## 8-byte Reload
	vmovss	-4(%rsi,%rdx,4), %xmm2          ## xmm2 = mem[0],zero,zero,zero
	vmulss	-4(%rsi,%rcx,4), %xmm2, %xmm2
	cmpl	$16, -128(%rsp)                 ## 4-byte Folded Reload
	jae	LBB20_18
## %bb.15:                              ##   in Loop: Header=BB20_6 Depth=1
	xorl	%ecx, %ecx
	jmp	LBB20_25
	.p2align	4, 0x90
LBB20_18:                               ## %vector.ph54
                                        ##   in Loop: Header=BB20_6 Depth=1
	vbroadcastss	%xmm2, %ymm3
	cmpq	$0, 48(%rsp)                    ## 8-byte Folded Reload
	je	LBB20_19
## %bb.20:                              ## %vector.body52.preheader
                                        ##   in Loop: Header=BB20_6 Depth=1
	movq	-16(%rsp), %rcx                 ## 8-byte Reload
	addq	%rax, %rcx
	movq	-8(%rsp), %rdx                  ## 8-byte Reload
	leaq	(%rdx,%rcx,4), %rcx
	movq	24(%rsp), %rdx                  ## 8-byte Reload
	xorl	%esi, %esi
	movq	-120(%rsp), %r9                 ## 8-byte Reload
	.p2align	4, 0x90
LBB20_21:                               ## %vector.body52
                                        ##   Parent Loop BB20_6 Depth=1
                                        ## =>  This Inner Loop Header: Depth=2
	vmulps	-96(%r12,%rsi,4), %ymm3, %ymm4
	vmulps	-64(%r12,%rsi,4), %ymm3, %ymm5
	vfmadd213ps	-96(%rcx,%rsi,4), %ymm1, %ymm4 ## ymm4 = (ymm1 * ymm4) + mem
	vfmadd213ps	-64(%rcx,%rsi,4), %ymm1, %ymm5 ## ymm5 = (ymm1 * ymm5) + mem
	vmovups	%ymm4, -96(%rcx,%rsi,4)
	vmovups	%ymm5, -64(%rcx,%rsi,4)
	vmulps	-32(%r12,%rsi,4), %ymm3, %ymm4
	vmulps	(%r12,%rsi,4), %ymm3, %ymm5
	vfmadd213ps	-32(%rcx,%rsi,4), %ymm1, %ymm4 ## ymm4 = (ymm1 * ymm4) + mem
	vfmadd213ps	(%rcx,%rsi,4), %ymm1, %ymm5 ## ymm5 = (ymm1 * ymm5) + mem
	vmovups	%ymm4, -32(%rcx,%rsi,4)
	vmovups	%ymm5, (%rcx,%rsi,4)
	addq	$32, %rsi
	addq	$2, %rdx
	jne	LBB20_21
## %bb.22:                              ## %middle.block50.unr-lcssa
                                        ##   in Loop: Header=BB20_6 Depth=1
	testb	$1, 40(%rsp)                    ## 1-byte Folded Reload
	je	LBB20_24
LBB20_23:                               ## %vector.body52.epil
                                        ##   in Loop: Header=BB20_6 Depth=1
	movq	-32(%rsp), %rcx                 ## 8-byte Reload
	addl	%r9d, %ecx
	movq	-48(%rsp), %rdx                 ## 8-byte Reload
	addq	%r9, %rdx
	imull	%ebx, %ecx
	imulq	-72(%rsp), %rdx                 ## 8-byte Folded Reload
	addl	-60(%rsp), %ecx                 ## 4-byte Folded Reload
	movslq	%ecx, %rcx
	addq	-16(%rsp), %rsi                 ## 8-byte Folded Reload
	addq	%rsi, %rcx
	addq	%rdx, %rsi
	movq	-56(%rsp), %rdx                 ## 8-byte Reload
	vmulps	(%rdx,%rsi,4), %ymm3, %ymm4
	vmulps	32(%rdx,%rsi,4), %ymm3, %ymm3
	movq	-80(%rsp), %rdx                 ## 8-byte Reload
	vfmadd213ps	(%rdx,%rcx,4), %ymm1, %ymm4 ## ymm4 = (ymm1 * ymm4) + mem
	vfmadd213ps	32(%rdx,%rcx,4), %ymm1, %ymm3 ## ymm3 = (ymm1 * ymm3) + mem
	vmovups	%ymm4, (%rdx,%rcx,4)
	vmovups	%ymm3, 32(%rdx,%rcx,4)
LBB20_24:                               ## %middle.block50
                                        ##   in Loop: Header=BB20_6 Depth=1
	movq	56(%rsp), %rdx                  ## 8-byte Reload
	movq	%rdx, %rcx
	cmpq	%r14, %rdx
	je	LBB20_17
LBB20_25:                               ## %"for relu1_0_d_def__.s12.n.ni.rebased.us.us.preheader"
                                        ##   in Loop: Header=BB20_6 Depth=1
	movq	72(%rsp), %rdx                  ## 8-byte Reload
	leaq	(%rdx,%rax,4), %rax
	.p2align	4, 0x90
LBB20_16:                               ## %"for relu1_0_d_def__.s12.n.ni.rebased.us.us"
                                        ##   Parent Loop BB20_6 Depth=1
                                        ## =>  This Inner Loop Header: Depth=2
	vmulss	(%r15,%rcx,4), %xmm2, %xmm3
	vfmadd213ss	(%rax,%rcx,4), %xmm0, %xmm3 ## xmm3 = (xmm0 * xmm3) + mem
	vmovss	%xmm3, (%rax,%rcx,4)
	incq	%rcx
	cmpq	%rcx, %r14
	jne	LBB20_16
	jmp	LBB20_17
LBB20_19:                               ##   in Loop: Header=BB20_6 Depth=1
	xorl	%esi, %esi
	movq	-120(%rsp), %r9                 ## 8-byte Reload
	testb	$1, 40(%rsp)                    ## 1-byte Folded Reload
	jne	LBB20_23
	jmp	LBB20_24
LBB20_2:                                ## %"for relu1_0_d_def__.s12.w.wi.preheader20"
	decl	%edx
	vmovd	%r12d, %xmm0
	vpbroadcastd	%xmm0, %ymm0
	vpor	LCPI20_0(%rip), %ymm0, %ymm0
	vmovd	%edx, %xmm1
	vpbroadcastd	%xmm1, %ymm1
	vpminsd	%ymm0, %ymm1, %ymm0
	movq	-72(%rsp), %rcx                 ## 8-byte Reload
	movslq	-96(%rsp), %rdi                 ## 4-byte Folded Reload
	movl	-104(%rsp), %r14d               ## 4-byte Reload
	imulq	%rcx, %r13
	addq	%r12, %r13
	movl	%ebx, %eax
	leaq	(,%r13,4), %rbx
	addq	%rbp, %rbx
	shlq	$2, %rcx
	movq	%rcx, %r15
	shll	$6, %r8d
	shll	$6, %eax
	subl	%eax, %r8d
	orl	$13, %r8d
	movq	-88(%rsp), %rax                 ## 8-byte Reload
	imull	%r8d, %eax
	movq	-120(%rsp), %rcx                ## 8-byte Reload
	leal	(%rax,%rcx,8), %edx
	vbroadcastss	LCPI20_1(%rip), %ymm1   ## ymm1 = [9.99999971E-10,9.99999971E-10,9.99999971E-10,9.99999971E-10,9.99999971E-10,9.99999971E-10,9.99999971E-10,9.99999971E-10]
	movl	-112(%rsp), %r8d                ## 4-byte Reload
	movl	-124(%rsp), %r10d               ## 4-byte Reload
	movq	-40(%rsp), %rax                 ## 8-byte Reload
	.p2align	4, 0x90
LBB20_3:                                ## %"for relu1_0_d_def__.s12.w.wi"
                                        ## =>This Inner Loop Header: Depth=1
	testq	%rdi, %rdi
	movl	$0, %esi
	cmovgl	%edi, %esi
	imull	%r10d, %esi
	leal	(%rsi,%r9), %ecx
	vmovd	%ecx, %xmm2
	vpbroadcastd	%xmm2, %ymm2
	vpaddd	%ymm0, %ymm2, %ymm3
	vextracti128	$1, %ymm3, %xmm2
	vmovd	%xmm2, %ecx
	movslq	%ecx, %rcx
	vpextrd	$1, %xmm2, %ebp
	movslq	%ebp, %rbp
	vmovss	(%rax,%rcx,4), %xmm4            ## xmm4 = mem[0],zero,zero,zero
	vpextrd	$2, %xmm2, %ecx
	vinsertps	$16, (%rax,%rbp,4), %xmm4, %xmm4 ## xmm4 = xmm4[0],mem[0],xmm4[2,3]
	vmovd	%xmm3, %ebp
	movslq	%ecx, %rcx
	vinsertps	$32, (%rax,%rcx,4), %xmm4, %xmm4 ## xmm4 = xmm4[0,1],mem[0],xmm4[3]
	movslq	%ebp, %rcx
	vpextrd	$3, %xmm2, %ebp
	movslq	%ebp, %rbp
	vinsertps	$48, (%rax,%rbp,4), %xmm4, %xmm2 ## xmm2 = xmm4[0,1,2],mem[0]
	vpextrd	$1, %xmm3, %ebp
	vmovss	(%rax,%rcx,4), %xmm4            ## xmm4 = mem[0],zero,zero,zero
	movslq	%ebp, %rcx
	vinsertps	$16, (%rax,%rcx,4), %xmm4, %xmm4 ## xmm4 = xmm4[0],mem[0],xmm4[2,3]
	vpextrd	$2, %xmm3, %ecx
	movslq	%ecx, %rcx
	vinsertps	$32, (%rax,%rcx,4), %xmm4, %xmm4 ## xmm4 = xmm4[0,1],mem[0],xmm4[3]
	vpextrd	$3, %xmm3, %ecx
	movslq	%ecx, %rcx
	vinsertps	$48, (%rax,%rcx,4), %xmm4, %xmm3 ## xmm3 = xmm4[0,1,2],mem[0]
	addl	%r11d, %esi
	vmovd	%esi, %xmm4
	vpbroadcastd	%xmm4, %ymm4
	vpaddd	%ymm0, %ymm4, %ymm4
	vextracti128	$1, %ymm4, %xmm5
	vmovd	%xmm5, %ecx
	movslq	%ecx, %rcx
	vmovss	(%rax,%rcx,4), %xmm6            ## xmm6 = mem[0],zero,zero,zero
	vpextrd	$1, %xmm5, %ecx
	movslq	%ecx, %rcx
	vinsertps	$16, (%rax,%rcx,4), %xmm6, %xmm6 ## xmm6 = xmm6[0],mem[0],xmm6[2,3]
	vpextrd	$2, %xmm5, %ecx
	movslq	%ecx, %rcx
	vinsertps	$32, (%rax,%rcx,4), %xmm6, %xmm6 ## xmm6 = xmm6[0,1],mem[0],xmm6[3]
	vpextrd	$3, %xmm5, %ecx
	movslq	%ecx, %rcx
	vinsertps	$48, (%rax,%rcx,4), %xmm6, %xmm5 ## xmm5 = xmm6[0,1,2],mem[0]
	vmovd	%xmm4, %ecx
	movslq	%ecx, %rcx
	vmovss	(%rax,%rcx,4), %xmm6            ## xmm6 = mem[0],zero,zero,zero
	vpextrd	$1, %xmm4, %ecx
	movslq	%ecx, %rcx
	vinsertps	$16, (%rax,%rcx,4), %xmm6, %xmm6 ## xmm6 = xmm6[0],mem[0],xmm6[2,3]
	vpextrd	$2, %xmm4, %ecx
	movslq	%ecx, %rcx
	vinsertps	$32, (%rax,%rcx,4), %xmm6, %xmm6 ## xmm6 = xmm6[0,1],mem[0],xmm6[3]
	vpextrd	$3, %xmm4, %ecx
	movslq	%ecx, %rcx
	vinsertps	$48, (%rax,%rcx,4), %xmm6, %xmm4 ## xmm4 = xmm6[0,1,2],mem[0]
	vinsertf128	$1, %xmm2, %ymm3, %ymm2
	movslq	%edx, %rdx
	vinsertf128	$1, %xmm5, %ymm4, %ymm3
	vmulps	%ymm3, %ymm2, %ymm2
	vmulps	(%rbx), %ymm2, %ymm2
	movq	-80(%rsp), %rcx                 ## 8-byte Reload
	vfmadd213ps	(%rcx,%rdx,4), %ymm1, %ymm2 ## ymm2 = (ymm1 * ymm2) + mem
	vmovups	%ymm2, (%rcx,%rdx,4)
	addq	%r15, %rbx
	incq	%rdi
	addl	%r8d, %edx
	decq	%r14
	jne	LBB20_3
LBB20_40:                               ## %destructor_block
	xorl	%eax, %eax
	addq	$112, %rsp
	popq	%rbx
	popq	%r12
	popq	%r13
	popq	%r14
	popq	%r15
	popq	%rbp
	vzeroupper
	retq
LBB20_26:                               ## %"for relu1_0_d_def__.s12.w.wi.preheader.split.us.split"
	movl	%ecx, -120(%rsp)                ## 4-byte Spill
	testl	%r14d, %r14d
	movl	-124(%rsp), %edx                ## 4-byte Reload
	jle	LBB20_40
## %bb.27:                              ## %"for relu1_0_d_def__.s12.w.wi.us.us4.preheader"
	movq	%rbp, %r9
	movslq	%esi, %r11
	movl	-128(%rsp), %ebp                ## 4-byte Reload
	movslq	-96(%rsp), %r10                 ## 4-byte Folded Reload
	movl	-104(%rsp), %r15d               ## 4-byte Reload
	movl	%ebp, %r12d
	andl	$-16, %r12d
	leaq	-16(%r12), %r14
	movq	%r14, -96(%rsp)                 ## 8-byte Spill
	shrq	$4, %r14
	incq	%r14
	movq	-72(%rsp), %rcx                 ## 8-byte Reload
	movq	%rcx, %rax
	imulq	-48(%rsp), %rax                 ## 8-byte Folded Reload
	addq	%r11, %rax
	movq	-56(%rsp), %r13                 ## 8-byte Reload
	leaq	96(,%rax,4), %rdi
	addq	%r13, %rdi
	leaq	(,%rcx,4), %rbx
	movq	-80(%rsp), %rcx                 ## 8-byte Reload
	leaq	96(%rcx), %rsi
	movq	%rsi, -104(%rsp)                ## 8-byte Spill
	shll	$6, %r8d
	movl	-120(%rsp), %esi                ## 4-byte Reload
	shll	$6, %esi
	subl	%esi, %r8d
	orl	$13, %r8d
	imull	%r8d, %r9d
	movq	%r14, %rsi
	andq	$-2, %rsi
	negq	%rsi
	movq	%rsi, -88(%rsp)                 ## 8-byte Spill
	leaq	(%r13,%rax,4), %r13
	movq	%r11, -120(%rsp)                ## 8-byte Spill
	leaq	(%rcx,%r11,4), %r8
	vmovss	LCPI20_1(%rip), %xmm0           ## xmm0 = mem[0],zero,zero,zero
	vbroadcastss	LCPI20_1(%rip), %ymm1   ## ymm1 = [9.99999971E-10,9.99999971E-10,9.99999971E-10,9.99999971E-10,9.99999971E-10,9.99999971E-10,9.99999971E-10,9.99999971E-10]
	xorl	%r11d, %r11d
	jmp	LBB20_28
LBB20_39:                               ## %after_bb.loopexit.us.us17
                                        ##   in Loop: Header=BB20_28 Depth=1
	incq	%r11
	addq	%rbx, %rdi
	addl	-112(%rsp), %r9d                ## 4-byte Folded Reload
	addq	%rbx, %r13
	cmpq	%r15, %r11
	movl	-124(%rsp), %edx                ## 4-byte Reload
	je	LBB20_40
LBB20_28:                               ## %"for relu1_0_d_def__.s12.w.wi.us.us4"
                                        ## =>This Loop Header: Depth=1
                                        ##     Child Loop BB20_33 Depth 2
                                        ##     Child Loop BB20_38 Depth 2
	movslq	%r9d, %rax
	leaq	(%r11,%r10), %rcx
	testq	%rcx, %rcx
	movl	$0, %esi
	cmovlel	%esi, %ecx
	imull	%edx, %ecx
	movslq	%ecx, %rcx
	movq	8(%rsp), %rdx                   ## 8-byte Reload
	addq	%rcx, %rdx
	addq	(%rsp), %rcx                    ## 8-byte Folded Reload
	movq	-40(%rsp), %rsi                 ## 8-byte Reload
	vmovss	-4(%rsi,%rdx,4), %xmm2          ## xmm2 = mem[0],zero,zero,zero
	vmulss	-4(%rsi,%rcx,4), %xmm2, %xmm2
	cmpl	$16, -128(%rsp)                 ## 4-byte Folded Reload
	jae	LBB20_30
## %bb.29:                              ##   in Loop: Header=BB20_28 Depth=1
	xorl	%ecx, %ecx
	jmp	LBB20_37
LBB20_30:                               ## %vector.ph
                                        ##   in Loop: Header=BB20_28 Depth=1
	vbroadcastss	%xmm2, %ymm3
	cmpq	$0, -96(%rsp)                   ## 8-byte Folded Reload
	je	LBB20_31
## %bb.32:                              ## %vector.body.preheader
                                        ##   in Loop: Header=BB20_28 Depth=1
	movq	-120(%rsp), %rcx                ## 8-byte Reload
	addq	%rax, %rcx
	movq	-104(%rsp), %rdx                ## 8-byte Reload
	leaq	(%rdx,%rcx,4), %rcx
	movq	-88(%rsp), %rdx                 ## 8-byte Reload
	xorl	%esi, %esi
LBB20_33:                               ## %vector.body
                                        ##   Parent Loop BB20_28 Depth=1
                                        ## =>  This Inner Loop Header: Depth=2
	vmulps	-96(%rdi,%rsi,4), %ymm3, %ymm4
	vmulps	-64(%rdi,%rsi,4), %ymm3, %ymm5
	vfmadd213ps	-96(%rcx,%rsi,4), %ymm1, %ymm4 ## ymm4 = (ymm1 * ymm4) + mem
	vfmadd213ps	-64(%rcx,%rsi,4), %ymm1, %ymm5 ## ymm5 = (ymm1 * ymm5) + mem
	vmovups	%ymm4, -96(%rcx,%rsi,4)
	vmovups	%ymm5, -64(%rcx,%rsi,4)
	vmulps	-32(%rdi,%rsi,4), %ymm3, %ymm4
	vmulps	(%rdi,%rsi,4), %ymm3, %ymm5
	vfmadd213ps	-32(%rcx,%rsi,4), %ymm1, %ymm4 ## ymm4 = (ymm1 * ymm4) + mem
	vfmadd213ps	(%rcx,%rsi,4), %ymm1, %ymm5 ## ymm5 = (ymm1 * ymm5) + mem
	vmovups	%ymm4, -32(%rcx,%rsi,4)
	vmovups	%ymm5, (%rcx,%rsi,4)
	addq	$32, %rsi
	addq	$2, %rdx
	jne	LBB20_33
	jmp	LBB20_34
LBB20_31:                               ##   in Loop: Header=BB20_28 Depth=1
	xorl	%esi, %esi
LBB20_34:                               ## %middle.block.unr-lcssa
                                        ##   in Loop: Header=BB20_28 Depth=1
	testb	$1, %r14b
	je	LBB20_36
## %bb.35:                              ## %vector.body.epil
                                        ##   in Loop: Header=BB20_28 Depth=1
	movq	-32(%rsp), %rcx                 ## 8-byte Reload
	addl	%r11d, %ecx
	imull	-112(%rsp), %ecx                ## 4-byte Folded Reload
	movq	-48(%rsp), %rdx                 ## 8-byte Reload
	addq	%r11, %rdx
	addl	-60(%rsp), %ecx                 ## 4-byte Folded Reload
	imulq	-72(%rsp), %rdx                 ## 8-byte Folded Reload
	movslq	%ecx, %rcx
	addq	-120(%rsp), %rsi                ## 8-byte Folded Reload
	addq	%rsi, %rcx
	addq	%rdx, %rsi
	movq	-56(%rsp), %rdx                 ## 8-byte Reload
	vmulps	(%rdx,%rsi,4), %ymm3, %ymm4
	vmulps	32(%rdx,%rsi,4), %ymm3, %ymm3
	movq	-80(%rsp), %rdx                 ## 8-byte Reload
	vfmadd213ps	(%rdx,%rcx,4), %ymm1, %ymm4 ## ymm4 = (ymm1 * ymm4) + mem
	vfmadd213ps	32(%rdx,%rcx,4), %ymm1, %ymm3 ## ymm3 = (ymm1 * ymm3) + mem
	vmovups	%ymm4, (%rdx,%rcx,4)
	vmovups	%ymm3, 32(%rdx,%rcx,4)
LBB20_36:                               ## %middle.block
                                        ##   in Loop: Header=BB20_28 Depth=1
	movq	%r12, %rcx
	cmpq	%rbp, %r12
	je	LBB20_39
LBB20_37:                               ## %"for relu1_0_d_def__.s12.n.ni.rebased.us.us9.preheader"
                                        ##   in Loop: Header=BB20_28 Depth=1
	leaq	(%r8,%rax,4), %rax
LBB20_38:                               ## %"for relu1_0_d_def__.s12.n.ni.rebased.us.us9"
                                        ##   Parent Loop BB20_28 Depth=1
                                        ## =>  This Inner Loop Header: Depth=2
	vmulss	(%r13,%rcx,4), %xmm2, %xmm3
	vfmadd213ss	(%rax,%rcx,4), %xmm0, %xmm3 ## xmm3 = (xmm0 * xmm3) + mem
	vmovss	%xmm3, (%rax,%rcx,4)
	incq	%rcx
	cmpq	%rcx, %rbp
	jne	LBB20_38
	jmp	LBB20_39
                                        ## -- End function
	.section	__TEXT,__const
	.p2align	5                               ## -- Begin function train_cost_model.par_for.relu1_0_d_def__.s13.n.n.n
LCPI21_0:
	.long	0                               ## 0x0
	.long	1                               ## 0x1
	.long	2                               ## 0x2
	.long	3                               ## 0x3
	.long	4                               ## 0x4
	.long	5                               ## 0x5
	.long	6                               ## 0x6
	.long	7                               ## 0x7
	.section	__TEXT,__literal4,4byte_literals
	.p2align	2
LCPI21_1:
	.long	0x3089705f                      ## float 9.99999971E-10
	.section	__TEXT,__text,regular,pure_instructions
	.p2align	4, 0x90
_train_cost_model.par_for.relu1_0_d_def__.s13.n.n.n: ## @train_cost_model.par_for.relu1_0_d_def__.s13.n.n.n
## %bb.0:                               ## %entry
	pushq	%rbp
	pushq	%r15
	pushq	%r14
	pushq	%r13
	pushq	%r12
	pushq	%rbx
	subq	$112, %rsp
	movq	%rdx, %rdi
	movl	12(%rdx), %r10d
	movl	24(%rdx), %r13d
	movl	%esi, %r9d
	sarl	$31, %r9d
	xorl	%ecx, %ecx
	testl	%r13d, %r13d
	sete	%cl
	movl	%ecx, %ebx
	negl	%ebx
	movl	%r13d, %ebp
	sarl	$31, %ebp
	subl	%r9d, %esi
	orl	%r13d, %ebx
	movl	%esi, %eax
	cltd
	idivl	%ebx
	movl	%edx, %r14d
	movl	%ebp, %r15d
	leal	(%rcx,%r13), %ebx
	movl	%esi, %eax
	cltd
	idivl	%ebx
	notl	%r15d
	decl	%ecx
	movl	%r15d, %r8d
	subl	%ebp, %r8d
	andl	%r9d, %r8d
	addl	%eax, %r8d
	andl	%ecx, %r8d
	leal	(%r8,%r8), %eax
	movl	%eax, -96(%rsp)                 ## 4-byte Spill
	subl	%eax, %r10d
	cmpl	$2, %r10d
	movl	$2, %eax
	cmovll	%r10d, %eax
	xorl	%ebx, %ebx
	testl	%r10d, %r10d
	cmovlel	%ebx, %eax
	movl	%eax, -104(%rsp)                ## 4-byte Spill
	jle	LBB21_40
## %bb.1:                               ## %"for relu1_0_d_def__.s13.w.wi.preheader"
	movl	(%rdi), %edx
	movl	16(%rdi), %r12d
	movl	32(%rdi), %r10d
	movl	36(%rdi), %eax
	movl	40(%rdi), %r11d
	xorl	%r13d, %ebp
	addl	%r15d, %ebp
	andl	%r9d, %ebp
	addl	%ebp, %r14d
	andl	%ecx, %r14d
	movq	%r14, -120(%rsp)                ## 8-byte Spill
	leal	(,%r14,8), %esi
	movl	%eax, -128(%rsp)                ## 4-byte Spill
	movl	%eax, %r9d
	subl	%r10d, %r9d
	movl	%r11d, %r15d
	subl	%r10d, %r11d
	movslq	8(%rdi), %rax
	movslq	%r8d, %r13
	subq	%rax, %r13
	addq	%r13, %r13
	movq	%r12, -88(%rsp)                 ## 8-byte Spill
	movl	%r12d, %eax
	shll	$5, %eax
	movl	%eax, -112(%rsp)                ## 4-byte Spill
	movl	%edx, %eax
	subl	%esi, %eax
	cmpl	$8, %eax
	movl	$8, %ecx
	cmovll	%eax, %ecx
	testl	%eax, %eax
	cmovgl	%ecx, %ebx
	movl	%ebx, -108(%rsp)                ## 4-byte Spill
	movl	20(%rdi), %ecx
	movl	%ecx, -124(%rsp)                ## 4-byte Spill
	movl	%esi, %ebx
	subl	%edx, %ebx
	movl	%ebx, %ecx
	sarl	$31, %ecx
	andl	%ebx, %ecx
	cmpl	$-8, %ecx
	movl	$-8, %r14d
	cmovgl	%ecx, %r14d
	movl	28(%rdi), %ebx
	movq	48(%rdi), %rbp
	movq	64(%rdi), %rcx
	movq	%rcx, -80(%rsp)                 ## 8-byte Spill
	movq	80(%rdi), %rcx
	movq	%rcx, -40(%rsp)                 ## 8-byte Spill
	movslq	4(%rdi), %rcx
	movq	%rcx, -72(%rsp)                 ## 8-byte Spill
	leal	8(%rsi), %ecx
	movslq	%esi, %r12
	cmpl	%edx, %ecx
	jle	LBB21_2
## %bb.4:                               ## %"for relu1_0_d_def__.s13.w.wi.preheader.split.us"
	movq	%rbp, -56(%rsp)                 ## 8-byte Spill
	movq	%r13, -48(%rsp)                 ## 8-byte Spill
	movq	-88(%rsp), %rbp                 ## 8-byte Reload
	leal	(,%rbp,4), %ecx
	leal	(%rcx,%rcx,2), %ecx
	movl	%ecx, -60(%rsp)                 ## 4-byte Spill
	movl	%r8d, %ecx
	subl	%ebx, %ecx
	addl	%ecx, %ecx
	movq	%rcx, -32(%rsp)                 ## 8-byte Spill
	subl	%r10d, %edx
	movl	-128(%rsp), %edi                ## 4-byte Reload
	addl	%edx, %edi
	addl	%r15d, %edx
	movl	-108(%rsp), %ecx                ## 4-byte Reload
	addl	%ecx, %esi
	addl	%ecx, %r14d
	movslq	%edi, %rcx
	movq	%rcx, 8(%rsp)                   ## 8-byte Spill
	movslq	%edx, %rcx
	movq	%rcx, (%rsp)                    ## 8-byte Spill
	testl	%eax, %eax
	movl	%r14d, -128(%rsp)               ## 4-byte Spill
	movl	%ebx, %ecx
	jle	LBB21_26
## %bb.5:                               ## %"for relu1_0_d_def__.s13.w.wi.us.us.preheader"
	addl	%r12d, %r11d
	addl	%r12d, %r9d
	movslq	%r9d, %rax
	movq	%rax, -24(%rsp)                 ## 8-byte Spill
	movslq	%r11d, %r9
	movl	-108(%rsp), %r10d               ## 4-byte Reload
	movslq	%esi, %rsi
	movl	%r14d, %r14d
	movslq	-96(%rsp), %rax                 ## 4-byte Folded Reload
	movq	%rax, 16(%rsp)                  ## 8-byte Spill
	movl	-104(%rsp), %eax                ## 4-byte Reload
	movq	%rax, 104(%rsp)                 ## 8-byte Spill
	movl	%r14d, %eax
	andl	$-16, %eax
	movq	%rax, 56(%rsp)                  ## 8-byte Spill
	addq	$-16, %rax
	movq	%rax, 48(%rsp)                  ## 8-byte Spill
	movq	%rax, %rbx
	shrq	$4, %rbx
	incq	%rbx
	movl	%r10d, %r11d
	andl	$2147483616, %r11d              ## imm = 0x7FFFFFE0
	shll	$6, %r8d
	shll	$6, %ecx
	subl	%ecx, %r8d
	movq	-72(%rsp), %rdx                 ## 8-byte Reload
	movq	%rdx, %rax
	imulq	-48(%rsp), %rax                 ## 8-byte Folded Reload
	addq	%rax, %r12
	orl	$12, %r8d
	imull	%r8d, %ebp
	movq	-56(%rsp), %rcx                 ## 8-byte Reload
	movq	%rdx, %r8
	leaq	(%rcx,%r12,4), %rdi
	addq	$96, %rdi
	movq	%rbp, -88(%rsp)                 ## 8-byte Spill
	movq	-120(%rsp), %rdx                ## 8-byte Reload
	leal	(%rbp,%rdx,8), %r13d
	leaq	(%rcx,%r12,4), %rbp
	addq	%rsi, %rax
	movq	%rbx, %rdx
	movq	%rbx, 40(%rsp)                  ## 8-byte Spill
	andq	$-2, %rbx
	negq	%rbx
	movq	%rbx, 24(%rsp)                  ## 8-byte Spill
	leaq	(%rcx,%rax,4), %r12
	addq	$96, %r12
	leaq	(%rcx,%rax,4), %r15
	vmovss	LCPI21_1(%rip), %xmm0           ## xmm0 = mem[0],zero,zero,zero
	vbroadcastss	LCPI21_1(%rip), %ymm1   ## ymm1 = [9.99999971E-10,9.99999971E-10,9.99999971E-10,9.99999971E-10,9.99999971E-10,9.99999971E-10,9.99999971E-10,9.99999971E-10]
	movq	-40(%rsp), %rcx                 ## 8-byte Reload
	leaq	96(%rcx), %rax
	movq	%rax, 32(%rsp)                  ## 8-byte Spill
	leaq	(,%r8,4), %rax
	movq	%rax, 96(%rsp)                  ## 8-byte Spill
	movq	-80(%rsp), %rax                 ## 8-byte Reload
	leaq	96(%rax), %rdx
	movq	%rdx, -8(%rsp)                  ## 8-byte Spill
	movq	%r9, 64(%rsp)                   ## 8-byte Spill
	leaq	(%rcx,%r9,4), %rdx
	movq	%rdx, 88(%rsp)                  ## 8-byte Spill
	movq	-24(%rsp), %rdx                 ## 8-byte Reload
	leaq	(%rcx,%rdx,4), %rcx
	movq	%rcx, 80(%rsp)                  ## 8-byte Spill
	movq	%rsi, -16(%rsp)                 ## 8-byte Spill
	leaq	(%rax,%rsi,4), %rax
	movq	%rax, 72(%rsp)                  ## 8-byte Spill
	movq	16(%rsp), %rdx                  ## 8-byte Reload
	xorl	%ebx, %ebx
	movl	-124(%rsp), %esi                ## 4-byte Reload
	movl	-108(%rsp), %ecx                ## 4-byte Reload
	jmp	LBB21_6
	.p2align	4, 0x90
LBB21_17:                               ## %after_bb.us.us
                                        ##   in Loop: Header=BB21_6 Depth=1
	movl	%ebx, %ecx
	movq	-120(%rsp), %rbx                ## 8-byte Reload
	incq	%rbx
	movq	-104(%rsp), %rdx                ## 8-byte Reload
	incq	%rdx
	movq	96(%rsp), %rax                  ## 8-byte Reload
	addq	%rax, %rdi
	movl	-96(%rsp), %r13d                ## 4-byte Reload
	addl	%ecx, %r13d
	addq	%rax, %rbp
	addq	%rax, %r12
	addl	%ecx, %r8d
	movq	%r8, -88(%rsp)                  ## 8-byte Spill
	addq	%rax, %r15
	cmpq	104(%rsp), %rbx                 ## 8-byte Folded Reload
	movl	-124(%rsp), %esi                ## 4-byte Reload
	movl	-108(%rsp), %ecx                ## 4-byte Reload
	je	LBB21_40
LBB21_6:                                ## %"for relu1_0_d_def__.s13.w.wi.us.us"
                                        ## =>This Loop Header: Depth=1
                                        ##     Child Loop BB21_9 Depth 2
                                        ##     Child Loop BB21_12 Depth 2
                                        ##     Child Loop BB21_21 Depth 2
                                        ##     Child Loop BB21_16 Depth 2
	testq	%rdx, %rdx
	movl	$0, %eax
	movq	%rdx, -104(%rsp)                ## 8-byte Spill
	cmovgl	%edx, %eax
	imull	%esi, %eax
	cltq
	movq	16(%rsp), %rdx                  ## 8-byte Reload
	movq	%rbx, -120(%rsp)                ## 8-byte Spill
	leaq	(%rbx,%rdx), %r9
	testq	%r9, %r9
	movl	$0, %edx
	cmovlel	%edx, %r9d
	movslq	%r13d, %rsi
	cmpl	$32, %ecx
	movl	%r13d, -96(%rsp)                ## 4-byte Spill
	jae	LBB21_8
## %bb.7:                               ##   in Loop: Header=BB21_6 Depth=1
	xorl	%ecx, %ecx
	jmp	LBB21_11
	.p2align	4, 0x90
LBB21_8:                                ## %vector.body71.preheader
                                        ##   in Loop: Header=BB21_6 Depth=1
	movq	64(%rsp), %rcx                  ## 8-byte Reload
	addq	%rax, %rcx
	movq	-24(%rsp), %rdx                 ## 8-byte Reload
	addq	%rax, %rdx
	movq	32(%rsp), %rbx                  ## 8-byte Reload
	leaq	(%rbx,%rcx,4), %r13
	leaq	(%rbx,%rdx,4), %r8
	movq	-8(%rsp), %rcx                  ## 8-byte Reload
	movq	%rsi, %rbx
	leaq	(%rcx,%rsi,4), %rcx
	xorl	%edx, %edx
	.p2align	4, 0x90
LBB21_9:                                ## %vector.body71
                                        ##   Parent Loop BB21_6 Depth=1
                                        ## =>  This Inner Loop Header: Depth=2
	vmovups	-96(%r8,%rdx,4), %ymm2
	vmovups	-64(%r8,%rdx,4), %ymm3
	vmovups	-32(%r8,%rdx,4), %ymm4
	vmovups	(%r8,%rdx,4), %ymm5
	vmulps	-96(%r13,%rdx,4), %ymm2, %ymm2
	vmulps	-64(%r13,%rdx,4), %ymm3, %ymm3
	vmulps	-32(%r13,%rdx,4), %ymm4, %ymm4
	vmulps	(%r13,%rdx,4), %ymm5, %ymm5
	vmulps	-96(%rdi,%rdx,4), %ymm2, %ymm2
	vmulps	-64(%rdi,%rdx,4), %ymm3, %ymm3
	vmulps	-32(%rdi,%rdx,4), %ymm4, %ymm4
	vmulps	(%rdi,%rdx,4), %ymm5, %ymm5
	vfmadd213ps	-96(%rcx,%rdx,4), %ymm1, %ymm2 ## ymm2 = (ymm1 * ymm2) + mem
	vfmadd213ps	-64(%rcx,%rdx,4), %ymm1, %ymm3 ## ymm3 = (ymm1 * ymm3) + mem
	vfmadd213ps	-32(%rcx,%rdx,4), %ymm1, %ymm4 ## ymm4 = (ymm1 * ymm4) + mem
	vfmadd213ps	(%rcx,%rdx,4), %ymm1, %ymm5 ## ymm5 = (ymm1 * ymm5) + mem
	vmovups	%ymm2, -96(%rcx,%rdx,4)
	vmovups	%ymm3, -64(%rcx,%rdx,4)
	vmovups	%ymm4, -32(%rcx,%rdx,4)
	vmovups	%ymm5, (%rcx,%rdx,4)
	addq	$32, %rdx
	cmpq	%rdx, %r11
	jne	LBB21_9
## %bb.10:                              ## %middle.block69
                                        ##   in Loop: Header=BB21_6 Depth=1
	movq	%r11, %rcx
	cmpq	%r10, %r11
	movq	%rbx, %rsi
	je	LBB21_13
LBB21_11:                               ## %"for relu1_0_d_def__.s13.n.ni.us.us.preheader"
                                        ##   in Loop: Header=BB21_6 Depth=1
	movq	88(%rsp), %rdx                  ## 8-byte Reload
	leaq	(%rdx,%rax,4), %rdx
	movq	80(%rsp), %rbx                  ## 8-byte Reload
	leaq	(%rbx,%rax,4), %rax
	movq	-80(%rsp), %rbx                 ## 8-byte Reload
	leaq	(%rbx,%rsi,4), %rbx
	.p2align	4, 0x90
LBB21_12:                               ## %"for relu1_0_d_def__.s13.n.ni.us.us"
                                        ##   Parent Loop BB21_6 Depth=1
                                        ## =>  This Inner Loop Header: Depth=2
	vmovss	(%rax,%rcx,4), %xmm2            ## xmm2 = mem[0],zero,zero,zero
	vmulss	(%rdx,%rcx,4), %xmm2, %xmm2
	vmulss	(%rbp,%rcx,4), %xmm2, %xmm2
	vfmadd213ss	(%rbx,%rcx,4), %xmm0, %xmm2 ## xmm2 = (xmm0 * xmm2) + mem
	vmovss	%xmm2, (%rbx,%rcx,4)
	incq	%rcx
	cmpq	%rcx, %r10
	jne	LBB21_12
LBB21_13:                               ## %"end for relu1_0_d_def__.s13.n.ni.loopexit.us.us"
                                        ##   in Loop: Header=BB21_6 Depth=1
	cmpl	$0, -128(%rsp)                  ## 4-byte Folded Reload
	movl	-112(%rsp), %ebx                ## 4-byte Reload
	movq	-88(%rsp), %r8                  ## 8-byte Reload
	jle	LBB21_17
## %bb.14:                              ## %"for relu1_0_d_def__.s13.n.ni.rebased.preheader.us.us"
                                        ##   in Loop: Header=BB21_6 Depth=1
	movslq	%r8d, %rax
	imull	-124(%rsp), %r9d                ## 4-byte Folded Reload
	movslq	%r9d, %rcx
	movq	8(%rsp), %rdx                   ## 8-byte Reload
	addq	%rcx, %rdx
	addq	(%rsp), %rcx                    ## 8-byte Folded Reload
	movq	-40(%rsp), %rsi                 ## 8-byte Reload
	vmovss	-4(%rsi,%rdx,4), %xmm2          ## xmm2 = mem[0],zero,zero,zero
	vmulss	-4(%rsi,%rcx,4), %xmm2, %xmm2
	cmpl	$16, -128(%rsp)                 ## 4-byte Folded Reload
	jae	LBB21_18
## %bb.15:                              ##   in Loop: Header=BB21_6 Depth=1
	xorl	%ecx, %ecx
	jmp	LBB21_25
	.p2align	4, 0x90
LBB21_18:                               ## %vector.ph54
                                        ##   in Loop: Header=BB21_6 Depth=1
	vbroadcastss	%xmm2, %ymm3
	cmpq	$0, 48(%rsp)                    ## 8-byte Folded Reload
	je	LBB21_19
## %bb.20:                              ## %vector.body52.preheader
                                        ##   in Loop: Header=BB21_6 Depth=1
	movq	-16(%rsp), %rcx                 ## 8-byte Reload
	addq	%rax, %rcx
	movq	-8(%rsp), %rdx                  ## 8-byte Reload
	leaq	(%rdx,%rcx,4), %rcx
	movq	24(%rsp), %rdx                  ## 8-byte Reload
	xorl	%esi, %esi
	movq	-120(%rsp), %r9                 ## 8-byte Reload
	.p2align	4, 0x90
LBB21_21:                               ## %vector.body52
                                        ##   Parent Loop BB21_6 Depth=1
                                        ## =>  This Inner Loop Header: Depth=2
	vmulps	-96(%r12,%rsi,4), %ymm3, %ymm4
	vmulps	-64(%r12,%rsi,4), %ymm3, %ymm5
	vfmadd213ps	-96(%rcx,%rsi,4), %ymm1, %ymm4 ## ymm4 = (ymm1 * ymm4) + mem
	vfmadd213ps	-64(%rcx,%rsi,4), %ymm1, %ymm5 ## ymm5 = (ymm1 * ymm5) + mem
	vmovups	%ymm4, -96(%rcx,%rsi,4)
	vmovups	%ymm5, -64(%rcx,%rsi,4)
	vmulps	-32(%r12,%rsi,4), %ymm3, %ymm4
	vmulps	(%r12,%rsi,4), %ymm3, %ymm5
	vfmadd213ps	-32(%rcx,%rsi,4), %ymm1, %ymm4 ## ymm4 = (ymm1 * ymm4) + mem
	vfmadd213ps	(%rcx,%rsi,4), %ymm1, %ymm5 ## ymm5 = (ymm1 * ymm5) + mem
	vmovups	%ymm4, -32(%rcx,%rsi,4)
	vmovups	%ymm5, (%rcx,%rsi,4)
	addq	$32, %rsi
	addq	$2, %rdx
	jne	LBB21_21
## %bb.22:                              ## %middle.block50.unr-lcssa
                                        ##   in Loop: Header=BB21_6 Depth=1
	testb	$1, 40(%rsp)                    ## 1-byte Folded Reload
	je	LBB21_24
LBB21_23:                               ## %vector.body52.epil
                                        ##   in Loop: Header=BB21_6 Depth=1
	movq	-32(%rsp), %rcx                 ## 8-byte Reload
	addl	%r9d, %ecx
	movq	-48(%rsp), %rdx                 ## 8-byte Reload
	addq	%r9, %rdx
	imull	%ebx, %ecx
	imulq	-72(%rsp), %rdx                 ## 8-byte Folded Reload
	addl	-60(%rsp), %ecx                 ## 4-byte Folded Reload
	movslq	%ecx, %rcx
	addq	-16(%rsp), %rsi                 ## 8-byte Folded Reload
	addq	%rsi, %rcx
	addq	%rdx, %rsi
	movq	-56(%rsp), %rdx                 ## 8-byte Reload
	vmulps	(%rdx,%rsi,4), %ymm3, %ymm4
	vmulps	32(%rdx,%rsi,4), %ymm3, %ymm3
	movq	-80(%rsp), %rdx                 ## 8-byte Reload
	vfmadd213ps	(%rdx,%rcx,4), %ymm1, %ymm4 ## ymm4 = (ymm1 * ymm4) + mem
	vfmadd213ps	32(%rdx,%rcx,4), %ymm1, %ymm3 ## ymm3 = (ymm1 * ymm3) + mem
	vmovups	%ymm4, (%rdx,%rcx,4)
	vmovups	%ymm3, 32(%rdx,%rcx,4)
LBB21_24:                               ## %middle.block50
                                        ##   in Loop: Header=BB21_6 Depth=1
	movq	56(%rsp), %rdx                  ## 8-byte Reload
	movq	%rdx, %rcx
	cmpq	%r14, %rdx
	je	LBB21_17
LBB21_25:                               ## %"for relu1_0_d_def__.s13.n.ni.rebased.us.us.preheader"
                                        ##   in Loop: Header=BB21_6 Depth=1
	movq	72(%rsp), %rdx                  ## 8-byte Reload
	leaq	(%rdx,%rax,4), %rax
	.p2align	4, 0x90
LBB21_16:                               ## %"for relu1_0_d_def__.s13.n.ni.rebased.us.us"
                                        ##   Parent Loop BB21_6 Depth=1
                                        ## =>  This Inner Loop Header: Depth=2
	vmulss	(%r15,%rcx,4), %xmm2, %xmm3
	vfmadd213ss	(%rax,%rcx,4), %xmm0, %xmm3 ## xmm3 = (xmm0 * xmm3) + mem
	vmovss	%xmm3, (%rax,%rcx,4)
	incq	%rcx
	cmpq	%rcx, %r14
	jne	LBB21_16
	jmp	LBB21_17
LBB21_19:                               ##   in Loop: Header=BB21_6 Depth=1
	xorl	%esi, %esi
	movq	-120(%rsp), %r9                 ## 8-byte Reload
	testb	$1, 40(%rsp)                    ## 1-byte Folded Reload
	jne	LBB21_23
	jmp	LBB21_24
LBB21_2:                                ## %"for relu1_0_d_def__.s13.w.wi.preheader20"
	decl	%edx
	vmovd	%r12d, %xmm0
	vpbroadcastd	%xmm0, %ymm0
	vpor	LCPI21_0(%rip), %ymm0, %ymm0
	vmovd	%edx, %xmm1
	vpbroadcastd	%xmm1, %ymm1
	vpminsd	%ymm0, %ymm1, %ymm0
	movq	-72(%rsp), %rcx                 ## 8-byte Reload
	movslq	-96(%rsp), %rdi                 ## 4-byte Folded Reload
	movl	-104(%rsp), %r14d               ## 4-byte Reload
	imulq	%rcx, %r13
	addq	%r12, %r13
	movl	%ebx, %eax
	leaq	(,%r13,4), %rbx
	addq	%rbp, %rbx
	shlq	$2, %rcx
	movq	%rcx, %r15
	shll	$6, %r8d
	shll	$6, %eax
	subl	%eax, %r8d
	orl	$12, %r8d
	movq	-88(%rsp), %rax                 ## 8-byte Reload
	imull	%r8d, %eax
	movq	-120(%rsp), %rcx                ## 8-byte Reload
	leal	(%rax,%rcx,8), %edx
	vbroadcastss	LCPI21_1(%rip), %ymm1   ## ymm1 = [9.99999971E-10,9.99999971E-10,9.99999971E-10,9.99999971E-10,9.99999971E-10,9.99999971E-10,9.99999971E-10,9.99999971E-10]
	movl	-112(%rsp), %r8d                ## 4-byte Reload
	movl	-124(%rsp), %r10d               ## 4-byte Reload
	movq	-40(%rsp), %rax                 ## 8-byte Reload
	.p2align	4, 0x90
LBB21_3:                                ## %"for relu1_0_d_def__.s13.w.wi"
                                        ## =>This Inner Loop Header: Depth=1
	testq	%rdi, %rdi
	movl	$0, %esi
	cmovgl	%edi, %esi
	imull	%r10d, %esi
	leal	(%rsi,%r9), %ecx
	vmovd	%ecx, %xmm2
	vpbroadcastd	%xmm2, %ymm2
	vpaddd	%ymm0, %ymm2, %ymm3
	vextracti128	$1, %ymm3, %xmm2
	vmovd	%xmm2, %ecx
	movslq	%ecx, %rcx
	vpextrd	$1, %xmm2, %ebp
	movslq	%ebp, %rbp
	vmovss	(%rax,%rcx,4), %xmm4            ## xmm4 = mem[0],zero,zero,zero
	vpextrd	$2, %xmm2, %ecx
	vinsertps	$16, (%rax,%rbp,4), %xmm4, %xmm4 ## xmm4 = xmm4[0],mem[0],xmm4[2,3]
	vmovd	%xmm3, %ebp
	movslq	%ecx, %rcx
	vinsertps	$32, (%rax,%rcx,4), %xmm4, %xmm4 ## xmm4 = xmm4[0,1],mem[0],xmm4[3]
	movslq	%ebp, %rcx
	vpextrd	$3, %xmm2, %ebp
	movslq	%ebp, %rbp
	vinsertps	$48, (%rax,%rbp,4), %xmm4, %xmm2 ## xmm2 = xmm4[0,1,2],mem[0]
	vpextrd	$1, %xmm3, %ebp
	vmovss	(%rax,%rcx,4), %xmm4            ## xmm4 = mem[0],zero,zero,zero
	movslq	%ebp, %rcx
	vinsertps	$16, (%rax,%rcx,4), %xmm4, %xmm4 ## xmm4 = xmm4[0],mem[0],xmm4[2,3]
	vpextrd	$2, %xmm3, %ecx
	movslq	%ecx, %rcx
	vinsertps	$32, (%rax,%rcx,4), %xmm4, %xmm4 ## xmm4 = xmm4[0,1],mem[0],xmm4[3]
	vpextrd	$3, %xmm3, %ecx
	movslq	%ecx, %rcx
	vinsertps	$48, (%rax,%rcx,4), %xmm4, %xmm3 ## xmm3 = xmm4[0,1,2],mem[0]
	addl	%r11d, %esi
	vmovd	%esi, %xmm4
	vpbroadcastd	%xmm4, %ymm4
	vpaddd	%ymm0, %ymm4, %ymm4
	vextracti128	$1, %ymm4, %xmm5
	vmovd	%xmm5, %ecx
	movslq	%ecx, %rcx
	vmovss	(%rax,%rcx,4), %xmm6            ## xmm6 = mem[0],zero,zero,zero
	vpextrd	$1, %xmm5, %ecx
	movslq	%ecx, %rcx
	vinsertps	$16, (%rax,%rcx,4), %xmm6, %xmm6 ## xmm6 = xmm6[0],mem[0],xmm6[2,3]
	vpextrd	$2, %xmm5, %ecx
	movslq	%ecx, %rcx
	vinsertps	$32, (%rax,%rcx,4), %xmm6, %xmm6 ## xmm6 = xmm6[0,1],mem[0],xmm6[3]
	vpextrd	$3, %xmm5, %ecx
	movslq	%ecx, %rcx
	vinsertps	$48, (%rax,%rcx,4), %xmm6, %xmm5 ## xmm5 = xmm6[0,1,2],mem[0]
	vmovd	%xmm4, %ecx
	movslq	%ecx, %rcx
	vmovss	(%rax,%rcx,4), %xmm6            ## xmm6 = mem[0],zero,zero,zero
	vpextrd	$1, %xmm4, %ecx
	movslq	%ecx, %rcx
	vinsertps	$16, (%rax,%rcx,4), %xmm6, %xmm6 ## xmm6 = xmm6[0],mem[0],xmm6[2,3]
	vpextrd	$2, %xmm4, %ecx
	movslq	%ecx, %rcx
	vinsertps	$32, (%rax,%rcx,4), %xmm6, %xmm6 ## xmm6 = xmm6[0,1],mem[0],xmm6[3]
	vpextrd	$3, %xmm4, %ecx
	movslq	%ecx, %rcx
	vinsertps	$48, (%rax,%rcx,4), %xmm6, %xmm4 ## xmm4 = xmm6[0,1,2],mem[0]
	vinsertf128	$1, %xmm2, %ymm3, %ymm2
	movslq	%edx, %rdx
	vinsertf128	$1, %xmm5, %ymm4, %ymm3
	vmulps	%ymm3, %ymm2, %ymm2
	vmulps	(%rbx), %ymm2, %ymm2
	movq	-80(%rsp), %rcx                 ## 8-byte Reload
	vfmadd213ps	(%rcx,%rdx,4), %ymm1, %ymm2 ## ymm2 = (ymm1 * ymm2) + mem
	vmovups	%ymm2, (%rcx,%rdx,4)
	addq	%r15, %rbx
	incq	%rdi
	addl	%r8d, %edx
	decq	%r14
	jne	LBB21_3
LBB21_40:                               ## %destructor_block
	xorl	%eax, %eax
	addq	$112, %rsp
	popq	%rbx
	popq	%r12
	popq	%r13
	popq	%r14
	popq	%r15
	popq	%rbp
	vzeroupper
	retq
LBB21_26:                               ## %"for relu1_0_d_def__.s13.w.wi.preheader.split.us.split"
	movl	%ecx, -120(%rsp)                ## 4-byte Spill
	testl	%r14d, %r14d
	movl	-124(%rsp), %edx                ## 4-byte Reload
	jle	LBB21_40
## %bb.27:                              ## %"for relu1_0_d_def__.s13.w.wi.us.us4.preheader"
	movq	%rbp, %r9
	movslq	%esi, %r11
	movl	-128(%rsp), %ebp                ## 4-byte Reload
	movslq	-96(%rsp), %r10                 ## 4-byte Folded Reload
	movl	-104(%rsp), %r15d               ## 4-byte Reload
	movl	%ebp, %r12d
	andl	$-16, %r12d
	leaq	-16(%r12), %r14
	movq	%r14, -96(%rsp)                 ## 8-byte Spill
	shrq	$4, %r14
	incq	%r14
	movq	-72(%rsp), %rcx                 ## 8-byte Reload
	movq	%rcx, %rax
	imulq	-48(%rsp), %rax                 ## 8-byte Folded Reload
	addq	%r11, %rax
	movq	-56(%rsp), %r13                 ## 8-byte Reload
	leaq	96(,%rax,4), %rdi
	addq	%r13, %rdi
	leaq	(,%rcx,4), %rbx
	movq	-80(%rsp), %rcx                 ## 8-byte Reload
	leaq	96(%rcx), %rsi
	movq	%rsi, -104(%rsp)                ## 8-byte Spill
	shll	$6, %r8d
	movl	-120(%rsp), %esi                ## 4-byte Reload
	shll	$6, %esi
	subl	%esi, %r8d
	orl	$12, %r8d
	imull	%r8d, %r9d
	movq	%r14, %rsi
	andq	$-2, %rsi
	negq	%rsi
	movq	%rsi, -88(%rsp)                 ## 8-byte Spill
	leaq	(%r13,%rax,4), %r13
	movq	%r11, -120(%rsp)                ## 8-byte Spill
	leaq	(%rcx,%r11,4), %r8
	vmovss	LCPI21_1(%rip), %xmm0           ## xmm0 = mem[0],zero,zero,zero
	vbroadcastss	LCPI21_1(%rip), %ymm1   ## ymm1 = [9.99999971E-10,9.99999971E-10,9.99999971E-10,9.99999971E-10,9.99999971E-10,9.99999971E-10,9.99999971E-10,9.99999971E-10]
	xorl	%r11d, %r11d
	jmp	LBB21_28
LBB21_39:                               ## %after_bb.loopexit.us.us17
                                        ##   in Loop: Header=BB21_28 Depth=1
	incq	%r11
	addq	%rbx, %rdi
	addl	-112(%rsp), %r9d                ## 4-byte Folded Reload
	addq	%rbx, %r13
	cmpq	%r15, %r11
	movl	-124(%rsp), %edx                ## 4-byte Reload
	je	LBB21_40
LBB21_28:                               ## %"for relu1_0_d_def__.s13.w.wi.us.us4"
                                        ## =>This Loop Header: Depth=1
                                        ##     Child Loop BB21_33 Depth 2
                                        ##     Child Loop BB21_38 Depth 2
	movslq	%r9d, %rax
	leaq	(%r11,%r10), %rcx
	testq	%rcx, %rcx
	movl	$0, %esi
	cmovlel	%esi, %ecx
	imull	%edx, %ecx
	movslq	%ecx, %rcx
	movq	8(%rsp), %rdx                   ## 8-byte Reload
	addq	%rcx, %rdx
	addq	(%rsp), %rcx                    ## 8-byte Folded Reload
	movq	-40(%rsp), %rsi                 ## 8-byte Reload
	vmovss	-4(%rsi,%rdx,4), %xmm2          ## xmm2 = mem[0],zero,zero,zero
	vmulss	-4(%rsi,%rcx,4), %xmm2, %xmm2
	cmpl	$16, -128(%rsp)                 ## 4-byte Folded Reload
	jae	LBB21_30
## %bb.29:                              ##   in Loop: Header=BB21_28 Depth=1
	xorl	%ecx, %ecx
	jmp	LBB21_37
LBB21_30:                               ## %vector.ph
                                        ##   in Loop: Header=BB21_28 Depth=1
	vbroadcastss	%xmm2, %ymm3
	cmpq	$0, -96(%rsp)                   ## 8-byte Folded Reload
	je	LBB21_31
## %bb.32:                              ## %vector.body.preheader
                                        ##   in Loop: Header=BB21_28 Depth=1
	movq	-120(%rsp), %rcx                ## 8-byte Reload
	addq	%rax, %rcx
	movq	-104(%rsp), %rdx                ## 8-byte Reload
	leaq	(%rdx,%rcx,4), %rcx
	movq	-88(%rsp), %rdx                 ## 8-byte Reload
	xorl	%esi, %esi
LBB21_33:                               ## %vector.body
                                        ##   Parent Loop BB21_28 Depth=1
                                        ## =>  This Inner Loop Header: Depth=2
	vmulps	-96(%rdi,%rsi,4), %ymm3, %ymm4
	vmulps	-64(%rdi,%rsi,4), %ymm3, %ymm5
	vfmadd213ps	-96(%rcx,%rsi,4), %ymm1, %ymm4 ## ymm4 = (ymm1 * ymm4) + mem
	vfmadd213ps	-64(%rcx,%rsi,4), %ymm1, %ymm5 ## ymm5 = (ymm1 * ymm5) + mem
	vmovups	%ymm4, -96(%rcx,%rsi,4)
	vmovups	%ymm5, -64(%rcx,%rsi,4)
	vmulps	-32(%rdi,%rsi,4), %ymm3, %ymm4
	vmulps	(%rdi,%rsi,4), %ymm3, %ymm5
	vfmadd213ps	-32(%rcx,%rsi,4), %ymm1, %ymm4 ## ymm4 = (ymm1 * ymm4) + mem
	vfmadd213ps	(%rcx,%rsi,4), %ymm1, %ymm5 ## ymm5 = (ymm1 * ymm5) + mem
	vmovups	%ymm4, -32(%rcx,%rsi,4)
	vmovups	%ymm5, (%rcx,%rsi,4)
	addq	$32, %rsi
	addq	$2, %rdx
	jne	LBB21_33
	jmp	LBB21_34
LBB21_31:                               ##   in Loop: Header=BB21_28 Depth=1
	xorl	%esi, %esi
LBB21_34:                               ## %middle.block.unr-lcssa
                                        ##   in Loop: Header=BB21_28 Depth=1
	testb	$1, %r14b
	je	LBB21_36
## %bb.35:                              ## %vector.body.epil
                                        ##   in Loop: Header=BB21_28 Depth=1
	movq	-32(%rsp), %rcx                 ## 8-byte Reload
	addl	%r11d, %ecx
	imull	-112(%rsp), %ecx                ## 4-byte Folded Reload
	movq	-48(%rsp), %rdx                 ## 8-byte Reload
	addq	%r11, %rdx
	addl	-60(%rsp), %ecx                 ## 4-byte Folded Reload
	imulq	-72(%rsp), %rdx                 ## 8-byte Folded Reload
	movslq	%ecx, %rcx
	addq	-120(%rsp), %rsi                ## 8-byte Folded Reload
	addq	%rsi, %rcx
	addq	%rdx, %rsi
	movq	-56(%rsp), %rdx                 ## 8-byte Reload
	vmulps	(%rdx,%rsi,4), %ymm3, %ymm4
	vmulps	32(%rdx,%rsi,4), %ymm3, %ymm3
	movq	-80(%rsp), %rdx                 ## 8-byte Reload
	vfmadd213ps	(%rdx,%rcx,4), %ymm1, %ymm4 ## ymm4 = (ymm1 * ymm4) + mem
	vfmadd213ps	32(%rdx,%rcx,4), %ymm1, %ymm3 ## ymm3 = (ymm1 * ymm3) + mem
	vmovups	%ymm4, (%rdx,%rcx,4)
	vmovups	%ymm3, 32(%rdx,%rcx,4)
LBB21_36:                               ## %middle.block
                                        ##   in Loop: Header=BB21_28 Depth=1
	movq	%r12, %rcx
	cmpq	%rbp, %r12
	je	LBB21_39
LBB21_37:                               ## %"for relu1_0_d_def__.s13.n.ni.rebased.us.us9.preheader"
                                        ##   in Loop: Header=BB21_28 Depth=1
	leaq	(%r8,%rax,4), %rax
LBB21_38:                               ## %"for relu1_0_d_def__.s13.n.ni.rebased.us.us9"
                                        ##   Parent Loop BB21_28 Depth=1
                                        ## =>  This Inner Loop Header: Depth=2
	vmulss	(%r13,%rcx,4), %xmm2, %xmm3
	vfmadd213ss	(%rax,%rcx,4), %xmm0, %xmm3 ## xmm3 = (xmm0 * xmm3) + mem
	vmovss	%xmm3, (%rax,%rcx,4)
	incq	%rcx
	cmpq	%rcx, %rbp
	jne	LBB21_38
	jmp	LBB21_39
                                        ## -- End function
	.section	__TEXT,__const
	.p2align	5                               ## -- Begin function train_cost_model.par_for.relu1_0_d_def__.s14.n.n.n
LCPI22_0:
	.long	0                               ## 0x0
	.long	1                               ## 0x1
	.long	2                               ## 0x2
	.long	3                               ## 0x3
	.long	4                               ## 0x4
	.long	5                               ## 0x5
	.long	6                               ## 0x6
	.long	7                               ## 0x7
	.section	__TEXT,__literal4,4byte_literals
	.p2align	2
LCPI22_1:
	.long	0x3089705f                      ## float 9.99999971E-10
	.section	__TEXT,__text,regular,pure_instructions
	.p2align	4, 0x90
_train_cost_model.par_for.relu1_0_d_def__.s14.n.n.n: ## @train_cost_model.par_for.relu1_0_d_def__.s14.n.n.n
## %bb.0:                               ## %entry
	pushq	%rbp
	pushq	%r15
	pushq	%r14
	pushq	%r13
	pushq	%r12
	pushq	%rbx
	subq	$112, %rsp
	movq	%rdx, %rdi
	movl	12(%rdx), %r10d
	movl	24(%rdx), %r13d
	movl	%esi, %r9d
	sarl	$31, %r9d
	xorl	%ecx, %ecx
	testl	%r13d, %r13d
	sete	%cl
	movl	%ecx, %ebx
	negl	%ebx
	movl	%r13d, %ebp
	sarl	$31, %ebp
	subl	%r9d, %esi
	orl	%r13d, %ebx
	movl	%esi, %eax
	cltd
	idivl	%ebx
	movl	%edx, %r14d
	movl	%ebp, %r15d
	leal	(%rcx,%r13), %ebx
	movl	%esi, %eax
	cltd
	idivl	%ebx
	notl	%r15d
	decl	%ecx
	movl	%r15d, %r8d
	subl	%ebp, %r8d
	andl	%r9d, %r8d
	addl	%eax, %r8d
	andl	%ecx, %r8d
	leal	(%r8,%r8), %eax
	movl	%eax, -96(%rsp)                 ## 4-byte Spill
	subl	%eax, %r10d
	cmpl	$2, %r10d
	movl	$2, %eax
	cmovll	%r10d, %eax
	xorl	%ebx, %ebx
	testl	%r10d, %r10d
	cmovlel	%ebx, %eax
	movl	%eax, -104(%rsp)                ## 4-byte Spill
	jle	LBB22_40
## %bb.1:                               ## %"for relu1_0_d_def__.s14.w.wi.preheader"
	movl	(%rdi), %edx
	movl	16(%rdi), %r12d
	movl	32(%rdi), %r10d
	movl	36(%rdi), %eax
	movl	40(%rdi), %r11d
	xorl	%r13d, %ebp
	addl	%r15d, %ebp
	andl	%r9d, %ebp
	addl	%ebp, %r14d
	andl	%ecx, %r14d
	movq	%r14, -120(%rsp)                ## 8-byte Spill
	leal	(,%r14,8), %esi
	movl	%eax, -128(%rsp)                ## 4-byte Spill
	movl	%eax, %r9d
	subl	%r10d, %r9d
	movl	%r11d, %r15d
	subl	%r10d, %r11d
	movslq	8(%rdi), %rax
	movslq	%r8d, %r13
	subq	%rax, %r13
	addq	%r13, %r13
	movq	%r12, -88(%rsp)                 ## 8-byte Spill
	movl	%r12d, %eax
	shll	$5, %eax
	movl	%eax, -112(%rsp)                ## 4-byte Spill
	movl	%edx, %eax
	subl	%esi, %eax
	cmpl	$8, %eax
	movl	$8, %ecx
	cmovll	%eax, %ecx
	testl	%eax, %eax
	cmovgl	%ecx, %ebx
	movl	%ebx, -108(%rsp)                ## 4-byte Spill
	movl	20(%rdi), %ecx
	movl	%ecx, -124(%rsp)                ## 4-byte Spill
	movl	%esi, %ebx
	subl	%edx, %ebx
	movl	%ebx, %ecx
	sarl	$31, %ecx
	andl	%ebx, %ecx
	cmpl	$-8, %ecx
	movl	$-8, %r14d
	cmovgl	%ecx, %r14d
	movl	28(%rdi), %ebx
	movq	48(%rdi), %rbp
	movq	64(%rdi), %rcx
	movq	%rcx, -80(%rsp)                 ## 8-byte Spill
	movq	80(%rdi), %rcx
	movq	%rcx, -40(%rsp)                 ## 8-byte Spill
	movslq	4(%rdi), %rcx
	movq	%rcx, -72(%rsp)                 ## 8-byte Spill
	leal	8(%rsi), %ecx
	movslq	%esi, %r12
	cmpl	%edx, %ecx
	jle	LBB22_2
## %bb.4:                               ## %"for relu1_0_d_def__.s14.w.wi.preheader.split.us"
	movq	%rbp, -56(%rsp)                 ## 8-byte Spill
	movq	%r13, -48(%rsp)                 ## 8-byte Spill
	movq	-88(%rsp), %rbp                 ## 8-byte Reload
	leal	(%rbp,%rbp,4), %ecx
	leal	(%rbp,%rcx,2), %ecx
	movl	%ecx, -60(%rsp)                 ## 4-byte Spill
	movl	%r8d, %ecx
	subl	%ebx, %ecx
	addl	%ecx, %ecx
	movq	%rcx, -32(%rsp)                 ## 8-byte Spill
	subl	%r10d, %edx
	movl	-128(%rsp), %edi                ## 4-byte Reload
	addl	%edx, %edi
	addl	%r15d, %edx
	movl	-108(%rsp), %ecx                ## 4-byte Reload
	addl	%ecx, %esi
	addl	%ecx, %r14d
	movslq	%edi, %rcx
	movq	%rcx, 8(%rsp)                   ## 8-byte Spill
	movslq	%edx, %rcx
	movq	%rcx, (%rsp)                    ## 8-byte Spill
	testl	%eax, %eax
	movl	%r14d, -128(%rsp)               ## 4-byte Spill
	movl	%ebx, %ecx
	jle	LBB22_26
## %bb.5:                               ## %"for relu1_0_d_def__.s14.w.wi.us.us.preheader"
	addl	%r12d, %r11d
	addl	%r12d, %r9d
	movslq	%r9d, %rax
	movq	%rax, -24(%rsp)                 ## 8-byte Spill
	movslq	%r11d, %r9
	movl	-108(%rsp), %r10d               ## 4-byte Reload
	movslq	%esi, %rsi
	movl	%r14d, %r14d
	movslq	-96(%rsp), %rax                 ## 4-byte Folded Reload
	movq	%rax, 16(%rsp)                  ## 8-byte Spill
	movl	-104(%rsp), %eax                ## 4-byte Reload
	movq	%rax, 104(%rsp)                 ## 8-byte Spill
	movl	%r14d, %eax
	andl	$-16, %eax
	movq	%rax, 56(%rsp)                  ## 8-byte Spill
	addq	$-16, %rax
	movq	%rax, 48(%rsp)                  ## 8-byte Spill
	movq	%rax, %rbx
	shrq	$4, %rbx
	incq	%rbx
	movl	%r10d, %r11d
	andl	$2147483616, %r11d              ## imm = 0x7FFFFFE0
	shll	$6, %r8d
	shll	$6, %ecx
	subl	%ecx, %r8d
	movq	-72(%rsp), %rdx                 ## 8-byte Reload
	movq	%rdx, %rax
	imulq	-48(%rsp), %rax                 ## 8-byte Folded Reload
	addq	%rax, %r12
	orl	$11, %r8d
	imull	%r8d, %ebp
	movq	-56(%rsp), %rcx                 ## 8-byte Reload
	movq	%rdx, %r8
	leaq	(%rcx,%r12,4), %rdi
	addq	$96, %rdi
	movq	%rbp, -88(%rsp)                 ## 8-byte Spill
	movq	-120(%rsp), %rdx                ## 8-byte Reload
	leal	(%rbp,%rdx,8), %r13d
	leaq	(%rcx,%r12,4), %rbp
	addq	%rsi, %rax
	movq	%rbx, %rdx
	movq	%rbx, 40(%rsp)                  ## 8-byte Spill
	andq	$-2, %rbx
	negq	%rbx
	movq	%rbx, 24(%rsp)                  ## 8-byte Spill
	leaq	(%rcx,%rax,4), %r12
	addq	$96, %r12
	leaq	(%rcx,%rax,4), %r15
	vmovss	LCPI22_1(%rip), %xmm0           ## xmm0 = mem[0],zero,zero,zero
	vbroadcastss	LCPI22_1(%rip), %ymm1   ## ymm1 = [9.99999971E-10,9.99999971E-10,9.99999971E-10,9.99999971E-10,9.99999971E-10,9.99999971E-10,9.99999971E-10,9.99999971E-10]
	movq	-40(%rsp), %rcx                 ## 8-byte Reload
	leaq	96(%rcx), %rax
	movq	%rax, 32(%rsp)                  ## 8-byte Spill
	leaq	(,%r8,4), %rax
	movq	%rax, 96(%rsp)                  ## 8-byte Spill
	movq	-80(%rsp), %rax                 ## 8-byte Reload
	leaq	96(%rax), %rdx
	movq	%rdx, -8(%rsp)                  ## 8-byte Spill
	movq	%r9, 64(%rsp)                   ## 8-byte Spill
	leaq	(%rcx,%r9,4), %rdx
	movq	%rdx, 88(%rsp)                  ## 8-byte Spill
	movq	-24(%rsp), %rdx                 ## 8-byte Reload
	leaq	(%rcx,%rdx,4), %rcx
	movq	%rcx, 80(%rsp)                  ## 8-byte Spill
	movq	%rsi, -16(%rsp)                 ## 8-byte Spill
	leaq	(%rax,%rsi,4), %rax
	movq	%rax, 72(%rsp)                  ## 8-byte Spill
	movq	16(%rsp), %rdx                  ## 8-byte Reload
	xorl	%ebx, %ebx
	movl	-124(%rsp), %esi                ## 4-byte Reload
	movl	-108(%rsp), %ecx                ## 4-byte Reload
	jmp	LBB22_6
	.p2align	4, 0x90
LBB22_17:                               ## %after_bb.us.us
                                        ##   in Loop: Header=BB22_6 Depth=1
	movl	%ebx, %ecx
	movq	-120(%rsp), %rbx                ## 8-byte Reload
	incq	%rbx
	movq	-104(%rsp), %rdx                ## 8-byte Reload
	incq	%rdx
	movq	96(%rsp), %rax                  ## 8-byte Reload
	addq	%rax, %rdi
	movl	-96(%rsp), %r13d                ## 4-byte Reload
	addl	%ecx, %r13d
	addq	%rax, %rbp
	addq	%rax, %r12
	addl	%ecx, %r8d
	movq	%r8, -88(%rsp)                  ## 8-byte Spill
	addq	%rax, %r15
	cmpq	104(%rsp), %rbx                 ## 8-byte Folded Reload
	movl	-124(%rsp), %esi                ## 4-byte Reload
	movl	-108(%rsp), %ecx                ## 4-byte Reload
	je	LBB22_40
LBB22_6:                                ## %"for relu1_0_d_def__.s14.w.wi.us.us"
                                        ## =>This Loop Header: Depth=1
                                        ##     Child Loop BB22_9 Depth 2
                                        ##     Child Loop BB22_12 Depth 2
                                        ##     Child Loop BB22_21 Depth 2
                                        ##     Child Loop BB22_16 Depth 2
	testq	%rdx, %rdx
	movl	$0, %eax
	movq	%rdx, -104(%rsp)                ## 8-byte Spill
	cmovgl	%edx, %eax
	imull	%esi, %eax
	cltq
	movq	16(%rsp), %rdx                  ## 8-byte Reload
	movq	%rbx, -120(%rsp)                ## 8-byte Spill
	leaq	(%rbx,%rdx), %r9
	testq	%r9, %r9
	movl	$0, %edx
	cmovlel	%edx, %r9d
	movslq	%r13d, %rsi
	cmpl	$32, %ecx
	movl	%r13d, -96(%rsp)                ## 4-byte Spill
	jae	LBB22_8
## %bb.7:                               ##   in Loop: Header=BB22_6 Depth=1
	xorl	%ecx, %ecx
	jmp	LBB22_11
	.p2align	4, 0x90
LBB22_8:                                ## %vector.body71.preheader
                                        ##   in Loop: Header=BB22_6 Depth=1
	movq	64(%rsp), %rcx                  ## 8-byte Reload
	addq	%rax, %rcx
	movq	-24(%rsp), %rdx                 ## 8-byte Reload
	addq	%rax, %rdx
	movq	32(%rsp), %rbx                  ## 8-byte Reload
	leaq	(%rbx,%rcx,4), %r13
	leaq	(%rbx,%rdx,4), %r8
	movq	-8(%rsp), %rcx                  ## 8-byte Reload
	movq	%rsi, %rbx
	leaq	(%rcx,%rsi,4), %rcx
	xorl	%edx, %edx
	.p2align	4, 0x90
LBB22_9:                                ## %vector.body71
                                        ##   Parent Loop BB22_6 Depth=1
                                        ## =>  This Inner Loop Header: Depth=2
	vmovups	-96(%r8,%rdx,4), %ymm2
	vmovups	-64(%r8,%rdx,4), %ymm3
	vmovups	-32(%r8,%rdx,4), %ymm4
	vmovups	(%r8,%rdx,4), %ymm5
	vmulps	-96(%r13,%rdx,4), %ymm2, %ymm2
	vmulps	-64(%r13,%rdx,4), %ymm3, %ymm3
	vmulps	-32(%r13,%rdx,4), %ymm4, %ymm4
	vmulps	(%r13,%rdx,4), %ymm5, %ymm5
	vmulps	-96(%rdi,%rdx,4), %ymm2, %ymm2
	vmulps	-64(%rdi,%rdx,4), %ymm3, %ymm3
	vmulps	-32(%rdi,%rdx,4), %ymm4, %ymm4
	vmulps	(%rdi,%rdx,4), %ymm5, %ymm5
	vfmadd213ps	-96(%rcx,%rdx,4), %ymm1, %ymm2 ## ymm2 = (ymm1 * ymm2) + mem
	vfmadd213ps	-64(%rcx,%rdx,4), %ymm1, %ymm3 ## ymm3 = (ymm1 * ymm3) + mem
	vfmadd213ps	-32(%rcx,%rdx,4), %ymm1, %ymm4 ## ymm4 = (ymm1 * ymm4) + mem
	vfmadd213ps	(%rcx,%rdx,4), %ymm1, %ymm5 ## ymm5 = (ymm1 * ymm5) + mem
	vmovups	%ymm2, -96(%rcx,%rdx,4)
	vmovups	%ymm3, -64(%rcx,%rdx,4)
	vmovups	%ymm4, -32(%rcx,%rdx,4)
	vmovups	%ymm5, (%rcx,%rdx,4)
	addq	$32, %rdx
	cmpq	%rdx, %r11
	jne	LBB22_9
## %bb.10:                              ## %middle.block69
                                        ##   in Loop: Header=BB22_6 Depth=1
	movq	%r11, %rcx
	cmpq	%r10, %r11
	movq	%rbx, %rsi
	je	LBB22_13
LBB22_11:                               ## %"for relu1_0_d_def__.s14.n.ni.us.us.preheader"
                                        ##   in Loop: Header=BB22_6 Depth=1
	movq	88(%rsp), %rdx                  ## 8-byte Reload
	leaq	(%rdx,%rax,4), %rdx
	movq	80(%rsp), %rbx                  ## 8-byte Reload
	leaq	(%rbx,%rax,4), %rax
	movq	-80(%rsp), %rbx                 ## 8-byte Reload
	leaq	(%rbx,%rsi,4), %rbx
	.p2align	4, 0x90
LBB22_12:                               ## %"for relu1_0_d_def__.s14.n.ni.us.us"
                                        ##   Parent Loop BB22_6 Depth=1
                                        ## =>  This Inner Loop Header: Depth=2
	vmovss	(%rax,%rcx,4), %xmm2            ## xmm2 = mem[0],zero,zero,zero
	vmulss	(%rdx,%rcx,4), %xmm2, %xmm2
	vmulss	(%rbp,%rcx,4), %xmm2, %xmm2
	vfmadd213ss	(%rbx,%rcx,4), %xmm0, %xmm2 ## xmm2 = (xmm0 * xmm2) + mem
	vmovss	%xmm2, (%rbx,%rcx,4)
	incq	%rcx
	cmpq	%rcx, %r10
	jne	LBB22_12
LBB22_13:                               ## %"end for relu1_0_d_def__.s14.n.ni.loopexit.us.us"
                                        ##   in Loop: Header=BB22_6 Depth=1
	cmpl	$0, -128(%rsp)                  ## 4-byte Folded Reload
	movl	-112(%rsp), %ebx                ## 4-byte Reload
	movq	-88(%rsp), %r8                  ## 8-byte Reload
	jle	LBB22_17
## %bb.14:                              ## %"for relu1_0_d_def__.s14.n.ni.rebased.preheader.us.us"
                                        ##   in Loop: Header=BB22_6 Depth=1
	movslq	%r8d, %rax
	imull	-124(%rsp), %r9d                ## 4-byte Folded Reload
	movslq	%r9d, %rcx
	movq	8(%rsp), %rdx                   ## 8-byte Reload
	addq	%rcx, %rdx
	addq	(%rsp), %rcx                    ## 8-byte Folded Reload
	movq	-40(%rsp), %rsi                 ## 8-byte Reload
	vmovss	-4(%rsi,%rdx,4), %xmm2          ## xmm2 = mem[0],zero,zero,zero
	vmulss	-4(%rsi,%rcx,4), %xmm2, %xmm2
	cmpl	$16, -128(%rsp)                 ## 4-byte Folded Reload
	jae	LBB22_18
## %bb.15:                              ##   in Loop: Header=BB22_6 Depth=1
	xorl	%ecx, %ecx
	jmp	LBB22_25
	.p2align	4, 0x90
LBB22_18:                               ## %vector.ph54
                                        ##   in Loop: Header=BB22_6 Depth=1
	vbroadcastss	%xmm2, %ymm3
	cmpq	$0, 48(%rsp)                    ## 8-byte Folded Reload
	je	LBB22_19
## %bb.20:                              ## %vector.body52.preheader
                                        ##   in Loop: Header=BB22_6 Depth=1
	movq	-16(%rsp), %rcx                 ## 8-byte Reload
	addq	%rax, %rcx
	movq	-8(%rsp), %rdx                  ## 8-byte Reload
	leaq	(%rdx,%rcx,4), %rcx
	movq	24(%rsp), %rdx                  ## 8-byte Reload
	xorl	%esi, %esi
	movq	-120(%rsp), %r9                 ## 8-byte Reload
	.p2align	4, 0x90
LBB22_21:                               ## %vector.body52
                                        ##   Parent Loop BB22_6 Depth=1
                                        ## =>  This Inner Loop Header: Depth=2
	vmulps	-96(%r12,%rsi,4), %ymm3, %ymm4
	vmulps	-64(%r12,%rsi,4), %ymm3, %ymm5
	vfmadd213ps	-96(%rcx,%rsi,4), %ymm1, %ymm4 ## ymm4 = (ymm1 * ymm4) + mem
	vfmadd213ps	-64(%rcx,%rsi,4), %ymm1, %ymm5 ## ymm5 = (ymm1 * ymm5) + mem
	vmovups	%ymm4, -96(%rcx,%rsi,4)
	vmovups	%ymm5, -64(%rcx,%rsi,4)
	vmulps	-32(%r12,%rsi,4), %ymm3, %ymm4
	vmulps	(%r12,%rsi,4), %ymm3, %ymm5
	vfmadd213ps	-32(%rcx,%rsi,4), %ymm1, %ymm4 ## ymm4 = (ymm1 * ymm4) + mem
	vfmadd213ps	(%rcx,%rsi,4), %ymm1, %ymm5 ## ymm5 = (ymm1 * ymm5) + mem
	vmovups	%ymm4, -32(%rcx,%rsi,4)
	vmovups	%ymm5, (%rcx,%rsi,4)
	addq	$32, %rsi
	addq	$2, %rdx
	jne	LBB22_21
## %bb.22:                              ## %middle.block50.unr-lcssa
                                        ##   in Loop: Header=BB22_6 Depth=1
	testb	$1, 40(%rsp)                    ## 1-byte Folded Reload
	je	LBB22_24
LBB22_23:                               ## %vector.body52.epil
                                        ##   in Loop: Header=BB22_6 Depth=1
	movq	-32(%rsp), %rcx                 ## 8-byte Reload
	addl	%r9d, %ecx
	movq	-48(%rsp), %rdx                 ## 8-byte Reload
	addq	%r9, %rdx
	imull	%ebx, %ecx
	imulq	-72(%rsp), %rdx                 ## 8-byte Folded Reload
	addl	-60(%rsp), %ecx                 ## 4-byte Folded Reload
	movslq	%ecx, %rcx
	addq	-16(%rsp), %rsi                 ## 8-byte Folded Reload
	addq	%rsi, %rcx
	addq	%rdx, %rsi
	movq	-56(%rsp), %rdx                 ## 8-byte Reload
	vmulps	(%rdx,%rsi,4), %ymm3, %ymm4
	vmulps	32(%rdx,%rsi,4), %ymm3, %ymm3
	movq	-80(%rsp), %rdx                 ## 8-byte Reload
	vfmadd213ps	(%rdx,%rcx,4), %ymm1, %ymm4 ## ymm4 = (ymm1 * ymm4) + mem
	vfmadd213ps	32(%rdx,%rcx,4), %ymm1, %ymm3 ## ymm3 = (ymm1 * ymm3) + mem
	vmovups	%ymm4, (%rdx,%rcx,4)
	vmovups	%ymm3, 32(%rdx,%rcx,4)
LBB22_24:                               ## %middle.block50
                                        ##   in Loop: Header=BB22_6 Depth=1
	movq	56(%rsp), %rdx                  ## 8-byte Reload
	movq	%rdx, %rcx
	cmpq	%r14, %rdx
	je	LBB22_17
LBB22_25:                               ## %"for relu1_0_d_def__.s14.n.ni.rebased.us.us.preheader"
                                        ##   in Loop: Header=BB22_6 Depth=1
	movq	72(%rsp), %rdx                  ## 8-byte Reload
	leaq	(%rdx,%rax,4), %rax
	.p2align	4, 0x90
LBB22_16:                               ## %"for relu1_0_d_def__.s14.n.ni.rebased.us.us"
                                        ##   Parent Loop BB22_6 Depth=1
                                        ## =>  This Inner Loop Header: Depth=2
	vmulss	(%r15,%rcx,4), %xmm2, %xmm3
	vfmadd213ss	(%rax,%rcx,4), %xmm0, %xmm3 ## xmm3 = (xmm0 * xmm3) + mem
	vmovss	%xmm3, (%rax,%rcx,4)
	incq	%rcx
	cmpq	%rcx, %r14
	jne	LBB22_16
	jmp	LBB22_17
LBB22_19:                               ##   in Loop: Header=BB22_6 Depth=1
	xorl	%esi, %esi
	movq	-120(%rsp), %r9                 ## 8-byte Reload
	testb	$1, 40(%rsp)                    ## 1-byte Folded Reload
	jne	LBB22_23
	jmp	LBB22_24
LBB22_2:                                ## %"for relu1_0_d_def__.s14.w.wi.preheader20"
	decl	%edx
	vmovd	%r12d, %xmm0
	vpbroadcastd	%xmm0, %ymm0
	vpor	LCPI22_0(%rip), %ymm0, %ymm0
	vmovd	%edx, %xmm1
	vpbroadcastd	%xmm1, %ymm1
	vpminsd	%ymm0, %ymm1, %ymm0
	movq	-72(%rsp), %rcx                 ## 8-byte Reload
	movslq	-96(%rsp), %rdi                 ## 4-byte Folded Reload
	movl	-104(%rsp), %r14d               ## 4-byte Reload
	imulq	%rcx, %r13
	addq	%r12, %r13
	movl	%ebx, %eax
	leaq	(,%r13,4), %rbx
	addq	%rbp, %rbx
	shlq	$2, %rcx
	movq	%rcx, %r15
	shll	$6, %r8d
	shll	$6, %eax
	subl	%eax, %r8d
	orl	$11, %r8d
	movq	-88(%rsp), %rax                 ## 8-byte Reload
	imull	%r8d, %eax
	movq	-120(%rsp), %rcx                ## 8-byte Reload
	leal	(%rax,%rcx,8), %edx
	vbroadcastss	LCPI22_1(%rip), %ymm1   ## ymm1 = [9.99999971E-10,9.99999971E-10,9.99999971E-10,9.99999971E-10,9.99999971E-10,9.99999971E-10,9.99999971E-10,9.99999971E-10]
	movl	-112(%rsp), %r8d                ## 4-byte Reload
	movl	-124(%rsp), %r10d               ## 4-byte Reload
	movq	-40(%rsp), %rax                 ## 8-byte Reload
	.p2align	4, 0x90
LBB22_3:                                ## %"for relu1_0_d_def__.s14.w.wi"
                                        ## =>This Inner Loop Header: Depth=1
	testq	%rdi, %rdi
	movl	$0, %esi
	cmovgl	%edi, %esi
	imull	%r10d, %esi
	leal	(%rsi,%r9), %ecx
	vmovd	%ecx, %xmm2
	vpbroadcastd	%xmm2, %ymm2
	vpaddd	%ymm0, %ymm2, %ymm3
	vextracti128	$1, %ymm3, %xmm2
	vmovd	%xmm2, %ecx
	movslq	%ecx, %rcx
	vpextrd	$1, %xmm2, %ebp
	movslq	%ebp, %rbp
	vmovss	(%rax,%rcx,4), %xmm4            ## xmm4 = mem[0],zero,zero,zero
	vpextrd	$2, %xmm2, %ecx
	vinsertps	$16, (%rax,%rbp,4), %xmm4, %xmm4 ## xmm4 = xmm4[0],mem[0],xmm4[2,3]
	vmovd	%xmm3, %ebp
	movslq	%ecx, %rcx
	vinsertps	$32, (%rax,%rcx,4), %xmm4, %xmm4 ## xmm4 = xmm4[0,1],mem[0],xmm4[3]
	movslq	%ebp, %rcx
	vpextrd	$3, %xmm2, %ebp
	movslq	%ebp, %rbp
	vinsertps	$48, (%rax,%rbp,4), %xmm4, %xmm2 ## xmm2 = xmm4[0,1,2],mem[0]
	vpextrd	$1, %xmm3, %ebp
	vmovss	(%rax,%rcx,4), %xmm4            ## xmm4 = mem[0],zero,zero,zero
	movslq	%ebp, %rcx
	vinsertps	$16, (%rax,%rcx,4), %xmm4, %xmm4 ## xmm4 = xmm4[0],mem[0],xmm4[2,3]
	vpextrd	$2, %xmm3, %ecx
	movslq	%ecx, %rcx
	vinsertps	$32, (%rax,%rcx,4), %xmm4, %xmm4 ## xmm4 = xmm4[0,1],mem[0],xmm4[3]
	vpextrd	$3, %xmm3, %ecx
	movslq	%ecx, %rcx
	vinsertps	$48, (%rax,%rcx,4), %xmm4, %xmm3 ## xmm3 = xmm4[0,1,2],mem[0]
	addl	%r11d, %esi
	vmovd	%esi, %xmm4
	vpbroadcastd	%xmm4, %ymm4
	vpaddd	%ymm0, %ymm4, %ymm4
	vextracti128	$1, %ymm4, %xmm5
	vmovd	%xmm5, %ecx
	movslq	%ecx, %rcx
	vmovss	(%rax,%rcx,4), %xmm6            ## xmm6 = mem[0],zero,zero,zero
	vpextrd	$1, %xmm5, %ecx
	movslq	%ecx, %rcx
	vinsertps	$16, (%rax,%rcx,4), %xmm6, %xmm6 ## xmm6 = xmm6[0],mem[0],xmm6[2,3]
	vpextrd	$2, %xmm5, %ecx
	movslq	%ecx, %rcx
	vinsertps	$32, (%rax,%rcx,4), %xmm6, %xmm6 ## xmm6 = xmm6[0,1],mem[0],xmm6[3]
	vpextrd	$3, %xmm5, %ecx
	movslq	%ecx, %rcx
	vinsertps	$48, (%rax,%rcx,4), %xmm6, %xmm5 ## xmm5 = xmm6[0,1,2],mem[0]
	vmovd	%xmm4, %ecx
	movslq	%ecx, %rcx
	vmovss	(%rax,%rcx,4), %xmm6            ## xmm6 = mem[0],zero,zero,zero
	vpextrd	$1, %xmm4, %ecx
	movslq	%ecx, %rcx
	vinsertps	$16, (%rax,%rcx,4), %xmm6, %xmm6 ## xmm6 = xmm6[0],mem[0],xmm6[2,3]
	vpextrd	$2, %xmm4, %ecx
	movslq	%ecx, %rcx
	vinsertps	$32, (%rax,%rcx,4), %xmm6, %xmm6 ## xmm6 = xmm6[0,1],mem[0],xmm6[3]
	vpextrd	$3, %xmm4, %ecx
	movslq	%ecx, %rcx
	vinsertps	$48, (%rax,%rcx,4), %xmm6, %xmm4 ## xmm4 = xmm6[0,1,2],mem[0]
	vinsertf128	$1, %xmm2, %ymm3, %ymm2
	movslq	%edx, %rdx
	vinsertf128	$1, %xmm5, %ymm4, %ymm3
	vmulps	%ymm3, %ymm2, %ymm2
	vmulps	(%rbx), %ymm2, %ymm2
	movq	-80(%rsp), %rcx                 ## 8-byte Reload
	vfmadd213ps	(%rcx,%rdx,4), %ymm1, %ymm2 ## ymm2 = (ymm1 * ymm2) + mem
	vmovups	%ymm2, (%rcx,%rdx,4)
	addq	%r15, %rbx
	incq	%rdi
	addl	%r8d, %edx
	decq	%r14
	jne	LBB22_3
LBB22_40:                               ## %destructor_block
	xorl	%eax, %eax
	addq	$112, %rsp
	popq	%rbx
	popq	%r12
	popq	%r13
	popq	%r14
	popq	%r15
	popq	%rbp
	vzeroupper
	retq
LBB22_26:                               ## %"for relu1_0_d_def__.s14.w.wi.preheader.split.us.split"
	movl	%ecx, -120(%rsp)                ## 4-byte Spill
	testl	%r14d, %r14d
	movl	-124(%rsp), %edx                ## 4-byte Reload
	jle	LBB22_40
## %bb.27:                              ## %"for relu1_0_d_def__.s14.w.wi.us.us4.preheader"
	movq	%rbp, %r9
	movslq	%esi, %r11
	movl	-128(%rsp), %ebp                ## 4-byte Reload
	movslq	-96(%rsp), %r10                 ## 4-byte Folded Reload
	movl	-104(%rsp), %r15d               ## 4-byte Reload
	movl	%ebp, %r12d
	andl	$-16, %r12d
	leaq	-16(%r12), %r14
	movq	%r14, -96(%rsp)                 ## 8-byte Spill
	shrq	$4, %r14
	incq	%r14
	movq	-72(%rsp), %rcx                 ## 8-byte Reload
	movq	%rcx, %rax
	imulq	-48(%rsp), %rax                 ## 8-byte Folded Reload
	addq	%r11, %rax
	movq	-56(%rsp), %r13                 ## 8-byte Reload
	leaq	96(,%rax,4), %rdi
	addq	%r13, %rdi
	leaq	(,%rcx,4), %rbx
	movq	-80(%rsp), %rcx                 ## 8-byte Reload
	leaq	96(%rcx), %rsi
	movq	%rsi, -104(%rsp)                ## 8-byte Spill
	shll	$6, %r8d
	movl	-120(%rsp), %esi                ## 4-byte Reload
	shll	$6, %esi
	subl	%esi, %r8d
	orl	$11, %r8d
	imull	%r8d, %r9d
	movq	%r14, %rsi
	andq	$-2, %rsi
	negq	%rsi
	movq	%rsi, -88(%rsp)                 ## 8-byte Spill
	leaq	(%r13,%rax,4), %r13
	movq	%r11, -120(%rsp)                ## 8-byte Spill
	leaq	(%rcx,%r11,4), %r8
	vmovss	LCPI22_1(%rip), %xmm0           ## xmm0 = mem[0],zero,zero,zero
	vbroadcastss	LCPI22_1(%rip), %ymm1   ## ymm1 = [9.99999971E-10,9.99999971E-10,9.99999971E-10,9.99999971E-10,9.99999971E-10,9.99999971E-10,9.99999971E-10,9.99999971E-10]
	xorl	%r11d, %r11d
	jmp	LBB22_28
LBB22_39:                               ## %after_bb.loopexit.us.us17
                                        ##   in Loop: Header=BB22_28 Depth=1
	incq	%r11
	addq	%rbx, %rdi
	addl	-112(%rsp), %r9d                ## 4-byte Folded Reload
	addq	%rbx, %r13
	cmpq	%r15, %r11
	movl	-124(%rsp), %edx                ## 4-byte Reload
	je	LBB22_40
LBB22_28:                               ## %"for relu1_0_d_def__.s14.w.wi.us.us4"
                                        ## =>This Loop Header: Depth=1
                                        ##     Child Loop BB22_33 Depth 2
                                        ##     Child Loop BB22_38 Depth 2
	movslq	%r9d, %rax
	leaq	(%r11,%r10), %rcx
	testq	%rcx, %rcx
	movl	$0, %esi
	cmovlel	%esi, %ecx
	imull	%edx, %ecx
	movslq	%ecx, %rcx
	movq	8(%rsp), %rdx                   ## 8-byte Reload
	addq	%rcx, %rdx
	addq	(%rsp), %rcx                    ## 8-byte Folded Reload
	movq	-40(%rsp), %rsi                 ## 8-byte Reload
	vmovss	-4(%rsi,%rdx,4), %xmm2          ## xmm2 = mem[0],zero,zero,zero
	vmulss	-4(%rsi,%rcx,4), %xmm2, %xmm2
	cmpl	$16, -128(%rsp)                 ## 4-byte Folded Reload
	jae	LBB22_30
## %bb.29:                              ##   in Loop: Header=BB22_28 Depth=1
	xorl	%ecx, %ecx
	jmp	LBB22_37
LBB22_30:                               ## %vector.ph
                                        ##   in Loop: Header=BB22_28 Depth=1
	vbroadcastss	%xmm2, %ymm3
	cmpq	$0, -96(%rsp)                   ## 8-byte Folded Reload
	je	LBB22_31
## %bb.32:                              ## %vector.body.preheader
                                        ##   in Loop: Header=BB22_28 Depth=1
	movq	-120(%rsp), %rcx                ## 8-byte Reload
	addq	%rax, %rcx
	movq	-104(%rsp), %rdx                ## 8-byte Reload
	leaq	(%rdx,%rcx,4), %rcx
	movq	-88(%rsp), %rdx                 ## 8-byte Reload
	xorl	%esi, %esi
LBB22_33:                               ## %vector.body
                                        ##   Parent Loop BB22_28 Depth=1
                                        ## =>  This Inner Loop Header: Depth=2
	vmulps	-96(%rdi,%rsi,4), %ymm3, %ymm4
	vmulps	-64(%rdi,%rsi,4), %ymm3, %ymm5
	vfmadd213ps	-96(%rcx,%rsi,4), %ymm1, %ymm4 ## ymm4 = (ymm1 * ymm4) + mem
	vfmadd213ps	-64(%rcx,%rsi,4), %ymm1, %ymm5 ## ymm5 = (ymm1 * ymm5) + mem
	vmovups	%ymm4, -96(%rcx,%rsi,4)
	vmovups	%ymm5, -64(%rcx,%rsi,4)
	vmulps	-32(%rdi,%rsi,4), %ymm3, %ymm4
	vmulps	(%rdi,%rsi,4), %ymm3, %ymm5
	vfmadd213ps	-32(%rcx,%rsi,4), %ymm1, %ymm4 ## ymm4 = (ymm1 * ymm4) + mem
	vfmadd213ps	(%rcx,%rsi,4), %ymm1, %ymm5 ## ymm5 = (ymm1 * ymm5) + mem
	vmovups	%ymm4, -32(%rcx,%rsi,4)
	vmovups	%ymm5, (%rcx,%rsi,4)
	addq	$32, %rsi
	addq	$2, %rdx
	jne	LBB22_33
	jmp	LBB22_34
LBB22_31:                               ##   in Loop: Header=BB22_28 Depth=1
	xorl	%esi, %esi
LBB22_34:                               ## %middle.block.unr-lcssa
                                        ##   in Loop: Header=BB22_28 Depth=1
	testb	$1, %r14b
	je	LBB22_36
## %bb.35:                              ## %vector.body.epil
                                        ##   in Loop: Header=BB22_28 Depth=1
	movq	-32(%rsp), %rcx                 ## 8-byte Reload
	addl	%r11d, %ecx
	imull	-112(%rsp), %ecx                ## 4-byte Folded Reload
	movq	-48(%rsp), %rdx                 ## 8-byte Reload
	addq	%r11, %rdx
	addl	-60(%rsp), %ecx                 ## 4-byte Folded Reload
	imulq	-72(%rsp), %rdx                 ## 8-byte Folded Reload
	movslq	%ecx, %rcx
	addq	-120(%rsp), %rsi                ## 8-byte Folded Reload
	addq	%rsi, %rcx
	addq	%rdx, %rsi
	movq	-56(%rsp), %rdx                 ## 8-byte Reload
	vmulps	(%rdx,%rsi,4), %ymm3, %ymm4
	vmulps	32(%rdx,%rsi,4), %ymm3, %ymm3
	movq	-80(%rsp), %rdx                 ## 8-byte Reload
	vfmadd213ps	(%rdx,%rcx,4), %ymm1, %ymm4 ## ymm4 = (ymm1 * ymm4) + mem
	vfmadd213ps	32(%rdx,%rcx,4), %ymm1, %ymm3 ## ymm3 = (ymm1 * ymm3) + mem
	vmovups	%ymm4, (%rdx,%rcx,4)
	vmovups	%ymm3, 32(%rdx,%rcx,4)
LBB22_36:                               ## %middle.block
                                        ##   in Loop: Header=BB22_28 Depth=1
	movq	%r12, %rcx
	cmpq	%rbp, %r12
	je	LBB22_39
LBB22_37:                               ## %"for relu1_0_d_def__.s14.n.ni.rebased.us.us9.preheader"
                                        ##   in Loop: Header=BB22_28 Depth=1
	leaq	(%r8,%rax,4), %rax
LBB22_38:                               ## %"for relu1_0_d_def__.s14.n.ni.rebased.us.us9"
                                        ##   Parent Loop BB22_28 Depth=1
                                        ## =>  This Inner Loop Header: Depth=2
	vmulss	(%r13,%rcx,4), %xmm2, %xmm3
	vfmadd213ss	(%rax,%rcx,4), %xmm0, %xmm3 ## xmm3 = (xmm0 * xmm3) + mem
	vmovss	%xmm3, (%rax,%rcx,4)
	incq	%rcx
	cmpq	%rcx, %rbp
	jne	LBB22_38
	jmp	LBB22_39
                                        ## -- End function
	.section	__TEXT,__const
	.p2align	5                               ## -- Begin function train_cost_model.par_for.relu1_0_d_def__.s15.n.n.n
LCPI23_0:
	.long	0                               ## 0x0
	.long	1                               ## 0x1
	.long	2                               ## 0x2
	.long	3                               ## 0x3
	.long	4                               ## 0x4
	.long	5                               ## 0x5
	.long	6                               ## 0x6
	.long	7                               ## 0x7
	.section	__TEXT,__literal4,4byte_literals
	.p2align	2
LCPI23_1:
	.long	0x3089705f                      ## float 9.99999971E-10
	.section	__TEXT,__text,regular,pure_instructions
	.p2align	4, 0x90
_train_cost_model.par_for.relu1_0_d_def__.s15.n.n.n: ## @train_cost_model.par_for.relu1_0_d_def__.s15.n.n.n
## %bb.0:                               ## %entry
	pushq	%rbp
	pushq	%r15
	pushq	%r14
	pushq	%r13
	pushq	%r12
	pushq	%rbx
	subq	$112, %rsp
	movq	%rdx, %rdi
	movl	12(%rdx), %r10d
	movl	24(%rdx), %r13d
	movl	%esi, %r9d
	sarl	$31, %r9d
	xorl	%ecx, %ecx
	testl	%r13d, %r13d
	sete	%cl
	movl	%ecx, %ebx
	negl	%ebx
	movl	%r13d, %ebp
	sarl	$31, %ebp
	subl	%r9d, %esi
	orl	%r13d, %ebx
	movl	%esi, %eax
	cltd
	idivl	%ebx
	movl	%edx, %r14d
	movl	%ebp, %r15d
	leal	(%rcx,%r13), %ebx
	movl	%esi, %eax
	cltd
	idivl	%ebx
	notl	%r15d
	decl	%ecx
	movl	%r15d, %r8d
	subl	%ebp, %r8d
	andl	%r9d, %r8d
	addl	%eax, %r8d
	andl	%ecx, %r8d
	leal	(%r8,%r8), %eax
	movl	%eax, -96(%rsp)                 ## 4-byte Spill
	subl	%eax, %r10d
	cmpl	$2, %r10d
	movl	$2, %eax
	cmovll	%r10d, %eax
	xorl	%ebx, %ebx
	testl	%r10d, %r10d
	cmovlel	%ebx, %eax
	movl	%eax, -104(%rsp)                ## 4-byte Spill
	jle	LBB23_40
## %bb.1:                               ## %"for relu1_0_d_def__.s15.w.wi.preheader"
	movl	(%rdi), %edx
	movl	16(%rdi), %r12d
	movl	32(%rdi), %r10d
	movl	36(%rdi), %eax
	movl	40(%rdi), %r11d
	xorl	%r13d, %ebp
	addl	%r15d, %ebp
	andl	%r9d, %ebp
	addl	%ebp, %r14d
	andl	%ecx, %r14d
	movq	%r14, -120(%rsp)                ## 8-byte Spill
	leal	(,%r14,8), %esi
	movl	%eax, -128(%rsp)                ## 4-byte Spill
	movl	%eax, %r9d
	subl	%r10d, %r9d
	movl	%r11d, %r15d
	subl	%r10d, %r11d
	movslq	8(%rdi), %rax
	movslq	%r8d, %r13
	subq	%rax, %r13
	addq	%r13, %r13
	movq	%r12, -88(%rsp)                 ## 8-byte Spill
	movl	%r12d, %eax
	shll	$5, %eax
	movl	%eax, -112(%rsp)                ## 4-byte Spill
	movl	%edx, %eax
	subl	%esi, %eax
	cmpl	$8, %eax
	movl	$8, %ecx
	cmovll	%eax, %ecx
	testl	%eax, %eax
	cmovgl	%ecx, %ebx
	movl	%ebx, -108(%rsp)                ## 4-byte Spill
	movl	20(%rdi), %ecx
	movl	%ecx, -124(%rsp)                ## 4-byte Spill
	movl	%esi, %ebx
	subl	%edx, %ebx
	movl	%ebx, %ecx
	sarl	$31, %ecx
	andl	%ebx, %ecx
	cmpl	$-8, %ecx
	movl	$-8, %r14d
	cmovgl	%ecx, %r14d
	movl	28(%rdi), %ebx
	movq	48(%rdi), %rbp
	movq	64(%rdi), %rcx
	movq	%rcx, -80(%rsp)                 ## 8-byte Spill
	movq	80(%rdi), %rcx
	movq	%rcx, -40(%rsp)                 ## 8-byte Spill
	movslq	4(%rdi), %rcx
	movq	%rcx, -72(%rsp)                 ## 8-byte Spill
	leal	8(%rsi), %ecx
	movslq	%esi, %r12
	cmpl	%edx, %ecx
	jle	LBB23_2
## %bb.4:                               ## %"for relu1_0_d_def__.s15.w.wi.preheader.split.us"
	movq	%rbp, -56(%rsp)                 ## 8-byte Spill
	movq	%r13, -48(%rsp)                 ## 8-byte Spill
	movq	-88(%rsp), %rbp                 ## 8-byte Reload
	leal	(%rbp,%rbp), %ecx
	leal	(%rcx,%rcx,4), %ecx
	movl	%ecx, -60(%rsp)                 ## 4-byte Spill
	movl	%r8d, %ecx
	subl	%ebx, %ecx
	addl	%ecx, %ecx
	movq	%rcx, -32(%rsp)                 ## 8-byte Spill
	subl	%r10d, %edx
	movl	-128(%rsp), %edi                ## 4-byte Reload
	addl	%edx, %edi
	addl	%r15d, %edx
	movl	-108(%rsp), %ecx                ## 4-byte Reload
	addl	%ecx, %esi
	addl	%ecx, %r14d
	movslq	%edi, %rcx
	movq	%rcx, 8(%rsp)                   ## 8-byte Spill
	movslq	%edx, %rcx
	movq	%rcx, (%rsp)                    ## 8-byte Spill
	testl	%eax, %eax
	movl	%r14d, -128(%rsp)               ## 4-byte Spill
	movl	%ebx, %ecx
	jle	LBB23_26
## %bb.5:                               ## %"for relu1_0_d_def__.s15.w.wi.us.us.preheader"
	addl	%r12d, %r11d
	addl	%r12d, %r9d
	movslq	%r9d, %rax
	movq	%rax, -24(%rsp)                 ## 8-byte Spill
	movslq	%r11d, %r9
	movl	-108(%rsp), %r10d               ## 4-byte Reload
	movslq	%esi, %rsi
	movl	%r14d, %r14d
	movslq	-96(%rsp), %rax                 ## 4-byte Folded Reload
	movq	%rax, 16(%rsp)                  ## 8-byte Spill
	movl	-104(%rsp), %eax                ## 4-byte Reload
	movq	%rax, 104(%rsp)                 ## 8-byte Spill
	movl	%r14d, %eax
	andl	$-16, %eax
	movq	%rax, 56(%rsp)                  ## 8-byte Spill
	addq	$-16, %rax
	movq	%rax, 48(%rsp)                  ## 8-byte Spill
	movq	%rax, %rbx
	shrq	$4, %rbx
	incq	%rbx
	movl	%r10d, %r11d
	andl	$2147483616, %r11d              ## imm = 0x7FFFFFE0
	shll	$6, %r8d
	shll	$6, %ecx
	subl	%ecx, %r8d
	movq	-72(%rsp), %rdx                 ## 8-byte Reload
	movq	%rdx, %rax
	imulq	-48(%rsp), %rax                 ## 8-byte Folded Reload
	addq	%rax, %r12
	orl	$10, %r8d
	imull	%r8d, %ebp
	movq	-56(%rsp), %rcx                 ## 8-byte Reload
	movq	%rdx, %r8
	leaq	(%rcx,%r12,4), %rdi
	addq	$96, %rdi
	movq	%rbp, -88(%rsp)                 ## 8-byte Spill
	movq	-120(%rsp), %rdx                ## 8-byte Reload
	leal	(%rbp,%rdx,8), %r13d
	leaq	(%rcx,%r12,4), %rbp
	addq	%rsi, %rax
	movq	%rbx, %rdx
	movq	%rbx, 40(%rsp)                  ## 8-byte Spill
	andq	$-2, %rbx
	negq	%rbx
	movq	%rbx, 24(%rsp)                  ## 8-byte Spill
	leaq	(%rcx,%rax,4), %r12
	addq	$96, %r12
	leaq	(%rcx,%rax,4), %r15
	vmovss	LCPI23_1(%rip), %xmm0           ## xmm0 = mem[0],zero,zero,zero
	vbroadcastss	LCPI23_1(%rip), %ymm1   ## ymm1 = [9.99999971E-10,9.99999971E-10,9.99999971E-10,9.99999971E-10,9.99999971E-10,9.99999971E-10,9.99999971E-10,9.99999971E-10]
	movq	-40(%rsp), %rcx                 ## 8-byte Reload
	leaq	96(%rcx), %rax
	movq	%rax, 32(%rsp)                  ## 8-byte Spill
	leaq	(,%r8,4), %rax
	movq	%rax, 96(%rsp)                  ## 8-byte Spill
	movq	-80(%rsp), %rax                 ## 8-byte Reload
	leaq	96(%rax), %rdx
	movq	%rdx, -8(%rsp)                  ## 8-byte Spill
	movq	%r9, 64(%rsp)                   ## 8-byte Spill
	leaq	(%rcx,%r9,4), %rdx
	movq	%rdx, 88(%rsp)                  ## 8-byte Spill
	movq	-24(%rsp), %rdx                 ## 8-byte Reload
	leaq	(%rcx,%rdx,4), %rcx
	movq	%rcx, 80(%rsp)                  ## 8-byte Spill
	movq	%rsi, -16(%rsp)                 ## 8-byte Spill
	leaq	(%rax,%rsi,4), %rax
	movq	%rax, 72(%rsp)                  ## 8-byte Spill
	movq	16(%rsp), %rdx                  ## 8-byte Reload
	xorl	%ebx, %ebx
	movl	-124(%rsp), %esi                ## 4-byte Reload
	movl	-108(%rsp), %ecx                ## 4-byte Reload
	jmp	LBB23_6
	.p2align	4, 0x90
LBB23_17:                               ## %after_bb.us.us
                                        ##   in Loop: Header=BB23_6 Depth=1
	movl	%ebx, %ecx
	movq	-120(%rsp), %rbx                ## 8-byte Reload
	incq	%rbx
	movq	-104(%rsp), %rdx                ## 8-byte Reload
	incq	%rdx
	movq	96(%rsp), %rax                  ## 8-byte Reload
	addq	%rax, %rdi
	movl	-96(%rsp), %r13d                ## 4-byte Reload
	addl	%ecx, %r13d
	addq	%rax, %rbp
	addq	%rax, %r12
	addl	%ecx, %r8d
	movq	%r8, -88(%rsp)                  ## 8-byte Spill
	addq	%rax, %r15
	cmpq	104(%rsp), %rbx                 ## 8-byte Folded Reload
	movl	-124(%rsp), %esi                ## 4-byte Reload
	movl	-108(%rsp), %ecx                ## 4-byte Reload
	je	LBB23_40
LBB23_6:                                ## %"for relu1_0_d_def__.s15.w.wi.us.us"
                                        ## =>This Loop Header: Depth=1
                                        ##     Child Loop BB23_9 Depth 2
                                        ##     Child Loop BB23_12 Depth 2
                                        ##     Child Loop BB23_21 Depth 2
                                        ##     Child Loop BB23_16 Depth 2
	testq	%rdx, %rdx
	movl	$0, %eax
	movq	%rdx, -104(%rsp)                ## 8-byte Spill
	cmovgl	%edx, %eax
	imull	%esi, %eax
	cltq
	movq	16(%rsp), %rdx                  ## 8-byte Reload
	movq	%rbx, -120(%rsp)                ## 8-byte Spill
	leaq	(%rbx,%rdx), %r9
	testq	%r9, %r9
	movl	$0, %edx
	cmovlel	%edx, %r9d
	movslq	%r13d, %rsi
	cmpl	$32, %ecx
	movl	%r13d, -96(%rsp)                ## 4-byte Spill
	jae	LBB23_8
## %bb.7:                               ##   in Loop: Header=BB23_6 Depth=1
	xorl	%ecx, %ecx
	jmp	LBB23_11
	.p2align	4, 0x90
LBB23_8:                                ## %vector.body71.preheader
                                        ##   in Loop: Header=BB23_6 Depth=1
	movq	64(%rsp), %rcx                  ## 8-byte Reload
	addq	%rax, %rcx
	movq	-24(%rsp), %rdx                 ## 8-byte Reload
	addq	%rax, %rdx
	movq	32(%rsp), %rbx                  ## 8-byte Reload
	leaq	(%rbx,%rcx,4), %r8
	leaq	(%rbx,%rdx,4), %r13
	movq	-8(%rsp), %rcx                  ## 8-byte Reload
	movq	%rsi, %rbx
	leaq	(%rcx,%rsi,4), %rcx
	xorl	%edx, %edx
	.p2align	4, 0x90
LBB23_9:                                ## %vector.body71
                                        ##   Parent Loop BB23_6 Depth=1
                                        ## =>  This Inner Loop Header: Depth=2
	vmovups	-96(%r13,%rdx,4), %ymm2
	vmovups	-64(%r13,%rdx,4), %ymm3
	vmovups	-32(%r13,%rdx,4), %ymm4
	vmovups	(%r13,%rdx,4), %ymm5
	vmulps	-96(%r8,%rdx,4), %ymm2, %ymm2
	vmulps	-64(%r8,%rdx,4), %ymm3, %ymm3
	vmulps	-32(%r8,%rdx,4), %ymm4, %ymm4
	vmulps	(%r8,%rdx,4), %ymm5, %ymm5
	vmulps	-96(%rdi,%rdx,4), %ymm2, %ymm2
	vmulps	-64(%rdi,%rdx,4), %ymm3, %ymm3
	vmulps	-32(%rdi,%rdx,4), %ymm4, %ymm4
	vmulps	(%rdi,%rdx,4), %ymm5, %ymm5
	vfmadd213ps	-96(%rcx,%rdx,4), %ymm1, %ymm2 ## ymm2 = (ymm1 * ymm2) + mem
	vfmadd213ps	-64(%rcx,%rdx,4), %ymm1, %ymm3 ## ymm3 = (ymm1 * ymm3) + mem
	vfmadd213ps	-32(%rcx,%rdx,4), %ymm1, %ymm4 ## ymm4 = (ymm1 * ymm4) + mem
	vfmadd213ps	(%rcx,%rdx,4), %ymm1, %ymm5 ## ymm5 = (ymm1 * ymm5) + mem
	vmovups	%ymm2, -96(%rcx,%rdx,4)
	vmovups	%ymm3, -64(%rcx,%rdx,4)
	vmovups	%ymm4, -32(%rcx,%rdx,4)
	vmovups	%ymm5, (%rcx,%rdx,4)
	addq	$32, %rdx
	cmpq	%rdx, %r11
	jne	LBB23_9
## %bb.10:                              ## %middle.block69
                                        ##   in Loop: Header=BB23_6 Depth=1
	movq	%r11, %rcx
	cmpq	%r10, %r11
	movq	%rbx, %rsi
	je	LBB23_13
LBB23_11:                               ## %"for relu1_0_d_def__.s15.n.ni.us.us.preheader"
                                        ##   in Loop: Header=BB23_6 Depth=1
	movq	88(%rsp), %rdx                  ## 8-byte Reload
	leaq	(%rdx,%rax,4), %rdx
	movq	80(%rsp), %rbx                  ## 8-byte Reload
	leaq	(%rbx,%rax,4), %rax
	movq	-80(%rsp), %rbx                 ## 8-byte Reload
	leaq	(%rbx,%rsi,4), %rbx
	.p2align	4, 0x90
LBB23_12:                               ## %"for relu1_0_d_def__.s15.n.ni.us.us"
                                        ##   Parent Loop BB23_6 Depth=1
                                        ## =>  This Inner Loop Header: Depth=2
	vmovss	(%rax,%rcx,4), %xmm2            ## xmm2 = mem[0],zero,zero,zero
	vmulss	(%rdx,%rcx,4), %xmm2, %xmm2
	vmulss	(%rbp,%rcx,4), %xmm2, %xmm2
	vfmadd213ss	(%rbx,%rcx,4), %xmm0, %xmm2 ## xmm2 = (xmm0 * xmm2) + mem
	vmovss	%xmm2, (%rbx,%rcx,4)
	incq	%rcx
	cmpq	%rcx, %r10
	jne	LBB23_12
LBB23_13:                               ## %"end for relu1_0_d_def__.s15.n.ni.loopexit.us.us"
                                        ##   in Loop: Header=BB23_6 Depth=1
	cmpl	$0, -128(%rsp)                  ## 4-byte Folded Reload
	movl	-112(%rsp), %ebx                ## 4-byte Reload
	movq	-88(%rsp), %r8                  ## 8-byte Reload
	jle	LBB23_17
## %bb.14:                              ## %"for relu1_0_d_def__.s15.n.ni.rebased.preheader.us.us"
                                        ##   in Loop: Header=BB23_6 Depth=1
	movslq	%r8d, %rax
	imull	-124(%rsp), %r9d                ## 4-byte Folded Reload
	movslq	%r9d, %rcx
	movq	8(%rsp), %rdx                   ## 8-byte Reload
	addq	%rcx, %rdx
	addq	(%rsp), %rcx                    ## 8-byte Folded Reload
	movq	-40(%rsp), %rsi                 ## 8-byte Reload
	vmovss	-4(%rsi,%rdx,4), %xmm2          ## xmm2 = mem[0],zero,zero,zero
	vmulss	-4(%rsi,%rcx,4), %xmm2, %xmm2
	cmpl	$16, -128(%rsp)                 ## 4-byte Folded Reload
	jae	LBB23_18
## %bb.15:                              ##   in Loop: Header=BB23_6 Depth=1
	xorl	%ecx, %ecx
	jmp	LBB23_25
	.p2align	4, 0x90
LBB23_18:                               ## %vector.ph54
                                        ##   in Loop: Header=BB23_6 Depth=1
	vbroadcastss	%xmm2, %ymm3
	cmpq	$0, 48(%rsp)                    ## 8-byte Folded Reload
	je	LBB23_19
## %bb.20:                              ## %vector.body52.preheader
                                        ##   in Loop: Header=BB23_6 Depth=1
	movq	-16(%rsp), %rcx                 ## 8-byte Reload
	addq	%rax, %rcx
	movq	-8(%rsp), %rdx                  ## 8-byte Reload
	leaq	(%rdx,%rcx,4), %rcx
	movq	24(%rsp), %rdx                  ## 8-byte Reload
	xorl	%esi, %esi
	movq	-120(%rsp), %r9                 ## 8-byte Reload
	.p2align	4, 0x90
LBB23_21:                               ## %vector.body52
                                        ##   Parent Loop BB23_6 Depth=1
                                        ## =>  This Inner Loop Header: Depth=2
	vmulps	-96(%r12,%rsi,4), %ymm3, %ymm4
	vmulps	-64(%r12,%rsi,4), %ymm3, %ymm5
	vfmadd213ps	-96(%rcx,%rsi,4), %ymm1, %ymm4 ## ymm4 = (ymm1 * ymm4) + mem
	vfmadd213ps	-64(%rcx,%rsi,4), %ymm1, %ymm5 ## ymm5 = (ymm1 * ymm5) + mem
	vmovups	%ymm4, -96(%rcx,%rsi,4)
	vmovups	%ymm5, -64(%rcx,%rsi,4)
	vmulps	-32(%r12,%rsi,4), %ymm3, %ymm4
	vmulps	(%r12,%rsi,4), %ymm3, %ymm5
	vfmadd213ps	-32(%rcx,%rsi,4), %ymm1, %ymm4 ## ymm4 = (ymm1 * ymm4) + mem
	vfmadd213ps	(%rcx,%rsi,4), %ymm1, %ymm5 ## ymm5 = (ymm1 * ymm5) + mem
	vmovups	%ymm4, -32(%rcx,%rsi,4)
	vmovups	%ymm5, (%rcx,%rsi,4)
	addq	$32, %rsi
	addq	$2, %rdx
	jne	LBB23_21
## %bb.22:                              ## %middle.block50.unr-lcssa
                                        ##   in Loop: Header=BB23_6 Depth=1
	testb	$1, 40(%rsp)                    ## 1-byte Folded Reload
	je	LBB23_24
LBB23_23:                               ## %vector.body52.epil
                                        ##   in Loop: Header=BB23_6 Depth=1
	movq	-32(%rsp), %rcx                 ## 8-byte Reload
	addl	%r9d, %ecx
	movq	-48(%rsp), %rdx                 ## 8-byte Reload
	addq	%r9, %rdx
	imull	%ebx, %ecx
	imulq	-72(%rsp), %rdx                 ## 8-byte Folded Reload
	addl	-60(%rsp), %ecx                 ## 4-byte Folded Reload
	movslq	%ecx, %rcx
	addq	-16(%rsp), %rsi                 ## 8-byte Folded Reload
	addq	%rsi, %rcx
	addq	%rdx, %rsi
	movq	-56(%rsp), %rdx                 ## 8-byte Reload
	vmulps	(%rdx,%rsi,4), %ymm3, %ymm4
	vmulps	32(%rdx,%rsi,4), %ymm3, %ymm3
	movq	-80(%rsp), %rdx                 ## 8-byte Reload
	vfmadd213ps	(%rdx,%rcx,4), %ymm1, %ymm4 ## ymm4 = (ymm1 * ymm4) + mem
	vfmadd213ps	32(%rdx,%rcx,4), %ymm1, %ymm3 ## ymm3 = (ymm1 * ymm3) + mem
	vmovups	%ymm4, (%rdx,%rcx,4)
	vmovups	%ymm3, 32(%rdx,%rcx,4)
LBB23_24:                               ## %middle.block50
                                        ##   in Loop: Header=BB23_6 Depth=1
	movq	56(%rsp), %rdx                  ## 8-byte Reload
	movq	%rdx, %rcx
	cmpq	%r14, %rdx
	je	LBB23_17
LBB23_25:                               ## %"for relu1_0_d_def__.s15.n.ni.rebased.us.us.preheader"
                                        ##   in Loop: Header=BB23_6 Depth=1
	movq	72(%rsp), %rdx                  ## 8-byte Reload
	leaq	(%rdx,%rax,4), %rax
	.p2align	4, 0x90
LBB23_16:                               ## %"for relu1_0_d_def__.s15.n.ni.rebased.us.us"
                                        ##   Parent Loop BB23_6 Depth=1
                                        ## =>  This Inner Loop Header: Depth=2
	vmulss	(%r15,%rcx,4), %xmm2, %xmm3
	vfmadd213ss	(%rax,%rcx,4), %xmm0, %xmm3 ## xmm3 = (xmm0 * xmm3) + mem
	vmovss	%xmm3, (%rax,%rcx,4)
	incq	%rcx
	cmpq	%rcx, %r14
	jne	LBB23_16
	jmp	LBB23_17
LBB23_19:                               ##   in Loop: Header=BB23_6 Depth=1
	xorl	%esi, %esi
	movq	-120(%rsp), %r9                 ## 8-byte Reload
	testb	$1, 40(%rsp)                    ## 1-byte Folded Reload
	jne	LBB23_23
	jmp	LBB23_24
LBB23_2:                                ## %"for relu1_0_d_def__.s15.w.wi.preheader20"
	decl	%edx
	vmovd	%r12d, %xmm0
	vpbroadcastd	%xmm0, %ymm0
	vpor	LCPI23_0(%rip), %ymm0, %ymm0
	vmovd	%edx, %xmm1
	vpbroadcastd	%xmm1, %ymm1
	vpminsd	%ymm0, %ymm1, %ymm0
	movq	-72(%rsp), %rcx                 ## 8-byte Reload
	movslq	-96(%rsp), %rdi                 ## 4-byte Folded Reload
	movl	-104(%rsp), %r14d               ## 4-byte Reload
	imulq	%rcx, %r13
	addq	%r12, %r13
	movl	%ebx, %eax
	leaq	(,%r13,4), %rbx
	addq	%rbp, %rbx
	shlq	$2, %rcx
	movq	%rcx, %r15
	shll	$6, %r8d
	shll	$6, %eax
	subl	%eax, %r8d
	orl	$10, %r8d
	movq	-88(%rsp), %rax                 ## 8-byte Reload
	imull	%r8d, %eax
	movq	-120(%rsp), %rcx                ## 8-byte Reload
	leal	(%rax,%rcx,8), %edx
	vbroadcastss	LCPI23_1(%rip), %ymm1   ## ymm1 = [9.99999971E-10,9.99999971E-10,9.99999971E-10,9.99999971E-10,9.99999971E-10,9.99999971E-10,9.99999971E-10,9.99999971E-10]
	movl	-112(%rsp), %r8d                ## 4-byte Reload
	movl	-124(%rsp), %r10d               ## 4-byte Reload
	movq	-40(%rsp), %rax                 ## 8-byte Reload
	.p2align	4, 0x90
LBB23_3:                                ## %"for relu1_0_d_def__.s15.w.wi"
                                        ## =>This Inner Loop Header: Depth=1
	testq	%rdi, %rdi
	movl	$0, %esi
	cmovgl	%edi, %esi
	imull	%r10d, %esi
	leal	(%rsi,%r9), %ecx
	vmovd	%ecx, %xmm2
	vpbroadcastd	%xmm2, %ymm2
	vpaddd	%ymm0, %ymm2, %ymm3
	vextracti128	$1, %ymm3, %xmm2
	vmovd	%xmm2, %ecx
	movslq	%ecx, %rcx
	vpextrd	$1, %xmm2, %ebp
	movslq	%ebp, %rbp
	vmovss	(%rax,%rcx,4), %xmm4            ## xmm4 = mem[0],zero,zero,zero
	vpextrd	$2, %xmm2, %ecx
	vinsertps	$16, (%rax,%rbp,4), %xmm4, %xmm4 ## xmm4 = xmm4[0],mem[0],xmm4[2,3]
	vmovd	%xmm3, %ebp
	movslq	%ecx, %rcx
	vinsertps	$32, (%rax,%rcx,4), %xmm4, %xmm4 ## xmm4 = xmm4[0,1],mem[0],xmm4[3]
	movslq	%ebp, %rcx
	vpextrd	$3, %xmm2, %ebp
	movslq	%ebp, %rbp
	vinsertps	$48, (%rax,%rbp,4), %xmm4, %xmm2 ## xmm2 = xmm4[0,1,2],mem[0]
	vpextrd	$1, %xmm3, %ebp
	vmovss	(%rax,%rcx,4), %xmm4            ## xmm4 = mem[0],zero,zero,zero
	movslq	%ebp, %rcx
	vinsertps	$16, (%rax,%rcx,4), %xmm4, %xmm4 ## xmm4 = xmm4[0],mem[0],xmm4[2,3]
	vpextrd	$2, %xmm3, %ecx
	movslq	%ecx, %rcx
	vinsertps	$32, (%rax,%rcx,4), %xmm4, %xmm4 ## xmm4 = xmm4[0,1],mem[0],xmm4[3]
	vpextrd	$3, %xmm3, %ecx
	movslq	%ecx, %rcx
	vinsertps	$48, (%rax,%rcx,4), %xmm4, %xmm3 ## xmm3 = xmm4[0,1,2],mem[0]
	addl	%r11d, %esi
	vmovd	%esi, %xmm4
	vpbroadcastd	%xmm4, %ymm4
	vpaddd	%ymm0, %ymm4, %ymm4
	vextracti128	$1, %ymm4, %xmm5
	vmovd	%xmm5, %ecx
	movslq	%ecx, %rcx
	vmovss	(%rax,%rcx,4), %xmm6            ## xmm6 = mem[0],zero,zero,zero
	vpextrd	$1, %xmm5, %ecx
	movslq	%ecx, %rcx
	vinsertps	$16, (%rax,%rcx,4), %xmm6, %xmm6 ## xmm6 = xmm6[0],mem[0],xmm6[2,3]
	vpextrd	$2, %xmm5, %ecx
	movslq	%ecx, %rcx
	vinsertps	$32, (%rax,%rcx,4), %xmm6, %xmm6 ## xmm6 = xmm6[0,1],mem[0],xmm6[3]
	vpextrd	$3, %xmm5, %ecx
	movslq	%ecx, %rcx
	vinsertps	$48, (%rax,%rcx,4), %xmm6, %xmm5 ## xmm5 = xmm6[0,1,2],mem[0]
	vmovd	%xmm4, %ecx
	movslq	%ecx, %rcx
	vmovss	(%rax,%rcx,4), %xmm6            ## xmm6 = mem[0],zero,zero,zero
	vpextrd	$1, %xmm4, %ecx
	movslq	%ecx, %rcx
	vinsertps	$16, (%rax,%rcx,4), %xmm6, %xmm6 ## xmm6 = xmm6[0],mem[0],xmm6[2,3]
	vpextrd	$2, %xmm4, %ecx
	movslq	%ecx, %rcx
	vinsertps	$32, (%rax,%rcx,4), %xmm6, %xmm6 ## xmm6 = xmm6[0,1],mem[0],xmm6[3]
	vpextrd	$3, %xmm4, %ecx
	movslq	%ecx, %rcx
	vinsertps	$48, (%rax,%rcx,4), %xmm6, %xmm4 ## xmm4 = xmm6[0,1,2],mem[0]
	vinsertf128	$1, %xmm2, %ymm3, %ymm2
	movslq	%edx, %rdx
	vinsertf128	$1, %xmm5, %ymm4, %ymm3
	vmulps	%ymm3, %ymm2, %ymm2
	vmulps	(%rbx), %ymm2, %ymm2
	movq	-80(%rsp), %rcx                 ## 8-byte Reload
	vfmadd213ps	(%rcx,%rdx,4), %ymm1, %ymm2 ## ymm2 = (ymm1 * ymm2) + mem
	vmovups	%ymm2, (%rcx,%rdx,4)
	addq	%r15, %rbx
	incq	%rdi
	addl	%r8d, %edx
	decq	%r14
	jne	LBB23_3
LBB23_40:                               ## %destructor_block
	xorl	%eax, %eax
	addq	$112, %rsp
	popq	%rbx
	popq	%r12
	popq	%r13
	popq	%r14
	popq	%r15
	popq	%rbp
	vzeroupper
	retq
LBB23_26:                               ## %"for relu1_0_d_def__.s15.w.wi.preheader.split.us.split"
	movl	%ecx, -120(%rsp)                ## 4-byte Spill
	testl	%r14d, %r14d
	movl	-124(%rsp), %edx                ## 4-byte Reload
	jle	LBB23_40
## %bb.27:                              ## %"for relu1_0_d_def__.s15.w.wi.us.us4.preheader"
	movq	%rbp, %r9
	movslq	%esi, %r11
	movl	-128(%rsp), %ebp                ## 4-byte Reload
	movslq	-96(%rsp), %r10                 ## 4-byte Folded Reload
	movl	-104(%rsp), %r15d               ## 4-byte Reload
	movl	%ebp, %r12d
	andl	$-16, %r12d
	leaq	-16(%r12), %r14
	movq	%r14, -96(%rsp)                 ## 8-byte Spill
	shrq	$4, %r14
	incq	%r14
	movq	-72(%rsp), %rcx                 ## 8-byte Reload
	movq	%rcx, %rax
	imulq	-48(%rsp), %rax                 ## 8-byte Folded Reload
	addq	%r11, %rax
	movq	-56(%rsp), %r13                 ## 8-byte Reload
	leaq	96(,%rax,4), %rdi
	addq	%r13, %rdi
	leaq	(,%rcx,4), %rbx
	movq	-80(%rsp), %rcx                 ## 8-byte Reload
	leaq	96(%rcx), %rsi
	movq	%rsi, -104(%rsp)                ## 8-byte Spill
	shll	$6, %r8d
	movl	-120(%rsp), %esi                ## 4-byte Reload
	shll	$6, %esi
	subl	%esi, %r8d
	orl	$10, %r8d
	imull	%r8d, %r9d
	movq	%r14, %rsi
	andq	$-2, %rsi
	negq	%rsi
	movq	%rsi, -88(%rsp)                 ## 8-byte Spill
	leaq	(%r13,%rax,4), %r13
	movq	%r11, -120(%rsp)                ## 8-byte Spill
	leaq	(%rcx,%r11,4), %r8
	vmovss	LCPI23_1(%rip), %xmm0           ## xmm0 = mem[0],zero,zero,zero
	vbroadcastss	LCPI23_1(%rip), %ymm1   ## ymm1 = [9.99999971E-10,9.99999971E-10,9.99999971E-10,9.99999971E-10,9.99999971E-10,9.99999971E-10,9.99999971E-10,9.99999971E-10]
	xorl	%r11d, %r11d
	jmp	LBB23_28
LBB23_39:                               ## %after_bb.loopexit.us.us17
                                        ##   in Loop: Header=BB23_28 Depth=1
	incq	%r11
	addq	%rbx, %rdi
	addl	-112(%rsp), %r9d                ## 4-byte Folded Reload
	addq	%rbx, %r13
	cmpq	%r15, %r11
	movl	-124(%rsp), %edx                ## 4-byte Reload
	je	LBB23_40
LBB23_28:                               ## %"for relu1_0_d_def__.s15.w.wi.us.us4"
                                        ## =>This Loop Header: Depth=1
                                        ##     Child Loop BB23_33 Depth 2
                                        ##     Child Loop BB23_38 Depth 2
	movslq	%r9d, %rax
	leaq	(%r11,%r10), %rcx
	testq	%rcx, %rcx
	movl	$0, %esi
	cmovlel	%esi, %ecx
	imull	%edx, %ecx
	movslq	%ecx, %rcx
	movq	8(%rsp), %rdx                   ## 8-byte Reload
	addq	%rcx, %rdx
	addq	(%rsp), %rcx                    ## 8-byte Folded Reload
	movq	-40(%rsp), %rsi                 ## 8-byte Reload
	vmovss	-4(%rsi,%rdx,4), %xmm2          ## xmm2 = mem[0],zero,zero,zero
	vmulss	-4(%rsi,%rcx,4), %xmm2, %xmm2
	cmpl	$16, -128(%rsp)                 ## 4-byte Folded Reload
	jae	LBB23_30
## %bb.29:                              ##   in Loop: Header=BB23_28 Depth=1
	xorl	%ecx, %ecx
	jmp	LBB23_37
LBB23_30:                               ## %vector.ph
                                        ##   in Loop: Header=BB23_28 Depth=1
	vbroadcastss	%xmm2, %ymm3
	cmpq	$0, -96(%rsp)                   ## 8-byte Folded Reload
	je	LBB23_31
## %bb.32:                              ## %vector.body.preheader
                                        ##   in Loop: Header=BB23_28 Depth=1
	movq	-120(%rsp), %rcx                ## 8-byte Reload
	addq	%rax, %rcx
	movq	-104(%rsp), %rdx                ## 8-byte Reload
	leaq	(%rdx,%rcx,4), %rcx
	movq	-88(%rsp), %rdx                 ## 8-byte Reload
	xorl	%esi, %esi
LBB23_33:                               ## %vector.body
                                        ##   Parent Loop BB23_28 Depth=1
                                        ## =>  This Inner Loop Header: Depth=2
	vmulps	-96(%rdi,%rsi,4), %ymm3, %ymm4
	vmulps	-64(%rdi,%rsi,4), %ymm3, %ymm5
	vfmadd213ps	-96(%rcx,%rsi,4), %ymm1, %ymm4 ## ymm4 = (ymm1 * ymm4) + mem
	vfmadd213ps	-64(%rcx,%rsi,4), %ymm1, %ymm5 ## ymm5 = (ymm1 * ymm5) + mem
	vmovups	%ymm4, -96(%rcx,%rsi,4)
	vmovups	%ymm5, -64(%rcx,%rsi,4)
	vmulps	-32(%rdi,%rsi,4), %ymm3, %ymm4
	vmulps	(%rdi,%rsi,4), %ymm3, %ymm5
	vfmadd213ps	-32(%rcx,%rsi,4), %ymm1, %ymm4 ## ymm4 = (ymm1 * ymm4) + mem
	vfmadd213ps	(%rcx,%rsi,4), %ymm1, %ymm5 ## ymm5 = (ymm1 * ymm5) + mem
	vmovups	%ymm4, -32(%rcx,%rsi,4)
	vmovups	%ymm5, (%rcx,%rsi,4)
	addq	$32, %rsi
	addq	$2, %rdx
	jne	LBB23_33
	jmp	LBB23_34
LBB23_31:                               ##   in Loop: Header=BB23_28 Depth=1
	xorl	%esi, %esi
LBB23_34:                               ## %middle.block.unr-lcssa
                                        ##   in Loop: Header=BB23_28 Depth=1
	testb	$1, %r14b
	je	LBB23_36
## %bb.35:                              ## %vector.body.epil
                                        ##   in Loop: Header=BB23_28 Depth=1
	movq	-32(%rsp), %rcx                 ## 8-byte Reload
	addl	%r11d, %ecx
	imull	-112(%rsp), %ecx                ## 4-byte Folded Reload
	movq	-48(%rsp), %rdx                 ## 8-byte Reload
	addq	%r11, %rdx
	addl	-60(%rsp), %ecx                 ## 4-byte Folded Reload
	imulq	-72(%rsp), %rdx                 ## 8-byte Folded Reload
	movslq	%ecx, %rcx
	addq	-120(%rsp), %rsi                ## 8-byte Folded Reload
	addq	%rsi, %rcx
	addq	%rdx, %rsi
	movq	-56(%rsp), %rdx                 ## 8-byte Reload
	vmulps	(%rdx,%rsi,4), %ymm3, %ymm4
	vmulps	32(%rdx,%rsi,4), %ymm3, %ymm3
	movq	-80(%rsp), %rdx                 ## 8-byte Reload
	vfmadd213ps	(%rdx,%rcx,4), %ymm1, %ymm4 ## ymm4 = (ymm1 * ymm4) + mem
	vfmadd213ps	32(%rdx,%rcx,4), %ymm1, %ymm3 ## ymm3 = (ymm1 * ymm3) + mem
	vmovups	%ymm4, (%rdx,%rcx,4)
	vmovups	%ymm3, 32(%rdx,%rcx,4)
LBB23_36:                               ## %middle.block
                                        ##   in Loop: Header=BB23_28 Depth=1
	movq	%r12, %rcx
	cmpq	%rbp, %r12
	je	LBB23_39
LBB23_37:                               ## %"for relu1_0_d_def__.s15.n.ni.rebased.us.us9.preheader"
                                        ##   in Loop: Header=BB23_28 Depth=1
	leaq	(%r8,%rax,4), %rax
LBB23_38:                               ## %"for relu1_0_d_def__.s15.n.ni.rebased.us.us9"
                                        ##   Parent Loop BB23_28 Depth=1
                                        ## =>  This Inner Loop Header: Depth=2
	vmulss	(%r13,%rcx,4), %xmm2, %xmm3
	vfmadd213ss	(%rax,%rcx,4), %xmm0, %xmm3 ## xmm3 = (xmm0 * xmm3) + mem
	vmovss	%xmm3, (%rax,%rcx,4)
	incq	%rcx
	cmpq	%rcx, %rbp
	jne	LBB23_38
	jmp	LBB23_39
                                        ## -- End function
	.section	__TEXT,__const
	.p2align	5                               ## -- Begin function train_cost_model.par_for.relu1_0_d_def__.s16.n.n.n
LCPI24_0:
	.long	0                               ## 0x0
	.long	1                               ## 0x1
	.long	2                               ## 0x2
	.long	3                               ## 0x3
	.long	4                               ## 0x4
	.long	5                               ## 0x5
	.long	6                               ## 0x6
	.long	7                               ## 0x7
	.section	__TEXT,__literal4,4byte_literals
	.p2align	2
LCPI24_1:
	.long	0x3089705f                      ## float 9.99999971E-10
	.section	__TEXT,__text,regular,pure_instructions
	.p2align	4, 0x90
_train_cost_model.par_for.relu1_0_d_def__.s16.n.n.n: ## @train_cost_model.par_for.relu1_0_d_def__.s16.n.n.n
## %bb.0:                               ## %entry
	pushq	%rbp
	pushq	%r15
	pushq	%r14
	pushq	%r13
	pushq	%r12
	pushq	%rbx
	subq	$120, %rsp
	movq	%rdx, %rdi
	movl	12(%rdx), %r10d
	movl	24(%rdx), %r13d
	movl	%esi, %r8d
	sarl	$31, %r8d
	xorl	%ecx, %ecx
	testl	%r13d, %r13d
	sete	%cl
	movl	%ecx, %ebp
	negl	%ebp
	movl	%r13d, %ebx
	sarl	$31, %ebx
	subl	%r8d, %esi
	orl	%r13d, %ebp
	movl	%esi, %eax
	cltd
	idivl	%ebp
	movl	%edx, %r14d
	movl	%ebx, %r9d
	leal	(%rcx,%r13), %ebp
	movl	%esi, %eax
	cltd
	idivl	%ebp
	notl	%r9d
	decl	%ecx
	movl	%r9d, %r12d
	subl	%ebx, %r12d
	andl	%r8d, %r12d
	addl	%eax, %r12d
	andl	%ecx, %r12d
	leal	(%r12,%r12), %eax
	movl	%eax, -96(%rsp)                 ## 4-byte Spill
	subl	%eax, %r10d
	cmpl	$2, %r10d
	movl	$2, %edx
	cmovll	%r10d, %edx
	xorl	%eax, %eax
	testl	%r10d, %r10d
	cmovlel	%eax, %edx
	movl	%edx, -104(%rsp)                ## 4-byte Spill
	jle	LBB24_40
## %bb.1:                               ## %"for relu1_0_d_def__.s16.w.wi.preheader"
	movl	(%rdi), %edx
	movl	16(%rdi), %r10d
	movl	32(%rdi), %esi
	movl	36(%rdi), %ebp
	movl	40(%rdi), %eax
	xorl	%r13d, %ebx
	addl	%r9d, %ebx
	andl	%r8d, %ebx
	addl	%ebx, %r14d
	andl	%ecx, %r14d
	leal	(,%r14,8), %r11d
	movl	%ebp, -128(%rsp)                ## 4-byte Spill
	movl	%ebp, %r8d
	subl	%esi, %r8d
	movl	%eax, -40(%rsp)                 ## 4-byte Spill
	movl	%eax, %r9d
	movl	%esi, -48(%rsp)                 ## 4-byte Spill
	subl	%esi, %r9d
	movslq	8(%rdi), %rcx
	movslq	%r12d, %r13
	subq	%rcx, %r13
	addq	%r13, %r13
	movq	%r10, -88(%rsp)                 ## 8-byte Spill
	movl	%r10d, %eax
	shll	$5, %eax
	movl	%eax, -108(%rsp)                ## 4-byte Spill
	movl	%edx, %ebp
	subl	%r11d, %ebp
	cmpl	$8, %ebp
	movl	$8, %ecx
	cmovll	%ebp, %ecx
	testl	%ebp, %ebp
	movl	$0, %r15d
	cmovgl	%ecx, %r15d
	movl	20(%rdi), %eax
	movl	%eax, -124(%rsp)                ## 4-byte Spill
	movl	%r11d, %ebx
	subl	%edx, %ebx
	movl	%ebx, %ecx
	sarl	$31, %ecx
	andl	%ebx, %ecx
	cmpl	$-8, %ecx
	movq	%r14, %rsi
	movl	$-8, %r14d
	cmovgl	%ecx, %r14d
	movl	28(%rdi), %eax
	movl	%eax, -120(%rsp)                ## 4-byte Spill
	movq	48(%rdi), %r10
	movq	64(%rdi), %rax
	movq	%rax, -80(%rsp)                 ## 8-byte Spill
	movq	80(%rdi), %rax
	movq	%rax, -32(%rsp)                 ## 8-byte Spill
	movslq	4(%rdi), %rax
	leal	8(%r11), %ecx
	movslq	%r11d, %rbx
	cmpl	%edx, %ecx
	jle	LBB24_2
## %bb.4:                               ## %"for relu1_0_d_def__.s16.w.wi.preheader.split.us"
	movq	%rsi, -16(%rsp)                 ## 8-byte Spill
	movq	%r10, -64(%rsp)                 ## 8-byte Spill
	movq	%r13, -56(%rsp)                 ## 8-byte Spill
	movq	-88(%rsp), %rdi                 ## 8-byte Reload
	leal	(%rdi,%rdi,8), %ecx
	movl	%ecx, -68(%rsp)                 ## 4-byte Spill
	movl	%r12d, %ecx
	subl	-120(%rsp), %ecx                ## 4-byte Folded Reload
	addl	%ecx, %ecx
	movq	%rcx, (%rsp)                    ## 8-byte Spill
	subl	-48(%rsp), %edx                 ## 4-byte Folded Reload
	movl	-128(%rsp), %ecx                ## 4-byte Reload
	addl	%edx, %ecx
	addl	-40(%rsp), %edx                 ## 4-byte Folded Reload
	addl	%r15d, %r11d
	addl	%r15d, %r14d
	movslq	%ecx, %rcx
	movq	%rcx, -40(%rsp)                 ## 8-byte Spill
	movslq	%edx, %rcx
	movq	%rcx, -48(%rsp)                 ## 8-byte Spill
	testl	%ebp, %ebp
	movl	%r14d, -128(%rsp)               ## 4-byte Spill
	movq	%rax, -24(%rsp)                 ## 8-byte Spill
	jle	LBB24_26
## %bb.5:                               ## %"for relu1_0_d_def__.s16.w.wi.us.us.preheader"
	addl	%ebx, %r9d
	addl	%ebx, %r8d
	movslq	%r8d, %rcx
	movq	%rcx, 8(%rsp)                   ## 8-byte Spill
	movslq	%r9d, %r9
	movl	%r15d, -4(%rsp)                 ## 4-byte Spill
	movl	%r15d, %r10d
	movslq	%r11d, %r8
	movl	%r14d, %r14d
	movslq	-96(%rsp), %r13                 ## 4-byte Folded Reload
	movl	-104(%rsp), %ecx                ## 4-byte Reload
	movq	%rcx, 112(%rsp)                 ## 8-byte Spill
	movl	%r14d, %ecx
	andl	$-16, %ecx
	movq	%rcx, 64(%rsp)                  ## 8-byte Spill
	addq	$-16, %rcx
	movq	%rcx, 56(%rsp)                  ## 8-byte Spill
	movq	%rcx, %r15
	shrq	$4, %r15
	incq	%r15
	movl	%r10d, %r11d
	andl	$2147483616, %r11d              ## imm = 0x7FFFFFE0
	shll	$6, %r12d
	movl	-120(%rsp), %ecx                ## 4-byte Reload
	shll	$6, %ecx
	subl	%ecx, %r12d
	movq	%rax, %rdx
	imulq	-56(%rsp), %rax                 ## 8-byte Folded Reload
	addq	%rax, %rbx
	orl	$9, %r12d
	imull	%r12d, %edi
	movq	-64(%rsp), %rcx                 ## 8-byte Reload
	movq	%rdi, %rbp
	leaq	(%rcx,%rbx,4), %rdi
	addq	$96, %rdi
	movq	%rbp, -88(%rsp)                 ## 8-byte Spill
	movq	-16(%rsp), %rsi                 ## 8-byte Reload
	leal	(%rbp,%rsi,8), %esi
	leaq	(%rcx,%rbx,4), %rbp
	addq	%r8, %rax
	movq	%r15, %rbx
	movq	%r15, 48(%rsp)                  ## 8-byte Spill
	andq	$-2, %r15
	negq	%r15
	movq	%r15, 32(%rsp)                  ## 8-byte Spill
	leaq	(%rcx,%rax,4), %r12
	addq	$96, %r12
	leaq	(%rcx,%rax,4), %r15
	vmovss	LCPI24_1(%rip), %xmm0           ## xmm0 = mem[0],zero,zero,zero
	vbroadcastss	LCPI24_1(%rip), %ymm1   ## ymm1 = [9.99999971E-10,9.99999971E-10,9.99999971E-10,9.99999971E-10,9.99999971E-10,9.99999971E-10,9.99999971E-10,9.99999971E-10]
	movq	-32(%rsp), %rcx                 ## 8-byte Reload
	leaq	96(%rcx), %rax
	movq	%rax, 40(%rsp)                  ## 8-byte Spill
	leaq	(,%rdx,4), %rax
	movq	%rax, 104(%rsp)                 ## 8-byte Spill
	movl	%esi, %edx
	movq	-80(%rsp), %rax                 ## 8-byte Reload
	leaq	96(%rax), %rbx
	movq	%rbx, 24(%rsp)                  ## 8-byte Spill
	movq	%r9, 72(%rsp)                   ## 8-byte Spill
	leaq	(%rcx,%r9,4), %rbx
	movq	%rbx, 96(%rsp)                  ## 8-byte Spill
	movq	8(%rsp), %rsi                   ## 8-byte Reload
	leaq	(%rcx,%rsi,4), %rcx
	movq	%rcx, 88(%rsp)                  ## 8-byte Spill
	movq	%r8, 16(%rsp)                   ## 8-byte Spill
	leaq	(%rax,%r8,4), %rax
	movq	%rax, 80(%rsp)                  ## 8-byte Spill
	movq	%r13, %rax
	movq	%r13, -16(%rsp)                 ## 8-byte Spill
	movq	%r13, %rcx
	xorl	%esi, %esi
	movl	-124(%rsp), %r8d                ## 4-byte Reload
	jmp	LBB24_6
	.p2align	4, 0x90
LBB24_17:                               ## %after_bb.us.us
                                        ##   in Loop: Header=BB24_6 Depth=1
	movq	-120(%rsp), %rsi                ## 8-byte Reload
	incq	%rsi
	movq	-104(%rsp), %rcx                ## 8-byte Reload
	incq	%rcx
	movq	104(%rsp), %rax                 ## 8-byte Reload
	addq	%rax, %rdi
	movl	-96(%rsp), %edx                 ## 4-byte Reload
	addl	%r13d, %edx
	addq	%rax, %rbp
	addq	%rax, %r12
	addl	%r13d, %ebx
	movq	%rbx, -88(%rsp)                 ## 8-byte Spill
	addq	%rax, %r15
	cmpq	112(%rsp), %rsi                 ## 8-byte Folded Reload
	je	LBB24_40
LBB24_6:                                ## %"for relu1_0_d_def__.s16.w.wi.us.us"
                                        ## =>This Loop Header: Depth=1
                                        ##     Child Loop BB24_9 Depth 2
                                        ##     Child Loop BB24_12 Depth 2
                                        ##     Child Loop BB24_21 Depth 2
                                        ##     Child Loop BB24_16 Depth 2
	testq	%rcx, %rcx
	movl	$0, %eax
	movq	%rcx, -104(%rsp)                ## 8-byte Spill
	cmovgl	%ecx, %eax
	imull	%r8d, %eax
	cltq
	movq	-16(%rsp), %rcx                 ## 8-byte Reload
	movq	%rsi, -120(%rsp)                ## 8-byte Spill
	leaq	(%rsi,%rcx), %r9
	testq	%r9, %r9
	movl	$0, %ecx
	cmovlel	%ecx, %r9d
	movl	%edx, -96(%rsp)                 ## 4-byte Spill
	movslq	%edx, %rsi
	cmpl	$32, -4(%rsp)                   ## 4-byte Folded Reload
	jae	LBB24_8
## %bb.7:                               ##   in Loop: Header=BB24_6 Depth=1
	xorl	%ecx, %ecx
	jmp	LBB24_11
	.p2align	4, 0x90
LBB24_8:                                ## %vector.body71.preheader
                                        ##   in Loop: Header=BB24_6 Depth=1
	movq	72(%rsp), %rcx                  ## 8-byte Reload
	addq	%rax, %rcx
	movq	8(%rsp), %rdx                   ## 8-byte Reload
	addq	%rax, %rdx
	movq	40(%rsp), %rbx                  ## 8-byte Reload
	leaq	(%rbx,%rcx,4), %r13
	leaq	(%rbx,%rdx,4), %r8
	movq	24(%rsp), %rcx                  ## 8-byte Reload
	movq	%rsi, %rbx
	leaq	(%rcx,%rsi,4), %rcx
	xorl	%edx, %edx
	.p2align	4, 0x90
LBB24_9:                                ## %vector.body71
                                        ##   Parent Loop BB24_6 Depth=1
                                        ## =>  This Inner Loop Header: Depth=2
	vmovups	-96(%r8,%rdx,4), %ymm2
	vmovups	-64(%r8,%rdx,4), %ymm3
	vmovups	-32(%r8,%rdx,4), %ymm4
	vmovups	(%r8,%rdx,4), %ymm5
	vmulps	-96(%r13,%rdx,4), %ymm2, %ymm2
	vmulps	-64(%r13,%rdx,4), %ymm3, %ymm3
	vmulps	-32(%r13,%rdx,4), %ymm4, %ymm4
	vmulps	(%r13,%rdx,4), %ymm5, %ymm5
	vmulps	-96(%rdi,%rdx,4), %ymm2, %ymm2
	vmulps	-64(%rdi,%rdx,4), %ymm3, %ymm3
	vmulps	-32(%rdi,%rdx,4), %ymm4, %ymm4
	vmulps	(%rdi,%rdx,4), %ymm5, %ymm5
	vfmadd213ps	-96(%rcx,%rdx,4), %ymm1, %ymm2 ## ymm2 = (ymm1 * ymm2) + mem
	vfmadd213ps	-64(%rcx,%rdx,4), %ymm1, %ymm3 ## ymm3 = (ymm1 * ymm3) + mem
	vfmadd213ps	-32(%rcx,%rdx,4), %ymm1, %ymm4 ## ymm4 = (ymm1 * ymm4) + mem
	vfmadd213ps	(%rcx,%rdx,4), %ymm1, %ymm5 ## ymm5 = (ymm1 * ymm5) + mem
	vmovups	%ymm2, -96(%rcx,%rdx,4)
	vmovups	%ymm3, -64(%rcx,%rdx,4)
	vmovups	%ymm4, -32(%rcx,%rdx,4)
	vmovups	%ymm5, (%rcx,%rdx,4)
	addq	$32, %rdx
	cmpq	%rdx, %r11
	jne	LBB24_9
## %bb.10:                              ## %middle.block69
                                        ##   in Loop: Header=BB24_6 Depth=1
	movq	%r11, %rcx
	cmpq	%r10, %r11
	movl	-124(%rsp), %r8d                ## 4-byte Reload
	movq	%rbx, %rsi
	je	LBB24_13
LBB24_11:                               ## %"for relu1_0_d_def__.s16.n.ni.us.us.preheader"
                                        ##   in Loop: Header=BB24_6 Depth=1
	movq	96(%rsp), %rdx                  ## 8-byte Reload
	leaq	(%rdx,%rax,4), %rdx
	movq	88(%rsp), %rbx                  ## 8-byte Reload
	leaq	(%rbx,%rax,4), %rax
	movq	-80(%rsp), %rbx                 ## 8-byte Reload
	leaq	(%rbx,%rsi,4), %rbx
	.p2align	4, 0x90
LBB24_12:                               ## %"for relu1_0_d_def__.s16.n.ni.us.us"
                                        ##   Parent Loop BB24_6 Depth=1
                                        ## =>  This Inner Loop Header: Depth=2
	vmovss	(%rax,%rcx,4), %xmm2            ## xmm2 = mem[0],zero,zero,zero
	vmulss	(%rdx,%rcx,4), %xmm2, %xmm2
	vmulss	(%rbp,%rcx,4), %xmm2, %xmm2
	vfmadd213ss	(%rbx,%rcx,4), %xmm0, %xmm2 ## xmm2 = (xmm0 * xmm2) + mem
	vmovss	%xmm2, (%rbx,%rcx,4)
	incq	%rcx
	cmpq	%rcx, %r10
	jne	LBB24_12
LBB24_13:                               ## %"end for relu1_0_d_def__.s16.n.ni.loopexit.us.us"
                                        ##   in Loop: Header=BB24_6 Depth=1
	cmpl	$0, -128(%rsp)                  ## 4-byte Folded Reload
	movl	-108(%rsp), %r13d               ## 4-byte Reload
	movq	-88(%rsp), %rbx                 ## 8-byte Reload
	jle	LBB24_17
## %bb.14:                              ## %"for relu1_0_d_def__.s16.n.ni.rebased.preheader.us.us"
                                        ##   in Loop: Header=BB24_6 Depth=1
	movslq	%ebx, %rax
	imull	%r8d, %r9d
	movslq	%r9d, %rcx
	movq	-40(%rsp), %rdx                 ## 8-byte Reload
	addq	%rcx, %rdx
	addq	-48(%rsp), %rcx                 ## 8-byte Folded Reload
	movq	-32(%rsp), %rsi                 ## 8-byte Reload
	vmovss	-4(%rsi,%rdx,4), %xmm2          ## xmm2 = mem[0],zero,zero,zero
	vmulss	-4(%rsi,%rcx,4), %xmm2, %xmm2
	cmpl	$16, -128(%rsp)                 ## 4-byte Folded Reload
	jae	LBB24_18
## %bb.15:                              ##   in Loop: Header=BB24_6 Depth=1
	xorl	%ecx, %ecx
	jmp	LBB24_25
	.p2align	4, 0x90
LBB24_18:                               ## %vector.ph54
                                        ##   in Loop: Header=BB24_6 Depth=1
	vbroadcastss	%xmm2, %ymm3
	cmpq	$0, 56(%rsp)                    ## 8-byte Folded Reload
	je	LBB24_19
## %bb.20:                              ## %vector.body52.preheader
                                        ##   in Loop: Header=BB24_6 Depth=1
	movq	16(%rsp), %rcx                  ## 8-byte Reload
	addq	%rax, %rcx
	movq	24(%rsp), %rdx                  ## 8-byte Reload
	leaq	(%rdx,%rcx,4), %rcx
	movq	32(%rsp), %rdx                  ## 8-byte Reload
	xorl	%esi, %esi
	.p2align	4, 0x90
LBB24_21:                               ## %vector.body52
                                        ##   Parent Loop BB24_6 Depth=1
                                        ## =>  This Inner Loop Header: Depth=2
	vmulps	-96(%r12,%rsi,4), %ymm3, %ymm4
	vmulps	-64(%r12,%rsi,4), %ymm3, %ymm5
	vfmadd213ps	-96(%rcx,%rsi,4), %ymm1, %ymm4 ## ymm4 = (ymm1 * ymm4) + mem
	vfmadd213ps	-64(%rcx,%rsi,4), %ymm1, %ymm5 ## ymm5 = (ymm1 * ymm5) + mem
	vmovups	%ymm4, -96(%rcx,%rsi,4)
	vmovups	%ymm5, -64(%rcx,%rsi,4)
	vmulps	-32(%r12,%rsi,4), %ymm3, %ymm4
	vmulps	(%r12,%rsi,4), %ymm3, %ymm5
	vfmadd213ps	-32(%rcx,%rsi,4), %ymm1, %ymm4 ## ymm4 = (ymm1 * ymm4) + mem
	vfmadd213ps	(%rcx,%rsi,4), %ymm1, %ymm5 ## ymm5 = (ymm1 * ymm5) + mem
	vmovups	%ymm4, -32(%rcx,%rsi,4)
	vmovups	%ymm5, (%rcx,%rsi,4)
	addq	$32, %rsi
	addq	$2, %rdx
	jne	LBB24_21
## %bb.22:                              ## %middle.block50.unr-lcssa
                                        ##   in Loop: Header=BB24_6 Depth=1
	testb	$1, 48(%rsp)                    ## 1-byte Folded Reload
	je	LBB24_24
LBB24_23:                               ## %vector.body52.epil
                                        ##   in Loop: Header=BB24_6 Depth=1
	movq	(%rsp), %rcx                    ## 8-byte Reload
	movq	%rax, %r9
	movq	-120(%rsp), %rax                ## 8-byte Reload
	addl	%eax, %ecx
	movq	-56(%rsp), %rdx                 ## 8-byte Reload
	addq	%rax, %rdx
	movq	%r9, %rax
	imull	%r13d, %ecx
	imulq	-24(%rsp), %rdx                 ## 8-byte Folded Reload
	addl	-68(%rsp), %ecx                 ## 4-byte Folded Reload
	movslq	%ecx, %rcx
	addq	16(%rsp), %rsi                  ## 8-byte Folded Reload
	addq	%rsi, %rcx
	addq	%rdx, %rsi
	movq	-64(%rsp), %rdx                 ## 8-byte Reload
	vmulps	(%rdx,%rsi,4), %ymm3, %ymm4
	vmulps	32(%rdx,%rsi,4), %ymm3, %ymm3
	movq	-80(%rsp), %rdx                 ## 8-byte Reload
	vfmadd213ps	(%rdx,%rcx,4), %ymm1, %ymm4 ## ymm4 = (ymm1 * ymm4) + mem
	vfmadd213ps	32(%rdx,%rcx,4), %ymm1, %ymm3 ## ymm3 = (ymm1 * ymm3) + mem
	vmovups	%ymm4, (%rdx,%rcx,4)
	vmovups	%ymm3, 32(%rdx,%rcx,4)
LBB24_24:                               ## %middle.block50
                                        ##   in Loop: Header=BB24_6 Depth=1
	movq	64(%rsp), %rdx                  ## 8-byte Reload
	movq	%rdx, %rcx
	cmpq	%r14, %rdx
	je	LBB24_17
LBB24_25:                               ## %"for relu1_0_d_def__.s16.n.ni.rebased.us.us.preheader"
                                        ##   in Loop: Header=BB24_6 Depth=1
	movq	80(%rsp), %rdx                  ## 8-byte Reload
	leaq	(%rdx,%rax,4), %rax
	.p2align	4, 0x90
LBB24_16:                               ## %"for relu1_0_d_def__.s16.n.ni.rebased.us.us"
                                        ##   Parent Loop BB24_6 Depth=1
                                        ## =>  This Inner Loop Header: Depth=2
	vmulss	(%r15,%rcx,4), %xmm2, %xmm3
	vfmadd213ss	(%rax,%rcx,4), %xmm0, %xmm3 ## xmm3 = (xmm0 * xmm3) + mem
	vmovss	%xmm3, (%rax,%rcx,4)
	incq	%rcx
	cmpq	%rcx, %r14
	jne	LBB24_16
	jmp	LBB24_17
LBB24_19:                               ##   in Loop: Header=BB24_6 Depth=1
	xorl	%esi, %esi
	testb	$1, 48(%rsp)                    ## 1-byte Folded Reload
	jne	LBB24_23
	jmp	LBB24_24
LBB24_2:                                ## %"for relu1_0_d_def__.s16.w.wi.preheader20"
	decl	%edx
	vmovd	%ebx, %xmm0
	vpbroadcastd	%xmm0, %ymm0
	vpor	LCPI24_0(%rip), %ymm0, %ymm0
	vmovd	%edx, %xmm1
	vpbroadcastd	%xmm1, %ymm1
	vpminsd	%ymm0, %ymm1, %ymm0
	movslq	-96(%rsp), %rdi                 ## 4-byte Folded Reload
	movl	-104(%rsp), %r14d               ## 4-byte Reload
	imulq	%rax, %r13
	addq	%rbx, %r13
	leaq	(%r10,%r13,4), %rbx
	shlq	$2, %rax
	movq	%rax, %r15
	shll	$6, %r12d
	movl	-120(%rsp), %eax                ## 4-byte Reload
	shll	$6, %eax
	subl	%eax, %r12d
	orl	$9, %r12d
	movq	-88(%rsp), %rax                 ## 8-byte Reload
	imull	%r12d, %eax
	leal	(%rax,%rsi,8), %edx
	vbroadcastss	LCPI24_1(%rip), %ymm1   ## ymm1 = [9.99999971E-10,9.99999971E-10,9.99999971E-10,9.99999971E-10,9.99999971E-10,9.99999971E-10,9.99999971E-10,9.99999971E-10]
	movl	-108(%rsp), %r10d               ## 4-byte Reload
	movl	-124(%rsp), %r11d               ## 4-byte Reload
	movq	-32(%rsp), %rbp                 ## 8-byte Reload
	.p2align	4, 0x90
LBB24_3:                                ## %"for relu1_0_d_def__.s16.w.wi"
                                        ## =>This Inner Loop Header: Depth=1
	testq	%rdi, %rdi
	movl	$0, %esi
	cmovgl	%edi, %esi
	imull	%r11d, %esi
	leal	(%rsi,%r8), %ecx
	vmovd	%ecx, %xmm2
	vpbroadcastd	%xmm2, %ymm2
	vpaddd	%ymm0, %ymm2, %ymm3
	vextracti128	$1, %ymm3, %xmm2
	vmovd	%xmm2, %ecx
	movslq	%ecx, %rcx
	vpextrd	$1, %xmm2, %eax
	cltq
	vmovss	(%rbp,%rcx,4), %xmm4            ## xmm4 = mem[0],zero,zero,zero
	vpextrd	$2, %xmm2, %ecx
	vinsertps	$16, (%rbp,%rax,4), %xmm4, %xmm4 ## xmm4 = xmm4[0],mem[0],xmm4[2,3]
	vmovd	%xmm3, %eax
	movslq	%ecx, %rcx
	vinsertps	$32, (%rbp,%rcx,4), %xmm4, %xmm4 ## xmm4 = xmm4[0,1],mem[0],xmm4[3]
	cltq
	vpextrd	$3, %xmm2, %ecx
	movslq	%ecx, %rcx
	vinsertps	$48, (%rbp,%rcx,4), %xmm4, %xmm2 ## xmm2 = xmm4[0,1,2],mem[0]
	vpextrd	$1, %xmm3, %ecx
	vmovss	(%rbp,%rax,4), %xmm4            ## xmm4 = mem[0],zero,zero,zero
	movslq	%ecx, %rax
	vinsertps	$16, (%rbp,%rax,4), %xmm4, %xmm4 ## xmm4 = xmm4[0],mem[0],xmm4[2,3]
	vpextrd	$2, %xmm3, %eax
	cltq
	vinsertps	$32, (%rbp,%rax,4), %xmm4, %xmm4 ## xmm4 = xmm4[0,1],mem[0],xmm4[3]
	vpextrd	$3, %xmm3, %eax
	cltq
	vinsertps	$48, (%rbp,%rax,4), %xmm4, %xmm3 ## xmm3 = xmm4[0,1,2],mem[0]
	addl	%r9d, %esi
	vmovd	%esi, %xmm4
	vpbroadcastd	%xmm4, %ymm4
	vpaddd	%ymm0, %ymm4, %ymm4
	vextracti128	$1, %ymm4, %xmm5
	vmovd	%xmm5, %eax
	cltq
	vmovss	(%rbp,%rax,4), %xmm6            ## xmm6 = mem[0],zero,zero,zero
	vpextrd	$1, %xmm5, %eax
	cltq
	vinsertps	$16, (%rbp,%rax,4), %xmm6, %xmm6 ## xmm6 = xmm6[0],mem[0],xmm6[2,3]
	vpextrd	$2, %xmm5, %eax
	cltq
	vinsertps	$32, (%rbp,%rax,4), %xmm6, %xmm6 ## xmm6 = xmm6[0,1],mem[0],xmm6[3]
	vpextrd	$3, %xmm5, %eax
	cltq
	vinsertps	$48, (%rbp,%rax,4), %xmm6, %xmm5 ## xmm5 = xmm6[0,1,2],mem[0]
	vmovd	%xmm4, %eax
	cltq
	vmovss	(%rbp,%rax,4), %xmm6            ## xmm6 = mem[0],zero,zero,zero
	vpextrd	$1, %xmm4, %eax
	cltq
	vinsertps	$16, (%rbp,%rax,4), %xmm6, %xmm6 ## xmm6 = xmm6[0],mem[0],xmm6[2,3]
	vpextrd	$2, %xmm4, %eax
	cltq
	vinsertps	$32, (%rbp,%rax,4), %xmm6, %xmm6 ## xmm6 = xmm6[0,1],mem[0],xmm6[3]
	vpextrd	$3, %xmm4, %eax
	cltq
	vinsertps	$48, (%rbp,%rax,4), %xmm6, %xmm4 ## xmm4 = xmm6[0,1,2],mem[0]
	vinsertf128	$1, %xmm2, %ymm3, %ymm2
	movslq	%edx, %rdx
	vinsertf128	$1, %xmm5, %ymm4, %ymm3
	vmulps	%ymm3, %ymm2, %ymm2
	vmulps	(%rbx), %ymm2, %ymm2
	movq	-80(%rsp), %rax                 ## 8-byte Reload
	vfmadd213ps	(%rax,%rdx,4), %ymm1, %ymm2 ## ymm2 = (ymm1 * ymm2) + mem
	vmovups	%ymm2, (%rax,%rdx,4)
	addq	%r15, %rbx
	incq	%rdi
	addl	%r10d, %edx
	decq	%r14
	jne	LBB24_3
LBB24_40:                               ## %destructor_block
	xorl	%eax, %eax
	addq	$120, %rsp
	popq	%rbx
	popq	%r12
	popq	%r13
	popq	%r14
	popq	%r15
	popq	%rbp
	vzeroupper
	retq
LBB24_26:                               ## %"for relu1_0_d_def__.s16.w.wi.preheader.split.us.split"
	testl	%r14d, %r14d
	movl	-124(%rsp), %edx                ## 4-byte Reload
	jle	LBB24_40
## %bb.27:                              ## %"for relu1_0_d_def__.s16.w.wi.us.us4.preheader"
	movq	%rdi, %r8
	movslq	%r11d, %r10
	movl	-128(%rsp), %edi                ## 4-byte Reload
	movslq	-96(%rsp), %r9                  ## 4-byte Folded Reload
	movl	-104(%rsp), %r14d               ## 4-byte Reload
	movl	%edi, %r15d
	andl	$-16, %r15d
	leaq	-16(%r15), %r11
	movq	%r11, -96(%rsp)                 ## 8-byte Spill
	shrq	$4, %r11
	incq	%r11
	movq	-24(%rsp), %rsi                 ## 8-byte Reload
	movq	%rsi, %rcx
	imulq	-56(%rsp), %rcx                 ## 8-byte Folded Reload
	addq	%r10, %rcx
	movq	-64(%rsp), %r13                 ## 8-byte Reload
	leaq	96(,%rcx,4), %rax
	addq	%r13, %rax
	leaq	(,%rsi,4), %rbx
	movq	-80(%rsp), %rsi                 ## 8-byte Reload
	leaq	96(%rsi), %rbp
	movq	%rbp, -104(%rsp)                ## 8-byte Spill
	shll	$6, %r12d
	movl	-120(%rsp), %ebp                ## 4-byte Reload
	shll	$6, %ebp
	subl	%ebp, %r12d
	orl	$9, %r12d
	imull	%r12d, %r8d
	movq	%r11, %rbp
	andq	$-2, %rbp
	negq	%rbp
	movq	%rbp, -88(%rsp)                 ## 8-byte Spill
	leaq	(,%rcx,4), %rbp
	addq	%r13, %rbp
	movq	%r10, -120(%rsp)                ## 8-byte Spill
	leaq	(%rsi,%r10,4), %r12
	vmovss	LCPI24_1(%rip), %xmm0           ## xmm0 = mem[0],zero,zero,zero
	vbroadcastss	LCPI24_1(%rip), %ymm1   ## ymm1 = [9.99999971E-10,9.99999971E-10,9.99999971E-10,9.99999971E-10,9.99999971E-10,9.99999971E-10,9.99999971E-10,9.99999971E-10]
	xorl	%r10d, %r10d
	jmp	LBB24_28
LBB24_39:                               ## %after_bb.loopexit.us.us17
                                        ##   in Loop: Header=BB24_28 Depth=1
	incq	%r10
	addq	%rbx, %rax
	addl	-108(%rsp), %r8d                ## 4-byte Folded Reload
	addq	%rbx, %rbp
	cmpq	%r14, %r10
	movl	-124(%rsp), %edx                ## 4-byte Reload
	je	LBB24_40
LBB24_28:                               ## %"for relu1_0_d_def__.s16.w.wi.us.us4"
                                        ## =>This Loop Header: Depth=1
                                        ##     Child Loop BB24_33 Depth 2
                                        ##     Child Loop BB24_38 Depth 2
	movslq	%r8d, %r13
	leaq	(%r10,%r9), %rcx
	testq	%rcx, %rcx
	movl	$0, %esi
	cmovlel	%esi, %ecx
	imull	%edx, %ecx
	movslq	%ecx, %rcx
	movq	-40(%rsp), %rdx                 ## 8-byte Reload
	addq	%rcx, %rdx
	addq	-48(%rsp), %rcx                 ## 8-byte Folded Reload
	movq	-32(%rsp), %rsi                 ## 8-byte Reload
	vmovss	-4(%rsi,%rdx,4), %xmm2          ## xmm2 = mem[0],zero,zero,zero
	vmulss	-4(%rsi,%rcx,4), %xmm2, %xmm2
	cmpl	$16, -128(%rsp)                 ## 4-byte Folded Reload
	jae	LBB24_30
## %bb.29:                              ##   in Loop: Header=BB24_28 Depth=1
	xorl	%ecx, %ecx
	jmp	LBB24_37
LBB24_30:                               ## %vector.ph
                                        ##   in Loop: Header=BB24_28 Depth=1
	vbroadcastss	%xmm2, %ymm3
	cmpq	$0, -96(%rsp)                   ## 8-byte Folded Reload
	je	LBB24_31
## %bb.32:                              ## %vector.body.preheader
                                        ##   in Loop: Header=BB24_28 Depth=1
	movq	-120(%rsp), %rcx                ## 8-byte Reload
	addq	%r13, %rcx
	movq	-104(%rsp), %rdx                ## 8-byte Reload
	leaq	(%rdx,%rcx,4), %rcx
	movq	-88(%rsp), %rdx                 ## 8-byte Reload
	xorl	%esi, %esi
LBB24_33:                               ## %vector.body
                                        ##   Parent Loop BB24_28 Depth=1
                                        ## =>  This Inner Loop Header: Depth=2
	vmulps	-96(%rax,%rsi,4), %ymm3, %ymm4
	vmulps	-64(%rax,%rsi,4), %ymm3, %ymm5
	vfmadd213ps	-96(%rcx,%rsi,4), %ymm1, %ymm4 ## ymm4 = (ymm1 * ymm4) + mem
	vfmadd213ps	-64(%rcx,%rsi,4), %ymm1, %ymm5 ## ymm5 = (ymm1 * ymm5) + mem
	vmovups	%ymm4, -96(%rcx,%rsi,4)
	vmovups	%ymm5, -64(%rcx,%rsi,4)
	vmulps	-32(%rax,%rsi,4), %ymm3, %ymm4
	vmulps	(%rax,%rsi,4), %ymm3, %ymm5
	vfmadd213ps	-32(%rcx,%rsi,4), %ymm1, %ymm4 ## ymm4 = (ymm1 * ymm4) + mem
	vfmadd213ps	(%rcx,%rsi,4), %ymm1, %ymm5 ## ymm5 = (ymm1 * ymm5) + mem
	vmovups	%ymm4, -32(%rcx,%rsi,4)
	vmovups	%ymm5, (%rcx,%rsi,4)
	addq	$32, %rsi
	addq	$2, %rdx
	jne	LBB24_33
	jmp	LBB24_34
LBB24_31:                               ##   in Loop: Header=BB24_28 Depth=1
	xorl	%esi, %esi
LBB24_34:                               ## %middle.block.unr-lcssa
                                        ##   in Loop: Header=BB24_28 Depth=1
	testb	$1, %r11b
	je	LBB24_36
## %bb.35:                              ## %vector.body.epil
                                        ##   in Loop: Header=BB24_28 Depth=1
	movq	(%rsp), %rcx                    ## 8-byte Reload
	addl	%r10d, %ecx
	imull	-108(%rsp), %ecx                ## 4-byte Folded Reload
	movq	-56(%rsp), %rdx                 ## 8-byte Reload
	addq	%r10, %rdx
	addl	-68(%rsp), %ecx                 ## 4-byte Folded Reload
	imulq	-24(%rsp), %rdx                 ## 8-byte Folded Reload
	movslq	%ecx, %rcx
	addq	-120(%rsp), %rsi                ## 8-byte Folded Reload
	addq	%rsi, %rcx
	addq	%rdx, %rsi
	movq	-64(%rsp), %rdx                 ## 8-byte Reload
	vmulps	(%rdx,%rsi,4), %ymm3, %ymm4
	vmulps	32(%rdx,%rsi,4), %ymm3, %ymm3
	movq	-80(%rsp), %rdx                 ## 8-byte Reload
	vfmadd213ps	(%rdx,%rcx,4), %ymm1, %ymm4 ## ymm4 = (ymm1 * ymm4) + mem
	vfmadd213ps	32(%rdx,%rcx,4), %ymm1, %ymm3 ## ymm3 = (ymm1 * ymm3) + mem
	vmovups	%ymm4, (%rdx,%rcx,4)
	vmovups	%ymm3, 32(%rdx,%rcx,4)
LBB24_36:                               ## %middle.block
                                        ##   in Loop: Header=BB24_28 Depth=1
	movq	%r15, %rcx
	cmpq	%rdi, %r15
	je	LBB24_39
LBB24_37:                               ## %"for relu1_0_d_def__.s16.n.ni.rebased.us.us9.preheader"
                                        ##   in Loop: Header=BB24_28 Depth=1
	leaq	(%r12,%r13,4), %rdx
LBB24_38:                               ## %"for relu1_0_d_def__.s16.n.ni.rebased.us.us9"
                                        ##   Parent Loop BB24_28 Depth=1
                                        ## =>  This Inner Loop Header: Depth=2
	vmulss	(%rbp,%rcx,4), %xmm2, %xmm3
	vfmadd213ss	(%rdx,%rcx,4), %xmm0, %xmm3 ## xmm3 = (xmm0 * xmm3) + mem
	vmovss	%xmm3, (%rdx,%rcx,4)
	incq	%rcx
	cmpq	%rcx, %rdi
	jne	LBB24_38
	jmp	LBB24_39
                                        ## -- End function
	.section	__TEXT,__const
	.p2align	5                               ## -- Begin function train_cost_model.par_for.relu1_0_d_def__.s17.n.n.n
LCPI25_0:
	.long	0                               ## 0x0
	.long	1                               ## 0x1
	.long	2                               ## 0x2
	.long	3                               ## 0x3
	.long	4                               ## 0x4
	.long	5                               ## 0x5
	.long	6                               ## 0x6
	.long	7                               ## 0x7
	.section	__TEXT,__literal4,4byte_literals
	.p2align	2
LCPI25_1:
	.long	0x3089705f                      ## float 9.99999971E-10
	.section	__TEXT,__text,regular,pure_instructions
	.p2align	4, 0x90
_train_cost_model.par_for.relu1_0_d_def__.s17.n.n.n: ## @train_cost_model.par_for.relu1_0_d_def__.s17.n.n.n
## %bb.0:                               ## %entry
	pushq	%rbp
	pushq	%r15
	pushq	%r14
	pushq	%r13
	pushq	%r12
	pushq	%rbx
	subq	$136, %rsp
	movq	%rdx, %rdi
	movl	12(%rdx), %r9d
	movl	24(%rdx), %r14d
	movl	%esi, %r10d
	sarl	$31, %r10d
	xorl	%ebp, %ebp
	testl	%r14d, %r14d
	sete	%bpl
	movl	%ebp, %ecx
	negl	%ecx
	movl	%r14d, %ebx
	sarl	$31, %ebx
	subl	%r10d, %esi
	orl	%r14d, %ecx
	movl	%esi, %eax
	cltd
	idivl	%ecx
	movl	%edx, %ecx
	movl	%ebx, %r15d
	leal	(%r14,%rbp), %r8d
	movl	%esi, %eax
	cltd
	idivl	%r8d
	notl	%r15d
	decl	%ebp
	movl	%r15d, %r8d
	subl	%ebx, %r8d
	andl	%r10d, %r8d
	addl	%eax, %r8d
	andl	%ebp, %r8d
	leal	(%r8,%r8), %eax
	movl	%eax, -112(%rsp)                ## 4-byte Spill
	subl	%eax, %r9d
	cmpl	$2, %r9d
	movl	$2, %edx
	cmovll	%r9d, %edx
	xorl	%eax, %eax
	testl	%r9d, %r9d
	cmovlel	%eax, %edx
	movl	%edx, -120(%rsp)                ## 4-byte Spill
	jle	LBB25_40
## %bb.1:                               ## %"for relu1_0_d_def__.s17.w.wi.preheader"
	movl	(%rdi), %r13d
	movl	16(%rdi), %eax
	movq	%rax, -96(%rsp)                 ## 8-byte Spill
	xorl	%r12d, %r12d
	movl	32(%rdi), %edx
	movl	36(%rdi), %eax
	movl	40(%rdi), %esi
	xorl	%r14d, %ebx
	addl	%r15d, %ebx
	andl	%r10d, %ebx
	addl	%ebx, %ecx
	andl	%ebp, %ecx
	leal	(,%rcx,8), %r14d
	movl	%eax, -128(%rsp)                ## 4-byte Spill
	movl	%eax, %r10d
	subl	%edx, %r10d
	movl	%esi, -64(%rsp)                 ## 4-byte Spill
	movl	%esi, %r15d
	movl	%edx, -72(%rsp)                 ## 4-byte Spill
	subl	%edx, %r15d
	movslq	8(%rdi), %rax
	movslq	%r8d, %rbp
	subq	%rax, %rbp
	addq	%rbp, %rbp
	movl	%r13d, %eax
	subl	%r14d, %eax
	cmpl	$8, %eax
	movl	$8, %esi
	cmovll	%eax, %esi
	movl	20(%rdi), %ebx
	movl	%ebx, -124(%rsp)                ## 4-byte Spill
	testl	%eax, %eax
	cmovgl	%esi, %r12d
	movq	48(%rdi), %rdx
	movq	%rdx, -80(%rsp)                 ## 8-byte Spill
	movl	%r14d, %esi
	subl	%r13d, %esi
	movl	%esi, %ebx
	sarl	$31, %ebx
	andl	%esi, %ebx
	cmpl	$-8, %ebx
	movl	$-8, %r9d
	cmovgl	%ebx, %r9d
	movq	64(%rdi), %rsi
	movq	%rsi, -88(%rsp)                 ## 8-byte Spill
	movq	80(%rdi), %rsi
	movq	%rsi, -40(%rsp)                 ## 8-byte Spill
	movl	28(%rdi), %ebx
	movslq	4(%rdi), %r11
	leal	8(%r14), %esi
	movq	-96(%rsp), %rdx                 ## 8-byte Reload
	leal	(,%rdx,4), %edi
	movl	%edi, -100(%rsp)                ## 4-byte Spill
	movslq	%r14d, %rdx
	cmpl	%r13d, %esi
	jle	LBB25_2
## %bb.4:                               ## %"for relu1_0_d_def__.s17.w.wi.preheader.split.us"
	movq	%r11, %rdi
	movq	%rbp, -56(%rsp)                 ## 8-byte Spill
	movl	%r8d, %ecx
	subl	%ebx, %ecx
	subl	-72(%rsp), %r13d                ## 4-byte Folded Reload
	movl	-128(%rsp), %esi                ## 4-byte Reload
	addl	%r13d, %esi
	addl	-64(%rsp), %r13d                ## 4-byte Folded Reload
	addl	%ecx, %ecx
	movq	%rcx, -8(%rsp)                  ## 8-byte Spill
	leal	(%r12,%r14), %ecx
	addl	%r12d, %r9d
	movslq	%esi, %rsi
	movq	%rsi, 40(%rsp)                  ## 8-byte Spill
	movslq	%r13d, %rsi
	movq	%rsi, 32(%rsp)                  ## 8-byte Spill
	testl	%eax, %eax
	movl	%r9d, %eax
	movl	%r9d, -128(%rsp)                ## 4-byte Spill
	movq	%r11, -32(%rsp)                 ## 8-byte Spill
	jle	LBB25_26
## %bb.5:                               ## %"for relu1_0_d_def__.s17.w.wi.us.us.preheader"
	movq	%r12, %r11
	addl	%edx, %r15d
	addl	%edx, %r10d
	movslq	%r10d, %rsi
	movq	%rsi, 8(%rsp)                   ## 8-byte Spill
	movslq	%r15d, %rsi
	movq	%rsi, (%rsp)                    ## 8-byte Spill
	movl	%r11d, %r9d
	movslq	%ecx, %r13
	movl	%eax, %r10d
	movslq	-112(%rsp), %rax                ## 4-byte Folded Reload
	movq	%rax, -48(%rsp)                 ## 8-byte Spill
	movl	-120(%rsp), %eax                ## 4-byte Reload
	movq	%rax, 128(%rsp)                 ## 8-byte Spill
	movl	%r10d, %eax
	andl	$-16, %eax
	movq	%rax, 80(%rsp)                  ## 8-byte Spill
	addq	$-16, %rax
	movq	%rax, 72(%rsp)                  ## 8-byte Spill
	movq	%rax, %r15
	shrq	$4, %r15
	incq	%r15
	movl	%r9d, %r12d
	andl	$2147483616, %r12d              ## imm = 0x7FFFFFE0
	shll	$3, %r8d
	shll	$3, %ebx
	subl	%ebx, %r8d
	movq	%rdi, %rsi
	movq	%rdi, %rcx
	imulq	-56(%rsp), %rcx                 ## 8-byte Folded Reload
	addq	%rcx, %rdx
	orl	$1, %r8d
	movq	-96(%rsp), %rax                 ## 8-byte Reload
	imull	%eax, %r8d
	leal	(%r14,%r8,8), %edi
	movq	%rdx, %rbp
	movl	%r8d, %edx
	shll	$3, %edx
	movl	%edx, -120(%rsp)                ## 4-byte Spill
                                        ## kill: def $eax killed $eax killed $rax
	shll	$5, %eax
	movl	%eax, -12(%rsp)                 ## 4-byte Spill
	movq	%rsi, %r8
	movq	-80(%rsp), %rdx                 ## 8-byte Reload
	leaq	(%rdx,%rbp,4), %rbx
	addq	$96, %rbx
	leaq	(%rdx,%rbp,4), %rax
	addq	%r13, %rcx
	movq	%r15, %rsi
	movq	%r15, 64(%rsp)                  ## 8-byte Spill
	movl	%edi, %esi
	andq	$-2, %r15
	negq	%r15
	movq	%r15, 48(%rsp)                  ## 8-byte Spill
	leaq	(%rdx,%rcx,4), %r14
	addq	$96, %r14
	leaq	(%rdx,%rcx,4), %r15
	vmovss	LCPI25_1(%rip), %xmm0           ## xmm0 = mem[0],zero,zero,zero
	vbroadcastss	LCPI25_1(%rip), %ymm1   ## ymm1 = [9.99999971E-10,9.99999971E-10,9.99999971E-10,9.99999971E-10,9.99999971E-10,9.99999971E-10,9.99999971E-10,9.99999971E-10]
	movq	-40(%rsp), %rdx                 ## 8-byte Reload
	leaq	96(%rdx), %rcx
	movq	%rcx, 56(%rsp)                  ## 8-byte Spill
	leaq	(,%r8,4), %rcx
	movq	%rcx, 120(%rsp)                 ## 8-byte Spill
	movq	-88(%rsp), %rcx                 ## 8-byte Reload
	leaq	96(%rcx), %rbp
	movq	%rbp, 24(%rsp)                  ## 8-byte Spill
	movq	(%rsp), %rbp                    ## 8-byte Reload
	leaq	(%rdx,%rbp,4), %rbp
	movq	%rbp, 104(%rsp)                 ## 8-byte Spill
	movq	8(%rsp), %rbp                   ## 8-byte Reload
	leaq	(%rdx,%rbp,4), %rdx
	movq	%rdx, 96(%rsp)                  ## 8-byte Spill
	movq	%r13, 16(%rsp)                  ## 8-byte Spill
	leaq	(%rcx,%r13,4), %rcx
	movq	%rcx, 88(%rsp)                  ## 8-byte Spill
	movq	-48(%rsp), %rbp                 ## 8-byte Reload
	xorl	%edi, %edi
	movl	-124(%rsp), %r8d                ## 4-byte Reload
	movq	%r11, -24(%rsp)                 ## 8-byte Spill
	jmp	LBB25_6
	.p2align	4, 0x90
LBB25_17:                               ## %after_bb.us.us
                                        ##   in Loop: Header=BB25_6 Depth=1
	movq	-112(%rsp), %rdi                ## 8-byte Reload
	incq	%rdi
	movq	-72(%rsp), %rbp                 ## 8-byte Reload
	incq	%rbp
	movq	120(%rsp), %rdx                 ## 8-byte Reload
	addq	%rdx, %rbx
	movl	-64(%rsp), %esi                 ## 4-byte Reload
	movl	-12(%rsp), %ecx                 ## 4-byte Reload
	addl	%ecx, %esi
	addq	%rdx, %rax
	addq	%rdx, %r14
	addl	%ecx, -120(%rsp)                ## 4-byte Folded Spill
	addq	%rdx, %r15
	cmpq	128(%rsp), %rdi                 ## 8-byte Folded Reload
	movq	-24(%rsp), %r11                 ## 8-byte Reload
	je	LBB25_40
LBB25_6:                                ## %"for relu1_0_d_def__.s17.w.wi.us.us"
                                        ## =>This Loop Header: Depth=1
                                        ##     Child Loop BB25_9 Depth 2
                                        ##     Child Loop BB25_12 Depth 2
                                        ##     Child Loop BB25_21 Depth 2
                                        ##     Child Loop BB25_16 Depth 2
	testq	%rbp, %rbp
	movl	$0, %ecx
	movq	%rbp, -72(%rsp)                 ## 8-byte Spill
	cmovgl	%ebp, %ecx
	imull	%r8d, %ecx
	movslq	%ecx, %rdx
	movq	-48(%rsp), %rcx                 ## 8-byte Reload
	movq	%rdi, -112(%rsp)                ## 8-byte Spill
	leaq	(%rdi,%rcx), %r13
	testq	%r13, %r13
	movl	$0, %ecx
	cmovlel	%ecx, %r13d
	movl	%esi, -64(%rsp)                 ## 4-byte Spill
	movslq	%esi, %rbp
	cmpl	$32, %r11d
	jae	LBB25_8
## %bb.7:                               ##   in Loop: Header=BB25_6 Depth=1
	xorl	%esi, %esi
	jmp	LBB25_11
	.p2align	4, 0x90
LBB25_8:                                ## %vector.body70.preheader
                                        ##   in Loop: Header=BB25_6 Depth=1
	movq	(%rsp), %rcx                    ## 8-byte Reload
	addq	%rdx, %rcx
	movq	8(%rsp), %rsi                   ## 8-byte Reload
	addq	%rdx, %rsi
	movq	56(%rsp), %rdi                  ## 8-byte Reload
	leaq	(%rdi,%rcx,4), %r11
	leaq	(%rdi,%rsi,4), %rdi
	movq	24(%rsp), %rcx                  ## 8-byte Reload
	movq	%rbp, 112(%rsp)                 ## 8-byte Spill
	leaq	(%rcx,%rbp,4), %rsi
	xorl	%r8d, %r8d
	.p2align	4, 0x90
LBB25_9:                                ## %vector.body70
                                        ##   Parent Loop BB25_6 Depth=1
                                        ## =>  This Inner Loop Header: Depth=2
	vmovups	-96(%rdi,%r8,4), %ymm2
	vmovups	-64(%rdi,%r8,4), %ymm3
	vmovups	-32(%rdi,%r8,4), %ymm4
	vmovups	(%rdi,%r8,4), %ymm5
	vmulps	-96(%r11,%r8,4), %ymm2, %ymm2
	vmulps	-64(%r11,%r8,4), %ymm3, %ymm3
	vmulps	-32(%r11,%r8,4), %ymm4, %ymm4
	vmulps	(%r11,%r8,4), %ymm5, %ymm5
	vmulps	-96(%rbx,%r8,4), %ymm2, %ymm2
	vmulps	-64(%rbx,%r8,4), %ymm3, %ymm3
	vmulps	-32(%rbx,%r8,4), %ymm4, %ymm4
	vmulps	(%rbx,%r8,4), %ymm5, %ymm5
	vfmadd213ps	-96(%rsi,%r8,4), %ymm1, %ymm2 ## ymm2 = (ymm1 * ymm2) + mem
	vfmadd213ps	-64(%rsi,%r8,4), %ymm1, %ymm3 ## ymm3 = (ymm1 * ymm3) + mem
	vfmadd213ps	-32(%rsi,%r8,4), %ymm1, %ymm4 ## ymm4 = (ymm1 * ymm4) + mem
	vfmadd213ps	(%rsi,%r8,4), %ymm1, %ymm5 ## ymm5 = (ymm1 * ymm5) + mem
	vmovups	%ymm2, -96(%rsi,%r8,4)
	vmovups	%ymm3, -64(%rsi,%r8,4)
	vmovups	%ymm4, -32(%rsi,%r8,4)
	vmovups	%ymm5, (%rsi,%r8,4)
	addq	$32, %r8
	cmpq	%r8, %r12
	jne	LBB25_9
## %bb.10:                              ## %middle.block68
                                        ##   in Loop: Header=BB25_6 Depth=1
	movq	%r12, %rsi
	cmpq	%r9, %r12
	movl	-124(%rsp), %r8d                ## 4-byte Reload
	movq	112(%rsp), %rbp                 ## 8-byte Reload
	je	LBB25_13
LBB25_11:                               ## %"for relu1_0_d_def__.s17.n.ni.us.us.preheader"
                                        ##   in Loop: Header=BB25_6 Depth=1
	movq	104(%rsp), %rcx                 ## 8-byte Reload
	leaq	(%rcx,%rdx,4), %rdi
	movq	96(%rsp), %rcx                  ## 8-byte Reload
	leaq	(%rcx,%rdx,4), %rdx
	movq	-88(%rsp), %rcx                 ## 8-byte Reload
	leaq	(%rcx,%rbp,4), %rcx
	.p2align	4, 0x90
LBB25_12:                               ## %"for relu1_0_d_def__.s17.n.ni.us.us"
                                        ##   Parent Loop BB25_6 Depth=1
                                        ## =>  This Inner Loop Header: Depth=2
	vmovss	(%rdx,%rsi,4), %xmm2            ## xmm2 = mem[0],zero,zero,zero
	vmulss	(%rdi,%rsi,4), %xmm2, %xmm2
	vmulss	(%rax,%rsi,4), %xmm2, %xmm2
	vfmadd213ss	(%rcx,%rsi,4), %xmm0, %xmm2 ## xmm2 = (xmm0 * xmm2) + mem
	vmovss	%xmm2, (%rcx,%rsi,4)
	incq	%rsi
	cmpq	%rsi, %r9
	jne	LBB25_12
LBB25_13:                               ## %"end for relu1_0_d_def__.s17.n.ni.loopexit.us.us"
                                        ##   in Loop: Header=BB25_6 Depth=1
	cmpl	$0, -128(%rsp)                  ## 4-byte Folded Reload
	movq	-96(%rsp), %r11                 ## 8-byte Reload
	jle	LBB25_17
## %bb.14:                              ## %"for relu1_0_d_def__.s17.n.ni.rebased.preheader.us.us"
                                        ##   in Loop: Header=BB25_6 Depth=1
	movslq	-120(%rsp), %rdx                ## 4-byte Folded Reload
	imull	%r8d, %r13d
	movslq	%r13d, %rcx
	movq	40(%rsp), %rsi                  ## 8-byte Reload
	addq	%rcx, %rsi
	addq	32(%rsp), %rcx                  ## 8-byte Folded Reload
	movq	-40(%rsp), %rdi                 ## 8-byte Reload
	vmovss	-4(%rdi,%rsi,4), %xmm2          ## xmm2 = mem[0],zero,zero,zero
	vmulss	-4(%rdi,%rcx,4), %xmm2, %xmm2
	cmpl	$16, -128(%rsp)                 ## 4-byte Folded Reload
	jae	LBB25_18
## %bb.15:                              ##   in Loop: Header=BB25_6 Depth=1
	xorl	%esi, %esi
	jmp	LBB25_25
	.p2align	4, 0x90
LBB25_18:                               ## %vector.ph53
                                        ##   in Loop: Header=BB25_6 Depth=1
	vbroadcastss	%xmm2, %ymm3
	cmpq	$0, 72(%rsp)                    ## 8-byte Folded Reload
	je	LBB25_19
## %bb.20:                              ## %vector.body51.preheader
                                        ##   in Loop: Header=BB25_6 Depth=1
	movq	16(%rsp), %rcx                  ## 8-byte Reload
	addq	%rdx, %rcx
	movq	24(%rsp), %rsi                  ## 8-byte Reload
	leaq	(%rsi,%rcx,4), %rsi
	movq	48(%rsp), %rdi                  ## 8-byte Reload
	xorl	%ebp, %ebp
	.p2align	4, 0x90
LBB25_21:                               ## %vector.body51
                                        ##   Parent Loop BB25_6 Depth=1
                                        ## =>  This Inner Loop Header: Depth=2
	vmulps	-96(%r14,%rbp,4), %ymm3, %ymm4
	vmulps	-64(%r14,%rbp,4), %ymm3, %ymm5
	vfmadd213ps	-96(%rsi,%rbp,4), %ymm1, %ymm4 ## ymm4 = (ymm1 * ymm4) + mem
	vfmadd213ps	-64(%rsi,%rbp,4), %ymm1, %ymm5 ## ymm5 = (ymm1 * ymm5) + mem
	vmovups	%ymm4, -96(%rsi,%rbp,4)
	vmovups	%ymm5, -64(%rsi,%rbp,4)
	vmulps	-32(%r14,%rbp,4), %ymm3, %ymm4
	vmulps	(%r14,%rbp,4), %ymm3, %ymm5
	vfmadd213ps	-32(%rsi,%rbp,4), %ymm1, %ymm4 ## ymm4 = (ymm1 * ymm4) + mem
	vfmadd213ps	(%rsi,%rbp,4), %ymm1, %ymm5 ## ymm5 = (ymm1 * ymm5) + mem
	vmovups	%ymm4, -32(%rsi,%rbp,4)
	vmovups	%ymm5, (%rsi,%rbp,4)
	addq	$32, %rbp
	addq	$2, %rdi
	jne	LBB25_21
## %bb.22:                              ## %middle.block49.unr-lcssa
                                        ##   in Loop: Header=BB25_6 Depth=1
	testb	$1, 64(%rsp)                    ## 1-byte Folded Reload
	je	LBB25_24
LBB25_23:                               ## %vector.body51.epil
                                        ##   in Loop: Header=BB25_6 Depth=1
	movq	-8(%rsp), %rcx                  ## 8-byte Reload
	movq	-112(%rsp), %rdi                ## 8-byte Reload
	addl	%edi, %ecx
	imull	-100(%rsp), %ecx                ## 4-byte Folded Reload
	addl	%r11d, %ecx
	movq	-56(%rsp), %rsi                 ## 8-byte Reload
	addq	%rdi, %rsi
	shll	$3, %ecx
	imulq	-32(%rsp), %rsi                 ## 8-byte Folded Reload
	movslq	%ecx, %rcx
	addq	16(%rsp), %rbp                  ## 8-byte Folded Reload
	addq	%rbp, %rcx
	addq	%rsi, %rbp
	movq	-80(%rsp), %rsi                 ## 8-byte Reload
	vmulps	(%rsi,%rbp,4), %ymm3, %ymm4
	vmulps	32(%rsi,%rbp,4), %ymm3, %ymm3
	movq	-88(%rsp), %rsi                 ## 8-byte Reload
	vfmadd213ps	(%rsi,%rcx,4), %ymm1, %ymm4 ## ymm4 = (ymm1 * ymm4) + mem
	vfmadd213ps	32(%rsi,%rcx,4), %ymm1, %ymm3 ## ymm3 = (ymm1 * ymm3) + mem
	vmovups	%ymm4, (%rsi,%rcx,4)
	vmovups	%ymm3, 32(%rsi,%rcx,4)
LBB25_24:                               ## %middle.block49
                                        ##   in Loop: Header=BB25_6 Depth=1
	movq	80(%rsp), %rcx                  ## 8-byte Reload
	movq	%rcx, %rsi
	cmpq	%r10, %rcx
	je	LBB25_17
LBB25_25:                               ## %"for relu1_0_d_def__.s17.n.ni.rebased.us.us.preheader"
                                        ##   in Loop: Header=BB25_6 Depth=1
	movq	88(%rsp), %rcx                  ## 8-byte Reload
	leaq	(%rcx,%rdx,4), %rcx
	.p2align	4, 0x90
LBB25_16:                               ## %"for relu1_0_d_def__.s17.n.ni.rebased.us.us"
                                        ##   Parent Loop BB25_6 Depth=1
                                        ## =>  This Inner Loop Header: Depth=2
	vmulss	(%r15,%rsi,4), %xmm2, %xmm3
	vfmadd213ss	(%rcx,%rsi,4), %xmm0, %xmm3 ## xmm3 = (xmm0 * xmm3) + mem
	vmovss	%xmm3, (%rcx,%rsi,4)
	incq	%rsi
	cmpq	%rsi, %r10
	jne	LBB25_16
	jmp	LBB25_17
LBB25_19:                               ##   in Loop: Header=BB25_6 Depth=1
	xorl	%ebp, %ebp
	testb	$1, 64(%rsp)                    ## 1-byte Folded Reload
	jne	LBB25_23
	jmp	LBB25_24
LBB25_2:                                ## %"for relu1_0_d_def__.s17.w.wi.preheader19"
	decl	%r13d
	vmovd	%edx, %xmm0
	vpbroadcastd	%xmm0, %ymm0
	vpor	LCPI25_0(%rip), %ymm0, %ymm0
	vmovd	%r13d, %xmm1
	vpbroadcastd	%xmm1, %ymm1
	vpminsd	%ymm0, %ymm1, %ymm0
	movslq	-112(%rsp), %rdi                ## 4-byte Folded Reload
	movl	-120(%rsp), %r9d                ## 4-byte Reload
	imulq	%r11, %rbp
	addq	%rdx, %rbp
	movq	-80(%rsp), %rax                 ## 8-byte Reload
	leaq	(%rax,%rbp,4), %rdx
	shlq	$2, %r11
	shll	$3, %r8d
	shll	$3, %ebx
	subl	%ebx, %r8d
	orl	$1, %r8d
	movq	-96(%rsp), %rax                 ## 8-byte Reload
	imull	%r8d, %eax
	addl	%eax, %ecx
	vbroadcastss	LCPI25_1(%rip), %ymm1   ## ymm1 = [9.99999971E-10,9.99999971E-10,9.99999971E-10,9.99999971E-10,9.99999971E-10,9.99999971E-10,9.99999971E-10,9.99999971E-10]
	movl	-124(%rsp), %r8d                ## 4-byte Reload
	movq	-40(%rsp), %rbx                 ## 8-byte Reload
	.p2align	4, 0x90
LBB25_3:                                ## %"for relu1_0_d_def__.s17.w.wi"
                                        ## =>This Inner Loop Header: Depth=1
	testq	%rdi, %rdi
	movl	$0, %eax
	cmovgl	%edi, %eax
	imull	%r8d, %eax
	leal	(%rax,%r10), %esi
	vmovd	%esi, %xmm2
	vpbroadcastd	%xmm2, %ymm2
	vpaddd	%ymm0, %ymm2, %ymm3
	vextracti128	$1, %ymm3, %xmm2
	vmovd	%xmm2, %esi
	movslq	%esi, %rsi
	vpextrd	$1, %xmm2, %ebp
	movslq	%ebp, %rbp
	vmovss	(%rbx,%rsi,4), %xmm4            ## xmm4 = mem[0],zero,zero,zero
	vpextrd	$2, %xmm2, %esi
	vinsertps	$16, (%rbx,%rbp,4), %xmm4, %xmm4 ## xmm4 = xmm4[0],mem[0],xmm4[2,3]
	vmovd	%xmm3, %ebp
	movslq	%esi, %rsi
	vinsertps	$32, (%rbx,%rsi,4), %xmm4, %xmm4 ## xmm4 = xmm4[0,1],mem[0],xmm4[3]
	movslq	%ebp, %rsi
	vpextrd	$3, %xmm2, %ebp
	movslq	%ebp, %rbp
	vinsertps	$48, (%rbx,%rbp,4), %xmm4, %xmm2 ## xmm2 = xmm4[0,1,2],mem[0]
	vpextrd	$1, %xmm3, %ebp
	vmovss	(%rbx,%rsi,4), %xmm4            ## xmm4 = mem[0],zero,zero,zero
	movslq	%ebp, %rsi
	vinsertps	$16, (%rbx,%rsi,4), %xmm4, %xmm4 ## xmm4 = xmm4[0],mem[0],xmm4[2,3]
	vpextrd	$2, %xmm3, %esi
	movslq	%esi, %rsi
	vinsertps	$32, (%rbx,%rsi,4), %xmm4, %xmm4 ## xmm4 = xmm4[0,1],mem[0],xmm4[3]
	vpextrd	$3, %xmm3, %esi
	movslq	%esi, %rsi
	vinsertps	$48, (%rbx,%rsi,4), %xmm4, %xmm3 ## xmm3 = xmm4[0,1,2],mem[0]
	addl	%r15d, %eax
	vmovd	%eax, %xmm4
	vpbroadcastd	%xmm4, %ymm4
	vpaddd	%ymm0, %ymm4, %ymm4
	vextracti128	$1, %ymm4, %xmm5
	vmovd	%xmm5, %eax
	cltq
	vmovss	(%rbx,%rax,4), %xmm6            ## xmm6 = mem[0],zero,zero,zero
	vpextrd	$1, %xmm5, %eax
	cltq
	vinsertps	$16, (%rbx,%rax,4), %xmm6, %xmm6 ## xmm6 = xmm6[0],mem[0],xmm6[2,3]
	vpextrd	$2, %xmm5, %eax
	cltq
	vinsertps	$32, (%rbx,%rax,4), %xmm6, %xmm6 ## xmm6 = xmm6[0,1],mem[0],xmm6[3]
	vpextrd	$3, %xmm5, %eax
	cltq
	vinsertps	$48, (%rbx,%rax,4), %xmm6, %xmm5 ## xmm5 = xmm6[0,1,2],mem[0]
	vmovd	%xmm4, %eax
	cltq
	vmovss	(%rbx,%rax,4), %xmm6            ## xmm6 = mem[0],zero,zero,zero
	vpextrd	$1, %xmm4, %eax
	cltq
	vinsertps	$16, (%rbx,%rax,4), %xmm6, %xmm6 ## xmm6 = xmm6[0],mem[0],xmm6[2,3]
	vpextrd	$2, %xmm4, %eax
	cltq
	vinsertps	$32, (%rbx,%rax,4), %xmm6, %xmm6 ## xmm6 = xmm6[0,1],mem[0],xmm6[3]
	vinsertf128	$1, %xmm2, %ymm3, %ymm2
	vpextrd	$3, %xmm4, %eax
	movslq	%ecx, %rcx
	cltq
	vinsertps	$48, (%rbx,%rax,4), %xmm6, %xmm3 ## xmm3 = xmm6[0,1,2],mem[0]
	movq	%rcx, %rax
	shlq	$5, %rax
	vinsertf128	$1, %xmm5, %ymm3, %ymm3
	vmulps	%ymm3, %ymm2, %ymm2
	vmulps	(%rdx), %ymm2, %ymm2
	movq	-88(%rsp), %rsi                 ## 8-byte Reload
	vfmadd213ps	(%rsi,%rax), %ymm1, %ymm2 ## ymm2 = (ymm1 * ymm2) + mem
	vmovaps	%ymm2, (%rsi,%rax)
	addq	%r11, %rdx
	incq	%rdi
	addl	-100(%rsp), %ecx                ## 4-byte Folded Reload
	decq	%r9
	jne	LBB25_3
LBB25_40:                               ## %destructor_block
	xorl	%eax, %eax
	addq	$136, %rsp
	popq	%rbx
	popq	%r12
	popq	%r13
	popq	%r14
	popq	%r15
	popq	%rbp
	vzeroupper
	retq
LBB25_26:                               ## %"for relu1_0_d_def__.s17.w.wi.preheader.split.us.split"
	testl	%eax, %eax
	movl	-124(%rsp), %eax                ## 4-byte Reload
	jle	LBB25_40
## %bb.27:                              ## %"for relu1_0_d_def__.s17.w.wi.us.us4.preheader"
	movl	%ebx, %r15d
	movslq	%ecx, %r9
	movl	-128(%rsp), %ecx                ## 4-byte Reload
	movslq	-112(%rsp), %rdx                ## 4-byte Folded Reload
	movq	%rdx, -112(%rsp)                ## 8-byte Spill
	movl	-120(%rsp), %r14d               ## 4-byte Reload
	movl	%ecx, %r12d
	andl	$-16, %r12d
	leaq	-16(%r12), %rdx
	movq	%rdx, -64(%rsp)                 ## 8-byte Spill
	movq	%rdx, %rbx
	shrq	$4, %rbx
	incq	%rbx
	movq	-32(%rsp), %rsi                 ## 8-byte Reload
	movq	%rsi, %rdx
	imulq	-56(%rsp), %rdx                 ## 8-byte Folded Reload
	addq	%r9, %rdx
	movq	-80(%rsp), %r10                 ## 8-byte Reload
	leaq	(%r10,%rdx,4), %rdi
	addq	$96, %rdi
	leaq	(,%rsi,4), %r11
	movq	-88(%rsp), %rsi                 ## 8-byte Reload
	leaq	96(%rsi), %rbp
	movq	%rbp, -48(%rsp)                 ## 8-byte Spill
	shll	$3, %r8d
	shll	$3, %r15d
	subl	%r15d, %r8d
	orl	$1, %r8d
	movq	-96(%rsp), %rbp                 ## 8-byte Reload
	imull	%ebp, %r8d
	shll	$3, %r8d
	movl	%ebp, %r13d
	shll	$5, %r13d
	movq	%rbx, %rbp
	movq	%rbx, -72(%rsp)                 ## 8-byte Spill
	andq	$-2, %rbx
	negq	%rbx
	movq	%rbx, -24(%rsp)                 ## 8-byte Spill
	leaq	(%r10,%rdx,4), %r15
	movq	%r9, %rbp
	leaq	(%rsi,%r9,4), %rdx
	movq	%rdx, -120(%rsp)                ## 8-byte Spill
	vmovss	LCPI25_1(%rip), %xmm0           ## xmm0 = mem[0],zero,zero,zero
	vbroadcastss	LCPI25_1(%rip), %ymm1   ## ymm1 = [9.99999971E-10,9.99999971E-10,9.99999971E-10,9.99999971E-10,9.99999971E-10,9.99999971E-10,9.99999971E-10,9.99999971E-10]
	xorl	%r9d, %r9d
	jmp	LBB25_28
LBB25_39:                               ## %after_bb.loopexit.us.us16
                                        ##   in Loop: Header=BB25_28 Depth=1
	incq	%r9
	addq	%r11, %rdi
	addl	%r13d, %r8d
	addq	%r11, %r15
	cmpq	%r14, %r9
	movl	-124(%rsp), %eax                ## 4-byte Reload
	je	LBB25_40
LBB25_28:                               ## %"for relu1_0_d_def__.s17.w.wi.us.us4"
                                        ## =>This Loop Header: Depth=1
                                        ##     Child Loop BB25_33 Depth 2
                                        ##     Child Loop BB25_38 Depth 2
	movslq	%r8d, %rdx
	movq	-112(%rsp), %rsi                ## 8-byte Reload
	addq	%r9, %rsi
	testq	%rsi, %rsi
	movl	$0, %ebx
	cmovlel	%ebx, %esi
	imull	%eax, %esi
	movslq	%esi, %rsi
	movq	40(%rsp), %rax                  ## 8-byte Reload
	leaq	(%rsi,%rax), %rbx
	addq	32(%rsp), %rsi                  ## 8-byte Folded Reload
	movq	-40(%rsp), %rax                 ## 8-byte Reload
	vmovss	-4(%rax,%rbx,4), %xmm2          ## xmm2 = mem[0],zero,zero,zero
	vmulss	-4(%rax,%rsi,4), %xmm2, %xmm2
	cmpl	$16, -128(%rsp)                 ## 4-byte Folded Reload
	jae	LBB25_30
## %bb.29:                              ##   in Loop: Header=BB25_28 Depth=1
	xorl	%esi, %esi
	jmp	LBB25_37
LBB25_30:                               ## %vector.ph
                                        ##   in Loop: Header=BB25_28 Depth=1
	vbroadcastss	%xmm2, %ymm3
	cmpq	$0, -64(%rsp)                   ## 8-byte Folded Reload
	je	LBB25_31
## %bb.32:                              ## %vector.body.preheader
                                        ##   in Loop: Header=BB25_28 Depth=1
	leaq	(%rdx,%rbp), %rsi
	movq	-48(%rsp), %rax                 ## 8-byte Reload
	leaq	(%rax,%rsi,4), %rsi
	movq	-24(%rsp), %r10                 ## 8-byte Reload
	xorl	%ebx, %ebx
LBB25_33:                               ## %vector.body
                                        ##   Parent Loop BB25_28 Depth=1
                                        ## =>  This Inner Loop Header: Depth=2
	vmulps	-96(%rdi,%rbx,4), %ymm3, %ymm4
	vmulps	-64(%rdi,%rbx,4), %ymm3, %ymm5
	vfmadd213ps	-96(%rsi,%rbx,4), %ymm1, %ymm4 ## ymm4 = (ymm1 * ymm4) + mem
	vfmadd213ps	-64(%rsi,%rbx,4), %ymm1, %ymm5 ## ymm5 = (ymm1 * ymm5) + mem
	vmovups	%ymm4, -96(%rsi,%rbx,4)
	vmovups	%ymm5, -64(%rsi,%rbx,4)
	vmulps	-32(%rdi,%rbx,4), %ymm3, %ymm4
	vmulps	(%rdi,%rbx,4), %ymm3, %ymm5
	vfmadd213ps	-32(%rsi,%rbx,4), %ymm1, %ymm4 ## ymm4 = (ymm1 * ymm4) + mem
	vfmadd213ps	(%rsi,%rbx,4), %ymm1, %ymm5 ## ymm5 = (ymm1 * ymm5) + mem
	vmovups	%ymm4, -32(%rsi,%rbx,4)
	vmovups	%ymm5, (%rsi,%rbx,4)
	addq	$32, %rbx
	addq	$2, %r10
	jne	LBB25_33
	jmp	LBB25_34
LBB25_31:                               ##   in Loop: Header=BB25_28 Depth=1
	xorl	%ebx, %ebx
LBB25_34:                               ## %middle.block.unr-lcssa
                                        ##   in Loop: Header=BB25_28 Depth=1
	testb	$1, -72(%rsp)                   ## 1-byte Folded Reload
	je	LBB25_36
## %bb.35:                              ## %vector.body.epil
                                        ##   in Loop: Header=BB25_28 Depth=1
	movq	-8(%rsp), %rax                  ## 8-byte Reload
	leal	(%rax,%r9), %esi
	imull	-100(%rsp), %esi                ## 4-byte Folded Reload
	addl	-96(%rsp), %esi                 ## 4-byte Folded Reload
	shll	$3, %esi
	movq	-56(%rsp), %rax                 ## 8-byte Reload
	addq	%r9, %rax
	imulq	-32(%rsp), %rax                 ## 8-byte Folded Reload
	movslq	%esi, %rsi
	addq	%rbp, %rbx
	addq	%rbx, %rsi
	addq	%rax, %rbx
	movq	-80(%rsp), %rax                 ## 8-byte Reload
	vmulps	(%rax,%rbx,4), %ymm3, %ymm4
	vmulps	32(%rax,%rbx,4), %ymm3, %ymm3
	movq	-88(%rsp), %rax                 ## 8-byte Reload
	vfmadd213ps	(%rax,%rsi,4), %ymm1, %ymm4 ## ymm4 = (ymm1 * ymm4) + mem
	vfmadd213ps	32(%rax,%rsi,4), %ymm1, %ymm3 ## ymm3 = (ymm1 * ymm3) + mem
	vmovups	%ymm4, (%rax,%rsi,4)
	vmovups	%ymm3, 32(%rax,%rsi,4)
LBB25_36:                               ## %middle.block
                                        ##   in Loop: Header=BB25_28 Depth=1
	movq	%r12, %rsi
	cmpq	%rcx, %r12
	je	LBB25_39
LBB25_37:                               ## %"for relu1_0_d_def__.s17.n.ni.rebased.us.us8.preheader"
                                        ##   in Loop: Header=BB25_28 Depth=1
	movq	-120(%rsp), %rax                ## 8-byte Reload
	leaq	(%rax,%rdx,4), %rdx
LBB25_38:                               ## %"for relu1_0_d_def__.s17.n.ni.rebased.us.us8"
                                        ##   Parent Loop BB25_28 Depth=1
                                        ## =>  This Inner Loop Header: Depth=2
	vmulss	(%r15,%rsi,4), %xmm2, %xmm3
	vfmadd213ss	(%rdx,%rsi,4), %xmm0, %xmm3 ## xmm3 = (xmm0 * xmm3) + mem
	vmovss	%xmm3, (%rdx,%rsi,4)
	incq	%rsi
	cmpq	%rsi, %rcx
	jne	LBB25_38
	jmp	LBB25_39
                                        ## -- End function
	.section	__TEXT,__const
	.p2align	5                               ## -- Begin function train_cost_model.par_for.relu1_0_d_def__.s18.n.n.n
LCPI26_0:
	.long	0                               ## 0x0
	.long	1                               ## 0x1
	.long	2                               ## 0x2
	.long	3                               ## 0x3
	.long	4                               ## 0x4
	.long	5                               ## 0x5
	.long	6                               ## 0x6
	.long	7                               ## 0x7
	.section	__TEXT,__literal4,4byte_literals
	.p2align	2
LCPI26_1:
	.long	0x3089705f                      ## float 9.99999971E-10
	.section	__TEXT,__text,regular,pure_instructions
	.p2align	4, 0x90
_train_cost_model.par_for.relu1_0_d_def__.s18.n.n.n: ## @train_cost_model.par_for.relu1_0_d_def__.s18.n.n.n
## %bb.0:                               ## %entry
	pushq	%rbp
	pushq	%r15
	pushq	%r14
	pushq	%r13
	pushq	%r12
	pushq	%rbx
	subq	$112, %rsp
	movq	%rdx, %rdi
	movl	12(%rdx), %r10d
	movl	24(%rdx), %r13d
	movl	%esi, %r9d
	sarl	$31, %r9d
	xorl	%ecx, %ecx
	testl	%r13d, %r13d
	sete	%cl
	movl	%ecx, %ebx
	negl	%ebx
	movl	%r13d, %ebp
	sarl	$31, %ebp
	subl	%r9d, %esi
	orl	%r13d, %ebx
	movl	%esi, %eax
	cltd
	idivl	%ebx
	movl	%edx, %r14d
	movl	%ebp, %r15d
	leal	(%rcx,%r13), %ebx
	movl	%esi, %eax
	cltd
	idivl	%ebx
	notl	%r15d
	decl	%ecx
	movl	%r15d, %r8d
	subl	%ebp, %r8d
	andl	%r9d, %r8d
	addl	%eax, %r8d
	andl	%ecx, %r8d
	leal	(%r8,%r8), %eax
	movl	%eax, -96(%rsp)                 ## 4-byte Spill
	subl	%eax, %r10d
	cmpl	$2, %r10d
	movl	$2, %eax
	cmovll	%r10d, %eax
	xorl	%ebx, %ebx
	testl	%r10d, %r10d
	cmovlel	%ebx, %eax
	movl	%eax, -104(%rsp)                ## 4-byte Spill
	jle	LBB26_40
## %bb.1:                               ## %"for relu1_0_d_def__.s18.w.wi.preheader"
	movl	(%rdi), %edx
	movl	16(%rdi), %r12d
	movl	32(%rdi), %r10d
	movl	36(%rdi), %eax
	movl	40(%rdi), %r11d
	xorl	%r13d, %ebp
	addl	%r15d, %ebp
	andl	%r9d, %ebp
	addl	%ebp, %r14d
	andl	%ecx, %r14d
	movq	%r14, -120(%rsp)                ## 8-byte Spill
	leal	(,%r14,8), %esi
	movl	%eax, -128(%rsp)                ## 4-byte Spill
	movl	%eax, %r9d
	subl	%r10d, %r9d
	movl	%r11d, %r15d
	subl	%r10d, %r11d
	movslq	8(%rdi), %rax
	movslq	%r8d, %r13
	subq	%rax, %r13
	addq	%r13, %r13
	movq	%r12, -88(%rsp)                 ## 8-byte Spill
	movl	%r12d, %eax
	shll	$5, %eax
	movl	%eax, -112(%rsp)                ## 4-byte Spill
	movl	%edx, %eax
	subl	%esi, %eax
	cmpl	$8, %eax
	movl	$8, %ecx
	cmovll	%eax, %ecx
	testl	%eax, %eax
	cmovgl	%ecx, %ebx
	movl	%ebx, -108(%rsp)                ## 4-byte Spill
	movl	20(%rdi), %ecx
	movl	%ecx, -124(%rsp)                ## 4-byte Spill
	movl	%esi, %ebx
	subl	%edx, %ebx
	movl	%ebx, %ecx
	sarl	$31, %ecx
	andl	%ebx, %ecx
	cmpl	$-8, %ecx
	movl	$-8, %r14d
	cmovgl	%ecx, %r14d
	movl	28(%rdi), %ebx
	movq	48(%rdi), %rbp
	movq	64(%rdi), %rcx
	movq	%rcx, -80(%rsp)                 ## 8-byte Spill
	movq	80(%rdi), %rcx
	movq	%rcx, -40(%rsp)                 ## 8-byte Spill
	movslq	4(%rdi), %rcx
	movq	%rcx, -72(%rsp)                 ## 8-byte Spill
	leal	8(%rsi), %ecx
	movslq	%esi, %r12
	cmpl	%edx, %ecx
	jle	LBB26_2
## %bb.4:                               ## %"for relu1_0_d_def__.s18.w.wi.preheader.split.us"
	movq	%rbp, -56(%rsp)                 ## 8-byte Spill
	movq	%r13, -48(%rsp)                 ## 8-byte Spill
	movq	-88(%rsp), %rbp                 ## 8-byte Reload
	leal	(,%rbp,8), %ecx
	subl	%ebp, %ecx
	movl	%ecx, -60(%rsp)                 ## 4-byte Spill
	movl	%r8d, %ecx
	subl	%ebx, %ecx
	addl	%ecx, %ecx
	movq	%rcx, -32(%rsp)                 ## 8-byte Spill
	subl	%r10d, %edx
	movl	-128(%rsp), %edi                ## 4-byte Reload
	addl	%edx, %edi
	addl	%r15d, %edx
	movl	-108(%rsp), %ecx                ## 4-byte Reload
	addl	%ecx, %esi
	addl	%ecx, %r14d
	movslq	%edi, %rcx
	movq	%rcx, 8(%rsp)                   ## 8-byte Spill
	movslq	%edx, %rcx
	movq	%rcx, (%rsp)                    ## 8-byte Spill
	testl	%eax, %eax
	movl	%r14d, -128(%rsp)               ## 4-byte Spill
	movl	%ebx, %ecx
	jle	LBB26_26
## %bb.5:                               ## %"for relu1_0_d_def__.s18.w.wi.us.us.preheader"
	addl	%r12d, %r11d
	addl	%r12d, %r9d
	movslq	%r9d, %rax
	movq	%rax, -24(%rsp)                 ## 8-byte Spill
	movslq	%r11d, %r9
	movl	-108(%rsp), %r10d               ## 4-byte Reload
	movslq	%esi, %rsi
	movl	%r14d, %r14d
	movslq	-96(%rsp), %rax                 ## 4-byte Folded Reload
	movq	%rax, 16(%rsp)                  ## 8-byte Spill
	movl	-104(%rsp), %eax                ## 4-byte Reload
	movq	%rax, 104(%rsp)                 ## 8-byte Spill
	movl	%r14d, %eax
	andl	$-16, %eax
	movq	%rax, 56(%rsp)                  ## 8-byte Spill
	addq	$-16, %rax
	movq	%rax, 48(%rsp)                  ## 8-byte Spill
	movq	%rax, %rbx
	shrq	$4, %rbx
	incq	%rbx
	movl	%r10d, %r11d
	andl	$2147483616, %r11d              ## imm = 0x7FFFFFE0
	shll	$6, %r8d
	shll	$6, %ecx
	subl	%ecx, %r8d
	movq	-72(%rsp), %rdx                 ## 8-byte Reload
	movq	%rdx, %rax
	imulq	-48(%rsp), %rax                 ## 8-byte Folded Reload
	addq	%rax, %r12
	orl	$7, %r8d
	imull	%r8d, %ebp
	movq	-56(%rsp), %rcx                 ## 8-byte Reload
	movq	%rdx, %r8
	leaq	(%rcx,%r12,4), %rdi
	addq	$96, %rdi
	movq	%rbp, -88(%rsp)                 ## 8-byte Spill
	movq	-120(%rsp), %rdx                ## 8-byte Reload
	leal	(%rbp,%rdx,8), %r13d
	leaq	(%rcx,%r12,4), %rbp
	addq	%rsi, %rax
	movq	%rbx, %rdx
	movq	%rbx, 40(%rsp)                  ## 8-byte Spill
	andq	$-2, %rbx
	negq	%rbx
	movq	%rbx, 24(%rsp)                  ## 8-byte Spill
	leaq	(%rcx,%rax,4), %r12
	addq	$96, %r12
	leaq	(%rcx,%rax,4), %r15
	vmovss	LCPI26_1(%rip), %xmm0           ## xmm0 = mem[0],zero,zero,zero
	vbroadcastss	LCPI26_1(%rip), %ymm1   ## ymm1 = [9.99999971E-10,9.99999971E-10,9.99999971E-10,9.99999971E-10,9.99999971E-10,9.99999971E-10,9.99999971E-10,9.99999971E-10]
	movq	-40(%rsp), %rcx                 ## 8-byte Reload
	leaq	96(%rcx), %rax
	movq	%rax, 32(%rsp)                  ## 8-byte Spill
	leaq	(,%r8,4), %rax
	movq	%rax, 96(%rsp)                  ## 8-byte Spill
	movq	-80(%rsp), %rax                 ## 8-byte Reload
	leaq	96(%rax), %rdx
	movq	%rdx, -8(%rsp)                  ## 8-byte Spill
	movq	%r9, 64(%rsp)                   ## 8-byte Spill
	leaq	(%rcx,%r9,4), %rdx
	movq	%rdx, 88(%rsp)                  ## 8-byte Spill
	movq	-24(%rsp), %rdx                 ## 8-byte Reload
	leaq	(%rcx,%rdx,4), %rcx
	movq	%rcx, 80(%rsp)                  ## 8-byte Spill
	movq	%rsi, -16(%rsp)                 ## 8-byte Spill
	leaq	(%rax,%rsi,4), %rax
	movq	%rax, 72(%rsp)                  ## 8-byte Spill
	movq	16(%rsp), %rdx                  ## 8-byte Reload
	xorl	%ebx, %ebx
	movl	-124(%rsp), %esi                ## 4-byte Reload
	movl	-108(%rsp), %ecx                ## 4-byte Reload
	jmp	LBB26_6
	.p2align	4, 0x90
LBB26_17:                               ## %after_bb.us.us
                                        ##   in Loop: Header=BB26_6 Depth=1
	movl	%ebx, %ecx
	movq	-120(%rsp), %rbx                ## 8-byte Reload
	incq	%rbx
	movq	-104(%rsp), %rdx                ## 8-byte Reload
	incq	%rdx
	movq	96(%rsp), %rax                  ## 8-byte Reload
	addq	%rax, %rdi
	movl	-96(%rsp), %r13d                ## 4-byte Reload
	addl	%ecx, %r13d
	addq	%rax, %rbp
	addq	%rax, %r12
	addl	%ecx, %r8d
	movq	%r8, -88(%rsp)                  ## 8-byte Spill
	addq	%rax, %r15
	cmpq	104(%rsp), %rbx                 ## 8-byte Folded Reload
	movl	-124(%rsp), %esi                ## 4-byte Reload
	movl	-108(%rsp), %ecx                ## 4-byte Reload
	je	LBB26_40
LBB26_6:                                ## %"for relu1_0_d_def__.s18.w.wi.us.us"
                                        ## =>This Loop Header: Depth=1
                                        ##     Child Loop BB26_9 Depth 2
                                        ##     Child Loop BB26_12 Depth 2
                                        ##     Child Loop BB26_21 Depth 2
                                        ##     Child Loop BB26_16 Depth 2
	testq	%rdx, %rdx
	movl	$0, %eax
	movq	%rdx, -104(%rsp)                ## 8-byte Spill
	cmovgl	%edx, %eax
	imull	%esi, %eax
	cltq
	movq	16(%rsp), %rdx                  ## 8-byte Reload
	movq	%rbx, -120(%rsp)                ## 8-byte Spill
	leaq	(%rbx,%rdx), %r9
	testq	%r9, %r9
	movl	$0, %edx
	cmovlel	%edx, %r9d
	movslq	%r13d, %rsi
	cmpl	$32, %ecx
	movl	%r13d, -96(%rsp)                ## 4-byte Spill
	jae	LBB26_8
## %bb.7:                               ##   in Loop: Header=BB26_6 Depth=1
	xorl	%ecx, %ecx
	jmp	LBB26_11
	.p2align	4, 0x90
LBB26_8:                                ## %vector.body71.preheader
                                        ##   in Loop: Header=BB26_6 Depth=1
	movq	64(%rsp), %rcx                  ## 8-byte Reload
	addq	%rax, %rcx
	movq	-24(%rsp), %rdx                 ## 8-byte Reload
	addq	%rax, %rdx
	movq	32(%rsp), %rbx                  ## 8-byte Reload
	leaq	(%rbx,%rcx,4), %r13
	leaq	(%rbx,%rdx,4), %r8
	movq	-8(%rsp), %rcx                  ## 8-byte Reload
	movq	%rsi, %rbx
	leaq	(%rcx,%rsi,4), %rcx
	xorl	%edx, %edx
	.p2align	4, 0x90
LBB26_9:                                ## %vector.body71
                                        ##   Parent Loop BB26_6 Depth=1
                                        ## =>  This Inner Loop Header: Depth=2
	vmovups	-96(%r8,%rdx,4), %ymm2
	vmovups	-64(%r8,%rdx,4), %ymm3
	vmovups	-32(%r8,%rdx,4), %ymm4
	vmovups	(%r8,%rdx,4), %ymm5
	vmulps	-96(%r13,%rdx,4), %ymm2, %ymm2
	vmulps	-64(%r13,%rdx,4), %ymm3, %ymm3
	vmulps	-32(%r13,%rdx,4), %ymm4, %ymm4
	vmulps	(%r13,%rdx,4), %ymm5, %ymm5
	vmulps	-96(%rdi,%rdx,4), %ymm2, %ymm2
	vmulps	-64(%rdi,%rdx,4), %ymm3, %ymm3
	vmulps	-32(%rdi,%rdx,4), %ymm4, %ymm4
	vmulps	(%rdi,%rdx,4), %ymm5, %ymm5
	vfmadd213ps	-96(%rcx,%rdx,4), %ymm1, %ymm2 ## ymm2 = (ymm1 * ymm2) + mem
	vfmadd213ps	-64(%rcx,%rdx,4), %ymm1, %ymm3 ## ymm3 = (ymm1 * ymm3) + mem
	vfmadd213ps	-32(%rcx,%rdx,4), %ymm1, %ymm4 ## ymm4 = (ymm1 * ymm4) + mem
	vfmadd213ps	(%rcx,%rdx,4), %ymm1, %ymm5 ## ymm5 = (ymm1 * ymm5) + mem
	vmovups	%ymm2, -96(%rcx,%rdx,4)
	vmovups	%ymm3, -64(%rcx,%rdx,4)
	vmovups	%ymm4, -32(%rcx,%rdx,4)
	vmovups	%ymm5, (%rcx,%rdx,4)
	addq	$32, %rdx
	cmpq	%rdx, %r11
	jne	LBB26_9
## %bb.10:                              ## %middle.block69
                                        ##   in Loop: Header=BB26_6 Depth=1
	movq	%r11, %rcx
	cmpq	%r10, %r11
	movq	%rbx, %rsi
	je	LBB26_13
LBB26_11:                               ## %"for relu1_0_d_def__.s18.n.ni.us.us.preheader"
                                        ##   in Loop: Header=BB26_6 Depth=1
	movq	88(%rsp), %rdx                  ## 8-byte Reload
	leaq	(%rdx,%rax,4), %rdx
	movq	80(%rsp), %rbx                  ## 8-byte Reload
	leaq	(%rbx,%rax,4), %rax
	movq	-80(%rsp), %rbx                 ## 8-byte Reload
	leaq	(%rbx,%rsi,4), %rbx
	.p2align	4, 0x90
LBB26_12:                               ## %"for relu1_0_d_def__.s18.n.ni.us.us"
                                        ##   Parent Loop BB26_6 Depth=1
                                        ## =>  This Inner Loop Header: Depth=2
	vmovss	(%rax,%rcx,4), %xmm2            ## xmm2 = mem[0],zero,zero,zero
	vmulss	(%rdx,%rcx,4), %xmm2, %xmm2
	vmulss	(%rbp,%rcx,4), %xmm2, %xmm2
	vfmadd213ss	(%rbx,%rcx,4), %xmm0, %xmm2 ## xmm2 = (xmm0 * xmm2) + mem
	vmovss	%xmm2, (%rbx,%rcx,4)
	incq	%rcx
	cmpq	%rcx, %r10
	jne	LBB26_12
LBB26_13:                               ## %"end for relu1_0_d_def__.s18.n.ni.loopexit.us.us"
                                        ##   in Loop: Header=BB26_6 Depth=1
	cmpl	$0, -128(%rsp)                  ## 4-byte Folded Reload
	movl	-112(%rsp), %ebx                ## 4-byte Reload
	movq	-88(%rsp), %r8                  ## 8-byte Reload
	jle	LBB26_17
## %bb.14:                              ## %"for relu1_0_d_def__.s18.n.ni.rebased.preheader.us.us"
                                        ##   in Loop: Header=BB26_6 Depth=1
	movslq	%r8d, %rax
	imull	-124(%rsp), %r9d                ## 4-byte Folded Reload
	movslq	%r9d, %rcx
	movq	8(%rsp), %rdx                   ## 8-byte Reload
	addq	%rcx, %rdx
	addq	(%rsp), %rcx                    ## 8-byte Folded Reload
	movq	-40(%rsp), %rsi                 ## 8-byte Reload
	vmovss	-4(%rsi,%rdx,4), %xmm2          ## xmm2 = mem[0],zero,zero,zero
	vmulss	-4(%rsi,%rcx,4), %xmm2, %xmm2
	cmpl	$16, -128(%rsp)                 ## 4-byte Folded Reload
	jae	LBB26_18
## %bb.15:                              ##   in Loop: Header=BB26_6 Depth=1
	xorl	%ecx, %ecx
	jmp	LBB26_25
	.p2align	4, 0x90
LBB26_18:                               ## %vector.ph54
                                        ##   in Loop: Header=BB26_6 Depth=1
	vbroadcastss	%xmm2, %ymm3
	cmpq	$0, 48(%rsp)                    ## 8-byte Folded Reload
	je	LBB26_19
## %bb.20:                              ## %vector.body52.preheader
                                        ##   in Loop: Header=BB26_6 Depth=1
	movq	-16(%rsp), %rcx                 ## 8-byte Reload
	addq	%rax, %rcx
	movq	-8(%rsp), %rdx                  ## 8-byte Reload
	leaq	(%rdx,%rcx,4), %rcx
	movq	24(%rsp), %rdx                  ## 8-byte Reload
	xorl	%esi, %esi
	movq	-120(%rsp), %r9                 ## 8-byte Reload
	.p2align	4, 0x90
LBB26_21:                               ## %vector.body52
                                        ##   Parent Loop BB26_6 Depth=1
                                        ## =>  This Inner Loop Header: Depth=2
	vmulps	-96(%r12,%rsi,4), %ymm3, %ymm4
	vmulps	-64(%r12,%rsi,4), %ymm3, %ymm5
	vfmadd213ps	-96(%rcx,%rsi,4), %ymm1, %ymm4 ## ymm4 = (ymm1 * ymm4) + mem
	vfmadd213ps	-64(%rcx,%rsi,4), %ymm1, %ymm5 ## ymm5 = (ymm1 * ymm5) + mem
	vmovups	%ymm4, -96(%rcx,%rsi,4)
	vmovups	%ymm5, -64(%rcx,%rsi,4)
	vmulps	-32(%r12,%rsi,4), %ymm3, %ymm4
	vmulps	(%r12,%rsi,4), %ymm3, %ymm5
	vfmadd213ps	-32(%rcx,%rsi,4), %ymm1, %ymm4 ## ymm4 = (ymm1 * ymm4) + mem
	vfmadd213ps	(%rcx,%rsi,4), %ymm1, %ymm5 ## ymm5 = (ymm1 * ymm5) + mem
	vmovups	%ymm4, -32(%rcx,%rsi,4)
	vmovups	%ymm5, (%rcx,%rsi,4)
	addq	$32, %rsi
	addq	$2, %rdx
	jne	LBB26_21
## %bb.22:                              ## %middle.block50.unr-lcssa
                                        ##   in Loop: Header=BB26_6 Depth=1
	testb	$1, 40(%rsp)                    ## 1-byte Folded Reload
	je	LBB26_24
LBB26_23:                               ## %vector.body52.epil
                                        ##   in Loop: Header=BB26_6 Depth=1
	movq	-32(%rsp), %rcx                 ## 8-byte Reload
	addl	%r9d, %ecx
	movq	-48(%rsp), %rdx                 ## 8-byte Reload
	addq	%r9, %rdx
	imull	%ebx, %ecx
	imulq	-72(%rsp), %rdx                 ## 8-byte Folded Reload
	addl	-60(%rsp), %ecx                 ## 4-byte Folded Reload
	movslq	%ecx, %rcx
	addq	-16(%rsp), %rsi                 ## 8-byte Folded Reload
	addq	%rsi, %rcx
	addq	%rdx, %rsi
	movq	-56(%rsp), %rdx                 ## 8-byte Reload
	vmulps	(%rdx,%rsi,4), %ymm3, %ymm4
	vmulps	32(%rdx,%rsi,4), %ymm3, %ymm3
	movq	-80(%rsp), %rdx                 ## 8-byte Reload
	vfmadd213ps	(%rdx,%rcx,4), %ymm1, %ymm4 ## ymm4 = (ymm1 * ymm4) + mem
	vfmadd213ps	32(%rdx,%rcx,4), %ymm1, %ymm3 ## ymm3 = (ymm1 * ymm3) + mem
	vmovups	%ymm4, (%rdx,%rcx,4)
	vmovups	%ymm3, 32(%rdx,%rcx,4)
LBB26_24:                               ## %middle.block50
                                        ##   in Loop: Header=BB26_6 Depth=1
	movq	56(%rsp), %rdx                  ## 8-byte Reload
	movq	%rdx, %rcx
	cmpq	%r14, %rdx
	je	LBB26_17
LBB26_25:                               ## %"for relu1_0_d_def__.s18.n.ni.rebased.us.us.preheader"
                                        ##   in Loop: Header=BB26_6 Depth=1
	movq	72(%rsp), %rdx                  ## 8-byte Reload
	leaq	(%rdx,%rax,4), %rax
	.p2align	4, 0x90
LBB26_16:                               ## %"for relu1_0_d_def__.s18.n.ni.rebased.us.us"
                                        ##   Parent Loop BB26_6 Depth=1
                                        ## =>  This Inner Loop Header: Depth=2
	vmulss	(%r15,%rcx,4), %xmm2, %xmm3
	vfmadd213ss	(%rax,%rcx,4), %xmm0, %xmm3 ## xmm3 = (xmm0 * xmm3) + mem
	vmovss	%xmm3, (%rax,%rcx,4)
	incq	%rcx
	cmpq	%rcx, %r14
	jne	LBB26_16
	jmp	LBB26_17
LBB26_19:                               ##   in Loop: Header=BB26_6 Depth=1
	xorl	%esi, %esi
	movq	-120(%rsp), %r9                 ## 8-byte Reload
	testb	$1, 40(%rsp)                    ## 1-byte Folded Reload
	jne	LBB26_23
	jmp	LBB26_24
LBB26_2:                                ## %"for relu1_0_d_def__.s18.w.wi.preheader20"
	decl	%edx
	vmovd	%r12d, %xmm0
	vpbroadcastd	%xmm0, %ymm0
	vpor	LCPI26_0(%rip), %ymm0, %ymm0
	vmovd	%edx, %xmm1
	vpbroadcastd	%xmm1, %ymm1
	vpminsd	%ymm0, %ymm1, %ymm0
	movq	-72(%rsp), %rcx                 ## 8-byte Reload
	movslq	-96(%rsp), %rdi                 ## 4-byte Folded Reload
	movl	-104(%rsp), %r14d               ## 4-byte Reload
	imulq	%rcx, %r13
	addq	%r12, %r13
	movl	%ebx, %eax
	leaq	(,%r13,4), %rbx
	addq	%rbp, %rbx
	shlq	$2, %rcx
	movq	%rcx, %r15
	shll	$6, %r8d
	shll	$6, %eax
	subl	%eax, %r8d
	orl	$7, %r8d
	movq	-88(%rsp), %rax                 ## 8-byte Reload
	imull	%r8d, %eax
	movq	-120(%rsp), %rcx                ## 8-byte Reload
	leal	(%rax,%rcx,8), %edx
	vbroadcastss	LCPI26_1(%rip), %ymm1   ## ymm1 = [9.99999971E-10,9.99999971E-10,9.99999971E-10,9.99999971E-10,9.99999971E-10,9.99999971E-10,9.99999971E-10,9.99999971E-10]
	movl	-112(%rsp), %r8d                ## 4-byte Reload
	movl	-124(%rsp), %r10d               ## 4-byte Reload
	movq	-40(%rsp), %rax                 ## 8-byte Reload
	.p2align	4, 0x90
LBB26_3:                                ## %"for relu1_0_d_def__.s18.w.wi"
                                        ## =>This Inner Loop Header: Depth=1
	testq	%rdi, %rdi
	movl	$0, %esi
	cmovgl	%edi, %esi
	imull	%r10d, %esi
	leal	(%rsi,%r9), %ecx
	vmovd	%ecx, %xmm2
	vpbroadcastd	%xmm2, %ymm2
	vpaddd	%ymm0, %ymm2, %ymm3
	vextracti128	$1, %ymm3, %xmm2
	vmovd	%xmm2, %ecx
	movslq	%ecx, %rcx
	vpextrd	$1, %xmm2, %ebp
	movslq	%ebp, %rbp
	vmovss	(%rax,%rcx,4), %xmm4            ## xmm4 = mem[0],zero,zero,zero
	vpextrd	$2, %xmm2, %ecx
	vinsertps	$16, (%rax,%rbp,4), %xmm4, %xmm4 ## xmm4 = xmm4[0],mem[0],xmm4[2,3]
	vmovd	%xmm3, %ebp
	movslq	%ecx, %rcx
	vinsertps	$32, (%rax,%rcx,4), %xmm4, %xmm4 ## xmm4 = xmm4[0,1],mem[0],xmm4[3]
	movslq	%ebp, %rcx
	vpextrd	$3, %xmm2, %ebp
	movslq	%ebp, %rbp
	vinsertps	$48, (%rax,%rbp,4), %xmm4, %xmm2 ## xmm2 = xmm4[0,1,2],mem[0]
	vpextrd	$1, %xmm3, %ebp
	vmovss	(%rax,%rcx,4), %xmm4            ## xmm4 = mem[0],zero,zero,zero
	movslq	%ebp, %rcx
	vinsertps	$16, (%rax,%rcx,4), %xmm4, %xmm4 ## xmm4 = xmm4[0],mem[0],xmm4[2,3]
	vpextrd	$2, %xmm3, %ecx
	movslq	%ecx, %rcx
	vinsertps	$32, (%rax,%rcx,4), %xmm4, %xmm4 ## xmm4 = xmm4[0,1],mem[0],xmm4[3]
	vpextrd	$3, %xmm3, %ecx
	movslq	%ecx, %rcx
	vinsertps	$48, (%rax,%rcx,4), %xmm4, %xmm3 ## xmm3 = xmm4[0,1,2],mem[0]
	addl	%r11d, %esi
	vmovd	%esi, %xmm4
	vpbroadcastd	%xmm4, %ymm4
	vpaddd	%ymm0, %ymm4, %ymm4
	vextracti128	$1, %ymm4, %xmm5
	vmovd	%xmm5, %ecx
	movslq	%ecx, %rcx
	vmovss	(%rax,%rcx,4), %xmm6            ## xmm6 = mem[0],zero,zero,zero
	vpextrd	$1, %xmm5, %ecx
	movslq	%ecx, %rcx
	vinsertps	$16, (%rax,%rcx,4), %xmm6, %xmm6 ## xmm6 = xmm6[0],mem[0],xmm6[2,3]
	vpextrd	$2, %xmm5, %ecx
	movslq	%ecx, %rcx
	vinsertps	$32, (%rax,%rcx,4), %xmm6, %xmm6 ## xmm6 = xmm6[0,1],mem[0],xmm6[3]
	vpextrd	$3, %xmm5, %ecx
	movslq	%ecx, %rcx
	vinsertps	$48, (%rax,%rcx,4), %xmm6, %xmm5 ## xmm5 = xmm6[0,1,2],mem[0]
	vmovd	%xmm4, %ecx
	movslq	%ecx, %rcx
	vmovss	(%rax,%rcx,4), %xmm6            ## xmm6 = mem[0],zero,zero,zero
	vpextrd	$1, %xmm4, %ecx
	movslq	%ecx, %rcx
	vinsertps	$16, (%rax,%rcx,4), %xmm6, %xmm6 ## xmm6 = xmm6[0],mem[0],xmm6[2,3]
	vpextrd	$2, %xmm4, %ecx
	movslq	%ecx, %rcx
	vinsertps	$32, (%rax,%rcx,4), %xmm6, %xmm6 ## xmm6 = xmm6[0,1],mem[0],xmm6[3]
	vpextrd	$3, %xmm4, %ecx
	movslq	%ecx, %rcx
	vinsertps	$48, (%rax,%rcx,4), %xmm6, %xmm4 ## xmm4 = xmm6[0,1,2],mem[0]
	vinsertf128	$1, %xmm2, %ymm3, %ymm2
	movslq	%edx, %rdx
	vinsertf128	$1, %xmm5, %ymm4, %ymm3
	vmulps	%ymm3, %ymm2, %ymm2
	vmulps	(%rbx), %ymm2, %ymm2
	movq	-80(%rsp), %rcx                 ## 8-byte Reload
	vfmadd213ps	(%rcx,%rdx,4), %ymm1, %ymm2 ## ymm2 = (ymm1 * ymm2) + mem
	vmovups	%ymm2, (%rcx,%rdx,4)
	addq	%r15, %rbx
	incq	%rdi
	addl	%r8d, %edx
	decq	%r14
	jne	LBB26_3
LBB26_40:                               ## %destructor_block
	xorl	%eax, %eax
	addq	$112, %rsp
	popq	%rbx
	popq	%r12
	popq	%r13
	popq	%r14
	popq	%r15
	popq	%rbp
	vzeroupper
	retq
LBB26_26:                               ## %"for relu1_0_d_def__.s18.w.wi.preheader.split.us.split"
	movl	%ecx, -120(%rsp)                ## 4-byte Spill
	testl	%r14d, %r14d
	movl	-124(%rsp), %edx                ## 4-byte Reload
	jle	LBB26_40
## %bb.27:                              ## %"for relu1_0_d_def__.s18.w.wi.us.us4.preheader"
	movq	%rbp, %r9
	movslq	%esi, %r11
	movl	-128(%rsp), %ebp                ## 4-byte Reload
	movslq	-96(%rsp), %r10                 ## 4-byte Folded Reload
	movl	-104(%rsp), %r15d               ## 4-byte Reload
	movl	%ebp, %r12d
	andl	$-16, %r12d
	leaq	-16(%r12), %r14
	movq	%r14, -96(%rsp)                 ## 8-byte Spill
	shrq	$4, %r14
	incq	%r14
	movq	-72(%rsp), %rcx                 ## 8-byte Reload
	movq	%rcx, %rax
	imulq	-48(%rsp), %rax                 ## 8-byte Folded Reload
	addq	%r11, %rax
	movq	-56(%rsp), %r13                 ## 8-byte Reload
	leaq	96(,%rax,4), %rdi
	addq	%r13, %rdi
	leaq	(,%rcx,4), %rbx
	movq	-80(%rsp), %rcx                 ## 8-byte Reload
	leaq	96(%rcx), %rsi
	movq	%rsi, -104(%rsp)                ## 8-byte Spill
	shll	$6, %r8d
	movl	-120(%rsp), %esi                ## 4-byte Reload
	shll	$6, %esi
	subl	%esi, %r8d
	orl	$7, %r8d
	imull	%r8d, %r9d
	movq	%r14, %rsi
	andq	$-2, %rsi
	negq	%rsi
	movq	%rsi, -88(%rsp)                 ## 8-byte Spill
	leaq	(%r13,%rax,4), %r13
	movq	%r11, -120(%rsp)                ## 8-byte Spill
	leaq	(%rcx,%r11,4), %r8
	vmovss	LCPI26_1(%rip), %xmm0           ## xmm0 = mem[0],zero,zero,zero
	vbroadcastss	LCPI26_1(%rip), %ymm1   ## ymm1 = [9.99999971E-10,9.99999971E-10,9.99999971E-10,9.99999971E-10,9.99999971E-10,9.99999971E-10,9.99999971E-10,9.99999971E-10]
	xorl	%r11d, %r11d
	jmp	LBB26_28
LBB26_39:                               ## %after_bb.loopexit.us.us17
                                        ##   in Loop: Header=BB26_28 Depth=1
	incq	%r11
	addq	%rbx, %rdi
	addl	-112(%rsp), %r9d                ## 4-byte Folded Reload
	addq	%rbx, %r13
	cmpq	%r15, %r11
	movl	-124(%rsp), %edx                ## 4-byte Reload
	je	LBB26_40
LBB26_28:                               ## %"for relu1_0_d_def__.s18.w.wi.us.us4"
                                        ## =>This Loop Header: Depth=1
                                        ##     Child Loop BB26_33 Depth 2
                                        ##     Child Loop BB26_38 Depth 2
	movslq	%r9d, %rax
	leaq	(%r11,%r10), %rcx
	testq	%rcx, %rcx
	movl	$0, %esi
	cmovlel	%esi, %ecx
	imull	%edx, %ecx
	movslq	%ecx, %rcx
	movq	8(%rsp), %rdx                   ## 8-byte Reload
	addq	%rcx, %rdx
	addq	(%rsp), %rcx                    ## 8-byte Folded Reload
	movq	-40(%rsp), %rsi                 ## 8-byte Reload
	vmovss	-4(%rsi,%rdx,4), %xmm2          ## xmm2 = mem[0],zero,zero,zero
	vmulss	-4(%rsi,%rcx,4), %xmm2, %xmm2
	cmpl	$16, -128(%rsp)                 ## 4-byte Folded Reload
	jae	LBB26_30
## %bb.29:                              ##   in Loop: Header=BB26_28 Depth=1
	xorl	%ecx, %ecx
	jmp	LBB26_37
LBB26_30:                               ## %vector.ph
                                        ##   in Loop: Header=BB26_28 Depth=1
	vbroadcastss	%xmm2, %ymm3
	cmpq	$0, -96(%rsp)                   ## 8-byte Folded Reload
	je	LBB26_31
## %bb.32:                              ## %vector.body.preheader
                                        ##   in Loop: Header=BB26_28 Depth=1
	movq	-120(%rsp), %rcx                ## 8-byte Reload
	addq	%rax, %rcx
	movq	-104(%rsp), %rdx                ## 8-byte Reload
	leaq	(%rdx,%rcx,4), %rcx
	movq	-88(%rsp), %rdx                 ## 8-byte Reload
	xorl	%esi, %esi
LBB26_33:                               ## %vector.body
                                        ##   Parent Loop BB26_28 Depth=1
                                        ## =>  This Inner Loop Header: Depth=2
	vmulps	-96(%rdi,%rsi,4), %ymm3, %ymm4
	vmulps	-64(%rdi,%rsi,4), %ymm3, %ymm5
	vfmadd213ps	-96(%rcx,%rsi,4), %ymm1, %ymm4 ## ymm4 = (ymm1 * ymm4) + mem
	vfmadd213ps	-64(%rcx,%rsi,4), %ymm1, %ymm5 ## ymm5 = (ymm1 * ymm5) + mem
	vmovups	%ymm4, -96(%rcx,%rsi,4)
	vmovups	%ymm5, -64(%rcx,%rsi,4)
	vmulps	-32(%rdi,%rsi,4), %ymm3, %ymm4
	vmulps	(%rdi,%rsi,4), %ymm3, %ymm5
	vfmadd213ps	-32(%rcx,%rsi,4), %ymm1, %ymm4 ## ymm4 = (ymm1 * ymm4) + mem
	vfmadd213ps	(%rcx,%rsi,4), %ymm1, %ymm5 ## ymm5 = (ymm1 * ymm5) + mem
	vmovups	%ymm4, -32(%rcx,%rsi,4)
	vmovups	%ymm5, (%rcx,%rsi,4)
	addq	$32, %rsi
	addq	$2, %rdx
	jne	LBB26_33
	jmp	LBB26_34
LBB26_31:                               ##   in Loop: Header=BB26_28 Depth=1
	xorl	%esi, %esi
LBB26_34:                               ## %middle.block.unr-lcssa
                                        ##   in Loop: Header=BB26_28 Depth=1
	testb	$1, %r14b
	je	LBB26_36
## %bb.35:                              ## %vector.body.epil
                                        ##   in Loop: Header=BB26_28 Depth=1
	movq	-32(%rsp), %rcx                 ## 8-byte Reload
	addl	%r11d, %ecx
	imull	-112(%rsp), %ecx                ## 4-byte Folded Reload
	movq	-48(%rsp), %rdx                 ## 8-byte Reload
	addq	%r11, %rdx
	addl	-60(%rsp), %ecx                 ## 4-byte Folded Reload
	imulq	-72(%rsp), %rdx                 ## 8-byte Folded Reload
	movslq	%ecx, %rcx
	addq	-120(%rsp), %rsi                ## 8-byte Folded Reload
	addq	%rsi, %rcx
	addq	%rdx, %rsi
	movq	-56(%rsp), %rdx                 ## 8-byte Reload
	vmulps	(%rdx,%rsi,4), %ymm3, %ymm4
	vmulps	32(%rdx,%rsi,4), %ymm3, %ymm3
	movq	-80(%rsp), %rdx                 ## 8-byte Reload
	vfmadd213ps	(%rdx,%rcx,4), %ymm1, %ymm4 ## ymm4 = (ymm1 * ymm4) + mem
	vfmadd213ps	32(%rdx,%rcx,4), %ymm1, %ymm3 ## ymm3 = (ymm1 * ymm3) + mem
	vmovups	%ymm4, (%rdx,%rcx,4)
	vmovups	%ymm3, 32(%rdx,%rcx,4)
LBB26_36:                               ## %middle.block
                                        ##   in Loop: Header=BB26_28 Depth=1
	movq	%r12, %rcx
	cmpq	%rbp, %r12
	je	LBB26_39
LBB26_37:                               ## %"for relu1_0_d_def__.s18.n.ni.rebased.us.us9.preheader"
                                        ##   in Loop: Header=BB26_28 Depth=1
	leaq	(%r8,%rax,4), %rax
LBB26_38:                               ## %"for relu1_0_d_def__.s18.n.ni.rebased.us.us9"
                                        ##   Parent Loop BB26_28 Depth=1
                                        ## =>  This Inner Loop Header: Depth=2
	vmulss	(%r13,%rcx,4), %xmm2, %xmm3
	vfmadd213ss	(%rax,%rcx,4), %xmm0, %xmm3 ## xmm3 = (xmm0 * xmm3) + mem
	vmovss	%xmm3, (%rax,%rcx,4)
	incq	%rcx
	cmpq	%rcx, %rbp
	jne	LBB26_38
	jmp	LBB26_39
                                        ## -- End function
	.section	__TEXT,__const
	.p2align	5                               ## -- Begin function train_cost_model.par_for.relu1_0_d_def__.s19.n.n.n
LCPI27_0:
	.long	0                               ## 0x0
	.long	1                               ## 0x1
	.long	2                               ## 0x2
	.long	3                               ## 0x3
	.long	4                               ## 0x4
	.long	5                               ## 0x5
	.long	6                               ## 0x6
	.long	7                               ## 0x7
	.section	__TEXT,__literal4,4byte_literals
	.p2align	2
LCPI27_1:
	.long	0x3089705f                      ## float 9.99999971E-10
	.section	__TEXT,__text,regular,pure_instructions
	.p2align	4, 0x90
_train_cost_model.par_for.relu1_0_d_def__.s19.n.n.n: ## @train_cost_model.par_for.relu1_0_d_def__.s19.n.n.n
## %bb.0:                               ## %entry
	pushq	%rbp
	pushq	%r15
	pushq	%r14
	pushq	%r13
	pushq	%r12
	pushq	%rbx
	subq	$120, %rsp
	movq	%rdx, %rdi
	movl	12(%rdx), %r11d
	movl	32(%rdx), %r10d
	movl	%esi, %r8d
	sarl	$31, %r8d
	xorl	%ebp, %ebp
	testl	%r10d, %r10d
	sete	%bpl
	movl	%ebp, %ecx
	negl	%ecx
	movl	%r10d, %ebx
	sarl	$31, %ebx
	subl	%r8d, %esi
	orl	%r10d, %ecx
	movl	%esi, %eax
	cltd
	idivl	%ecx
	movl	%edx, %r13d
	movl	%ebx, %r12d
	leal	(%r10,%rbp), %ecx
	movl	%esi, %eax
	cltd
	idivl	%ecx
	notl	%r12d
	decl	%ebp
	movl	%r12d, %r15d
	subl	%ebx, %r15d
	andl	%r8d, %r15d
	addl	%eax, %r15d
	andl	%ebp, %r15d
	leal	(%r15,%r15), %eax
	movl	%eax, -104(%rsp)                ## 4-byte Spill
	subl	%eax, %r11d
	cmpl	$2, %r11d
	movl	$2, %r14d
	cmovll	%r11d, %r14d
	xorl	%eax, %eax
	testl	%r11d, %r11d
	cmovlel	%eax, %r14d
	jle	LBB27_40
## %bb.1:                               ## %"for relu1_0_d_def__.s19.w.wi.preheader"
	movl	(%rdi), %edx
	movl	16(%rdi), %r9d
	movl	28(%rdi), %eax
	movl	%eax, -124(%rsp)                ## 4-byte Spill
	movl	36(%rdi), %eax
	movl	%eax, -120(%rsp)                ## 4-byte Spill
	movq	56(%rdi), %rax
	movq	%rax, -88(%rsp)                 ## 8-byte Spill
	movq	72(%rdi), %rax
	movq	%rax, -80(%rsp)                 ## 8-byte Spill
	movq	88(%rdi), %rax
	movq	%rax, -64(%rsp)                 ## 8-byte Spill
	xorl	%r10d, %ebx
	addl	%r12d, %ebx
	andl	%r8d, %ebx
	addl	%ebx, %r13d
	andl	%ebp, %r13d
	leal	(,%r13,8), %ebx
	movslq	8(%rdi), %rax
	movslq	%r15d, %rcx
	subq	%rax, %rcx
	addq	%rcx, %rcx
	movslq	4(%rdi), %r11
	leal	8(%rbx), %r8d
	movl	%r9d, %eax
	shll	$5, %eax
	movl	%eax, -108(%rsp)                ## 4-byte Spill
	movl	%edx, %eax
	subl	%ebx, %eax
	cmpl	$8, %eax
	movl	$8, %esi
	cmovll	%eax, %esi
	testl	%eax, %eax
	movl	$0, %r10d
	cmovgl	%esi, %r10d
	movl	%ebx, %ebp
	subl	%edx, %ebp
	movl	%ebp, %esi
	sarl	$31, %esi
	andl	%ebp, %esi
	cmpl	$-8, %esi
	movl	$-8, %r12d
	cmovgl	%esi, %r12d
	movslq	%ebx, %rbp
	cmpl	%edx, %r8d
	jle	LBB27_2
## %bb.4:                               ## %"for relu1_0_d_def__.s19.w.wi.preheader.split.us"
	movq	%r13, -72(%rsp)                 ## 8-byte Spill
	movslq	44(%rdi), %r8
	movl	52(%rdi), %edi
	leal	(%r9,%r9), %esi
	leal	(%rsi,%rsi,2), %esi
	movl	%esi, -52(%rsp)                 ## 4-byte Spill
	movl	%r15d, %esi
	subl	-120(%rsp), %esi                ## 4-byte Folded Reload
	addl	%esi, %esi
	movq	%rsi, -16(%rsp)                 ## 8-byte Spill
	subl	%r8d, %edx
	addl	%r10d, %ebx
	addl	%r10d, %r12d
	movslq	%edx, %rsi
	movq	%rsi, 16(%rsp)                  ## 8-byte Spill
	addl	%edi, %edx
	movslq	%edx, %rdx
	movq	%rdx, 8(%rsp)                   ## 8-byte Spill
	testl	%eax, %eax
	movl	%r12d, -92(%rsp)                ## 4-byte Spill
	movq	%rcx, -40(%rsp)                 ## 8-byte Spill
	jle	LBB27_26
## %bb.5:                               ## %"for relu1_0_d_def__.s19.w.wi.us.us.preheader"
	subl	%r8d, %edi
	movq	%rbp, %rsi
	leal	(%rdi,%rbp), %eax
	cltq
	movq	%rax, 40(%rsp)                  ## 8-byte Spill
	movl	%r10d, -20(%rsp)                ## 4-byte Spill
	movl	%r10d, %edi
	movslq	%ebx, %rbx
	movl	%r12d, %r13d
	movslq	-104(%rsp), %rax                ## 4-byte Folded Reload
	movq	%rax, 24(%rsp)                  ## 8-byte Spill
	movl	%r14d, %eax
	movq	%rax, 104(%rsp)                 ## 8-byte Spill
	movl	%r13d, %eax
	andl	$-16, %eax
	movq	%rax, 80(%rsp)                  ## 8-byte Spill
	addq	$-16, %rax
	movq	%rax, 72(%rsp)                  ## 8-byte Spill
	movq	%rax, %rbp
	shrq	$4, %rbp
	incq	%rbp
	movq	%r11, %r10
	movq	%r11, %rax
	imulq	%rcx, %rax
	leaq	(%rax,%rsi), %r14
	subq	%r8, %rsi
	movq	%rdi, 32(%rsp)                  ## 8-byte Spill
	movl	%edi, %r12d
	andl	$2147483616, %r12d              ## imm = 0x7FFFFFE0
	shll	$6, %r15d
	movl	-120(%rsp), %ecx                ## 4-byte Reload
	shll	$6, %ecx
	subl	%ecx, %r15d
	orl	$6, %r15d
	imull	%r15d, %r9d
	movq	%rsi, %r15
	movq	%r9, -104(%rsp)                 ## 8-byte Spill
	movq	-72(%rsp), %rcx                 ## 8-byte Reload
	leal	(%r9,%rcx,8), %esi
	addq	%rbx, %rax
	movq	-88(%rsp), %rcx                 ## 8-byte Reload
	leaq	(%rcx,%rax,4), %r9
	addq	$96, %r9
	movq	%rbp, %rdx
	movq	%rbp, 64(%rsp)                  ## 8-byte Spill
	andq	$-2, %rbp
	negq	%rbp
	movq	%rbp, 48(%rsp)                  ## 8-byte Spill
	leaq	(%rcx,%rax,4), %r11
	vmovss	LCPI27_1(%rip), %xmm0           ## xmm0 = mem[0],zero,zero,zero
	vbroadcastss	LCPI27_1(%rip), %ymm1   ## ymm1 = [9.99999971E-10,9.99999971E-10,9.99999971E-10,9.99999971E-10,9.99999971E-10,9.99999971E-10,9.99999971E-10,9.99999971E-10]
	movq	-64(%rsp), %rax                 ## 8-byte Reload
	addq	$96, %rax
	movq	%rax, 56(%rsp)                  ## 8-byte Spill
	movq	%r14, -48(%rsp)                 ## 8-byte Spill
	leaq	96(%rcx,%r14,4), %r14
	leaq	(,%r10,4), %rax
	movq	%rax, 96(%rsp)                  ## 8-byte Spill
	movq	-80(%rsp), %rax                 ## 8-byte Reload
	leaq	96(%rax), %rcx
	movq	%rcx, (%rsp)                    ## 8-byte Spill
	movq	%rbx, -8(%rsp)                  ## 8-byte Spill
	leaq	(%rax,%rbx,4), %rax
	movq	%rax, 88(%rsp)                  ## 8-byte Spill
	movq	24(%rsp), %rcx                  ## 8-byte Reload
	xorl	%edx, %edx
	movl	-124(%rsp), %r8d                ## 4-byte Reload
	movq	%r10, 112(%rsp)                 ## 8-byte Spill
	jmp	LBB27_6
	.p2align	4, 0x90
LBB27_17:                               ## %after_bb.us.us
                                        ##   in Loop: Header=BB27_6 Depth=1
	movq	-120(%rsp), %rdx                ## 8-byte Reload
	incq	%rdx
	movq	-32(%rsp), %rcx                 ## 8-byte Reload
	incq	%rcx
	movq	96(%rsp), %rax                  ## 8-byte Reload
	addq	%rax, %r14
	movl	-72(%rsp), %esi                 ## 4-byte Reload
	addl	%ebx, %esi
	addq	%r10, -48(%rsp)                 ## 8-byte Folded Spill
	addq	%rax, %r9
	addl	%ebx, %ebp
	movq	%rbp, -104(%rsp)                ## 8-byte Spill
	addq	%rax, %r11
	cmpq	104(%rsp), %rdx                 ## 8-byte Folded Reload
	je	LBB27_40
LBB27_6:                                ## %"for relu1_0_d_def__.s19.w.wi.us.us"
                                        ## =>This Loop Header: Depth=1
                                        ##     Child Loop BB27_9 Depth 2
                                        ##     Child Loop BB27_12 Depth 2
                                        ##     Child Loop BB27_21 Depth 2
                                        ##     Child Loop BB27_16 Depth 2
	testq	%rcx, %rcx
	movl	$0, %eax
	movq	%rcx, -32(%rsp)                 ## 8-byte Spill
	cmovgl	%ecx, %eax
	imull	%r8d, %eax
	cltq
	movq	24(%rsp), %rcx                  ## 8-byte Reload
	movq	%rdx, -120(%rsp)                ## 8-byte Spill
	addq	%rcx, %rdx
	testq	%rdx, %rdx
	movl	$0, %ecx
	cmovlel	%ecx, %edx
	movl	%esi, -72(%rsp)                 ## 4-byte Spill
	movslq	%esi, %r10
	cmpl	$32, -20(%rsp)                  ## 4-byte Folded Reload
	jae	LBB27_8
## %bb.7:                               ##   in Loop: Header=BB27_6 Depth=1
	xorl	%ebx, %ebx
	jmp	LBB27_11
	.p2align	4, 0x90
LBB27_8:                                ## %vector.body72.preheader
                                        ##   in Loop: Header=BB27_6 Depth=1
	movq	40(%rsp), %rcx                  ## 8-byte Reload
	addq	%rax, %rcx
	movq	%r15, %rbp
	leaq	(%r15,%rax), %rsi
	movq	56(%rsp), %rdi                  ## 8-byte Reload
	leaq	(%rdi,%rcx,4), %r15
	leaq	(%rdi,%rsi,4), %rcx
	movq	(%rsp), %rsi                    ## 8-byte Reload
	leaq	(%rsi,%r10,4), %r8
	xorl	%edi, %edi
	.p2align	4, 0x90
LBB27_9:                                ## %vector.body72
                                        ##   Parent Loop BB27_6 Depth=1
                                        ## =>  This Inner Loop Header: Depth=2
	vmovups	-96(%rcx,%rdi,4), %ymm2
	vmovups	-64(%rcx,%rdi,4), %ymm3
	vmovups	-32(%rcx,%rdi,4), %ymm4
	vmovups	(%rcx,%rdi,4), %ymm5
	vmulps	-96(%r15,%rdi,4), %ymm2, %ymm2
	vmulps	-64(%r15,%rdi,4), %ymm3, %ymm3
	vmulps	-32(%r15,%rdi,4), %ymm4, %ymm4
	vmulps	(%r15,%rdi,4), %ymm5, %ymm5
	vmulps	-96(%r14,%rdi,4), %ymm2, %ymm2
	vmulps	-64(%r14,%rdi,4), %ymm3, %ymm3
	vmulps	-32(%r14,%rdi,4), %ymm4, %ymm4
	vmulps	(%r14,%rdi,4), %ymm5, %ymm5
	vfmadd213ps	-96(%r8,%rdi,4), %ymm1, %ymm2 ## ymm2 = (ymm1 * ymm2) + mem
	vfmadd213ps	-64(%r8,%rdi,4), %ymm1, %ymm3 ## ymm3 = (ymm1 * ymm3) + mem
	vfmadd213ps	-32(%r8,%rdi,4), %ymm1, %ymm4 ## ymm4 = (ymm1 * ymm4) + mem
	vfmadd213ps	(%r8,%rdi,4), %ymm1, %ymm5 ## ymm5 = (ymm1 * ymm5) + mem
	vmovups	%ymm2, -96(%r8,%rdi,4)
	vmovups	%ymm3, -64(%r8,%rdi,4)
	vmovups	%ymm4, -32(%r8,%rdi,4)
	vmovups	%ymm5, (%r8,%rdi,4)
	addq	$32, %rdi
	cmpq	%rdi, %r12
	jne	LBB27_9
## %bb.10:                              ## %middle.block70
                                        ##   in Loop: Header=BB27_6 Depth=1
	movq	%r12, %rbx
	cmpq	32(%rsp), %r12                  ## 8-byte Folded Reload
	movl	-124(%rsp), %r8d                ## 4-byte Reload
	movq	%rbp, %r15
	je	LBB27_13
LBB27_11:                               ## %"for relu1_0_d_def__.s19.n.ni.us.us.preheader"
                                        ##   in Loop: Header=BB27_6 Depth=1
	movq	32(%rsp), %rcx                  ## 8-byte Reload
	subq	%rbx, %rcx
	movq	40(%rsp), %rsi                  ## 8-byte Reload
	addq	%rbx, %rsi
	addq	%rax, %rsi
	movq	-64(%rsp), %rbp                 ## 8-byte Reload
	leaq	(,%rsi,4), %rdi
	addq	%rbp, %rdi
	leaq	(%r15,%rbx), %rsi
	addq	%rax, %rsi
	leaq	(,%rsi,4), %rax
	addq	%rbp, %rax
	movq	-48(%rsp), %rsi                 ## 8-byte Reload
	addq	%rbx, %rsi
	movq	-88(%rsp), %rbp                 ## 8-byte Reload
	leaq	(,%rsi,4), %rsi
	addq	%rbp, %rsi
	addq	%rbx, %r10
	movq	-80(%rsp), %rbx                 ## 8-byte Reload
	leaq	(%rbx,%r10,4), %rbx
	xorl	%ebp, %ebp
	.p2align	4, 0x90
LBB27_12:                               ## %"for relu1_0_d_def__.s19.n.ni.us.us"
                                        ##   Parent Loop BB27_6 Depth=1
                                        ## =>  This Inner Loop Header: Depth=2
	vmovss	(%rax,%rbp,4), %xmm2            ## xmm2 = mem[0],zero,zero,zero
	vmulss	(%rdi,%rbp,4), %xmm2, %xmm2
	vmulss	(%rsi,%rbp,4), %xmm2, %xmm2
	vfmadd213ss	(%rbx,%rbp,4), %xmm0, %xmm2 ## xmm2 = (xmm0 * xmm2) + mem
	vmovss	%xmm2, (%rbx,%rbp,4)
	incq	%rbp
	cmpq	%rbp, %rcx
	jne	LBB27_12
LBB27_13:                               ## %"end for relu1_0_d_def__.s19.n.ni.loopexit.us.us"
                                        ##   in Loop: Header=BB27_6 Depth=1
	movl	-92(%rsp), %edi                 ## 4-byte Reload
	testl	%edi, %edi
	movq	112(%rsp), %r10                 ## 8-byte Reload
	movl	-108(%rsp), %ebx                ## 4-byte Reload
	movq	-104(%rsp), %rbp                ## 8-byte Reload
	jle	LBB27_17
## %bb.14:                              ## %"for relu1_0_d_def__.s19.n.ni.rebased.preheader.us.us"
                                        ##   in Loop: Header=BB27_6 Depth=1
	movslq	%ebp, %rax
	imull	%r8d, %edx
	movslq	%edx, %rcx
	movq	16(%rsp), %rdx                  ## 8-byte Reload
	addq	%rcx, %rdx
	addq	8(%rsp), %rcx                   ## 8-byte Folded Reload
	movq	-64(%rsp), %rsi                 ## 8-byte Reload
	vmovss	-4(%rsi,%rdx,4), %xmm2          ## xmm2 = mem[0],zero,zero,zero
	vmulss	-4(%rsi,%rcx,4), %xmm2, %xmm2
	cmpl	$16, %edi
	jae	LBB27_18
## %bb.15:                              ##   in Loop: Header=BB27_6 Depth=1
	xorl	%ecx, %ecx
	jmp	LBB27_25
	.p2align	4, 0x90
LBB27_18:                               ## %vector.ph55
                                        ##   in Loop: Header=BB27_6 Depth=1
	vbroadcastss	%xmm2, %ymm3
	cmpq	$0, 72(%rsp)                    ## 8-byte Folded Reload
	je	LBB27_19
## %bb.20:                              ## %vector.body53.preheader
                                        ##   in Loop: Header=BB27_6 Depth=1
	movq	-8(%rsp), %rcx                  ## 8-byte Reload
	addq	%rax, %rcx
	movq	(%rsp), %rdx                    ## 8-byte Reload
	leaq	(%rdx,%rcx,4), %rcx
	movq	48(%rsp), %rdi                  ## 8-byte Reload
	xorl	%edx, %edx
	.p2align	4, 0x90
LBB27_21:                               ## %vector.body53
                                        ##   Parent Loop BB27_6 Depth=1
                                        ## =>  This Inner Loop Header: Depth=2
	vmulps	-96(%r9,%rdx,4), %ymm3, %ymm4
	vmulps	-64(%r9,%rdx,4), %ymm3, %ymm5
	vfmadd213ps	-96(%rcx,%rdx,4), %ymm1, %ymm4 ## ymm4 = (ymm1 * ymm4) + mem
	vfmadd213ps	-64(%rcx,%rdx,4), %ymm1, %ymm5 ## ymm5 = (ymm1 * ymm5) + mem
	vmovups	%ymm4, -96(%rcx,%rdx,4)
	vmovups	%ymm5, -64(%rcx,%rdx,4)
	vmulps	-32(%r9,%rdx,4), %ymm3, %ymm4
	vmulps	(%r9,%rdx,4), %ymm3, %ymm5
	vfmadd213ps	-32(%rcx,%rdx,4), %ymm1, %ymm4 ## ymm4 = (ymm1 * ymm4) + mem
	vfmadd213ps	(%rcx,%rdx,4), %ymm1, %ymm5 ## ymm5 = (ymm1 * ymm5) + mem
	vmovups	%ymm4, -32(%rcx,%rdx,4)
	vmovups	%ymm5, (%rcx,%rdx,4)
	addq	$32, %rdx
	addq	$2, %rdi
	jne	LBB27_21
## %bb.22:                              ## %middle.block51.unr-lcssa
                                        ##   in Loop: Header=BB27_6 Depth=1
	testb	$1, 64(%rsp)                    ## 1-byte Folded Reload
	je	LBB27_24
LBB27_23:                               ## %vector.body53.epil
                                        ##   in Loop: Header=BB27_6 Depth=1
	movq	-16(%rsp), %rcx                 ## 8-byte Reload
	movq	-120(%rsp), %rdi                ## 8-byte Reload
	addl	%edi, %ecx
	movq	-40(%rsp), %rsi                 ## 8-byte Reload
	addq	%rdi, %rsi
	imull	%ebx, %ecx
	imulq	%r10, %rsi
	addl	-52(%rsp), %ecx                 ## 4-byte Folded Reload
	movslq	%ecx, %rcx
	addq	-8(%rsp), %rdx                  ## 8-byte Folded Reload
	addq	%rdx, %rcx
	addq	%rsi, %rdx
	movq	-88(%rsp), %rsi                 ## 8-byte Reload
	vmulps	(%rsi,%rdx,4), %ymm3, %ymm4
	vmulps	32(%rsi,%rdx,4), %ymm3, %ymm3
	movq	-80(%rsp), %rdx                 ## 8-byte Reload
	vfmadd213ps	(%rdx,%rcx,4), %ymm1, %ymm4 ## ymm4 = (ymm1 * ymm4) + mem
	vfmadd213ps	32(%rdx,%rcx,4), %ymm1, %ymm3 ## ymm3 = (ymm1 * ymm3) + mem
	vmovups	%ymm4, (%rdx,%rcx,4)
	vmovups	%ymm3, 32(%rdx,%rcx,4)
LBB27_24:                               ## %middle.block51
                                        ##   in Loop: Header=BB27_6 Depth=1
	movq	80(%rsp), %rdx                  ## 8-byte Reload
	movq	%rdx, %rcx
	cmpq	%r13, %rdx
	je	LBB27_17
LBB27_25:                               ## %"for relu1_0_d_def__.s19.n.ni.rebased.us.us.preheader"
                                        ##   in Loop: Header=BB27_6 Depth=1
	movq	88(%rsp), %rdx                  ## 8-byte Reload
	leaq	(%rdx,%rax,4), %rax
	.p2align	4, 0x90
LBB27_16:                               ## %"for relu1_0_d_def__.s19.n.ni.rebased.us.us"
                                        ##   Parent Loop BB27_6 Depth=1
                                        ## =>  This Inner Loop Header: Depth=2
	vmulss	(%r11,%rcx,4), %xmm2, %xmm3
	vfmadd213ss	(%rax,%rcx,4), %xmm0, %xmm3 ## xmm3 = (xmm0 * xmm3) + mem
	vmovss	%xmm3, (%rax,%rcx,4)
	incq	%rcx
	cmpq	%rcx, %r13
	jne	LBB27_16
	jmp	LBB27_17
LBB27_19:                               ##   in Loop: Header=BB27_6 Depth=1
	xorl	%edx, %edx
	testb	$1, 64(%rsp)                    ## 1-byte Folded Reload
	jne	LBB27_23
	jmp	LBB27_24
LBB27_2:                                ## %"for relu1_0_d_def__.s19.w.wi.preheader20"
	movl	20(%rdi), %eax
	movl	48(%rdi), %esi
	movq	%r11, %r12
	movl	40(%rdi), %r10d
	subl	%eax, %r10d
	leal	(%rsi,%rax), %r8d
	subl	%esi, %r10d
	decl	%edx
	vmovd	%edx, %xmm0
	vmovd	%ebp, %xmm1
	imulq	%r11, %rcx
	addq	%rbp, %rcx
	movq	-88(%rsp), %rax                 ## 8-byte Reload
	leaq	(%rax,%rcx,4), %rdx
	shll	$6, %r15d
	movl	-120(%rsp), %eax                ## 4-byte Reload
	shll	$6, %eax
	subl	%eax, %r15d
	orl	$6, %r15d
	imull	%r15d, %r9d
	leal	(%r9,%r13,8), %ecx
	movl	24(%rdi), %r11d
	vpbroadcastd	%xmm1, %ymm1
	vpor	LCPI27_0(%rip), %ymm1, %ymm1
	vpbroadcastd	%xmm0, %ymm0
	vpminsd	%ymm1, %ymm0, %ymm0
	movslq	-104(%rsp), %rax                ## 4-byte Folded Reload
	movl	%r14d, %r14d
	shlq	$2, %r12
	vbroadcastss	LCPI27_1(%rip), %ymm1   ## ymm1 = [9.99999971E-10,9.99999971E-10,9.99999971E-10,9.99999971E-10,9.99999971E-10,9.99999971E-10,9.99999971E-10,9.99999971E-10]
	movl	-124(%rsp), %r9d                ## 4-byte Reload
	movl	-108(%rsp), %r15d               ## 4-byte Reload
	movq	-64(%rsp), %rbp                 ## 8-byte Reload
	.p2align	4, 0x90
LBB27_3:                                ## %"for relu1_0_d_def__.s19.w.wi"
                                        ## =>This Inner Loop Header: Depth=1
	testq	%rax, %rax
	movl	$0, %ebx
	cmovgl	%eax, %ebx
	movl	%ebx, %esi
	subl	%r11d, %esi
	imull	%r9d, %esi
	subl	%r8d, %esi
	vmovd	%esi, %xmm2
	vpbroadcastd	%xmm2, %ymm2
	vpaddd	%ymm0, %ymm2, %ymm3
	vextracti128	$1, %ymm3, %xmm2
	vmovd	%xmm2, %esi
	movslq	%esi, %rsi
	vpextrd	$1, %xmm2, %edi
	movslq	%edi, %rdi
	vmovss	(%rbp,%rsi,4), %xmm4            ## xmm4 = mem[0],zero,zero,zero
	vpextrd	$2, %xmm2, %esi
	vinsertps	$16, (%rbp,%rdi,4), %xmm4, %xmm4 ## xmm4 = xmm4[0],mem[0],xmm4[2,3]
	vmovd	%xmm3, %edi
	movslq	%esi, %rsi
	vinsertps	$32, (%rbp,%rsi,4), %xmm4, %xmm4 ## xmm4 = xmm4[0,1],mem[0],xmm4[3]
	movslq	%edi, %rsi
	vpextrd	$3, %xmm2, %edi
	movslq	%edi, %rdi
	vinsertps	$48, (%rbp,%rdi,4), %xmm4, %xmm2 ## xmm2 = xmm4[0,1,2],mem[0]
	vpextrd	$1, %xmm3, %edi
	vmovss	(%rbp,%rsi,4), %xmm4            ## xmm4 = mem[0],zero,zero,zero
	movslq	%edi, %rsi
	vinsertps	$16, (%rbp,%rsi,4), %xmm4, %xmm4 ## xmm4 = xmm4[0],mem[0],xmm4[2,3]
	vpextrd	$2, %xmm3, %esi
	movslq	%esi, %rsi
	vinsertps	$32, (%rbp,%rsi,4), %xmm4, %xmm4 ## xmm4 = xmm4[0,1],mem[0],xmm4[3]
	vpextrd	$3, %xmm3, %esi
	movslq	%esi, %rsi
	vinsertps	$48, (%rbp,%rsi,4), %xmm4, %xmm3 ## xmm3 = xmm4[0,1,2],mem[0]
	imull	%r9d, %ebx
	addl	%r10d, %ebx
	vmovd	%ebx, %xmm4
	vpbroadcastd	%xmm4, %ymm4
	vpaddd	%ymm0, %ymm4, %ymm4
	vextracti128	$1, %ymm4, %xmm5
	vmovd	%xmm5, %esi
	movslq	%esi, %rsi
	vmovss	(%rbp,%rsi,4), %xmm6            ## xmm6 = mem[0],zero,zero,zero
	vpextrd	$1, %xmm5, %esi
	movslq	%esi, %rsi
	vinsertps	$16, (%rbp,%rsi,4), %xmm6, %xmm6 ## xmm6 = xmm6[0],mem[0],xmm6[2,3]
	vpextrd	$2, %xmm5, %esi
	movslq	%esi, %rsi
	vinsertps	$32, (%rbp,%rsi,4), %xmm6, %xmm6 ## xmm6 = xmm6[0,1],mem[0],xmm6[3]
	vpextrd	$3, %xmm5, %esi
	movslq	%esi, %rsi
	vinsertps	$48, (%rbp,%rsi,4), %xmm6, %xmm5 ## xmm5 = xmm6[0,1,2],mem[0]
	vmovd	%xmm4, %esi
	movslq	%esi, %rsi
	vmovss	(%rbp,%rsi,4), %xmm6            ## xmm6 = mem[0],zero,zero,zero
	vpextrd	$1, %xmm4, %esi
	movslq	%esi, %rsi
	vinsertps	$16, (%rbp,%rsi,4), %xmm6, %xmm6 ## xmm6 = xmm6[0],mem[0],xmm6[2,3]
	vpextrd	$2, %xmm4, %esi
	movslq	%esi, %rsi
	vinsertps	$32, (%rbp,%rsi,4), %xmm6, %xmm6 ## xmm6 = xmm6[0,1],mem[0],xmm6[3]
	vpextrd	$3, %xmm4, %esi
	movslq	%esi, %rsi
	vinsertps	$48, (%rbp,%rsi,4), %xmm6, %xmm4 ## xmm4 = xmm6[0,1,2],mem[0]
	vinsertf128	$1, %xmm2, %ymm3, %ymm2
	movslq	%ecx, %rcx
	vinsertf128	$1, %xmm5, %ymm4, %ymm3
	vmulps	%ymm3, %ymm2, %ymm2
	vmulps	(%rdx), %ymm2, %ymm2
	movq	-80(%rsp), %rsi                 ## 8-byte Reload
	vfmadd213ps	(%rsi,%rcx,4), %ymm1, %ymm2 ## ymm2 = (ymm1 * ymm2) + mem
	vmovups	%ymm2, (%rsi,%rcx,4)
	addq	%r12, %rdx
	incq	%rax
	addl	%r15d, %ecx
	decq	%r14
	jne	LBB27_3
LBB27_40:                               ## %destructor_block
	xorl	%eax, %eax
	addq	$120, %rsp
	popq	%rbx
	popq	%r12
	popq	%r13
	popq	%r14
	popq	%r15
	popq	%rbp
	vzeroupper
	retq
LBB27_26:                               ## %"for relu1_0_d_def__.s19.w.wi.preheader.split.us.split"
	testl	%r12d, %r12d
	movl	-124(%rsp), %ecx                ## 4-byte Reload
	jle	LBB27_40
## %bb.27:                              ## %"for relu1_0_d_def__.s19.w.wi.us.us4.preheader"
	movq	%r11, %rsi
	movslq	%ebx, %r11
	movl	-92(%rsp), %r13d                ## 4-byte Reload
	movslq	-104(%rsp), %rax                ## 4-byte Folded Reload
	movq	%rax, -104(%rsp)                ## 8-byte Spill
	movl	%r14d, %r12d
	movl	%r13d, %r14d
	andl	$-16, %r14d
	leaq	-16(%r14), %rdx
	movq	%rdx, -72(%rsp)                 ## 8-byte Spill
	shrq	$4, %rdx
	incq	%rdx
	movq	%rsi, %rax
	imulq	-40(%rsp), %rax                 ## 8-byte Folded Reload
	addq	%r11, %rax
	movq	-88(%rsp), %rbp                 ## 8-byte Reload
	leaq	96(,%rax,4), %rdi
	addq	%rbp, %rdi
	leaq	(,%rsi,4), %r8
	movq	-80(%rsp), %r10                 ## 8-byte Reload
	leaq	96(%r10), %rbx
	movq	%rbx, -32(%rsp)                 ## 8-byte Spill
	shll	$6, %r15d
	movl	-120(%rsp), %ebx                ## 4-byte Reload
	shll	$6, %ebx
	subl	%ebx, %r15d
	orl	$6, %r15d
	imull	%r15d, %r9d
	movq	%rdx, %rbx
	movq	%rdx, -120(%rsp)                ## 8-byte Spill
	andq	$-2, %rdx
	negq	%rdx
	movq	%rdx, -48(%rsp)                 ## 8-byte Spill
	leaq	(%rbp,%rax,4), %rbp
	movq	%r11, %r15
	leaq	(%r10,%r11,4), %r11
	vmovss	LCPI27_1(%rip), %xmm0           ## xmm0 = mem[0],zero,zero,zero
	vbroadcastss	LCPI27_1(%rip), %ymm1   ## ymm1 = [9.99999971E-10,9.99999971E-10,9.99999971E-10,9.99999971E-10,9.99999971E-10,9.99999971E-10,9.99999971E-10,9.99999971E-10]
	xorl	%ebx, %ebx
	jmp	LBB27_28
LBB27_39:                               ## %after_bb.loopexit.us.us17
                                        ##   in Loop: Header=BB27_28 Depth=1
	incq	%rbx
	addq	%r8, %rdi
	addl	-108(%rsp), %r9d                ## 4-byte Folded Reload
	addq	%r8, %rbp
	cmpq	%r12, %rbx
	movl	-124(%rsp), %ecx                ## 4-byte Reload
	je	LBB27_40
LBB27_28:                               ## %"for relu1_0_d_def__.s19.w.wi.us.us4"
                                        ## =>This Loop Header: Depth=1
                                        ##     Child Loop BB27_33 Depth 2
                                        ##     Child Loop BB27_38 Depth 2
	movslq	%r9d, %r10
	movq	-104(%rsp), %rax                ## 8-byte Reload
	addq	%rbx, %rax
	testq	%rax, %rax
	movl	$0, %edx
	cmovlel	%edx, %eax
	imull	%ecx, %eax
	cltq
	movq	16(%rsp), %rcx                  ## 8-byte Reload
	addq	%rax, %rcx
	addq	8(%rsp), %rax                   ## 8-byte Folded Reload
	movq	-64(%rsp), %rdx                 ## 8-byte Reload
	vmovss	-4(%rdx,%rcx,4), %xmm2          ## xmm2 = mem[0],zero,zero,zero
	vmulss	-4(%rdx,%rax,4), %xmm2, %xmm2
	cmpl	$16, -92(%rsp)                  ## 4-byte Folded Reload
	jae	LBB27_30
## %bb.29:                              ##   in Loop: Header=BB27_28 Depth=1
	xorl	%eax, %eax
	jmp	LBB27_37
LBB27_30:                               ## %vector.ph
                                        ##   in Loop: Header=BB27_28 Depth=1
	vbroadcastss	%xmm2, %ymm3
	cmpq	$0, -72(%rsp)                   ## 8-byte Folded Reload
	je	LBB27_31
## %bb.32:                              ## %vector.body.preheader
                                        ##   in Loop: Header=BB27_28 Depth=1
	leaq	(%r15,%r10), %rax
	movq	-32(%rsp), %rcx                 ## 8-byte Reload
	leaq	(%rcx,%rax,4), %rcx
	movq	-48(%rsp), %rax                 ## 8-byte Reload
	xorl	%edx, %edx
LBB27_33:                               ## %vector.body
                                        ##   Parent Loop BB27_28 Depth=1
                                        ## =>  This Inner Loop Header: Depth=2
	vmulps	-96(%rdi,%rdx,4), %ymm3, %ymm4
	vmulps	-64(%rdi,%rdx,4), %ymm3, %ymm5
	vfmadd213ps	-96(%rcx,%rdx,4), %ymm1, %ymm4 ## ymm4 = (ymm1 * ymm4) + mem
	vfmadd213ps	-64(%rcx,%rdx,4), %ymm1, %ymm5 ## ymm5 = (ymm1 * ymm5) + mem
	vmovups	%ymm4, -96(%rcx,%rdx,4)
	vmovups	%ymm5, -64(%rcx,%rdx,4)
	vmulps	-32(%rdi,%rdx,4), %ymm3, %ymm4
	vmulps	(%rdi,%rdx,4), %ymm3, %ymm5
	vfmadd213ps	-32(%rcx,%rdx,4), %ymm1, %ymm4 ## ymm4 = (ymm1 * ymm4) + mem
	vfmadd213ps	(%rcx,%rdx,4), %ymm1, %ymm5 ## ymm5 = (ymm1 * ymm5) + mem
	vmovups	%ymm4, -32(%rcx,%rdx,4)
	vmovups	%ymm5, (%rcx,%rdx,4)
	addq	$32, %rdx
	addq	$2, %rax
	jne	LBB27_33
	jmp	LBB27_34
LBB27_31:                               ##   in Loop: Header=BB27_28 Depth=1
	xorl	%edx, %edx
LBB27_34:                               ## %middle.block.unr-lcssa
                                        ##   in Loop: Header=BB27_28 Depth=1
	testb	$1, -120(%rsp)                  ## 1-byte Folded Reload
	je	LBB27_36
## %bb.35:                              ## %vector.body.epil
                                        ##   in Loop: Header=BB27_28 Depth=1
	movq	-16(%rsp), %rax                 ## 8-byte Reload
	addl	%ebx, %eax
	imull	-108(%rsp), %eax                ## 4-byte Folded Reload
	movq	-40(%rsp), %rcx                 ## 8-byte Reload
	addq	%rbx, %rcx
	addl	-52(%rsp), %eax                 ## 4-byte Folded Reload
	imulq	%rsi, %rcx
	cltq
	addq	%r15, %rdx
	addq	%rdx, %rax
	addq	%rcx, %rdx
	movq	-88(%rsp), %rcx                 ## 8-byte Reload
	vmulps	(%rcx,%rdx,4), %ymm3, %ymm4
	vmulps	32(%rcx,%rdx,4), %ymm3, %ymm3
	movq	-80(%rsp), %rcx                 ## 8-byte Reload
	vfmadd213ps	(%rcx,%rax,4), %ymm1, %ymm4 ## ymm4 = (ymm1 * ymm4) + mem
	vfmadd213ps	32(%rcx,%rax,4), %ymm1, %ymm3 ## ymm3 = (ymm1 * ymm3) + mem
	vmovups	%ymm4, (%rcx,%rax,4)
	vmovups	%ymm3, 32(%rcx,%rax,4)
LBB27_36:                               ## %middle.block
                                        ##   in Loop: Header=BB27_28 Depth=1
	movq	%r14, %rax
	cmpq	%r13, %r14
	je	LBB27_39
LBB27_37:                               ## %"for relu1_0_d_def__.s19.n.ni.rebased.us.us9.preheader"
                                        ##   in Loop: Header=BB27_28 Depth=1
	leaq	(%r11,%r10,4), %rcx
LBB27_38:                               ## %"for relu1_0_d_def__.s19.n.ni.rebased.us.us9"
                                        ##   Parent Loop BB27_28 Depth=1
                                        ## =>  This Inner Loop Header: Depth=2
	vmulss	(%rbp,%rax,4), %xmm2, %xmm3
	vfmadd213ss	(%rcx,%rax,4), %xmm0, %xmm3 ## xmm3 = (xmm0 * xmm3) + mem
	vmovss	%xmm3, (%rcx,%rax,4)
	incq	%rax
	cmpq	%rax, %r13
	jne	LBB27_38
	jmp	LBB27_39
                                        ## -- End function
	.section	__TEXT,__const
	.p2align	5                               ## -- Begin function train_cost_model.par_for.relu1_0_d_def__.s20.n.n.n
LCPI28_0:
	.long	0                               ## 0x0
	.long	1                               ## 0x1
	.long	2                               ## 0x2
	.long	3                               ## 0x3
	.long	4                               ## 0x4
	.long	5                               ## 0x5
	.long	6                               ## 0x6
	.long	7                               ## 0x7
	.section	__TEXT,__literal4,4byte_literals
	.p2align	2
LCPI28_1:
	.long	0x3089705f                      ## float 9.99999971E-10
	.section	__TEXT,__text,regular,pure_instructions
	.p2align	4, 0x90
_train_cost_model.par_for.relu1_0_d_def__.s20.n.n.n: ## @train_cost_model.par_for.relu1_0_d_def__.s20.n.n.n
## %bb.0:                               ## %entry
	pushq	%rbp
	pushq	%r15
	pushq	%r14
	pushq	%r13
	pushq	%r12
	pushq	%rbx
	subq	$128, %rsp
	movq	%rdx, %rdi
	movl	12(%rdx), %r8d
	movl	32(%rdx), %r15d
	movl	%esi, %r10d
	sarl	$31, %r10d
	xorl	%ebx, %ebx
	testl	%r15d, %r15d
	sete	%bl
	movl	%ebx, %ecx
	negl	%ecx
	movl	%r15d, %ebp
	sarl	$31, %ebp
	subl	%r10d, %esi
	orl	%r15d, %ecx
	movl	%esi, %eax
	cltd
	idivl	%ecx
	movl	%edx, %ecx
	movl	%ebp, %r12d
	leal	(%r15,%rbx), %r9d
	movl	%esi, %eax
	cltd
	idivl	%r9d
	notl	%r12d
	decl	%ebx
	movl	%r12d, %r14d
	subl	%ebp, %r14d
	andl	%r10d, %r14d
	addl	%eax, %r14d
	andl	%ebx, %r14d
	leal	(%r14,%r14), %eax
	movl	%eax, -104(%rsp)                ## 4-byte Spill
	subl	%eax, %r8d
	cmpl	$2, %r8d
	movl	$2, %r11d
	cmovll	%r8d, %r11d
	xorl	%eax, %eax
	testl	%r8d, %r8d
	cmovlel	%eax, %r11d
	jle	LBB28_40
## %bb.1:                               ## %"for relu1_0_d_def__.s20.w.wi.preheader"
	movl	(%rdi), %edx
	movl	16(%rdi), %r9d
	movl	28(%rdi), %eax
	movl	%eax, -124(%rsp)                ## 4-byte Spill
	movl	36(%rdi), %eax
	movl	%eax, -120(%rsp)                ## 4-byte Spill
	movq	56(%rdi), %rax
	movq	%rax, -96(%rsp)                 ## 8-byte Spill
	movq	72(%rdi), %rax
	movq	%rax, -88(%rsp)                 ## 8-byte Spill
	movq	88(%rdi), %rax
	movq	%rax, -72(%rsp)                 ## 8-byte Spill
	xorl	%r15d, %ebp
	addl	%r12d, %ebp
	andl	%r10d, %ebp
	addl	%ebp, %ecx
	andl	%ebx, %ecx
	leal	(,%rcx,8), %ebx
	movslq	8(%rdi), %rax
	movslq	%r14d, %r8
	subq	%rax, %r8
	addq	%r8, %r8
	movq	%rcx, %r12
	movslq	4(%rdi), %r13
	leal	8(%rbx), %r10d
	movl	%r9d, %eax
	shll	$5, %eax
	movl	%eax, -112(%rsp)                ## 4-byte Spill
	movl	%edx, %eax
	subl	%ebx, %eax
	cmpl	$8, %eax
	movl	$8, %esi
	cmovll	%eax, %esi
	testl	%eax, %eax
	movl	$0, %ecx
	cmovgl	%esi, %ecx
	movl	%ebx, %ebp
	subl	%edx, %ebp
	movl	%ebp, %esi
	sarl	$31, %esi
	andl	%ebp, %esi
	cmpl	$-8, %esi
	movl	$-8, %ebp
	cmovgl	%esi, %ebp
	movslq	%ebx, %r15
	cmpl	%edx, %r10d
	jle	LBB28_2
## %bb.4:                               ## %"for relu1_0_d_def__.s20.w.wi.preheader.split.us"
	movq	%r13, -32(%rsp)                 ## 8-byte Spill
	movq	%r12, -80(%rsp)                 ## 8-byte Spill
	movslq	44(%rdi), %r10
	movl	52(%rdi), %esi
	leal	(%r9,%r9,4), %edi
	movl	%edi, -60(%rsp)                 ## 4-byte Spill
	movl	%r14d, %edi
	subl	-120(%rsp), %edi                ## 4-byte Folded Reload
	addl	%edi, %edi
	movq	%rdi, -8(%rsp)                  ## 8-byte Spill
	subl	%r10d, %edx
	addl	%ecx, %ebx
	addl	%ecx, %ebp
	movslq	%edx, %rdi
	movq	%rdi, 24(%rsp)                  ## 8-byte Spill
	addl	%esi, %edx
	movslq	%edx, %rdx
	movq	%rdx, 16(%rsp)                  ## 8-byte Spill
	testl	%eax, %eax
	movq	%r8, -40(%rsp)                  ## 8-byte Spill
	movl	%ebp, -108(%rsp)                ## 4-byte Spill
	jle	LBB28_26
## %bb.5:                               ## %"for relu1_0_d_def__.s20.w.wi.us.us.preheader"
	subl	%r10d, %esi
	leal	(%r15,%rsi), %eax
	cltq
	movq	%rax, 40(%rsp)                  ## 8-byte Spill
	movl	%ecx, -12(%rsp)                 ## 4-byte Spill
	movl	%ecx, %ecx
	movslq	%ebx, %rdx
	movl	%ebp, %r12d
	movslq	-104(%rsp), %r13                ## 4-byte Folded Reload
	movl	%r11d, %eax
	movq	%rax, 112(%rsp)                 ## 8-byte Spill
	movl	%r12d, %eax
	andl	$-16, %eax
	movq	%rax, 88(%rsp)                  ## 8-byte Spill
	addq	$-16, %rax
	movq	%rax, 80(%rsp)                  ## 8-byte Spill
	movq	%rax, %rbx
	shrq	$4, %rbx
	incq	%rbx
	movq	-32(%rsp), %rsi                 ## 8-byte Reload
	movq	%rsi, %rax
	imulq	%r8, %rax
	leaq	(%rax,%r15), %r8
	subq	%r10, %r15
	movq	%r15, -56(%rsp)                 ## 8-byte Spill
	movq	%rcx, 32(%rsp)                  ## 8-byte Spill
                                        ## kill: def $ecx killed $ecx killed $rcx def $rcx
	andl	$2147483616, %ecx               ## imm = 0x7FFFFFE0
	movq	%rcx, 56(%rsp)                  ## 8-byte Spill
	shll	$6, %r14d
	movl	-120(%rsp), %ecx                ## 4-byte Reload
	shll	$6, %ecx
	subl	%ecx, %r14d
	orl	$5, %r14d
	imull	%r14d, %r9d
	movq	%r9, -104(%rsp)                 ## 8-byte Spill
	movq	-80(%rsp), %rcx                 ## 8-byte Reload
	leal	(%r9,%rcx,8), %r10d
	addq	%rdx, %rax
	movq	-96(%rsp), %rcx                 ## 8-byte Reload
	leaq	(%rcx,%rax,4), %r9
	addq	$96, %r9
	movq	%rbx, %rbp
	movq	%rbx, 72(%rsp)                  ## 8-byte Spill
	andq	$-2, %rbx
	negq	%rbx
	movq	%rbx, 48(%rsp)                  ## 8-byte Spill
	leaq	(%rcx,%rax,4), %r11
	vmovss	LCPI28_1(%rip), %xmm0           ## xmm0 = mem[0],zero,zero,zero
	vbroadcastss	LCPI28_1(%rip), %ymm1   ## ymm1 = [9.99999971E-10,9.99999971E-10,9.99999971E-10,9.99999971E-10,9.99999971E-10,9.99999971E-10,9.99999971E-10,9.99999971E-10]
	movq	-72(%rsp), %rax                 ## 8-byte Reload
	addq	$96, %rax
	movq	%rax, 64(%rsp)                  ## 8-byte Spill
	movq	%r8, -48(%rsp)                  ## 8-byte Spill
	leaq	96(%rcx,%r8,4), %r14
	movl	%r10d, %ecx
	leaq	(,%rsi,4), %rax
	movq	%rax, 104(%rsp)                 ## 8-byte Spill
	movq	-88(%rsp), %rax                 ## 8-byte Reload
	leaq	96(%rax), %rbp
	movq	%rbp, 8(%rsp)                   ## 8-byte Spill
	movq	%rdx, (%rsp)                    ## 8-byte Spill
	leaq	(%rax,%rdx,4), %rax
	movq	%rax, 96(%rsp)                  ## 8-byte Spill
	movq	%r13, %rax
	movq	%r13, 120(%rsp)                 ## 8-byte Spill
	movq	%r13, %rdx
	xorl	%ebx, %ebx
	movl	-124(%rsp), %r15d               ## 4-byte Reload
	jmp	LBB28_6
	.p2align	4, 0x90
LBB28_17:                               ## %after_bb.us.us
                                        ##   in Loop: Header=BB28_6 Depth=1
	movq	-120(%rsp), %rbx                ## 8-byte Reload
	incq	%rbx
	movq	-24(%rsp), %rdx                 ## 8-byte Reload
	incq	%rdx
	movq	104(%rsp), %rax                 ## 8-byte Reload
	addq	%rax, %r14
	movl	-80(%rsp), %ecx                 ## 4-byte Reload
	addl	%r13d, %ecx
	addq	%rsi, -48(%rsp)                 ## 8-byte Folded Spill
	addq	%rax, %r9
	addl	%r13d, %ebp
	movq	%rbp, -104(%rsp)                ## 8-byte Spill
	addq	%rax, %r11
	cmpq	112(%rsp), %rbx                 ## 8-byte Folded Reload
	je	LBB28_40
LBB28_6:                                ## %"for relu1_0_d_def__.s20.w.wi.us.us"
                                        ## =>This Loop Header: Depth=1
                                        ##     Child Loop BB28_9 Depth 2
                                        ##     Child Loop BB28_12 Depth 2
                                        ##     Child Loop BB28_21 Depth 2
                                        ##     Child Loop BB28_16 Depth 2
	testq	%rdx, %rdx
	movl	$0, %eax
	movq	%rdx, -24(%rsp)                 ## 8-byte Spill
	cmovgl	%edx, %eax
	imull	%r15d, %eax
	movslq	%eax, %rdx
	movq	120(%rsp), %rax                 ## 8-byte Reload
	movq	%rbx, -120(%rsp)                ## 8-byte Spill
	leaq	(%rbx,%rax), %r10
	testq	%r10, %r10
	movl	$0, %eax
	cmovlel	%eax, %r10d
	movl	%ecx, -80(%rsp)                 ## 4-byte Spill
	movslq	%ecx, %rdi
	cmpl	$32, -12(%rsp)                  ## 4-byte Folded Reload
	jae	LBB28_8
## %bb.7:                               ##   in Loop: Header=BB28_6 Depth=1
	xorl	%ebp, %ebp
	movq	-96(%rsp), %r8                  ## 8-byte Reload
	jmp	LBB28_11
	.p2align	4, 0x90
LBB28_8:                                ## %vector.body72.preheader
                                        ##   in Loop: Header=BB28_6 Depth=1
	movq	40(%rsp), %rax                  ## 8-byte Reload
	leaq	(%rax,%rdx), %rcx
	movq	-56(%rsp), %rax                 ## 8-byte Reload
	movq	%rdx, %rbx
	leaq	(%rax,%rdx), %rsi
	movq	64(%rsp), %rax                  ## 8-byte Reload
	leaq	(%rax,%rcx,4), %r15
	leaq	(%rax,%rsi,4), %rcx
	movq	8(%rsp), %rax                   ## 8-byte Reload
	movq	%rdi, %rsi
	leaq	(%rax,%rdi,4), %r13
	xorl	%r8d, %r8d
	movq	56(%rsp), %rax                  ## 8-byte Reload
	.p2align	4, 0x90
LBB28_9:                                ## %vector.body72
                                        ##   Parent Loop BB28_6 Depth=1
                                        ## =>  This Inner Loop Header: Depth=2
	vmovups	-96(%rcx,%r8,4), %ymm2
	vmovups	-64(%rcx,%r8,4), %ymm3
	vmovups	-32(%rcx,%r8,4), %ymm4
	vmovups	(%rcx,%r8,4), %ymm5
	vmulps	-96(%r15,%r8,4), %ymm2, %ymm2
	vmulps	-64(%r15,%r8,4), %ymm3, %ymm3
	vmulps	-32(%r15,%r8,4), %ymm4, %ymm4
	vmulps	(%r15,%r8,4), %ymm5, %ymm5
	vmulps	-96(%r14,%r8,4), %ymm2, %ymm2
	vmulps	-64(%r14,%r8,4), %ymm3, %ymm3
	vmulps	-32(%r14,%r8,4), %ymm4, %ymm4
	vmulps	(%r14,%r8,4), %ymm5, %ymm5
	vfmadd213ps	-96(%r13,%r8,4), %ymm1, %ymm2 ## ymm2 = (ymm1 * ymm2) + mem
	vfmadd213ps	-64(%r13,%r8,4), %ymm1, %ymm3 ## ymm3 = (ymm1 * ymm3) + mem
	vfmadd213ps	-32(%r13,%r8,4), %ymm1, %ymm4 ## ymm4 = (ymm1 * ymm4) + mem
	vfmadd213ps	(%r13,%r8,4), %ymm1, %ymm5 ## ymm5 = (ymm1 * ymm5) + mem
	vmovups	%ymm2, -96(%r13,%r8,4)
	vmovups	%ymm3, -64(%r13,%r8,4)
	vmovups	%ymm4, -32(%r13,%r8,4)
	vmovups	%ymm5, (%r13,%r8,4)
	addq	$32, %r8
	cmpq	%r8, %rax
	jne	LBB28_9
## %bb.10:                              ## %middle.block70
                                        ##   in Loop: Header=BB28_6 Depth=1
	movq	%rax, %rbp
	cmpq	32(%rsp), %rax                  ## 8-byte Folded Reload
	movl	-124(%rsp), %r15d               ## 4-byte Reload
	movq	-96(%rsp), %r8                  ## 8-byte Reload
	movq	%rbx, %rdx
	movq	%rsi, %rdi
	je	LBB28_13
LBB28_11:                               ## %"for relu1_0_d_def__.s20.n.ni.us.us.preheader"
                                        ##   in Loop: Header=BB28_6 Depth=1
	movq	32(%rsp), %rcx                  ## 8-byte Reload
	subq	%rbp, %rcx
	movq	40(%rsp), %rax                  ## 8-byte Reload
	leaq	(%rax,%rbp), %rsi
	addq	%rdx, %rsi
	movq	-72(%rsp), %rax                 ## 8-byte Reload
	leaq	(%rax,%rsi,4), %rsi
	movq	-56(%rsp), %rbx                 ## 8-byte Reload
	addq	%rbp, %rbx
	addq	%rdx, %rbx
	leaq	(%rax,%rbx,4), %rax
	movq	-48(%rsp), %rbx                 ## 8-byte Reload
	addq	%rbp, %rbx
	leaq	(%r8,%rbx,4), %rbx
	addq	%rbp, %rdi
	movq	-88(%rsp), %rbp                 ## 8-byte Reload
	leaq	(%rbp,%rdi,4), %rbp
	xorl	%edx, %edx
	.p2align	4, 0x90
LBB28_12:                               ## %"for relu1_0_d_def__.s20.n.ni.us.us"
                                        ##   Parent Loop BB28_6 Depth=1
                                        ## =>  This Inner Loop Header: Depth=2
	vmovss	(%rax,%rdx,4), %xmm2            ## xmm2 = mem[0],zero,zero,zero
	vmulss	(%rsi,%rdx,4), %xmm2, %xmm2
	vmulss	(%rbx,%rdx,4), %xmm2, %xmm2
	vfmadd213ss	(%rbp,%rdx,4), %xmm0, %xmm2 ## xmm2 = (xmm0 * xmm2) + mem
	vmovss	%xmm2, (%rbp,%rdx,4)
	incq	%rdx
	cmpq	%rdx, %rcx
	jne	LBB28_12
LBB28_13:                               ## %"end for relu1_0_d_def__.s20.n.ni.loopexit.us.us"
                                        ##   in Loop: Header=BB28_6 Depth=1
	cmpl	$0, -108(%rsp)                  ## 4-byte Folded Reload
	movq	-32(%rsp), %rsi                 ## 8-byte Reload
	movl	-112(%rsp), %r13d               ## 4-byte Reload
	movq	-104(%rsp), %rbp                ## 8-byte Reload
	jle	LBB28_17
## %bb.14:                              ## %"for relu1_0_d_def__.s20.n.ni.rebased.preheader.us.us"
                                        ##   in Loop: Header=BB28_6 Depth=1
	movslq	%ebp, %r8
	imull	%r15d, %r10d
	movslq	%r10d, %rax
	movq	24(%rsp), %rcx                  ## 8-byte Reload
	addq	%rax, %rcx
	addq	16(%rsp), %rax                  ## 8-byte Folded Reload
	movq	-72(%rsp), %rdx                 ## 8-byte Reload
	vmovss	-4(%rdx,%rcx,4), %xmm2          ## xmm2 = mem[0],zero,zero,zero
	vmulss	-4(%rdx,%rax,4), %xmm2, %xmm2
	cmpl	$16, -108(%rsp)                 ## 4-byte Folded Reload
	jae	LBB28_18
## %bb.15:                              ##   in Loop: Header=BB28_6 Depth=1
	xorl	%eax, %eax
	jmp	LBB28_25
	.p2align	4, 0x90
LBB28_18:                               ## %vector.ph55
                                        ##   in Loop: Header=BB28_6 Depth=1
	vbroadcastss	%xmm2, %ymm3
	cmpq	$0, 80(%rsp)                    ## 8-byte Folded Reload
	je	LBB28_19
## %bb.20:                              ## %vector.body53.preheader
                                        ##   in Loop: Header=BB28_6 Depth=1
	movq	(%rsp), %rax                    ## 8-byte Reload
	addq	%r8, %rax
	movq	8(%rsp), %rcx                   ## 8-byte Reload
	leaq	(%rcx,%rax,4), %rcx
	movq	48(%rsp), %rax                  ## 8-byte Reload
	xorl	%edx, %edx
	.p2align	4, 0x90
LBB28_21:                               ## %vector.body53
                                        ##   Parent Loop BB28_6 Depth=1
                                        ## =>  This Inner Loop Header: Depth=2
	vmulps	-96(%r9,%rdx,4), %ymm3, %ymm4
	vmulps	-64(%r9,%rdx,4), %ymm3, %ymm5
	vfmadd213ps	-96(%rcx,%rdx,4), %ymm1, %ymm4 ## ymm4 = (ymm1 * ymm4) + mem
	vfmadd213ps	-64(%rcx,%rdx,4), %ymm1, %ymm5 ## ymm5 = (ymm1 * ymm5) + mem
	vmovups	%ymm4, -96(%rcx,%rdx,4)
	vmovups	%ymm5, -64(%rcx,%rdx,4)
	vmulps	-32(%r9,%rdx,4), %ymm3, %ymm4
	vmulps	(%r9,%rdx,4), %ymm3, %ymm5
	vfmadd213ps	-32(%rcx,%rdx,4), %ymm1, %ymm4 ## ymm4 = (ymm1 * ymm4) + mem
	vfmadd213ps	(%rcx,%rdx,4), %ymm1, %ymm5 ## ymm5 = (ymm1 * ymm5) + mem
	vmovups	%ymm4, -32(%rcx,%rdx,4)
	vmovups	%ymm5, (%rcx,%rdx,4)
	addq	$32, %rdx
	addq	$2, %rax
	jne	LBB28_21
## %bb.22:                              ## %middle.block51.unr-lcssa
                                        ##   in Loop: Header=BB28_6 Depth=1
	testb	$1, 72(%rsp)                    ## 1-byte Folded Reload
	je	LBB28_24
LBB28_23:                               ## %vector.body53.epil
                                        ##   in Loop: Header=BB28_6 Depth=1
	movq	-8(%rsp), %rax                  ## 8-byte Reload
	movq	-120(%rsp), %rbx                ## 8-byte Reload
	addl	%ebx, %eax
	movq	-40(%rsp), %rcx                 ## 8-byte Reload
	addq	%rbx, %rcx
	imull	%r13d, %eax
	imulq	%rsi, %rcx
	addl	-60(%rsp), %eax                 ## 4-byte Folded Reload
	cltq
	addq	(%rsp), %rdx                    ## 8-byte Folded Reload
	addq	%rdx, %rax
	addq	%rcx, %rdx
	movq	-96(%rsp), %rcx                 ## 8-byte Reload
	vmulps	(%rcx,%rdx,4), %ymm3, %ymm4
	vmulps	32(%rcx,%rdx,4), %ymm3, %ymm3
	movq	-88(%rsp), %rcx                 ## 8-byte Reload
	vfmadd213ps	(%rcx,%rax,4), %ymm1, %ymm4 ## ymm4 = (ymm1 * ymm4) + mem
	vfmadd213ps	32(%rcx,%rax,4), %ymm1, %ymm3 ## ymm3 = (ymm1 * ymm3) + mem
	vmovups	%ymm4, (%rcx,%rax,4)
	vmovups	%ymm3, 32(%rcx,%rax,4)
LBB28_24:                               ## %middle.block51
                                        ##   in Loop: Header=BB28_6 Depth=1
	movq	88(%rsp), %rcx                  ## 8-byte Reload
	movq	%rcx, %rax
	cmpq	%r12, %rcx
	je	LBB28_17
LBB28_25:                               ## %"for relu1_0_d_def__.s20.n.ni.rebased.us.us.preheader"
                                        ##   in Loop: Header=BB28_6 Depth=1
	movq	96(%rsp), %rcx                  ## 8-byte Reload
	leaq	(%rcx,%r8,4), %rcx
	.p2align	4, 0x90
LBB28_16:                               ## %"for relu1_0_d_def__.s20.n.ni.rebased.us.us"
                                        ##   Parent Loop BB28_6 Depth=1
                                        ## =>  This Inner Loop Header: Depth=2
	vmulss	(%r11,%rax,4), %xmm2, %xmm3
	vfmadd213ss	(%rcx,%rax,4), %xmm0, %xmm3 ## xmm3 = (xmm0 * xmm3) + mem
	vmovss	%xmm3, (%rcx,%rax,4)
	incq	%rax
	cmpq	%rax, %r12
	jne	LBB28_16
	jmp	LBB28_17
LBB28_19:                               ##   in Loop: Header=BB28_6 Depth=1
	xorl	%edx, %edx
	testb	$1, 72(%rsp)                    ## 1-byte Folded Reload
	jne	LBB28_23
	jmp	LBB28_24
LBB28_2:                                ## %"for relu1_0_d_def__.s20.w.wi.preheader20"
	movl	20(%rdi), %eax
	movl	48(%rdi), %esi
	movl	40(%rdi), %r10d
	subl	%eax, %r10d
	movq	%r15, %rcx
	leal	(%rsi,%rax), %r15d
	subl	%esi, %r10d
	decl	%edx
	vmovd	%edx, %xmm0
	vmovd	%ecx, %xmm1
	imulq	%r13, %r8
	addq	%rcx, %r8
	movq	-96(%rsp), %rax                 ## 8-byte Reload
	leaq	(%rax,%r8,4), %rdx
	shll	$6, %r14d
	movl	-120(%rsp), %eax                ## 4-byte Reload
	shll	$6, %eax
	subl	%eax, %r14d
	orl	$5, %r14d
	imull	%r14d, %r9d
	leal	(%r9,%r12,8), %ecx
	movl	24(%rdi), %r8d
	vpbroadcastd	%xmm1, %ymm1
	vpor	LCPI28_0(%rip), %ymm1, %ymm1
	vpbroadcastd	%xmm0, %ymm0
	vpminsd	%ymm1, %ymm0, %ymm0
	movslq	-104(%rsp), %rax                ## 4-byte Folded Reload
	movl	%r11d, %r11d
	shlq	$2, %r13
	vbroadcastss	LCPI28_1(%rip), %ymm1   ## ymm1 = [9.99999971E-10,9.99999971E-10,9.99999971E-10,9.99999971E-10,9.99999971E-10,9.99999971E-10,9.99999971E-10,9.99999971E-10]
	movl	-124(%rsp), %r9d                ## 4-byte Reload
	movl	-112(%rsp), %r14d               ## 4-byte Reload
	movq	-72(%rsp), %rbp                 ## 8-byte Reload
	.p2align	4, 0x90
LBB28_3:                                ## %"for relu1_0_d_def__.s20.w.wi"
                                        ## =>This Inner Loop Header: Depth=1
	testq	%rax, %rax
	movl	$0, %ebx
	cmovgl	%eax, %ebx
	movl	%ebx, %esi
	subl	%r8d, %esi
	imull	%r9d, %esi
	subl	%r15d, %esi
	vmovd	%esi, %xmm2
	vpbroadcastd	%xmm2, %ymm2
	vpaddd	%ymm0, %ymm2, %ymm3
	vextracti128	$1, %ymm3, %xmm2
	vmovd	%xmm2, %esi
	movslq	%esi, %rsi
	vpextrd	$1, %xmm2, %edi
	movslq	%edi, %rdi
	vmovss	(%rbp,%rsi,4), %xmm4            ## xmm4 = mem[0],zero,zero,zero
	vpextrd	$2, %xmm2, %esi
	vinsertps	$16, (%rbp,%rdi,4), %xmm4, %xmm4 ## xmm4 = xmm4[0],mem[0],xmm4[2,3]
	vmovd	%xmm3, %edi
	movslq	%esi, %rsi
	vinsertps	$32, (%rbp,%rsi,4), %xmm4, %xmm4 ## xmm4 = xmm4[0,1],mem[0],xmm4[3]
	movslq	%edi, %rsi
	vpextrd	$3, %xmm2, %edi
	movslq	%edi, %rdi
	vinsertps	$48, (%rbp,%rdi,4), %xmm4, %xmm2 ## xmm2 = xmm4[0,1,2],mem[0]
	vpextrd	$1, %xmm3, %edi
	vmovss	(%rbp,%rsi,4), %xmm4            ## xmm4 = mem[0],zero,zero,zero
	movslq	%edi, %rsi
	vinsertps	$16, (%rbp,%rsi,4), %xmm4, %xmm4 ## xmm4 = xmm4[0],mem[0],xmm4[2,3]
	vpextrd	$2, %xmm3, %esi
	movslq	%esi, %rsi
	vinsertps	$32, (%rbp,%rsi,4), %xmm4, %xmm4 ## xmm4 = xmm4[0,1],mem[0],xmm4[3]
	vpextrd	$3, %xmm3, %esi
	movslq	%esi, %rsi
	vinsertps	$48, (%rbp,%rsi,4), %xmm4, %xmm3 ## xmm3 = xmm4[0,1,2],mem[0]
	imull	%r9d, %ebx
	addl	%r10d, %ebx
	vmovd	%ebx, %xmm4
	vpbroadcastd	%xmm4, %ymm4
	vpaddd	%ymm0, %ymm4, %ymm4
	vextracti128	$1, %ymm4, %xmm5
	vmovd	%xmm5, %esi
	movslq	%esi, %rsi
	vmovss	(%rbp,%rsi,4), %xmm6            ## xmm6 = mem[0],zero,zero,zero
	vpextrd	$1, %xmm5, %esi
	movslq	%esi, %rsi
	vinsertps	$16, (%rbp,%rsi,4), %xmm6, %xmm6 ## xmm6 = xmm6[0],mem[0],xmm6[2,3]
	vpextrd	$2, %xmm5, %esi
	movslq	%esi, %rsi
	vinsertps	$32, (%rbp,%rsi,4), %xmm6, %xmm6 ## xmm6 = xmm6[0,1],mem[0],xmm6[3]
	vpextrd	$3, %xmm5, %esi
	movslq	%esi, %rsi
	vinsertps	$48, (%rbp,%rsi,4), %xmm6, %xmm5 ## xmm5 = xmm6[0,1,2],mem[0]
	vmovd	%xmm4, %esi
	movslq	%esi, %rsi
	vmovss	(%rbp,%rsi,4), %xmm6            ## xmm6 = mem[0],zero,zero,zero
	vpextrd	$1, %xmm4, %esi
	movslq	%esi, %rsi
	vinsertps	$16, (%rbp,%rsi,4), %xmm6, %xmm6 ## xmm6 = xmm6[0],mem[0],xmm6[2,3]
	vpextrd	$2, %xmm4, %esi
	movslq	%esi, %rsi
	vinsertps	$32, (%rbp,%rsi,4), %xmm6, %xmm6 ## xmm6 = xmm6[0,1],mem[0],xmm6[3]
	vpextrd	$3, %xmm4, %esi
	movslq	%esi, %rsi
	vinsertps	$48, (%rbp,%rsi,4), %xmm6, %xmm4 ## xmm4 = xmm6[0,1,2],mem[0]
	vinsertf128	$1, %xmm2, %ymm3, %ymm2
	movslq	%ecx, %rcx
	vinsertf128	$1, %xmm5, %ymm4, %ymm3
	vmulps	%ymm3, %ymm2, %ymm2
	vmulps	(%rdx), %ymm2, %ymm2
	movq	-88(%rsp), %rsi                 ## 8-byte Reload
	vfmadd213ps	(%rsi,%rcx,4), %ymm1, %ymm2 ## ymm2 = (ymm1 * ymm2) + mem
	vmovups	%ymm2, (%rsi,%rcx,4)
	addq	%r13, %rdx
	incq	%rax
	addl	%r14d, %ecx
	decq	%r11
	jne	LBB28_3
LBB28_40:                               ## %destructor_block
	xorl	%eax, %eax
	addq	$128, %rsp
	popq	%rbx
	popq	%r12
	popq	%r13
	popq	%r14
	popq	%r15
	popq	%rbp
	vzeroupper
	retq
LBB28_26:                               ## %"for relu1_0_d_def__.s20.w.wi.preheader.split.us.split"
	testl	%ebp, %ebp
	movl	-124(%rsp), %ecx                ## 4-byte Reload
	movq	-32(%rsp), %rsi                 ## 8-byte Reload
	jle	LBB28_40
## %bb.27:                              ## %"for relu1_0_d_def__.s20.w.wi.us.us4.preheader"
	movq	%r9, %r12
	movslq	%ebx, %rdi
	movq	%rdi, -56(%rsp)                 ## 8-byte Spill
	movl	-108(%rsp), %r13d               ## 4-byte Reload
	movslq	-104(%rsp), %rax                ## 4-byte Folded Reload
	movq	%rax, -104(%rsp)                ## 8-byte Spill
	movl	%r11d, %r11d
	movl	%r13d, %r9d
	andl	$-16, %r9d
	leaq	-16(%r9), %rdx
	movq	%rdx, -80(%rsp)                 ## 8-byte Spill
	shrq	$4, %rdx
	incq	%rdx
	movq	%rsi, %rax
	imulq	-40(%rsp), %rax                 ## 8-byte Folded Reload
	addq	%rdi, %rax
	movq	-96(%rsp), %rbp                 ## 8-byte Reload
	leaq	96(,%rax,4), %rdi
	addq	%rbp, %rdi
	leaq	(,%rsi,4), %r15
	movq	-88(%rsp), %r10                 ## 8-byte Reload
	leaq	96(%r10), %rbx
	movq	%rbx, -24(%rsp)                 ## 8-byte Spill
	shll	$6, %r14d
	movl	-120(%rsp), %r8d                ## 4-byte Reload
	shll	$6, %r8d
	subl	%r8d, %r14d
	orl	$5, %r14d
	imull	%r14d, %r12d
	movq	%rdx, %rbx
	movq	%rdx, -120(%rsp)                ## 8-byte Spill
	andq	$-2, %rdx
	negq	%rdx
	movq	%rdx, -48(%rsp)                 ## 8-byte Spill
	leaq	(,%rax,4), %rbx
	addq	%rbp, %rbx
	movq	-56(%rsp), %rax                 ## 8-byte Reload
	movq	%rax, %r8
	leaq	(%r10,%rax,4), %r14
	vmovss	LCPI28_1(%rip), %xmm0           ## xmm0 = mem[0],zero,zero,zero
	vbroadcastss	LCPI28_1(%rip), %ymm1   ## ymm1 = [9.99999971E-10,9.99999971E-10,9.99999971E-10,9.99999971E-10,9.99999971E-10,9.99999971E-10,9.99999971E-10,9.99999971E-10]
	xorl	%ebp, %ebp
	jmp	LBB28_28
LBB28_39:                               ## %after_bb.loopexit.us.us17
                                        ##   in Loop: Header=BB28_28 Depth=1
	incq	%rbp
	addq	%r15, %rdi
	addl	-112(%rsp), %r12d               ## 4-byte Folded Reload
	addq	%r15, %rbx
	cmpq	%r11, %rbp
	movl	-124(%rsp), %ecx                ## 4-byte Reload
	je	LBB28_40
LBB28_28:                               ## %"for relu1_0_d_def__.s20.w.wi.us.us4"
                                        ## =>This Loop Header: Depth=1
                                        ##     Child Loop BB28_33 Depth 2
                                        ##     Child Loop BB28_38 Depth 2
	movslq	%r12d, %r10
	movq	-104(%rsp), %rax                ## 8-byte Reload
	addq	%rbp, %rax
	testq	%rax, %rax
	movl	$0, %edx
	cmovlel	%edx, %eax
	imull	%ecx, %eax
	cltq
	movq	24(%rsp), %rcx                  ## 8-byte Reload
	addq	%rax, %rcx
	addq	16(%rsp), %rax                  ## 8-byte Folded Reload
	movq	-72(%rsp), %rdx                 ## 8-byte Reload
	vmovss	-4(%rdx,%rcx,4), %xmm2          ## xmm2 = mem[0],zero,zero,zero
	vmulss	-4(%rdx,%rax,4), %xmm2, %xmm2
	cmpl	$16, -108(%rsp)                 ## 4-byte Folded Reload
	jae	LBB28_30
## %bb.29:                              ##   in Loop: Header=BB28_28 Depth=1
	xorl	%eax, %eax
	jmp	LBB28_37
LBB28_30:                               ## %vector.ph
                                        ##   in Loop: Header=BB28_28 Depth=1
	vbroadcastss	%xmm2, %ymm3
	cmpq	$0, -80(%rsp)                   ## 8-byte Folded Reload
	je	LBB28_31
## %bb.32:                              ## %vector.body.preheader
                                        ##   in Loop: Header=BB28_28 Depth=1
	leaq	(%r8,%r10), %rax
	movq	-24(%rsp), %rcx                 ## 8-byte Reload
	leaq	(%rcx,%rax,4), %rcx
	movq	-48(%rsp), %rax                 ## 8-byte Reload
	xorl	%edx, %edx
LBB28_33:                               ## %vector.body
                                        ##   Parent Loop BB28_28 Depth=1
                                        ## =>  This Inner Loop Header: Depth=2
	vmulps	-96(%rdi,%rdx,4), %ymm3, %ymm4
	vmulps	-64(%rdi,%rdx,4), %ymm3, %ymm5
	vfmadd213ps	-96(%rcx,%rdx,4), %ymm1, %ymm4 ## ymm4 = (ymm1 * ymm4) + mem
	vfmadd213ps	-64(%rcx,%rdx,4), %ymm1, %ymm5 ## ymm5 = (ymm1 * ymm5) + mem
	vmovups	%ymm4, -96(%rcx,%rdx,4)
	vmovups	%ymm5, -64(%rcx,%rdx,4)
	vmulps	-32(%rdi,%rdx,4), %ymm3, %ymm4
	vmulps	(%rdi,%rdx,4), %ymm3, %ymm5
	vfmadd213ps	-32(%rcx,%rdx,4), %ymm1, %ymm4 ## ymm4 = (ymm1 * ymm4) + mem
	vfmadd213ps	(%rcx,%rdx,4), %ymm1, %ymm5 ## ymm5 = (ymm1 * ymm5) + mem
	vmovups	%ymm4, -32(%rcx,%rdx,4)
	vmovups	%ymm5, (%rcx,%rdx,4)
	addq	$32, %rdx
	addq	$2, %rax
	jne	LBB28_33
	jmp	LBB28_34
LBB28_31:                               ##   in Loop: Header=BB28_28 Depth=1
	xorl	%edx, %edx
LBB28_34:                               ## %middle.block.unr-lcssa
                                        ##   in Loop: Header=BB28_28 Depth=1
	testb	$1, -120(%rsp)                  ## 1-byte Folded Reload
	je	LBB28_36
## %bb.35:                              ## %vector.body.epil
                                        ##   in Loop: Header=BB28_28 Depth=1
	movq	-8(%rsp), %rax                  ## 8-byte Reload
	addl	%ebp, %eax
	imull	-112(%rsp), %eax                ## 4-byte Folded Reload
	movq	-40(%rsp), %rcx                 ## 8-byte Reload
	addq	%rbp, %rcx
	addl	-60(%rsp), %eax                 ## 4-byte Folded Reload
	imulq	%rsi, %rcx
	cltq
	addq	%r8, %rdx
	addq	%rdx, %rax
	addq	%rcx, %rdx
	movq	-96(%rsp), %rcx                 ## 8-byte Reload
	vmulps	(%rcx,%rdx,4), %ymm3, %ymm4
	vmulps	32(%rcx,%rdx,4), %ymm3, %ymm3
	movq	-88(%rsp), %rcx                 ## 8-byte Reload
	vfmadd213ps	(%rcx,%rax,4), %ymm1, %ymm4 ## ymm4 = (ymm1 * ymm4) + mem
	vfmadd213ps	32(%rcx,%rax,4), %ymm1, %ymm3 ## ymm3 = (ymm1 * ymm3) + mem
	vmovups	%ymm4, (%rcx,%rax,4)
	vmovups	%ymm3, 32(%rcx,%rax,4)
LBB28_36:                               ## %middle.block
                                        ##   in Loop: Header=BB28_28 Depth=1
	movq	%r9, %rax
	cmpq	%r13, %r9
	je	LBB28_39
LBB28_37:                               ## %"for relu1_0_d_def__.s20.n.ni.rebased.us.us9.preheader"
                                        ##   in Loop: Header=BB28_28 Depth=1
	leaq	(%r14,%r10,4), %rcx
LBB28_38:                               ## %"for relu1_0_d_def__.s20.n.ni.rebased.us.us9"
                                        ##   Parent Loop BB28_28 Depth=1
                                        ## =>  This Inner Loop Header: Depth=2
	vmulss	(%rbx,%rax,4), %xmm2, %xmm3
	vfmadd213ss	(%rcx,%rax,4), %xmm0, %xmm3 ## xmm3 = (xmm0 * xmm3) + mem
	vmovss	%xmm3, (%rcx,%rax,4)
	incq	%rax
	cmpq	%rax, %r13
	jne	LBB28_38
	jmp	LBB28_39
                                        ## -- End function
	.section	__TEXT,__const
	.p2align	5                               ## -- Begin function train_cost_model.par_for.relu1_0_d_def__.s21.n.n.n
LCPI29_0:
	.long	0                               ## 0x0
	.long	1                               ## 0x1
	.long	2                               ## 0x2
	.long	3                               ## 0x3
	.long	4                               ## 0x4
	.long	5                               ## 0x5
	.long	6                               ## 0x6
	.long	7                               ## 0x7
	.section	__TEXT,__literal4,4byte_literals
	.p2align	2
LCPI29_1:
	.long	0x3f800000                      ## float 1
LCPI29_2:
	.long	0x45800000                      ## float 4096
LCPI29_3:
	.long	0x3109705f                      ## float 1.99999994E-9
	.section	__TEXT,__text,regular,pure_instructions
	.p2align	4, 0x90
_train_cost_model.par_for.relu1_0_d_def__.s21.n.n.n: ## @train_cost_model.par_for.relu1_0_d_def__.s21.n.n.n
## %bb.0:                               ## %entry
	pushq	%rbp
	pushq	%r15
	pushq	%r14
	pushq	%r13
	pushq	%r12
	pushq	%rbx
	subq	$168, %rsp
	movq	%rdx, %rdi
	movl	12(%rdx), %r10d
	movl	24(%rdx), %r9d
	movl	%esi, %r8d
	sarl	$31, %r8d
	xorl	%ecx, %ecx
	testl	%r9d, %r9d
	sete	%cl
	movl	%ecx, %ebp
	negl	%ebp
	movl	%r9d, %ebx
	sarl	$31, %ebx
	subl	%r8d, %esi
	orl	%r9d, %ebp
	movl	%esi, %eax
	cltd
	idivl	%ebp
	movl	%edx, %r15d
	movl	%ebx, %r14d
	leal	(%r9,%rcx), %ebp
	movl	%esi, %eax
	cltd
	idivl	%ebp
	notl	%r14d
	decl	%ecx
	movl	%r14d, %ebp
	subl	%ebx, %ebp
	andl	%r8d, %ebp
	addl	%eax, %ebp
	andl	%ecx, %ebp
	leal	(%rbp,%rbp), %eax
	movl	%eax, -72(%rsp)                 ## 4-byte Spill
	subl	%eax, %r10d
	cmpl	$2, %r10d
	movl	$2, %edx
	cmovll	%r10d, %edx
	xorl	%eax, %eax
	testl	%r10d, %r10d
	cmovlel	%eax, %edx
	movl	%edx, -104(%rsp)                ## 4-byte Spill
	jle	LBB29_4
## %bb.1:                               ## %"for relu1_0_d_def__.s21.w.wi.preheader"
	movl	(%rdi), %r11d
	movl	16(%rdi), %r10d
	movl	32(%rdi), %esi
	movl	40(%rdi), %edx
	movl	44(%rdi), %r12d
	movl	48(%rdi), %r13d
	xorl	%r9d, %ebx
	addl	%r14d, %ebx
	andl	%r8d, %ebx
	addl	%ebx, %r15d
	andl	%ecx, %r15d
	movq	%r15, -64(%rsp)                 ## 8-byte Spill
	leal	(,%r15,8), %eax
	movl	%edx, -40(%rsp)                 ## 4-byte Spill
	movl	%edx, %r8d
	subl	%esi, %r8d
	movl	%r12d, -80(%rsp)                ## 4-byte Spill
                                        ## kill: def $r12d killed $r12d def $r12
	subl	%esi, %r12d
	movl	%r13d, -120(%rsp)               ## 4-byte Spill
	movl	%r13d, %r14d
	movl	%esi, -96(%rsp)                 ## 4-byte Spill
	subl	%esi, %r14d
	movslq	8(%rdi), %rcx
	movslq	%ebp, %rbx
	subq	%rcx, %rbx
	addq	%rbx, %rbx
	movq	%r10, -112(%rsp)                ## 8-byte Spill
	movl	%r10d, %r15d
	shll	$5, %r15d
	movl	%r11d, %edx
	subl	%eax, %edx
	cmpl	$8, %edx
	movl	$8, %ecx
	cmovll	%edx, %ecx
	movl	%edx, -28(%rsp)                 ## 4-byte Spill
	testl	%edx, %edx
	movl	$0, %r10d
	cmovgl	%ecx, %r10d
	leal	8(%rax), %edx
	movslq	%eax, %r13
	subl	%r11d, %eax
	movl	%eax, %ecx
	sarl	$31, %ecx
	andl	%eax, %ecx
	cmpl	$-8, %ecx
	movl	$-8, %esi
	cmovgl	%ecx, %esi
	movl	20(%rdi), %eax
	movl	%eax, -116(%rsp)                ## 4-byte Spill
	movl	28(%rdi), %eax
	movl	36(%rdi), %r9d
	movq	56(%rdi), %rcx
	movq	%rcx, -88(%rsp)                 ## 8-byte Spill
	movq	72(%rdi), %rcx
	movq	%rcx, -56(%rsp)                 ## 8-byte Spill
	movq	88(%rdi), %rcx
	movq	%rcx, -48(%rsp)                 ## 8-byte Spill
	movslq	4(%rdi), %rcx
	cmpl	%r11d, %edx
	movl	%r15d, -124(%rsp)               ## 4-byte Spill
	jle	LBB29_2
## %bb.5:                               ## %"for relu1_0_d_def__.s21.w.wi.us.preheader"
	movq	-112(%rsp), %rdi                ## 8-byte Reload
	leal	(%rdi,%rdi,2), %edx
	shll	$3, %edx
	subl	%edi, %edx
	movl	%edx, -32(%rsp)                 ## 4-byte Spill
	subl	-96(%rsp), %r11d                ## 4-byte Folded Reload
	movq	%rcx, -24(%rsp)                 ## 8-byte Spill
	movl	-40(%rsp), %ecx                 ## 4-byte Reload
	addl	%r11d, %ecx
	addl	%r11d, -80(%rsp)                ## 4-byte Folded Spill
	movl	-120(%rsp), %edi                ## 4-byte Reload
	addl	%r11d, %edi
	addl	%r9d, %r11d
	movq	%r11, -40(%rsp)                 ## 8-byte Spill
	movq	-64(%rsp), %r15                 ## 8-byte Reload
	leal	(%r9,%r15,8), %edx
	subl	-96(%rsp), %edx                 ## 4-byte Folded Reload
	movq	%rdx, 160(%rsp)                 ## 8-byte Spill
	movl	%ebp, %edx
	subl	%eax, %edx
	addl	%edx, %edx
	movq	%rdx, 8(%rsp)                   ## 8-byte Spill
	addl	%r13d, %r14d
	addl	%r13d, %r12d
	addl	%r13d, %r8d
	addl	%r10d, %esi
	movl	%esi, -120(%rsp)                ## 4-byte Spill
	movslq	%r8d, %rdx
	movq	%rdx, -8(%rsp)                  ## 8-byte Spill
	movslq	%r12d, %r11
	movslq	%r14d, %r14
	movslq	%ecx, %rcx
	movq	%rcx, 152(%rsp)                 ## 8-byte Spill
	movslq	-80(%rsp), %rcx                 ## 4-byte Folded Reload
	movq	%rcx, 144(%rsp)                 ## 8-byte Spill
	movslq	%edi, %rcx
	movq	%rcx, 136(%rsp)                 ## 8-byte Spill
	shll	$6, %ebp
	shll	$6, %eax
	subl	%eax, %ebp
	leal	(%r10,%r13), %eax
	movslq	%eax, %rcx
	orl	$23, %ebp
	imull	-112(%rsp), %ebp                ## 4-byte Folded Reload
	movl	%esi, %r8d
	movslq	-72(%rsp), %r9                  ## 4-byte Folded Reload
	movl	-104(%rsp), %eax                ## 4-byte Reload
	movq	%rax, 120(%rsp)                 ## 8-byte Spill
	movl	%r8d, %eax
	andl	$-16, %eax
	movq	%rax, 56(%rsp)                  ## 8-byte Spill
	addq	$-16, %rax
	movq	%rax, 48(%rsp)                  ## 8-byte Spill
	movq	%rax, %rdx
	shrq	$4, %rdx
	incq	%rdx
	movq	%r10, -80(%rsp)                 ## 8-byte Spill
	movl	%r10d, %r12d
	movl	%r12d, %eax
	andl	$2147483640, %eax               ## imm = 0x7FFFFFF8
	movq	%rax, 32(%rsp)                  ## 8-byte Spill
	movq	-24(%rsp), %rdi                 ## 8-byte Reload
	movq	%rdi, %rax
	movq	%rbx, -16(%rsp)                 ## 8-byte Spill
	imulq	%rbx, %rax
	addq	%rax, %r13
	movq	-88(%rsp), %rsi                 ## 8-byte Reload
	leaq	(%rsi,%r13,4), %r13
	leal	(%rbp,%r15,8), %ebx
	movl	%ebx, -112(%rsp)                ## 4-byte Spill
	addq	%rcx, %rax
	movq	%rdx, %rbx
	movq	%rdx, 40(%rsp)                  ## 8-byte Spill
	andq	$-2, %rdx
	negq	%rdx
	movq	%rdx, 24(%rsp)                  ## 8-byte Spill
	leaq	(%rsi,%rax,4), %r10
	addq	$96, %r10
	leaq	(%rsi,%rax,4), %r15
	vmovss	LCPI29_1(%rip), %xmm0           ## xmm0 = mem[0],zero,zero,zero
	vmovss	LCPI29_2(%rip), %xmm1           ## xmm1 = mem[0],zero,zero,zero
	vmovss	LCPI29_3(%rip), %xmm2           ## xmm2 = mem[0],zero,zero,zero
	vbroadcastss	LCPI29_1(%rip), %ymm3   ## ymm3 = [1.0E+0,1.0E+0,1.0E+0,1.0E+0,1.0E+0,1.0E+0,1.0E+0,1.0E+0]
	vbroadcastss	LCPI29_2(%rip), %ymm4   ## ymm4 = [4.096E+3,4.096E+3,4.096E+3,4.096E+3,4.096E+3,4.096E+3,4.096E+3,4.096E+3]
	vbroadcastss	LCPI29_3(%rip), %ymm5   ## ymm5 = [1.99999994E-9,1.99999994E-9,1.99999994E-9,1.99999994E-9,1.99999994E-9,1.99999994E-9,1.99999994E-9,1.99999994E-9]
	leaq	(,%rdi,4), %rax
	movq	%rax, 112(%rsp)                 ## 8-byte Spill
	movq	%r14, 64(%rsp)                  ## 8-byte Spill
	movq	-48(%rsp), %rdx                 ## 8-byte Reload
	leaq	(%rdx,%r14,4), %rax
	movq	%rax, 104(%rsp)                 ## 8-byte Spill
	movq	%r11, 72(%rsp)                  ## 8-byte Spill
	leaq	(%rdx,%r11,4), %rax
	movq	%rax, 96(%rsp)                  ## 8-byte Spill
	movq	-8(%rsp), %rax                  ## 8-byte Reload
	leaq	(%rdx,%rax,4), %rax
	movq	%rax, 88(%rsp)                  ## 8-byte Spill
	movq	-56(%rsp), %rdx                 ## 8-byte Reload
	leaq	96(%rdx), %rax
	movq	%rax, 16(%rsp)                  ## 8-byte Spill
	movq	%rcx, (%rsp)                    ## 8-byte Spill
	leaq	(%rdx,%rcx,4), %rax
	movq	%rax, 80(%rsp)                  ## 8-byte Spill
	movq	%r9, %rax
	movq	%r9, 128(%rsp)                  ## 8-byte Spill
	movq	%r9, %rdi
	xorl	%r9d, %r9d
	movq	%rbp, %r11
	movl	-124(%rsp), %ebx                ## 4-byte Reload
	jmp	LBB29_6
	.p2align	4, 0x90
LBB29_14:                               ## %after_bb.us
                                        ##   in Loop: Header=BB29_6 Depth=1
	incq	%r9
	movq	-64(%rsp), %rdi                 ## 8-byte Reload
	incq	%rdi
	movq	112(%rsp), %rax                 ## 8-byte Reload
	addq	%rax, %r13
	addl	%ebx, -112(%rsp)                ## 4-byte Folded Spill
	addq	%rax, %r10
	addl	%ebx, %r11d
	addq	%rax, %r15
	cmpq	120(%rsp), %r9                  ## 8-byte Folded Reload
	je	LBB29_4
LBB29_6:                                ## %"for relu1_0_d_def__.s21.w.wi.us"
                                        ## =>This Loop Header: Depth=1
                                        ##     Child Loop BB29_24 Depth 2
                                        ##     Child Loop BB29_9 Depth 2
                                        ##     Child Loop BB29_18 Depth 2
                                        ##     Child Loop BB29_13 Depth 2
	movq	%r11, -96(%rsp)                 ## 8-byte Spill
	testq	%rdi, %rdi
	movl	$0, %eax
	movq	%rdi, -64(%rsp)                 ## 8-byte Spill
	cmovgl	%edi, %eax
	movq	128(%rsp), %rcx                 ## 8-byte Reload
	movq	%r9, -72(%rsp)                  ## 8-byte Spill
	leaq	(%r9,%rcx), %rdx
	testq	%rdx, %rdx
	movl	$0, %ecx
	cmovlel	%ecx, %edx
	cmpl	$0, -28(%rsp)                   ## 4-byte Folded Reload
	jle	LBB29_10
## %bb.7:                               ## %"for relu1_0_d_def__.s21.n.ni.preheader.us"
                                        ##   in Loop: Header=BB29_6 Depth=1
	movq	%rdx, -104(%rsp)                ## 8-byte Spill
	movl	%ebx, %ebp
	imull	-116(%rsp), %eax                ## 4-byte Folded Reload
	movslq	%eax, %r9
	movq	160(%rsp), %rax                 ## 8-byte Reload
	addl	%r9d, %eax
	cltq
	movq	-48(%rsp), %rbx                 ## 8-byte Reload
	leaq	(%rbx,%rax,4), %rax
	movslq	-112(%rsp), %rcx                ## 4-byte Folded Reload
	movq	-56(%rsp), %rdx                 ## 8-byte Reload
	leaq	(%rdx,%rcx,4), %rsi
	cmpl	$8, -80(%rsp)                   ## 4-byte Folded Reload
	jae	LBB29_23
## %bb.8:                               ##   in Loop: Header=BB29_6 Depth=1
	xorl	%r14d, %r14d
	movl	%ebp, %ebx
	movq	-104(%rsp), %rdx                ## 8-byte Reload
	jmp	LBB29_26
	.p2align	4, 0x90
LBB29_23:                               ## %vector.body27.preheader
                                        ##   in Loop: Header=BB29_6 Depth=1
	movq	64(%rsp), %rcx                  ## 8-byte Reload
	addq	%r9, %rcx
	movq	72(%rsp), %rdx                  ## 8-byte Reload
	addq	%r9, %rdx
	movq	-8(%rsp), %rbp                  ## 8-byte Reload
	leaq	(%rbp,%r9), %r11
	leaq	(%rbx,%rcx,4), %rbp
	leaq	(%rbx,%rdx,4), %rdx
	leaq	(%rbx,%r11,4), %rcx
	xorl	%ebx, %ebx
	movq	32(%rsp), %rdi                  ## 8-byte Reload
	.p2align	4, 0x90
LBB29_24:                               ## %vector.body27
                                        ##   Parent Loop BB29_6 Depth=1
                                        ## =>  This Inner Loop Header: Depth=2
	vmovups	(%rbp,%rbx,4), %ymm6
	vmaxps	%ymm3, %ymm6, %ymm6
	vdivps	%ymm6, %ymm4, %ymm6
	vmovups	(%rax,%rbx,4), %ymm7
	vminps	%ymm7, %ymm6, %ymm6
	vmulps	(%rdx,%rbx,4), %ymm6, %ymm6
	vmulps	%ymm6, %ymm7, %ymm6
	vmulps	(%rcx,%rbx,4), %ymm6, %ymm6
	vmulps	(%r13,%rbx,4), %ymm6, %ymm6
	vfmadd213ps	(%rsi,%rbx,4), %ymm5, %ymm6 ## ymm6 = (ymm5 * ymm6) + mem
	vmovups	%ymm6, (%rsi,%rbx,4)
	addq	$8, %rbx
	cmpq	%rbx, %rdi
	jne	LBB29_24
## %bb.25:                              ## %middle.block25
                                        ##   in Loop: Header=BB29_6 Depth=1
	movq	%rdi, %r14
	cmpq	%r12, %rdi
	movl	-124(%rsp), %ebx                ## 4-byte Reload
	movq	-104(%rsp), %rdx                ## 8-byte Reload
	je	LBB29_10
LBB29_26:                               ## %"for relu1_0_d_def__.s21.n.ni.us.preheader"
                                        ##   in Loop: Header=BB29_6 Depth=1
	movq	104(%rsp), %rcx                 ## 8-byte Reload
	leaq	(%rcx,%r9,4), %r11
	movq	96(%rsp), %rcx                  ## 8-byte Reload
	leaq	(%rcx,%r9,4), %rbp
	movq	88(%rsp), %rcx                  ## 8-byte Reload
	leaq	(%rcx,%r9,4), %rdi
	.p2align	4, 0x90
LBB29_9:                                ## %"for relu1_0_d_def__.s21.n.ni.us"
                                        ##   Parent Loop BB29_6 Depth=1
                                        ## =>  This Inner Loop Header: Depth=2
	vmovss	(%rax,%r14,4), %xmm6            ## xmm6 = mem[0],zero,zero,zero
	vmovss	(%r11,%r14,4), %xmm7            ## xmm7 = mem[0],zero,zero,zero
	vmaxss	%xmm0, %xmm7, %xmm7
	vdivss	%xmm7, %xmm1, %xmm7
	vminss	%xmm6, %xmm7, %xmm7
	vmulss	(%rbp,%r14,4), %xmm7, %xmm7
	vmulss	%xmm7, %xmm6, %xmm6
	vmulss	(%rdi,%r14,4), %xmm6, %xmm6
	vmulss	(%r13,%r14,4), %xmm6, %xmm6
	vfmadd213ss	(%rsi,%r14,4), %xmm2, %xmm6 ## xmm6 = (xmm2 * xmm6) + mem
	vmovss	%xmm6, (%rsi,%r14,4)
	incq	%r14
	cmpq	%r14, %r12
	jne	LBB29_9
LBB29_10:                               ## %"end for relu1_0_d_def__.s21.n.ni.us"
                                        ##   in Loop: Header=BB29_6 Depth=1
	movl	-120(%rsp), %r14d               ## 4-byte Reload
	testl	%r14d, %r14d
	movq	-96(%rsp), %r11                 ## 8-byte Reload
	movq	-72(%rsp), %r9                  ## 8-byte Reload
	jle	LBB29_14
## %bb.11:                              ## %"for relu1_0_d_def__.s21.n.ni.rebased.preheader.us"
                                        ##   in Loop: Header=BB29_6 Depth=1
	movslq	%r11d, %rax
	imull	-116(%rsp), %edx                ## 4-byte Folded Reload
	movq	-40(%rsp), %rcx                 ## 8-byte Reload
	addl	%edx, %ecx
	movslq	%edx, %rdx
	movq	152(%rsp), %rsi                 ## 8-byte Reload
	addq	%rdx, %rsi
	movq	144(%rsp), %rdi                 ## 8-byte Reload
	addq	%rdx, %rdi
	addq	136(%rsp), %rdx                 ## 8-byte Folded Reload
	movslq	%ecx, %rcx
	movq	-48(%rsp), %rbp                 ## 8-byte Reload
	vmovss	-4(%rbp,%rcx,4), %xmm6          ## xmm6 = mem[0],zero,zero,zero
	vmovss	-4(%rbp,%rdx,4), %xmm7          ## xmm7 = mem[0],zero,zero,zero
	vmaxss	%xmm0, %xmm7, %xmm7
	vdivss	%xmm7, %xmm1, %xmm7
	vminss	%xmm6, %xmm7, %xmm7
	vmulss	-4(%rbp,%rdi,4), %xmm7, %xmm7
	vmulss	%xmm7, %xmm6, %xmm6
	vmulss	-4(%rbp,%rsi,4), %xmm6, %xmm6
	cmpl	$16, %r14d
	jae	LBB29_15
## %bb.12:                              ##   in Loop: Header=BB29_6 Depth=1
	xorl	%ecx, %ecx
	jmp	LBB29_22
	.p2align	4, 0x90
LBB29_15:                               ## %vector.ph
                                        ##   in Loop: Header=BB29_6 Depth=1
	vbroadcastss	%xmm6, %ymm7
	cmpq	$0, 48(%rsp)                    ## 8-byte Folded Reload
	je	LBB29_16
## %bb.17:                              ## %vector.body.preheader
                                        ##   in Loop: Header=BB29_6 Depth=1
	movq	(%rsp), %rcx                    ## 8-byte Reload
	addq	%rax, %rcx
	movq	16(%rsp), %rdx                  ## 8-byte Reload
	leaq	(%rdx,%rcx,4), %rdi
	movq	24(%rsp), %rbx                  ## 8-byte Reload
	xorl	%esi, %esi
	movq	-16(%rsp), %rdx                 ## 8-byte Reload
	movq	-88(%rsp), %rbp                 ## 8-byte Reload
	.p2align	4, 0x90
LBB29_18:                               ## %vector.body
                                        ##   Parent Loop BB29_6 Depth=1
                                        ## =>  This Inner Loop Header: Depth=2
	vmulps	-96(%r10,%rsi,4), %ymm7, %ymm8
	vmulps	-64(%r10,%rsi,4), %ymm7, %ymm9
	vfmadd213ps	-96(%rdi,%rsi,4), %ymm5, %ymm8 ## ymm8 = (ymm5 * ymm8) + mem
	vfmadd213ps	-64(%rdi,%rsi,4), %ymm5, %ymm9 ## ymm9 = (ymm5 * ymm9) + mem
	vmovups	%ymm8, -96(%rdi,%rsi,4)
	vmovups	%ymm9, -64(%rdi,%rsi,4)
	vmulps	-32(%r10,%rsi,4), %ymm7, %ymm8
	vmulps	(%r10,%rsi,4), %ymm7, %ymm9
	vfmadd213ps	-32(%rdi,%rsi,4), %ymm5, %ymm8 ## ymm8 = (ymm5 * ymm8) + mem
	vfmadd213ps	(%rdi,%rsi,4), %ymm5, %ymm9 ## ymm9 = (ymm5 * ymm9) + mem
	vmovups	%ymm8, -32(%rdi,%rsi,4)
	vmovups	%ymm9, (%rdi,%rsi,4)
	addq	$32, %rsi
	addq	$2, %rbx
	jne	LBB29_18
## %bb.19:                              ## %middle.block.unr-lcssa
                                        ##   in Loop: Header=BB29_6 Depth=1
	testb	$1, 40(%rsp)                    ## 1-byte Folded Reload
	movl	-124(%rsp), %ebx                ## 4-byte Reload
	je	LBB29_21
LBB29_20:                               ## %vector.body.epil
                                        ##   in Loop: Header=BB29_6 Depth=1
	movq	8(%rsp), %rcx                   ## 8-byte Reload
	addl	%r9d, %ecx
	imull	%ebx, %ecx
	addl	-32(%rsp), %ecx                 ## 4-byte Folded Reload
	addq	%r9, %rdx
	imulq	-24(%rsp), %rdx                 ## 8-byte Folded Reload
	movslq	%ecx, %rcx
	addq	(%rsp), %rsi                    ## 8-byte Folded Reload
	addq	%rsi, %rcx
	addq	%rdx, %rsi
	vmulps	(%rbp,%rsi,4), %ymm7, %ymm8
	vmulps	32(%rbp,%rsi,4), %ymm7, %ymm7
	movq	-56(%rsp), %rdx                 ## 8-byte Reload
	vfmadd213ps	(%rdx,%rcx,4), %ymm5, %ymm8 ## ymm8 = (ymm5 * ymm8) + mem
	vfmadd213ps	32(%rdx,%rcx,4), %ymm5, %ymm7 ## ymm7 = (ymm5 * ymm7) + mem
	vmovups	%ymm8, (%rdx,%rcx,4)
	vmovups	%ymm7, 32(%rdx,%rcx,4)
LBB29_21:                               ## %middle.block
                                        ##   in Loop: Header=BB29_6 Depth=1
	movq	56(%rsp), %rdx                  ## 8-byte Reload
	movq	%rdx, %rcx
	cmpq	%r8, %rdx
	je	LBB29_14
LBB29_22:                               ## %"for relu1_0_d_def__.s21.n.ni.rebased.us.preheader"
                                        ##   in Loop: Header=BB29_6 Depth=1
	movq	80(%rsp), %rdx                  ## 8-byte Reload
	leaq	(%rdx,%rax,4), %rax
	.p2align	4, 0x90
LBB29_13:                               ## %"for relu1_0_d_def__.s21.n.ni.rebased.us"
                                        ##   Parent Loop BB29_6 Depth=1
                                        ## =>  This Inner Loop Header: Depth=2
	vmulss	(%r15,%rcx,4), %xmm6, %xmm7
	vfmadd213ss	(%rax,%rcx,4), %xmm2, %xmm7 ## xmm7 = (xmm2 * xmm7) + mem
	vmovss	%xmm7, (%rax,%rcx,4)
	incq	%rcx
	cmpq	%rcx, %r8
	jne	LBB29_13
	jmp	LBB29_14
LBB29_16:                               ##   in Loop: Header=BB29_6 Depth=1
	xorl	%esi, %esi
	movq	-16(%rsp), %rdx                 ## 8-byte Reload
	movq	-88(%rsp), %rbp                 ## 8-byte Reload
	testb	$1, 40(%rsp)                    ## 1-byte Folded Reload
	movl	-124(%rsp), %ebx                ## 4-byte Reload
	jne	LBB29_20
	jmp	LBB29_21
LBB29_2:                                ## %"for relu1_0_d_def__.s21.w.wi.preheader4"
	subl	-96(%rsp), %r9d                 ## 4-byte Folded Reload
	decl	%r11d
	vmovd	%r13d, %xmm0
	vpbroadcastd	%xmm0, %ymm0
	vpor	LCPI29_0(%rip), %ymm0, %ymm0
	vmovd	%r11d, %xmm1
	vpbroadcastd	%xmm1, %ymm1
	vpminsd	%ymm0, %ymm1, %ymm12
	movslq	-72(%rsp), %rdi                 ## 4-byte Folded Reload
	movl	-104(%rsp), %r11d               ## 4-byte Reload
	imulq	%rcx, %rbx
	addq	%r13, %rbx
	movq	-88(%rsp), %rdx                 ## 8-byte Reload
	leaq	(%rdx,%rbx,4), %r15
	shlq	$2, %rcx
	movq	%rcx, %r13
	shll	$6, %ebp
	shll	$6, %eax
	subl	%eax, %ebp
	orl	$23, %ebp
	movq	-112(%rsp), %rcx                ## 8-byte Reload
	imull	%ebp, %ecx
	movq	-64(%rsp), %rax                 ## 8-byte Reload
	leal	(%rcx,%rax,8), %edx
	vbroadcastss	LCPI29_1(%rip), %ymm8   ## ymm8 = [1.0E+0,1.0E+0,1.0E+0,1.0E+0,1.0E+0,1.0E+0,1.0E+0,1.0E+0]
	vbroadcastss	LCPI29_2(%rip), %ymm10  ## ymm10 = [4.096E+3,4.096E+3,4.096E+3,4.096E+3,4.096E+3,4.096E+3,4.096E+3,4.096E+3]
	vbroadcastss	LCPI29_3(%rip), %ymm11  ## ymm11 = [1.99999994E-9,1.99999994E-9,1.99999994E-9,1.99999994E-9,1.99999994E-9,1.99999994E-9,1.99999994E-9,1.99999994E-9]
	movq	-48(%rsp), %rbp                 ## 8-byte Reload
	.p2align	4, 0x90
LBB29_3:                                ## %"for relu1_0_d_def__.s21.w.wi"
                                        ## =>This Inner Loop Header: Depth=1
	testq	%rdi, %rdi
	movl	$0, %eax
	cmovgl	%edi, %eax
	imull	-116(%rsp), %eax                ## 4-byte Folded Reload
	leal	(%rax,%r9), %esi
	vmovd	%esi, %xmm1
	vpbroadcastd	%xmm1, %ymm1
	vpaddd	%ymm1, %ymm12, %ymm1
	vmovd	%xmm1, %esi
	vpextrd	$1, %xmm1, %ecx
	movslq	%esi, %rsi
	vpextrd	$2, %xmm1, %ebx
	movslq	%ecx, %rcx
	vmovss	(%rbp,%rsi,4), %xmm5            ## xmm5 = mem[0],zero,zero,zero
	vpextrd	$3, %xmm1, %esi
	movslq	%ebx, %rbx
	vextracti128	$1, %ymm1, %xmm4
	vinsertps	$16, (%rbp,%rcx,4), %xmm5, %xmm1 ## xmm1 = xmm5[0],mem[0],xmm5[2,3]
	vmovd	%xmm4, %ecx
	vinsertps	$32, (%rbp,%rbx,4), %xmm1, %xmm5 ## xmm5 = xmm1[0,1],mem[0],xmm1[3]
	leal	(%rax,%r8), %ebx
	movslq	%ecx, %rcx
	vmovd	%ebx, %xmm1
	vpbroadcastd	%xmm1, %ymm1
	vpaddd	%ymm1, %ymm12, %ymm6
	vmovss	(%rbp,%rcx,4), %xmm1            ## xmm1 = mem[0],zero,zero,zero
	vmovd	%xmm6, %ecx
	vpextrd	$1, %xmm6, %ebx
	movslq	%ecx, %rcx
	vmovss	(%rbp,%rcx,4), %xmm2            ## xmm2 = mem[0],zero,zero,zero
	vpextrd	$2, %xmm6, %ecx
	movslq	%esi, %rsi
	movslq	%ebx, %rbx
	movslq	%ecx, %rcx
	vinsertps	$48, (%rbp,%rsi,4), %xmm5, %xmm13 ## xmm13 = xmm5[0,1,2],mem[0]
	vpextrd	$3, %xmm6, %esi
	vextracti128	$1, %ymm6, %xmm7
	movslq	%esi, %rsi
	vinsertps	$16, (%rbp,%rbx,4), %xmm2, %xmm2 ## xmm2 = xmm2[0],mem[0],xmm2[2,3]
	vpextrd	$1, %xmm4, %ebx
	vinsertps	$32, (%rbp,%rcx,4), %xmm2, %xmm2 ## xmm2 = xmm2[0,1],mem[0],xmm2[3]
	leal	(%rax,%r12), %ecx
	movslq	%ebx, %rbx
	vmovd	%ecx, %xmm6
	vmovd	%xmm7, %ecx
	movslq	%ecx, %rcx
	vmovss	(%rbp,%rcx,4), %xmm3            ## xmm3 = mem[0],zero,zero,zero
	vpbroadcastd	%xmm6, %ymm6
	vpaddd	%ymm6, %ymm12, %ymm9
	vinsertps	$48, (%rbp,%rsi,4), %xmm2, %xmm14 ## xmm14 = xmm2[0,1,2],mem[0]
	vmovd	%xmm9, %ecx
	movslq	%ecx, %rcx
	vinsertps	$16, (%rbp,%rbx,4), %xmm1, %xmm2 ## xmm2 = xmm1[0],mem[0],xmm1[2,3]
	vpextrd	$1, %xmm9, %esi
	vpextrd	$1, %xmm7, %ebx
	vmovss	(%rbp,%rcx,4), %xmm0            ## xmm0 = mem[0],zero,zero,zero
	movslq	%esi, %rcx
	vextracti128	$1, %ymm9, %xmm1
	movslq	%ebx, %rsi
	vinsertps	$16, (%rbp,%rcx,4), %xmm0, %xmm0 ## xmm0 = xmm0[0],mem[0],xmm0[2,3]
	vinsertps	$16, (%rbp,%rsi,4), %xmm3, %xmm3 ## xmm3 = xmm3[0],mem[0],xmm3[2,3]
	vmovd	%xmm1, %ecx
	vpextrd	$1, %xmm1, %esi
	movslq	%ecx, %rcx
	vmovss	(%rbp,%rcx,4), %xmm5            ## xmm5 = mem[0],zero,zero,zero
	movslq	%esi, %rcx
	vinsertps	$16, (%rbp,%rcx,4), %xmm5, %xmm5 ## xmm5 = xmm5[0],mem[0],xmm5[2,3]
	vpextrd	$2, %xmm9, %ecx
	movslq	%ecx, %rcx
	vinsertps	$32, (%rbp,%rcx,4), %xmm0, %xmm0 ## xmm0 = xmm0[0,1],mem[0],xmm0[3]
	vpextrd	$2, %xmm4, %ecx
	movslq	%ecx, %rcx
	vinsertps	$32, (%rbp,%rcx,4), %xmm2, %xmm2 ## xmm2 = xmm2[0,1],mem[0],xmm2[3]
	vpextrd	$3, %xmm9, %ecx
	movslq	%ecx, %rcx
	vinsertps	$48, (%rbp,%rcx,4), %xmm0, %xmm9 ## xmm9 = xmm0[0,1,2],mem[0]
	vpextrd	$2, %xmm7, %ecx
	movslq	%ecx, %rcx
	vinsertps	$32, (%rbp,%rcx,4), %xmm3, %xmm0 ## xmm0 = xmm3[0,1],mem[0],xmm3[3]
	addl	%r14d, %eax
	vmovd	%eax, %xmm3
	vpextrd	$2, %xmm1, %eax
	cltq
	vpbroadcastd	%xmm3, %ymm3
	vinsertps	$32, (%rbp,%rax,4), %xmm5, %xmm15 ## xmm15 = xmm5[0,1],mem[0],xmm5[3]
	vpaddd	%ymm3, %ymm12, %ymm3
	vextracti128	$1, %ymm3, %xmm6
	vmovd	%xmm6, %eax
	cltq
	vmovss	(%rbp,%rax,4), %xmm5            ## xmm5 = mem[0],zero,zero,zero
	vpextrd	$1, %xmm6, %eax
	cltq
	vinsertps	$16, (%rbp,%rax,4), %xmm5, %xmm5 ## xmm5 = xmm5[0],mem[0],xmm5[2,3]
	vpextrd	$3, %xmm4, %eax
	cltq
	vinsertps	$48, (%rbp,%rax,4), %xmm2, %xmm2 ## xmm2 = xmm2[0,1,2],mem[0]
	vpextrd	$2, %xmm6, %eax
	cltq
	vinsertps	$32, (%rbp,%rax,4), %xmm5, %xmm4 ## xmm4 = xmm5[0,1],mem[0],xmm5[3]
	vmovd	%xmm3, %eax
	cltq
	vmovss	(%rbp,%rax,4), %xmm5            ## xmm5 = mem[0],zero,zero,zero
	vpextrd	$1, %xmm3, %eax
	cltq
	vinsertps	$16, (%rbp,%rax,4), %xmm5, %xmm5 ## xmm5 = xmm5[0],mem[0],xmm5[2,3]
	vpextrd	$3, %xmm7, %eax
	cltq
	vinsertps	$48, (%rbp,%rax,4), %xmm0, %xmm0 ## xmm0 = xmm0[0,1,2],mem[0]
	vpextrd	$2, %xmm3, %eax
	cltq
	vinsertps	$32, (%rbp,%rax,4), %xmm5, %xmm5 ## xmm5 = xmm5[0,1],mem[0],xmm5[3]
	vpextrd	$3, %xmm6, %eax
	cltq
	vinsertps	$48, (%rbp,%rax,4), %xmm4, %xmm4 ## xmm4 = xmm4[0,1,2],mem[0]
	vpextrd	$3, %xmm3, %eax
	cltq
	vinsertps	$48, (%rbp,%rax,4), %xmm5, %xmm3 ## xmm3 = xmm5[0,1,2],mem[0]
	vpextrd	$3, %xmm1, %eax
	cltq
	vinsertps	$48, (%rbp,%rax,4), %xmm15, %xmm1 ## xmm1 = xmm15[0,1,2],mem[0]
	vinsertf128	$1, %xmm4, %ymm3, %ymm3
	vmaxps	%ymm8, %ymm3, %ymm3
	vinsertf128	$1, %xmm2, %ymm13, %ymm2
	vdivps	%ymm3, %ymm10, %ymm3
	vinsertf128	$1, %xmm0, %ymm14, %ymm0
	vinsertf128	$1, %xmm1, %ymm9, %ymm1
	vmulps	%ymm2, %ymm1, %ymm1
	vmulps	%ymm0, %ymm1, %ymm0
	vminps	%ymm2, %ymm3, %ymm1
	vmulps	%ymm0, %ymm1, %ymm0
	vmulps	(%r15), %ymm0, %ymm0
	movslq	%edx, %rdx
	movq	-56(%rsp), %rax                 ## 8-byte Reload
	vfmadd213ps	(%rax,%rdx,4), %ymm11, %ymm0 ## ymm0 = (ymm11 * ymm0) + mem
	vmovups	%ymm0, (%rax,%rdx,4)
	addq	%r13, %r15
	incq	%rdi
	addl	-124(%rsp), %edx                ## 4-byte Folded Reload
	decq	%r11
	jne	LBB29_3
LBB29_4:                                ## %destructor_block
	xorl	%eax, %eax
	addq	$168, %rsp
	popq	%rbx
	popq	%r12
	popq	%r13
	popq	%r14
	popq	%r15
	popq	%rbp
	vzeroupper
	retq
                                        ## -- End function
	.section	__TEXT,__const
	.p2align	5                               ## -- Begin function train_cost_model.par_for.relu1_0_d_def__.s22.n.n.n
LCPI30_0:
	.long	0                               ## 0x0
	.long	1                               ## 0x1
	.long	2                               ## 0x2
	.long	3                               ## 0x3
	.long	4                               ## 0x4
	.long	5                               ## 0x5
	.long	6                               ## 0x6
	.long	7                               ## 0x7
	.section	__TEXT,__literal4,4byte_literals
	.p2align	2
LCPI30_1:
	.long	0x3f800000                      ## float 1
LCPI30_2:
	.long	0x3109705f                      ## float 1.99999994E-9
	.section	__TEXT,__text,regular,pure_instructions
	.p2align	4, 0x90
_train_cost_model.par_for.relu1_0_d_def__.s22.n.n.n: ## @train_cost_model.par_for.relu1_0_d_def__.s22.n.n.n
## %bb.0:                               ## %entry
	pushq	%rbp
	pushq	%r15
	pushq	%r14
	pushq	%r13
	pushq	%r12
	pushq	%rbx
	subq	$184, %rsp
	movq	%rdx, %r12
	movl	12(%rdx), %r9d
	movl	24(%rdx), %r14d
	movl	%esi, %r8d
	sarl	$31, %r8d
	xorl	%ecx, %ecx
	testl	%r14d, %r14d
	sete	%cl
	movl	%ecx, %ebx
	negl	%ebx
	movl	%r14d, %ebp
	sarl	$31, %ebp
	subl	%r8d, %esi
	orl	%r14d, %ebx
	movl	%esi, %eax
	cltd
	idivl	%ebx
	movl	%edx, %edi
	movl	%ebp, %r15d
	leal	(%r14,%rcx), %ebx
	movl	%esi, %eax
	cltd
	idivl	%ebx
	notl	%r15d
	decl	%ecx
	movl	%r15d, %edx
	subl	%ebp, %edx
	andl	%r8d, %edx
	addl	%eax, %edx
	andl	%ecx, %edx
	leal	(%rdx,%rdx), %eax
	movl	%eax, -48(%rsp)                 ## 4-byte Spill
	subl	%eax, %r9d
	cmpl	$2, %r9d
	movl	$2, %esi
	cmovll	%r9d, %esi
	xorl	%eax, %eax
	testl	%r9d, %r9d
	cmovlel	%eax, %esi
	movl	%esi, -56(%rsp)                 ## 4-byte Spill
	jle	LBB30_4
## %bb.1:                               ## %"for relu1_0_d_def__.s22.w.wi.preheader"
	movl	(%r12), %eax
	movl	%eax, -24(%rsp)                 ## 4-byte Spill
	movl	16(%r12), %eax
	movq	%rax, -128(%rsp)                ## 8-byte Spill
	movq	%rdx, %rsi
	movl	32(%r12), %r10d
	movl	36(%r12), %edx
	movl	40(%r12), %ebx
	movl	44(%r12), %r11d
	movl	48(%r12), %r9d
	xorl	%r14d, %ebp
	addl	%r15d, %ebp
	andl	%r8d, %ebp
	addl	%ebp, %edi
	andl	%ecx, %edi
	movq	%rdi, -104(%rsp)                ## 8-byte Spill
	leal	(,%rdi,8), %eax
	movl	%edx, -64(%rsp)                 ## 4-byte Spill
	movl	%edx, %r13d
	subl	%r10d, %r13d
	movl	%ebx, -72(%rsp)                 ## 4-byte Spill
	movl	%ebx, %r8d
	subl	%r10d, %r8d
	movl	%r11d, -112(%rsp)               ## 4-byte Spill
	subl	%r10d, %r11d
	movl	%r9d, -32(%rsp)                 ## 4-byte Spill
	movl	%r9d, %r15d
	movl	-24(%rsp), %ebx                 ## 4-byte Reload
	subl	%r10d, %r15d
	movslq	8(%r12), %rdx
	movq	%rsi, %r9
	movslq	%esi, %r14
	subq	%rdx, %r14
	addq	%r14, %r14
	movq	-128(%rsp), %rcx                ## 8-byte Reload
                                        ## kill: def $ecx killed $ecx killed $rcx
	shll	$5, %ecx
	movl	%ecx, -92(%rsp)                 ## 4-byte Spill
	movl	%ebx, %ecx
	subl	%eax, %ecx
	cmpl	$8, %ecx
	movl	$8, %edx
	cmovll	%ecx, %edx
	movl	%ecx, -12(%rsp)                 ## 4-byte Spill
	testl	%ecx, %ecx
	movl	$0, %ecx
	cmovgl	%edx, %ecx
	movq	%rcx, 40(%rsp)                  ## 8-byte Spill
	leal	8(%rax), %edx
	movslq	%eax, %rsi
	movq	%rsi, -80(%rsp)                 ## 8-byte Spill
	subl	%ebx, %eax
	movl	%eax, %esi
	sarl	$31, %esi
	andl	%eax, %esi
	cmpl	$-8, %esi
	movl	$-8, %eax
	cmovgl	%esi, %eax
	movl	%eax, -120(%rsp)                ## 4-byte Spill
	movl	20(%r12), %eax
	movl	%eax, -116(%rsp)                ## 4-byte Spill
	movl	28(%r12), %eax
	movq	56(%r12), %rcx
	movq	%rcx, -88(%rsp)                 ## 8-byte Spill
	movq	72(%r12), %rbp
	movq	88(%r12), %rdi
	movslq	4(%r12), %rcx
	cmpl	%ebx, %edx
	movq	%rbp, -40(%rsp)                 ## 8-byte Spill
	jle	LBB30_2
## %bb.5:                               ## %"for relu1_0_d_def__.s22.w.wi.us.preheader"
	subl	%r10d, %ebx
	addl	%ebx, -64(%rsp)                 ## 4-byte Folded Spill
	addl	%ebx, -72(%rsp)                 ## 4-byte Folded Spill
	addl	%ebx, -112(%rsp)                ## 4-byte Folded Spill
	addl	-32(%rsp), %ebx                 ## 4-byte Folded Reload
	movq	%r9, %r10
	movl	%r10d, %edx
	subl	%eax, %edx
	shll	$6, %r10d
	shll	$6, %eax
	subl	%eax, %r10d
	movq	-128(%rsp), %rsi                ## 8-byte Reload
	leal	(%rsi,%rsi,4), %eax
	leal	(%rsi,%rax,4), %eax
	addl	%esi, %eax
	movl	%eax, -16(%rsp)                 ## 4-byte Spill
	addl	%edx, %edx
	movq	%rdx, 56(%rsp)                  ## 8-byte Spill
	movq	%rcx, -8(%rsp)                  ## 8-byte Spill
	movq	-80(%rsp), %rcx                 ## 8-byte Reload
	addl	%ecx, %r15d
	addl	%ecx, %r11d
	addl	%ecx, %r8d
	addl	%ecx, %r13d
	movl	-120(%rsp), %eax                ## 4-byte Reload
	movq	%r14, %r12
	movq	40(%rsp), %r14                  ## 8-byte Reload
	addl	%r14d, %eax
	movl	%eax, %edx
	movl	%eax, -120(%rsp)                ## 4-byte Spill
	movslq	%r13d, %rax
	movq	%rax, 24(%rsp)                  ## 8-byte Spill
	movslq	%r15d, %rax
	movq	%rax, 16(%rsp)                  ## 8-byte Spill
	movslq	%r8d, %rax
	movq	%rax, 8(%rsp)                   ## 8-byte Spill
	movslq	%r11d, %r13
	movslq	-64(%rsp), %rax                 ## 4-byte Folded Reload
	movq	%rax, -24(%rsp)                 ## 8-byte Spill
	movslq	%ebx, %rax
	movq	%rax, -32(%rsp)                 ## 8-byte Spill
	movslq	-72(%rsp), %rax                 ## 4-byte Folded Reload
	movq	%rax, 176(%rsp)                 ## 8-byte Spill
	movslq	-112(%rsp), %rax                ## 4-byte Folded Reload
	movq	%rax, 168(%rsp)                 ## 8-byte Spill
	orl	$22, %r10d
	imull	%esi, %r10d
	leal	(%r14,%rcx), %eax
	movslq	%eax, %rsi
	movslq	-48(%rsp), %rax                 ## 4-byte Folded Reload
	movq	%rax, -112(%rsp)                ## 8-byte Spill
	movl	-56(%rsp), %eax                 ## 4-byte Reload
	movq	%rax, 160(%rsp)                 ## 8-byte Spill
	movl	%edx, %r8d
	movl	%r8d, %eax
	andl	$-8, %eax
	movq	%rax, 104(%rsp)                 ## 8-byte Spill
	leaq	-8(%rax), %rdx
	movq	%rdx, 96(%rsp)                  ## 8-byte Spill
	shrq	$3, %rdx
	incq	%rdx
	movl	%r14d, %r9d
	movl	%r9d, %eax
	andl	$2147483640, %eax               ## imm = 0x7FFFFFF8
	movq	%rax, 80(%rsp)                  ## 8-byte Spill
	movq	-8(%rsp), %r11                  ## 8-byte Reload
	movq	%r11, %rax
	movq	%r12, (%rsp)                    ## 8-byte Spill
	imulq	%r12, %rax
	addq	%rax, %rcx
	movq	-88(%rsp), %r12                 ## 8-byte Reload
	leaq	(%r12,%rcx,4), %r15
	movq	-104(%rsp), %rcx                ## 8-byte Reload
	leal	(%r10,%rcx,8), %ecx
	movl	%ecx, -104(%rsp)                ## 4-byte Spill
	addq	%rsi, %rax
	movq	%rdx, %rcx
	movq	%rdx, 88(%rsp)                  ## 8-byte Spill
	andq	$-2, %rdx
	negq	%rdx
	movq	%rdx, 72(%rsp)                  ## 8-byte Spill
	leaq	(%r12,%rax,4), %rbx
	addq	$32, %rbx
	leaq	(%r12,%rax,4), %r12
	vmovss	LCPI30_1(%rip), %xmm0           ## xmm0 = mem[0],zero,zero,zero
	vmovss	LCPI30_2(%rip), %xmm1           ## xmm1 = mem[0],zero,zero,zero
	vbroadcastss	LCPI30_1(%rip), %ymm2   ## ymm2 = [1.0E+0,1.0E+0,1.0E+0,1.0E+0,1.0E+0,1.0E+0,1.0E+0,1.0E+0]
	vbroadcastss	LCPI30_2(%rip), %ymm10  ## ymm10 = [1.99999994E-9,1.99999994E-9,1.99999994E-9,1.99999994E-9,1.99999994E-9,1.99999994E-9,1.99999994E-9,1.99999994E-9]
	leaq	(,%r11,4), %rax
	movq	%rax, -80(%rsp)                 ## 8-byte Spill
	movq	%r13, 112(%rsp)                 ## 8-byte Spill
	leaq	(%rdi,%r13,4), %rax
	movq	%rax, 152(%rsp)                 ## 8-byte Spill
	movq	8(%rsp), %rax                   ## 8-byte Reload
	leaq	(%rdi,%rax,4), %rax
	movq	%rax, 144(%rsp)                 ## 8-byte Spill
	movq	16(%rsp), %rax                  ## 8-byte Reload
	leaq	(%rdi,%rax,4), %rax
	movq	%rax, 136(%rsp)                 ## 8-byte Spill
	movq	24(%rsp), %rax                  ## 8-byte Reload
	leaq	(%rdi,%rax,4), %rax
	movq	%rax, 128(%rsp)                 ## 8-byte Spill
	leaq	32(%rbp), %rax
	movq	%rax, 64(%rsp)                  ## 8-byte Spill
	movq	%rsi, 32(%rsp)                  ## 8-byte Spill
	leaq	(,%rsi,4), %rax
	addq	%rbp, %rax
	movq	%rax, 120(%rsp)                 ## 8-byte Spill
	movq	-112(%rsp), %rcx                ## 8-byte Reload
	xorl	%edx, %edx
	movq	%r10, %r14
	movq	%rdi, 48(%rsp)                  ## 8-byte Spill
	jmp	LBB30_6
	.p2align	4, 0x90
LBB30_18:                               ## %after_bb.us
                                        ##   in Loop: Header=BB30_6 Depth=1
	movq	-128(%rsp), %rdx                ## 8-byte Reload
	incq	%rdx
	movq	-80(%rsp), %rax                 ## 8-byte Reload
	addq	%rax, %r15
	movq	-64(%rsp), %rcx                 ## 8-byte Reload
	incq	%rcx
	addl	%r10d, -104(%rsp)               ## 4-byte Folded Spill
	addq	%rax, %rbx
	addl	%r10d, %r14d
	addq	%rax, %r12
	cmpq	160(%rsp), %rdx                 ## 8-byte Folded Reload
	je	LBB30_4
LBB30_6:                                ## %"for relu1_0_d_def__.s22.w.wi.us"
                                        ## =>This Loop Header: Depth=1
                                        ##     Child Loop BB30_35 Depth 2
                                        ##     Child Loop BB30_9 Depth 2
                                        ##     Child Loop BB30_22 Depth 2
                                        ##     Child Loop BB30_15 Depth 2
	movq	%rbx, -56(%rsp)                 ## 8-byte Spill
	movq	%r14, -48(%rsp)                 ## 8-byte Spill
	testq	%rcx, %rcx
	movl	$0, %eax
	movq	%rcx, -64(%rsp)                 ## 8-byte Spill
	cmovgl	%ecx, %eax
	movq	-112(%rsp), %rcx                ## 8-byte Reload
	movq	%rdx, %rsi
	movq	%rdx, -128(%rsp)                ## 8-byte Spill
	addq	%rcx, %rdx
	testq	%rdx, %rdx
	movl	$0, %ecx
	cmovlel	%ecx, %edx
	movq	%rdx, -72(%rsp)                 ## 8-byte Spill
	cmpl	$0, -12(%rsp)                   ## 4-byte Folded Reload
	jle	LBB30_12
## %bb.7:                               ## %"for relu1_0_d_def__.s22.n.ni.preheader.us"
                                        ##   in Loop: Header=BB30_6 Depth=1
	imull	-116(%rsp), %eax                ## 4-byte Folded Reload
	movslq	%eax, %r14
	movslq	-104(%rsp), %rax                ## 4-byte Folded Reload
	leaq	(,%rax,4), %rax
	addq	%rbp, %rax
	cmpl	$8, 40(%rsp)                    ## 4-byte Folded Reload
	jae	LBB30_34
## %bb.8:                               ##   in Loop: Header=BB30_6 Depth=1
	xorl	%r13d, %r13d
	jmp	LBB30_37
	.p2align	4, 0x90
LBB30_34:                               ## %vector.body25.preheader
                                        ##   in Loop: Header=BB30_6 Depth=1
	movq	112(%rsp), %rcx                 ## 8-byte Reload
	addq	%r14, %rcx
	movq	8(%rsp), %rdx                   ## 8-byte Reload
	leaq	(%rdx,%r14), %rdi
	movq	16(%rsp), %rdx                  ## 8-byte Reload
	leaq	(%rdx,%r14), %rbx
	movq	24(%rsp), %rdx                  ## 8-byte Reload
	leaq	(%rdx,%r14), %r10
	movq	48(%rsp), %rsi                  ## 8-byte Reload
	leaq	(%rsi,%rcx,4), %rdx
	leaq	(%rsi,%rdi,4), %rbp
	leaq	(%rsi,%rbx,4), %rcx
	leaq	(%rsi,%r10,4), %rdi
	xorl	%ebx, %ebx
	movq	80(%rsp), %rsi                  ## 8-byte Reload
	.p2align	4, 0x90
LBB30_35:                               ## %vector.body25
                                        ##   Parent Loop BB30_6 Depth=1
                                        ## =>  This Inner Loop Header: Depth=2
	vcmpltps	(%rdi,%rbx,4), %ymm2, %ymm3
	vmulps	(%r15,%rbx,4), %ymm10, %ymm4
	vmovups	(%rcx,%rbx,4), %ymm5
	vmaxps	%ymm2, %ymm5, %ymm5
	vmovups	(%rbp,%rbx,4), %ymm6
	vaddps	(%rdx,%rbx,4), %ymm6, %ymm6
	vmulps	%ymm6, %ymm4, %ymm4
	vdivps	%ymm5, %ymm4, %ymm4
	vandps	%ymm4, %ymm3, %ymm3
	vaddps	(%rax,%rbx,4), %ymm3, %ymm3
	vmovups	%ymm3, (%rax,%rbx,4)
	addq	$8, %rbx
	cmpq	%rbx, %rsi
	jne	LBB30_35
## %bb.36:                              ## %middle.block23
                                        ##   in Loop: Header=BB30_6 Depth=1
	movq	%rsi, %r13
	cmpq	%r9, %rsi
	movq	-40(%rsp), %rbp                 ## 8-byte Reload
	jne	LBB30_37
LBB30_12:                               ## %"end for relu1_0_d_def__.s22.n.ni.us"
                                        ##   in Loop: Header=BB30_6 Depth=1
	movl	-120(%rsp), %r11d               ## 4-byte Reload
	testl	%r11d, %r11d
	movl	-92(%rsp), %r10d                ## 4-byte Reload
	movq	-48(%rsp), %r14                 ## 8-byte Reload
	movq	48(%rsp), %rdi                  ## 8-byte Reload
	movq	-56(%rsp), %rbx                 ## 8-byte Reload
	jle	LBB30_18
## %bb.13:                              ## %"for relu1_0_d_def__.s22.n.ni.rebased.preheader.us"
                                        ##   in Loop: Header=BB30_6 Depth=1
	movslq	%r14d, %r13
	movq	-72(%rsp), %rax                 ## 8-byte Reload
	imull	-116(%rsp), %eax                ## 4-byte Folded Reload
	cltq
	movq	-24(%rsp), %rcx                 ## 8-byte Reload
	addq	%rax, %rcx
	movq	-32(%rsp), %rdx                 ## 8-byte Reload
	addq	%rax, %rdx
	movq	176(%rsp), %rsi                 ## 8-byte Reload
	addq	%rax, %rsi
	addq	168(%rsp), %rax                 ## 8-byte Folded Reload
	vmovss	-4(%rdi,%rcx,4), %xmm4          ## xmm4 = mem[0],zero,zero,zero
	vmovss	-4(%rdi,%rdx,4), %xmm3          ## xmm3 = mem[0],zero,zero,zero
	vmaxss	%xmm0, %xmm3, %xmm5
	vmovss	-4(%rdi,%rsi,4), %xmm3          ## xmm3 = mem[0],zero,zero,zero
	vaddss	-4(%rdi,%rax,4), %xmm3, %xmm6
	cmpl	$8, %r11d
	jae	LBB30_19
## %bb.14:                              ##   in Loop: Header=BB30_6 Depth=1
	xorl	%eax, %eax
	jmp	LBB30_33
	.p2align	4, 0x90
LBB30_37:                               ## %"for relu1_0_d_def__.s22.n.ni.us.preheader"
                                        ##   in Loop: Header=BB30_6 Depth=1
	movq	152(%rsp), %rcx                 ## 8-byte Reload
	leaq	(%rcx,%r14,4), %rdx
	movq	144(%rsp), %rcx                 ## 8-byte Reload
	leaq	(%rcx,%r14,4), %r11
	movq	136(%rsp), %rcx                 ## 8-byte Reload
	leaq	(%rcx,%r14,4), %r10
	movq	128(%rsp), %rcx                 ## 8-byte Reload
	leaq	(%rcx,%r14,4), %rsi
	jmp	LBB30_9
	.p2align	4, 0x90
LBB30_11:                               ## %select.end
                                        ##   in Loop: Header=BB30_9 Depth=2
	vaddss	(%rax,%r13,4), %xmm5, %xmm3
	vmovss	%xmm3, (%rax,%r13,4)
	incq	%r13
	cmpq	%r13, %r9
	je	LBB30_12
LBB30_9:                                ## %"for relu1_0_d_def__.s22.n.ni.us"
                                        ##   Parent Loop BB30_6 Depth=1
                                        ## =>  This Inner Loop Header: Depth=2
	vucomiss	(%rsi,%r13,4), %xmm0
	setb	%cl
	vmulss	(%r15,%r13,4), %xmm1, %xmm4
	vxorps	%xmm5, %xmm5, %xmm5
	testb	$1, %cl
	je	LBB30_11
## %bb.10:                              ## %select.true.sink
                                        ##   in Loop: Header=BB30_9 Depth=2
	vmovss	(%r10,%r13,4), %xmm3            ## xmm3 = mem[0],zero,zero,zero
	vmaxss	%xmm0, %xmm3, %xmm3
	vmovss	(%r11,%r13,4), %xmm5            ## xmm5 = mem[0],zero,zero,zero
	vaddss	(%rdx,%r13,4), %xmm5, %xmm5
	vmulss	%xmm5, %xmm4, %xmm4
	vdivss	%xmm3, %xmm4, %xmm5
	jmp	LBB30_11
	.p2align	4, 0x90
LBB30_19:                               ## %vector.ph
                                        ##   in Loop: Header=BB30_6 Depth=1
	vbroadcastss	%xmm5, %ymm7
	cmpq	$0, 96(%rsp)                    ## 8-byte Folded Reload
	je	LBB30_20
## %bb.21:                              ## %vector.ph.new
                                        ##   in Loop: Header=BB30_6 Depth=1
	movq	32(%rsp), %rax                  ## 8-byte Reload
	addq	%r13, %rax
	movq	64(%rsp), %rcx                  ## 8-byte Reload
	leaq	(%rcx,%rax,4), %rsi
	vmulss	%xmm1, %xmm6, %xmm3
	vbroadcastss	%xmm3, %ymm8
	movq	72(%rsp), %rax                  ## 8-byte Reload
	xorl	%ebp, %ebp
	movq	(%rsp), %rcx                    ## 8-byte Reload
	movq	-88(%rsp), %rdx                 ## 8-byte Reload
	jmp	LBB30_22
	.p2align	4, 0x90
LBB30_27:                               ## %vector.body
                                        ##   in Loop: Header=BB30_22 Depth=2
	vaddps	(%rsi,%rbp,4), %ymm9, %ymm3
	vmovups	%ymm3, (%rsi,%rbp,4)
	addq	$16, %rbp
	addq	$2, %rax
	je	LBB30_28
LBB30_22:                               ## %vector.body
                                        ##   Parent Loop BB30_6 Depth=1
                                        ## =>  This Inner Loop Header: Depth=2
	vucomiss	%xmm0, %xmm4
	vxorps	%xmm9, %xmm9, %xmm9
	ja	LBB30_23
## %bb.24:                              ## %vector.body
                                        ##   in Loop: Header=BB30_22 Depth=2
	vxorps	%xmm3, %xmm3, %xmm3
	vaddps	-32(%rsi,%rbp,4), %ymm3, %ymm3
	vmovups	%ymm3, -32(%rsi,%rbp,4)
	jbe	LBB30_27
	jmp	LBB30_26
	.p2align	4, 0x90
LBB30_23:                               ##   in Loop: Header=BB30_22 Depth=2
	vmulps	-32(%rbx,%rbp,4), %ymm8, %ymm3
	vdivps	%ymm7, %ymm3, %ymm3
	vaddps	-32(%rsi,%rbp,4), %ymm3, %ymm3
	vmovups	%ymm3, -32(%rsi,%rbp,4)
	jbe	LBB30_27
LBB30_26:                               ##   in Loop: Header=BB30_22 Depth=2
	vmulps	(%rbx,%rbp,4), %ymm8, %ymm3
	vdivps	%ymm7, %ymm3, %ymm9
	jmp	LBB30_27
LBB30_20:                               ##   in Loop: Header=BB30_6 Depth=1
	xorl	%ebp, %ebp
	movq	(%rsp), %rcx                    ## 8-byte Reload
	movq	-88(%rsp), %rdx                 ## 8-byte Reload
LBB30_28:                               ## %middle.block.unr-lcssa
                                        ##   in Loop: Header=BB30_6 Depth=1
	testb	$1, 88(%rsp)                    ## 1-byte Folded Reload
	je	LBB30_32
## %bb.29:                              ## %vector.body.epil
                                        ##   in Loop: Header=BB30_6 Depth=1
	movq	56(%rsp), %rax                  ## 8-byte Reload
	movq	-128(%rsp), %rsi                ## 8-byte Reload
	addl	%esi, %eax
	imull	%r10d, %eax
	addl	-16(%rsp), %eax                 ## 4-byte Folded Reload
	cltq
	addq	32(%rsp), %rbp                  ## 8-byte Folded Reload
	addq	%rbp, %rax
	vucomiss	%xmm0, %xmm4
	vmulss	%xmm1, %xmm6, %xmm8
	vxorps	%xmm9, %xmm9, %xmm9
	jbe	LBB30_31
## %bb.30:                              ## %select.true.sink143
                                        ##   in Loop: Header=BB30_6 Depth=1
	movq	-128(%rsp), %rsi                ## 8-byte Reload
	addq	%rsi, %rcx
	imulq	-8(%rsp), %rcx                  ## 8-byte Folded Reload
	addq	%rcx, %rbp
	vbroadcastss	%xmm8, %ymm3
	vmulps	(%rdx,%rbp,4), %ymm3, %ymm3
	vdivps	%ymm7, %ymm3, %ymm9
LBB30_31:                               ## %select.end142
                                        ##   in Loop: Header=BB30_6 Depth=1
	movq	-40(%rsp), %rcx                 ## 8-byte Reload
	vaddps	(%rcx,%rax,4), %ymm9, %ymm3
	vmovups	%ymm3, (%rcx,%rax,4)
LBB30_32:                               ## %middle.block
                                        ##   in Loop: Header=BB30_6 Depth=1
	movq	104(%rsp), %rcx                 ## 8-byte Reload
	movq	%rcx, %rax
	cmpq	%r8, %rcx
	movq	-40(%rsp), %rbp                 ## 8-byte Reload
	je	LBB30_18
LBB30_33:                               ## %"for relu1_0_d_def__.s22.n.ni.rebased.us.preheader"
                                        ##   in Loop: Header=BB30_6 Depth=1
	movq	120(%rsp), %rcx                 ## 8-byte Reload
	leaq	(%rcx,%r13,4), %rdx
	jmp	LBB30_15
	.p2align	4, 0x90
LBB30_17:                               ## %select.end190
                                        ##   in Loop: Header=BB30_15 Depth=2
	vaddss	(%rdx,%rax,4), %xmm7, %xmm3
	vmovss	%xmm3, (%rdx,%rax,4)
	incq	%rax
	cmpq	%rax, %r8
	je	LBB30_18
LBB30_15:                               ## %"for relu1_0_d_def__.s22.n.ni.rebased.us"
                                        ##   Parent Loop BB30_6 Depth=1
                                        ## =>  This Inner Loop Header: Depth=2
	vucomiss	%xmm0, %xmm4
	vmulss	(%r12,%rax,4), %xmm1, %xmm3
	vxorps	%xmm7, %xmm7, %xmm7
	jbe	LBB30_17
## %bb.16:                              ## %select.true.sink191
                                        ##   in Loop: Header=BB30_15 Depth=2
	vmulss	%xmm6, %xmm3, %xmm3
	vdivss	%xmm5, %xmm3, %xmm7
	jmp	LBB30_17
LBB30_2:                                ## %"for relu1_0_d_def__.s22.w.wi.preheader4"
	decl	%ebx
	movq	-80(%rsp), %rsi                 ## 8-byte Reload
	vmovd	%esi, %xmm0
	vpbroadcastd	%xmm0, %ymm0
	vpor	LCPI30_0(%rip), %ymm0, %ymm0
	vmovd	%ebx, %xmm1
	vpbroadcastd	%xmm1, %ymm1
	vpminsd	%ymm0, %ymm1, %ymm11
	movq	%r9, %rdx
	movslq	-48(%rsp), %rbx                 ## 4-byte Folded Reload
	movl	-56(%rsp), %r9d                 ## 4-byte Reload
	imulq	%rcx, %r14
	addq	%rsi, %r14
	movq	-88(%rsp), %rsi                 ## 8-byte Reload
	leaq	(%rsi,%r14,4), %r14
	shlq	$2, %rcx
	movq	%rcx, %r12
	shll	$6, %edx
	shll	$6, %eax
	subl	%eax, %edx
	orl	$22, %edx
	movq	-128(%rsp), %rcx                ## 8-byte Reload
	imull	%edx, %ecx
	movq	-104(%rsp), %rax                ## 8-byte Reload
	leal	(%rcx,%rax,8), %edx
	vbroadcastss	LCPI30_1(%rip), %ymm10  ## ymm10 = [1.0E+0,1.0E+0,1.0E+0,1.0E+0,1.0E+0,1.0E+0,1.0E+0,1.0E+0]
	vbroadcastss	LCPI30_2(%rip), %ymm9   ## ymm9 = [1.99999994E-9,1.99999994E-9,1.99999994E-9,1.99999994E-9,1.99999994E-9,1.99999994E-9,1.99999994E-9,1.99999994E-9]
	movl	-92(%rsp), %r10d                ## 4-byte Reload
	.p2align	4, 0x90
LBB30_3:                                ## %"for relu1_0_d_def__.s22.w.wi"
                                        ## =>This Inner Loop Header: Depth=1
	testq	%rbx, %rbx
	movl	$0, %eax
	cmovgl	%ebx, %eax
	imull	-116(%rsp), %eax                ## 4-byte Folded Reload
	leal	(%rax,%r13), %esi
	vmovd	%esi, %xmm3
	vpbroadcastd	%xmm3, %ymm3
	vpaddd	%ymm3, %ymm11, %ymm3
	vmovd	%xmm3, %esi
	vpextrd	$1, %xmm3, %ecx
	movslq	%esi, %rsi
	vpextrd	$2, %xmm3, %ebp
	movslq	%ecx, %rcx
	vmovss	(%rdi,%rsi,4), %xmm4            ## xmm4 = mem[0],zero,zero,zero
	vpextrd	$3, %xmm3, %esi
	movslq	%ebp, %rbp
	vextracti128	$1, %ymm3, %xmm3
	vinsertps	$16, (%rdi,%rcx,4), %xmm4, %xmm4 ## xmm4 = xmm4[0],mem[0],xmm4[2,3]
	vmovd	%xmm3, %ecx
	vinsertps	$32, (%rdi,%rbp,4), %xmm4, %xmm4 ## xmm4 = xmm4[0,1],mem[0],xmm4[3]
	leal	(%rax,%r15), %ebp
	movslq	%ecx, %rcx
	vmovd	%ebp, %xmm5
	vpbroadcastd	%xmm5, %ymm5
	vpaddd	%ymm5, %ymm11, %ymm5
	vmovss	(%rdi,%rcx,4), %xmm7            ## xmm7 = mem[0],zero,zero,zero
	vmovd	%xmm5, %ecx
	vpextrd	$1, %xmm5, %ebp
	movslq	%ecx, %rcx
	vmovss	(%rdi,%rcx,4), %xmm6            ## xmm6 = mem[0],zero,zero,zero
	vpextrd	$2, %xmm5, %ecx
	movslq	%esi, %rsi
	movslq	%ebp, %rbp
	movslq	%ecx, %rcx
	vinsertps	$48, (%rdi,%rsi,4), %xmm4, %xmm12 ## xmm12 = xmm4[0,1,2],mem[0]
	vpextrd	$3, %xmm5, %esi
	vextracti128	$1, %ymm5, %xmm5
	movslq	%esi, %rsi
	vinsertps	$16, (%rdi,%rbp,4), %xmm6, %xmm6 ## xmm6 = xmm6[0],mem[0],xmm6[2,3]
	vpextrd	$1, %xmm3, %ebp
	vinsertps	$32, (%rdi,%rcx,4), %xmm6, %xmm6 ## xmm6 = xmm6[0,1],mem[0],xmm6[3]
	leal	(%rax,%r8), %ecx
	movslq	%ebp, %rbp
	vmovd	%ecx, %xmm2
	vmovd	%xmm5, %ecx
	movslq	%ecx, %rcx
	vmovss	(%rdi,%rcx,4), %xmm1            ## xmm1 = mem[0],zero,zero,zero
	vpbroadcastd	%xmm2, %ymm2
	vpaddd	%ymm2, %ymm11, %ymm2
	vinsertps	$48, (%rdi,%rsi,4), %xmm6, %xmm13 ## xmm13 = xmm6[0,1,2],mem[0]
	vmovd	%xmm2, %ecx
	movslq	%ecx, %rcx
	vinsertps	$16, (%rdi,%rbp,4), %xmm7, %xmm0 ## xmm0 = xmm7[0],mem[0],xmm7[2,3]
	vpextrd	$1, %xmm2, %esi
	vpextrd	$1, %xmm5, %ebp
	vmovss	(%rdi,%rcx,4), %xmm4            ## xmm4 = mem[0],zero,zero,zero
	movslq	%esi, %rcx
	vextracti128	$1, %ymm2, %xmm7
	movslq	%ebp, %rsi
	movq	-40(%rsp), %rbp                 ## 8-byte Reload
	vinsertps	$16, (%rdi,%rcx,4), %xmm4, %xmm4 ## xmm4 = xmm4[0],mem[0],xmm4[2,3]
	vinsertps	$16, (%rdi,%rsi,4), %xmm1, %xmm1 ## xmm1 = xmm1[0],mem[0],xmm1[2,3]
	vmovd	%xmm7, %ecx
	vpextrd	$1, %xmm7, %esi
	movslq	%ecx, %rcx
	vmovss	(%rdi,%rcx,4), %xmm6            ## xmm6 = mem[0],zero,zero,zero
	movslq	%esi, %rcx
	vinsertps	$16, (%rdi,%rcx,4), %xmm6, %xmm6 ## xmm6 = xmm6[0],mem[0],xmm6[2,3]
	vpextrd	$2, %xmm2, %ecx
	movslq	%ecx, %rcx
	vinsertps	$32, (%rdi,%rcx,4), %xmm4, %xmm4 ## xmm4 = xmm4[0,1],mem[0],xmm4[3]
	vpextrd	$2, %xmm3, %ecx
	movslq	%ecx, %rcx
	vinsertps	$32, (%rdi,%rcx,4), %xmm0, %xmm0 ## xmm0 = xmm0[0,1],mem[0],xmm0[3]
	vpextrd	$3, %xmm2, %ecx
	movslq	%ecx, %rcx
	vinsertps	$48, (%rdi,%rcx,4), %xmm4, %xmm8 ## xmm8 = xmm4[0,1,2],mem[0]
	vpextrd	$2, %xmm5, %ecx
	movslq	%ecx, %rcx
	vinsertps	$32, (%rdi,%rcx,4), %xmm1, %xmm14 ## xmm14 = xmm1[0,1],mem[0],xmm1[3]
	addl	%r11d, %eax
	vmovd	%eax, %xmm2
	vpextrd	$2, %xmm7, %eax
	cltq
	vpbroadcastd	%xmm2, %ymm2
	vinsertps	$32, (%rdi,%rax,4), %xmm6, %xmm4 ## xmm4 = xmm6[0,1],mem[0],xmm6[3]
	vpaddd	%ymm2, %ymm11, %ymm2
	vextracti128	$1, %ymm2, %xmm6
	vmovd	%xmm6, %eax
	cltq
	vmovss	(%rdi,%rax,4), %xmm1            ## xmm1 = mem[0],zero,zero,zero
	vpextrd	$1, %xmm6, %eax
	cltq
	vinsertps	$16, (%rdi,%rax,4), %xmm1, %xmm1 ## xmm1 = xmm1[0],mem[0],xmm1[2,3]
	vpextrd	$3, %xmm3, %eax
	cltq
	vinsertps	$48, (%rdi,%rax,4), %xmm0, %xmm0 ## xmm0 = xmm0[0,1,2],mem[0]
	vpextrd	$2, %xmm6, %eax
	cltq
	vinsertps	$32, (%rdi,%rax,4), %xmm1, %xmm1 ## xmm1 = xmm1[0,1],mem[0],xmm1[3]
	vmovd	%xmm2, %eax
	cltq
	vmovss	(%rdi,%rax,4), %xmm3            ## xmm3 = mem[0],zero,zero,zero
	vpextrd	$1, %xmm2, %eax
	cltq
	vinsertps	$16, (%rdi,%rax,4), %xmm3, %xmm3 ## xmm3 = xmm3[0],mem[0],xmm3[2,3]
	vpextrd	$3, %xmm7, %eax
	cltq
	vinsertps	$48, (%rdi,%rax,4), %xmm4, %xmm4 ## xmm4 = xmm4[0,1,2],mem[0]
	vpextrd	$2, %xmm2, %eax
	cltq
	vinsertps	$32, (%rdi,%rax,4), %xmm3, %xmm3 ## xmm3 = xmm3[0,1],mem[0],xmm3[3]
	vpextrd	$3, %xmm6, %eax
	cltq
	vinsertps	$48, (%rdi,%rax,4), %xmm1, %xmm1 ## xmm1 = xmm1[0,1,2],mem[0]
	vpextrd	$3, %xmm2, %eax
	cltq
	vinsertps	$48, (%rdi,%rax,4), %xmm3, %xmm2 ## xmm2 = xmm3[0,1,2],mem[0]
	vpextrd	$3, %xmm5, %eax
	cltq
	vinsertps	$48, (%rdi,%rax,4), %xmm14, %xmm3 ## xmm3 = xmm14[0,1,2],mem[0]
	vinsertf128	$1, %xmm0, %ymm12, %ymm0
	movslq	%edx, %rdx
	vinsertf128	$1, %xmm3, %ymm13, %ymm3
	vinsertf128	$1, %xmm4, %ymm8, %ymm4
	vmulps	(%r14), %ymm9, %ymm5
	vmaxps	%ymm10, %ymm3, %ymm3
	vinsertf128	$1, %xmm1, %ymm2, %ymm1
	vaddps	%ymm1, %ymm4, %ymm1
	vmulps	%ymm1, %ymm5, %ymm1
	vcmpltps	%ymm0, %ymm10, %ymm0
	vdivps	%ymm3, %ymm1, %ymm1
	vandps	%ymm1, %ymm0, %ymm0
	vaddps	(%rbp,%rdx,4), %ymm0, %ymm0
	vmovups	%ymm0, (%rbp,%rdx,4)
	addq	%r12, %r14
	incq	%rbx
	addl	%r10d, %edx
	decq	%r9
	jne	LBB30_3
LBB30_4:                                ## %destructor_block
	xorl	%eax, %eax
	addq	$184, %rsp
	popq	%rbx
	popq	%r12
	popq	%r13
	popq	%r14
	popq	%r15
	popq	%rbp
	vzeroupper
	retq
                                        ## -- End function
	.section	__TEXT,__literal4,4byte_literals
	.p2align	2                               ## -- Begin function train_cost_model.par_for.relu1_0_d_def__.s23.n.n.n
LCPI31_0:
	.long	2                               ## 0x2
LCPI31_2:
	.long	0x3f800000                      ## float 1
LCPI31_3:
	.long	0x3109705f                      ## float 1.99999994E-9
	.section	__TEXT,__const
	.p2align	5
LCPI31_1:
	.long	0                               ## 0x0
	.long	1                               ## 0x1
	.long	2                               ## 0x2
	.long	3                               ## 0x3
	.long	4                               ## 0x4
	.long	5                               ## 0x5
	.long	6                               ## 0x6
	.long	7                               ## 0x7
	.section	__TEXT,__text,regular,pure_instructions
	.p2align	4, 0x90
_train_cost_model.par_for.relu1_0_d_def__.s23.n.n.n: ## @train_cost_model.par_for.relu1_0_d_def__.s23.n.n.n
## %bb.0:                               ## %entry
	pushq	%rbp
	pushq	%r15
	pushq	%r14
	pushq	%r13
	pushq	%r12
	pushq	%rbx
	subq	$56, %rsp
	movq	%rdx, %rdi
	movl	8(%rdx), %ebp
	movl	24(%rdx), %r15d
	movl	%esi, %r10d
	sarl	$31, %r10d
	xorl	%r12d, %r12d
	testl	%r15d, %r15d
	sete	%r12b
	movl	%r12d, %ecx
	negl	%ecx
	subl	%r10d, %esi
	orl	%r15d, %ecx
	movl	%esi, %eax
	cltd
	idivl	%ecx
	movl	%edx, %r11d
	movl	%r15d, %ecx
	sarl	$31, %ecx
	movl	%ecx, %r13d
	notl	%r13d
	leal	(%r15,%r12), %ebx
	movl	%esi, %eax
	cltd
	idivl	%ebx
	decl	%r12d
	movl	%r13d, %r14d
	subl	%ecx, %r14d
	andl	%r10d, %r14d
	addl	%eax, %r14d
	andl	%r12d, %r14d
	leal	(%r14,%r14), %eax
	movl	%ebp, %esi
	subl	%eax, %esi
	movl	$1, %ebx
	subl	%eax, %ebx
	cmpl	$1, %ebp
	cmovlel	%esi, %ebx
	vmovd	%esi, %xmm0
	vpinsrd	$1, %ebx, %xmm0, %xmm0
	vpbroadcastd	LCPI31_0(%rip), %xmm1   ## xmm1 = [2,2,2,2]
	vpminsd	%xmm1, %xmm0, %xmm0
	vpxor	%xmm1, %xmm1, %xmm1
	vpmaxsd	%xmm1, %xmm0, %xmm0
	vmovd	%xmm0, %eax
	vpextrd	$1, %xmm0, %edx
	movq	%rdx, -72(%rsp)                 ## 8-byte Spill
	subl	%edx, %eax
	movl	%eax, -120(%rsp)                ## 4-byte Spill
	jle	LBB31_18
## %bb.1:                               ## %"for relu1_0_d_def__.s23.w.wi.rebased.preheader"
	movl	(%rdi), %ebx
	movl	12(%rdi), %edx
	movslq	20(%rdi), %rax
	movq	%rax, -104(%rsp)                ## 8-byte Spill
	movl	28(%rdi), %eax
	movl	%eax, -128(%rsp)                ## 4-byte Spill
	movslq	32(%rdi), %rax
	movq	%rax, -96(%rsp)                 ## 8-byte Spill
	movl	44(%rdi), %r8d
	movl	48(%rdi), %r9d
	movq	56(%rdi), %rax
	movq	%rax, -80(%rsp)                 ## 8-byte Spill
	movq	72(%rdi), %rax
	movq	%rax, -88(%rsp)                 ## 8-byte Spill
	movq	88(%rdi), %rsi
	xorl	%r15d, %ecx
	addl	%r13d, %ecx
	andl	%r10d, %ecx
	addl	%ecx, %r11d
	andl	%r12d, %r11d
	movq	%rdx, %r12
	leal	(,%r11,8), %ecx
	movl	%r12d, %eax
	shll	$5, %eax
	movl	%eax, -124(%rsp)                ## 4-byte Spill
	movl	%ebx, %eax
	subl	%ecx, %eax
	cmpl	$8, %eax
	movl	$8, %edx
	cmovll	%eax, %edx
	xorl	%ebp, %ebp
	testl	%eax, %eax
	cmovgl	%edx, %ebp
	movl	%ebp, -108(%rsp)                ## 4-byte Spill
	leal	8(%rcx), %edx
	movslq	%ecx, %rbp
	movq	-72(%rsp), %r13                 ## 8-byte Reload
	leal	(%r13,%r14,2), %r15d
	movslq	40(%rdi), %rcx
	movslq	4(%rdi), %r10
	cmpl	%ebx, %edx
	jle	LBB31_2
## %bb.4:                               ## %"for relu1_0_d_def__.s23.w.wi.rebased.preheader.split.us"
	movq	-96(%rsp), %rbx                 ## 8-byte Reload
	movq	%rcx, -56(%rsp)                 ## 8-byte Spill
	movq	%rbp, -48(%rsp)                 ## 8-byte Spill
	movq	%r10, -40(%rsp)                 ## 8-byte Spill
	testl	%eax, %eax
	movl	-108(%rsp), %ecx                ## 4-byte Reload
	jle	LBB31_18
## %bb.5:                               ## %"for relu1_0_d_def__.s23.w.wi.rebased.us.us.preheader"
	subl	%ebx, %r8d
	subl	%ebx, %r9d
	leal	(%r12,%r12,4), %eax
	leal	(%r12,%rax,4), %edx
	movq	-48(%rsp), %rdi                 ## 8-byte Reload
	movl	%edi, %eax
	subl	%ebx, %eax
	addl	%edi, %edx
	movl	%edx, -64(%rsp)                 ## 4-byte Spill
	addl	%edi, %r9d
	addl	%edi, %r8d
	movslq	%r8d, %r8
	movslq	%r9d, %r9
	cltq
	movq	%rax, -24(%rsp)                 ## 8-byte Spill
	movl	%ecx, %r10d
	movslq	%r15d, %rbp
	movl	-120(%rsp), %eax                ## 4-byte Reload
	movq	%rax, 48(%rsp)                  ## 8-byte Spill
	movl	%r10d, %edx
	andl	$2147483640, %edx               ## imm = 0x7FFFFFF8
	movq	-104(%rsp), %rax                ## 8-byte Reload
	imulq	%rbp, %rax
	leaq	(%rax,%rdi), %r15
	subq	%rbx, %r15
	movq	%rdx, 32(%rsp)                  ## 8-byte Spill
	leaq	-8(%rdx), %rbx
	leal	(%r13,%r14,2), %edx
	movq	%rbx, 24(%rsp)                  ## 8-byte Spill
	movq	%rbx, %rcx
	shrq	$3, %rcx
	incq	%rcx
	subl	-128(%rsp), %edx                ## 4-byte Folded Reload
	shll	$5, %edx
	orl	$21, %edx
	imull	%r12d, %edx
	movq	%rbp, %rbx
	movq	%rbp, -32(%rsp)                 ## 8-byte Spill
	subq	-56(%rsp), %rbp                 ## 8-byte Folded Reload
	movq	-40(%rsp), %rbx                 ## 8-byte Reload
	imulq	%rbx, %rbp
	addq	%rdi, %rbp
	leaq	(%rsi,%r15,4), %rdi
	leal	(%rdx,%r11,8), %r12d
	movq	%r9, -16(%rsp)                  ## 8-byte Spill
	addq	%rax, %r9
	movq	%r8, %rdx
	movq	%r8, -8(%rsp)                   ## 8-byte Spill
	addq	%r8, %rax
	movq	%rcx, %rdx
	movq	%rcx, 16(%rsp)                  ## 8-byte Spill
	andq	$-2, %rcx
	negq	%rcx
	movq	%rcx, 8(%rsp)                   ## 8-byte Spill
	movq	-80(%rsp), %rdx                 ## 8-byte Reload
	leaq	(%rdx,%rbp,4), %r15
	addq	$32, %r15
	leaq	(%rdx,%rbp,4), %r8
	leaq	32(%rsi,%r9,4), %r13
	leaq	(%rsi,%r9,4), %r9
	leaq	32(%rsi,%rax,4), %r14
	leaq	(%rsi,%rax,4), %r11
	vmovss	LCPI31_3(%rip), %xmm0           ## xmm0 = mem[0],zero,zero,zero
	vmovss	LCPI31_2(%rip), %xmm1           ## xmm1 = mem[0],zero,zero,zero
	vbroadcastss	LCPI31_2(%rip), %ymm2   ## ymm2 = [1.0E+0,1.0E+0,1.0E+0,1.0E+0,1.0E+0,1.0E+0,1.0E+0,1.0E+0]
	vbroadcastss	LCPI31_3(%rip), %ymm3   ## ymm3 = [1.99999994E-9,1.99999994E-9,1.99999994E-9,1.99999994E-9,1.99999994E-9,1.99999994E-9,1.99999994E-9,1.99999994E-9]
	leaq	(,%rbx,4), %rax
	movq	%rax, -96(%rsp)                 ## 8-byte Spill
	movq	-104(%rsp), %rax                ## 8-byte Reload
	leaq	(,%rax,4), %rax
	movq	%rax, -72(%rsp)                 ## 8-byte Spill
	movq	-88(%rsp), %rax                 ## 8-byte Reload
	addq	$32, %rax
	movq	%rax, (%rsp)                    ## 8-byte Spill
	xorl	%ebp, %ebp
	jmp	LBB31_6
	.p2align	4, 0x90
LBB31_17:                               ## %after_bb.loopexit.us.us
                                        ##   in Loop: Header=BB31_6 Depth=1
	incq	%rbp
	movq	-96(%rsp), %rax                 ## 8-byte Reload
	addq	%rax, %r15
	movq	-72(%rsp), %rdx                 ## 8-byte Reload
	addq	%rdx, %rdi
	addq	%rdx, %r13
	addq	%rdx, %r14
	addl	-124(%rsp), %r12d               ## 4-byte Folded Reload
	addq	%rax, %r8
	addq	%rdx, %r9
	addq	%rdx, %r11
	cmpq	48(%rsp), %rbp                  ## 8-byte Folded Reload
	je	LBB31_18
LBB31_6:                                ## %"for relu1_0_d_def__.s23.w.wi.rebased.us.us"
                                        ## =>This Loop Header: Depth=1
                                        ##     Child Loop BB31_11 Depth 2
                                        ##     Child Loop BB31_16 Depth 2
	movslq	%r12d, %rbx
	cmpl	$8, -108(%rsp)                  ## 4-byte Folded Reload
	jae	LBB31_8
## %bb.7:                               ##   in Loop: Header=BB31_6 Depth=1
	xorl	%eax, %eax
	jmp	LBB31_15
	.p2align	4, 0x90
LBB31_8:                                ## %vector.ph
                                        ##   in Loop: Header=BB31_6 Depth=1
	cmpq	$0, 24(%rsp)                    ## 8-byte Folded Reload
	movl	%r12d, -60(%rsp)                ## 4-byte Spill
	movq	%rbp, -120(%rsp)                ## 8-byte Spill
	movq	%rbx, 40(%rsp)                  ## 8-byte Spill
	je	LBB31_9
## %bb.10:                              ## %vector.body.preheader
                                        ##   in Loop: Header=BB31_6 Depth=1
	movq	(%rsp), %rax                    ## 8-byte Reload
	leaq	(%rax,%rbx,4), %r12
	movq	8(%rsp), %rax                   ## 8-byte Reload
	xorl	%edx, %edx
	.p2align	4, 0x90
LBB31_11:                               ## %vector.body
                                        ##   Parent Loop BB31_6 Depth=1
                                        ## =>  This Inner Loop Header: Depth=2
	vmovups	(%rdi,%rdx,4), %ymm4
	vmulps	-32(%r15,%rdx,4), %ymm4, %ymm4
	vmulps	-32(%r13,%rdx,4), %ymm4, %ymm4
	vmovups	-32(%r14,%rdx,4), %ymm5
	vmulps	%ymm3, %ymm4, %ymm4
	vcmpleps	%ymm2, %ymm5, %ymm5
	vandps	%ymm4, %ymm5, %ymm4
	vaddps	-32(%r12,%rdx,4), %ymm4, %ymm4
	vmovups	%ymm4, -32(%r12,%rdx,4)
	vmovups	32(%rdi,%rdx,4), %ymm4
	vmulps	(%r15,%rdx,4), %ymm4, %ymm4
	vmovups	(%r14,%rdx,4), %ymm5
	vmulps	(%r13,%rdx,4), %ymm4, %ymm4
	vmulps	%ymm3, %ymm4, %ymm4
	vcmpleps	%ymm2, %ymm5, %ymm5
	vandps	%ymm4, %ymm5, %ymm4
	vaddps	(%r12,%rdx,4), %ymm4, %ymm4
	vmovups	%ymm4, (%r12,%rdx,4)
	addq	$16, %rdx
	addq	$2, %rax
	jne	LBB31_11
## %bb.12:                              ## %middle.block.unr-lcssa
                                        ##   in Loop: Header=BB31_6 Depth=1
	testb	$1, 16(%rsp)                    ## 1-byte Folded Reload
	movq	-104(%rsp), %r12                ## 8-byte Reload
	je	LBB31_14
LBB31_13:                               ## %vector.body.epil
                                        ##   in Loop: Header=BB31_6 Depth=1
	movq	-32(%rsp), %rax                 ## 8-byte Reload
	movq	-120(%rsp), %rcx                ## 8-byte Reload
	addq	%rcx, %rax
	movl	%eax, %ebp
	movq	%rax, %rbx
	imulq	%r12, %rax
	addq	%rdx, %rax
	movq	-8(%rsp), %rcx                  ## 8-byte Reload
	leaq	(%rax,%rcx), %r12
	movq	-16(%rsp), %rcx                 ## 8-byte Reload
	addq	%rax, %rcx
	addq	-24(%rsp), %rax                 ## 8-byte Folded Reload
	vmovups	(%rsi,%rax,4), %ymm4
	subl	-128(%rsp), %ebp                ## 4-byte Folded Reload
	imull	-124(%rsp), %ebp                ## 4-byte Folded Reload
	addl	-64(%rsp), %ebp                 ## 4-byte Folded Reload
	subq	-56(%rsp), %rbx                 ## 8-byte Folded Reload
	imulq	-40(%rsp), %rbx                 ## 8-byte Folded Reload
	addq	-48(%rsp), %rbx                 ## 8-byte Folded Reload
	movslq	%ebp, %rax
	addq	%rdx, %rax
	addq	%rdx, %rbx
	movq	-80(%rsp), %rdx                 ## 8-byte Reload
	vmulps	(%rdx,%rbx,4), %ymm4, %ymm4
	vmulps	(%rsi,%rcx,4), %ymm4, %ymm4
	vmovups	(%rsi,%r12,4), %ymm5
	vmulps	%ymm3, %ymm4, %ymm4
	vcmpleps	%ymm2, %ymm5, %ymm5
	vandps	%ymm4, %ymm5, %ymm4
	movq	-88(%rsp), %rdx                 ## 8-byte Reload
	vaddps	(%rdx,%rax,4), %ymm4, %ymm4
	vmovups	%ymm4, (%rdx,%rax,4)
LBB31_14:                               ## %middle.block
                                        ##   in Loop: Header=BB31_6 Depth=1
	movq	32(%rsp), %rcx                  ## 8-byte Reload
	movq	%rcx, %rax
	cmpq	%r10, %rcx
	movl	-60(%rsp), %r12d                ## 4-byte Reload
	movq	-120(%rsp), %rbp                ## 8-byte Reload
	movq	40(%rsp), %rbx                  ## 8-byte Reload
	je	LBB31_17
LBB31_15:                               ## %"for relu1_0_d_def__.s23.n.ni.us.us.preheader"
                                        ##   in Loop: Header=BB31_6 Depth=1
	movq	-88(%rsp), %rdx                 ## 8-byte Reload
	leaq	(%rdx,%rbx,4), %rdx
	.p2align	4, 0x90
LBB31_16:                               ## %"for relu1_0_d_def__.s23.n.ni.us.us"
                                        ##   Parent Loop BB31_6 Depth=1
                                        ## =>  This Inner Loop Header: Depth=2
	vmovss	(%rdi,%rax,4), %xmm4            ## xmm4 = mem[0],zero,zero,zero
	vmulss	(%r8,%rax,4), %xmm4, %xmm4
	vmulss	(%r9,%rax,4), %xmm4, %xmm4
	vcmpltss	(%r11,%rax,4), %xmm1, %xmm5
	vmulss	%xmm0, %xmm4, %xmm4
	vandnps	%xmm4, %xmm5, %xmm4
	vaddss	(%rdx,%rax,4), %xmm4, %xmm4
	vmovss	%xmm4, (%rdx,%rax,4)
	incq	%rax
	cmpq	%rax, %r10
	jne	LBB31_16
	jmp	LBB31_17
LBB31_9:                                ##   in Loop: Header=BB31_6 Depth=1
	xorl	%edx, %edx
	testb	$1, 16(%rsp)                    ## 1-byte Folded Reload
	movq	-104(%rsp), %r12                ## 8-byte Reload
	jne	LBB31_13
	jmp	LBB31_14
LBB31_2:                                ## %"for relu1_0_d_def__.s23.w.wi.rebased.preheader4"
	decl	%ebx
	vmovd	%ebx, %xmm0
	movslq	%r15d, %rax
	subq	%rcx, %rax
	vmovd	%ebp, %xmm1
	imulq	%r10, %rax
	leal	(%r13,%r14,2), %edx
	movl	%edx, %ecx
	subl	16(%rdi), %ecx
	addq	%rbp, %rax
	movq	-104(%rsp), %rbx                ## 8-byte Reload
	imull	%ebx, %ecx
	subl	36(%rdi), %ecx
	movq	-80(%rsp), %rdi                 ## 8-byte Reload
	leaq	(%rdi,%rax,4), %rdi
	movl	%ebx, %eax
	imull	%edx, %eax
	subl	-128(%rsp), %edx                ## 4-byte Folded Reload
	shll	$5, %edx
	orl	$21, %edx
	imull	%edx, %r12d
	leal	(%r12,%r11,8), %edx
	addl	%eax, %r9d
	addl	%eax, %r8d
	movq	-96(%rsp), %rax                 ## 8-byte Reload
	subl	%eax, %r9d
	subl	%eax, %r8d
	vpbroadcastd	%xmm1, %ymm1
	vpor	LCPI31_1(%rip), %ymm1, %ymm1
	vpbroadcastd	%xmm0, %ymm0
	vpminsd	%ymm1, %ymm0, %ymm0
	movl	-120(%rsp), %eax                ## 4-byte Reload
	shlq	$2, %r10
	movq	%r10, %r11
	vbroadcastss	LCPI31_2(%rip), %ymm8   ## ymm8 = [1.0E+0,1.0E+0,1.0E+0,1.0E+0,1.0E+0,1.0E+0,1.0E+0,1.0E+0]
	vbroadcastss	LCPI31_3(%rip), %ymm9   ## ymm9 = [1.99999994E-9,1.99999994E-9,1.99999994E-9,1.99999994E-9,1.99999994E-9,1.99999994E-9,1.99999994E-9,1.99999994E-9]
	movq	%rbx, %r10
	.p2align	4, 0x90
LBB31_3:                                ## %"for relu1_0_d_def__.s23.w.wi.rebased"
                                        ## =>This Inner Loop Header: Depth=1
	vmovd	%r8d, %xmm3
	vpbroadcastd	%xmm3, %ymm3
	vpaddd	%ymm0, %ymm3, %ymm4
	vextracti128	$1, %ymm4, %xmm3
	vmovd	%xmm3, %ebx
	movslq	%ebx, %rbx
	vpextrd	$1, %xmm3, %ebp
	movslq	%ebp, %rbp
	vmovss	(%rsi,%rbx,4), %xmm5            ## xmm5 = mem[0],zero,zero,zero
	vpextrd	$2, %xmm3, %ebx
	vinsertps	$16, (%rsi,%rbp,4), %xmm5, %xmm5 ## xmm5 = xmm5[0],mem[0],xmm5[2,3]
	vmovd	%xmm4, %ebp
	movslq	%ebx, %rbx
	vinsertps	$32, (%rsi,%rbx,4), %xmm5, %xmm5 ## xmm5 = xmm5[0,1],mem[0],xmm5[3]
	movslq	%ebp, %rbp
	vpextrd	$3, %xmm3, %ebx
	movslq	%ebx, %rbx
	vinsertps	$48, (%rsi,%rbx,4), %xmm5, %xmm3 ## xmm3 = xmm5[0,1,2],mem[0]
	vpextrd	$1, %xmm4, %ebx
	movslq	%ebx, %rbx
	vmovss	(%rsi,%rbp,4), %xmm5            ## xmm5 = mem[0],zero,zero,zero
	vpextrd	$2, %xmm4, %ebp
	vinsertps	$16, (%rsi,%rbx,4), %xmm5, %xmm5 ## xmm5 = xmm5[0],mem[0],xmm5[2,3]
	vpextrd	$3, %xmm4, %ebx
	movslq	%ebp, %rbp
	movslq	%ebx, %rbx
	vinsertps	$32, (%rsi,%rbp,4), %xmm5, %xmm4 ## xmm4 = xmm5[0,1],mem[0],xmm5[3]
	vinsertps	$48, (%rsi,%rbx,4), %xmm4, %xmm4 ## xmm4 = xmm4[0,1,2],mem[0]
	vmovd	%r9d, %xmm5
	vpbroadcastd	%xmm5, %ymm5
	vpaddd	%ymm0, %ymm5, %ymm6
	vextracti128	$1, %ymm6, %xmm5
	vmovd	%xmm5, %ebp
	movslq	%ebp, %rbp
	vpextrd	$1, %xmm5, %ebx
	movslq	%ebx, %rbx
	vmovss	(%rsi,%rbp,4), %xmm7            ## xmm7 = mem[0],zero,zero,zero
	vpextrd	$2, %xmm5, %ebp
	vinsertps	$16, (%rsi,%rbx,4), %xmm7, %xmm7 ## xmm7 = xmm7[0],mem[0],xmm7[2,3]
	vmovd	%xmm6, %ebx
	movslq	%ebp, %rbp
	vinsertps	$32, (%rsi,%rbp,4), %xmm7, %xmm7 ## xmm7 = xmm7[0,1],mem[0],xmm7[3]
	movslq	%ebx, %rbp
	vpextrd	$3, %xmm5, %ebx
	movslq	%ebx, %rbx
	vinsertps	$48, (%rsi,%rbx,4), %xmm7, %xmm5 ## xmm5 = xmm7[0,1,2],mem[0]
	vpextrd	$1, %xmm6, %ebx
	movslq	%ebx, %rbx
	vmovss	(%rsi,%rbp,4), %xmm7            ## xmm7 = mem[0],zero,zero,zero
	vpextrd	$2, %xmm6, %ebp
	vinsertps	$16, (%rsi,%rbx,4), %xmm7, %xmm7 ## xmm7 = xmm7[0],mem[0],xmm7[2,3]
	vpextrd	$3, %xmm6, %ebx
	movslq	%ebp, %rbp
	movslq	%ebx, %rbx
	vinsertps	$32, (%rsi,%rbp,4), %xmm7, %xmm6 ## xmm6 = xmm7[0,1],mem[0],xmm7[3]
	vinsertps	$48, (%rsi,%rbx,4), %xmm6, %xmm6 ## xmm6 = xmm6[0,1,2],mem[0]
	vmovd	%ecx, %xmm7
	vpbroadcastd	%xmm7, %ymm7
	vpaddd	%ymm0, %ymm7, %ymm7
	vextracti128	$1, %ymm7, %xmm1
	vmovd	%xmm1, %ebp
	movslq	%ebp, %rbp
	vpextrd	$1, %xmm1, %ebx
	movslq	%ebx, %rbx
	vmovss	(%rsi,%rbp,4), %xmm2            ## xmm2 = mem[0],zero,zero,zero
	vpextrd	$2, %xmm1, %ebp
	vinsertps	$16, (%rsi,%rbx,4), %xmm2, %xmm2 ## xmm2 = xmm2[0],mem[0],xmm2[2,3]
	vmovd	%xmm7, %ebx
	movslq	%ebp, %rbp
	vinsertps	$32, (%rsi,%rbp,4), %xmm2, %xmm2 ## xmm2 = xmm2[0,1],mem[0],xmm2[3]
	movslq	%ebx, %rbp
	vpextrd	$3, %xmm1, %ebx
	movslq	%ebx, %rbx
	vinsertps	$48, (%rsi,%rbx,4), %xmm2, %xmm1 ## xmm1 = xmm2[0,1,2],mem[0]
	vpextrd	$1, %xmm7, %ebx
	vmovss	(%rsi,%rbp,4), %xmm2            ## xmm2 = mem[0],zero,zero,zero
	movslq	%ebx, %rbp
	vinsertps	$16, (%rsi,%rbp,4), %xmm2, %xmm2 ## xmm2 = xmm2[0],mem[0],xmm2[2,3]
	vpextrd	$2, %xmm7, %ebp
	movslq	%ebp, %rbp
	vinsertps	$32, (%rsi,%rbp,4), %xmm2, %xmm2 ## xmm2 = xmm2[0,1],mem[0],xmm2[3]
	vpextrd	$3, %xmm7, %ebp
	movslq	%ebp, %rbp
	vinsertps	$48, (%rsi,%rbp,4), %xmm2, %xmm2 ## xmm2 = xmm2[0,1,2],mem[0]
	vinsertf128	$1, %xmm3, %ymm4, %ymm3
	vinsertf128	$1, %xmm5, %ymm6, %ymm4
	movslq	%edx, %rdx
	vcmpleps	%ymm8, %ymm3, %ymm3
	vmulps	%ymm4, %ymm9, %ymm4
	vinsertf128	$1, %xmm1, %ymm2, %ymm1
	vmulps	(%rdi), %ymm1, %ymm1
	vmulps	%ymm4, %ymm1, %ymm1
	vandps	%ymm1, %ymm3, %ymm1
	movq	-88(%rsp), %rbp                 ## 8-byte Reload
	vaddps	(%rbp,%rdx,4), %ymm1, %ymm1
	vmovups	%ymm1, (%rbp,%rdx,4)
	addq	%r11, %rdi
	addl	%r10d, %ecx
	addl	-124(%rsp), %edx                ## 4-byte Folded Reload
	addl	%r10d, %r9d
	addl	%r10d, %r8d
	decq	%rax
	jne	LBB31_3
LBB31_18:                               ## %destructor_block
	xorl	%eax, %eax
	addq	$56, %rsp
	popq	%rbx
	popq	%r12
	popq	%r13
	popq	%r14
	popq	%r15
	popq	%rbp
	vzeroupper
	retq
                                        ## -- End function
	.section	__TEXT,__const
	.p2align	5                               ## -- Begin function train_cost_model.par_for.relu1_0_d_def__.s24.n.n.n
LCPI32_0:
	.long	0                               ## 0x0
	.long	1                               ## 0x1
	.long	2                               ## 0x2
	.long	3                               ## 0x3
	.long	4                               ## 0x4
	.long	5                               ## 0x5
	.long	6                               ## 0x6
	.long	7                               ## 0x7
	.section	__TEXT,__literal4,4byte_literals
	.p2align	2
LCPI32_1:
	.long	0x3f800000                      ## float 1
LCPI32_2:
	.long	0x3109705f                      ## float 1.99999994E-9
	.section	__TEXT,__text,regular,pure_instructions
	.p2align	4, 0x90
_train_cost_model.par_for.relu1_0_d_def__.s24.n.n.n: ## @train_cost_model.par_for.relu1_0_d_def__.s24.n.n.n
## %bb.0:                               ## %entry
	pushq	%rbp
	pushq	%r15
	pushq	%r14
	pushq	%r13
	pushq	%r12
	pushq	%rbx
	movq	%rdx, %rdi
	movl	12(%rdx), %r11d
	movl	20(%rdx), %r10d
	movl	%esi, %r8d
	sarl	$31, %r8d
	xorl	%ecx, %ecx
	testl	%r10d, %r10d
	sete	%cl
	movl	%ecx, %ebp
	negl	%ebp
	movl	%r10d, %ebx
	sarl	$31, %ebx
	subl	%r8d, %esi
	orl	%r10d, %ebp
	movl	%esi, %eax
	cltd
	idivl	%ebp
	movl	%edx, %r13d
	movl	%ebx, %r15d
	leal	(%r10,%rcx), %ebp
	movl	%esi, %eax
	cltd
	idivl	%ebp
	notl	%r15d
	decl	%ecx
	movl	%r15d, %r9d
	subl	%ebx, %r9d
	andl	%r8d, %r9d
	addl	%eax, %r9d
	andl	%ecx, %r9d
	leal	(%r9,%r9), %eax
	testl	%r11d, %r11d
	movl	$1, %edx
	cmovlel	%r11d, %edx
	subl	%eax, %edx
	cmpl	$2, %edx
	movl	$2, %esi
	cmovll	%edx, %esi
	xorl	%eax, %eax
	testl	%edx, %edx
	movl	$0, -60(%rsp)                   ## 4-byte Folded Spill
	cmovlel	%eax, %esi
	movl	%esi, -24(%rsp)                 ## 4-byte Spill
	jle	LBB32_18
## %bb.1:                               ## %"for relu1_0_d_def__.s24.w.wi.preheader"
	movl	(%rdi), %r14d
	movl	16(%rdi), %esi
	movl	24(%rdi), %eax
	movl	%eax, -40(%rsp)                 ## 4-byte Spill
	movslq	28(%rdi), %r12
	movl	36(%rdi), %edx
	movl	40(%rdi), %eax
	movq	48(%rdi), %rbp
	movq	%rbp, -56(%rsp)                 ## 8-byte Spill
	movq	64(%rdi), %rbp
	movq	%rbp, -48(%rsp)                 ## 8-byte Spill
	xorl	%r10d, %ebx
	addl	%r15d, %ebx
	andl	%r8d, %ebx
	movl	%eax, %r8d
	addl	%ebx, %r13d
	andl	%ecx, %r13d
	leal	(,%r13,8), %ecx
	subl	%r12d, %edx
	subl	%r12d, %r8d
	movslq	8(%rdi), %rbp
	movslq	%r9d, %r10
	subq	%rbp, %r10
	addq	%r10, %r10
	movq	%rsi, %rax
	movq	%rsi, -8(%rsp)                  ## 8-byte Spill
	movl	%esi, %r11d
	shll	$5, %r11d
	movl	%r14d, %ebp
	subl	%ecx, %ebp
	cmpl	$8, %ebp
	movl	$8, %ebx
	cmovll	%ebp, %ebx
	movq	80(%rdi), %r15
	testl	%ebp, %ebp
	movl	-60(%rsp), %eax                 ## 4-byte Reload
	cmovgl	%ebx, %eax
	movl	%eax, -60(%rsp)                 ## 4-byte Spill
	movslq	4(%rdi), %rax
	movq	%rax, -32(%rsp)                 ## 8-byte Spill
	leal	8(%rcx), %ebx
	movslq	%ecx, %rax
	movq	%rax, -16(%rsp)                 ## 8-byte Spill
	cmpl	%r14d, %ebx
	movl	%r11d, -20(%rsp)                ## 4-byte Spill
	jle	LBB32_12
## %bb.2:                               ## %"for relu1_0_d_def__.s24.w.wi.preheader.split.us"
	testl	%ebp, %ebp
	jle	LBB32_18
## %bb.3:                               ## %"for relu1_0_d_def__.s24.w.wi.us.us.preheader"
	movq	%r13, %rdi
	addl	%ecx, %r8d
	addl	%ecx, %edx
	movq	-16(%rsp), %rsi                 ## 8-byte Reload
	movl	%esi, %ecx
	subl	%r12d, %ecx
	movq	-32(%rsp), %rbx                 ## 8-byte Reload
	imulq	%rbx, %r10
	addq	%rsi, %r10
	subq	%r12, %rsi
	movslq	%edx, %rdx
	movslq	%r8d, %r13
	movslq	%ecx, %r8
	shll	$6, %r9d
	movl	-40(%rsp), %eax                 ## 4-byte Reload
	shll	$6, %eax
	subl	%eax, %r9d
	movl	-60(%rsp), %r14d                ## 4-byte Reload
	movl	-24(%rsp), %eax                 ## 4-byte Reload
	movq	%rax, -40(%rsp)                 ## 8-byte Spill
	orl	$20, %r9d
	movq	-8(%rsp), %rax                  ## 8-byte Reload
	imull	%r9d, %eax
	movl	%r14d, %r12d
	andl	$2147483616, %r12d              ## imm = 0x7FFFFFE0
	leal	(%rax,%rdi,8), %r9d
	movq	-56(%rsp), %rdi                 ## 8-byte Reload
	leaq	(%rdi,%r10,4), %rbp
	addq	$96, %rbp
	shlq	$2, %rbx
	movq	%rbx, -32(%rsp)                 ## 8-byte Spill
	leaq	(%r15,%rsi,4), %rsi
	addq	$96, %rsi
	leaq	96(%r15,%r13,4), %rax
	leaq	96(%r15,%rdx,4), %rcx
	leaq	(%rdi,%r10,4), %rbx
	movq	-48(%rsp), %rdx                 ## 8-byte Reload
	addq	$96, %rdx
	movq	%rdx, -56(%rsp)                 ## 8-byte Spill
	leaq	(%r15,%r8,4), %r13
	movq	-48(%rsp), %r8                  ## 8-byte Reload
	xorl	%r15d, %r15d
	vmovss	LCPI32_2(%rip), %xmm0           ## xmm0 = mem[0],zero,zero,zero
	vmovss	LCPI32_1(%rip), %xmm1           ## xmm1 = mem[0],zero,zero,zero
	vbroadcastss	LCPI32_1(%rip), %ymm2   ## ymm2 = [1.0E+0,1.0E+0,1.0E+0,1.0E+0,1.0E+0,1.0E+0,1.0E+0,1.0E+0]
	vbroadcastss	LCPI32_2(%rip), %ymm3   ## ymm3 = [1.99999994E-9,1.99999994E-9,1.99999994E-9,1.99999994E-9,1.99999994E-9,1.99999994E-9,1.99999994E-9,1.99999994E-9]
	jmp	LBB32_4
	.p2align	4, 0x90
LBB32_11:                               ## %after_bb.loopexit.us.us
                                        ##   in Loop: Header=BB32_4 Depth=1
	incq	%r15
	movq	-32(%rsp), %rdx                 ## 8-byte Reload
	addq	%rdx, %rbp
	addl	%r11d, %r9d
	addq	%rdx, %rbx
	cmpq	-40(%rsp), %r15                 ## 8-byte Folded Reload
	je	LBB32_18
LBB32_4:                                ## %"for relu1_0_d_def__.s24.w.wi.us.us"
                                        ## =>This Loop Header: Depth=1
                                        ##     Child Loop BB32_7 Depth 2
                                        ##     Child Loop BB32_10 Depth 2
	movslq	%r9d, %r10
	cmpl	$32, -60(%rsp)                  ## 4-byte Folded Reload
	jae	LBB32_6
## %bb.5:                               ##   in Loop: Header=BB32_4 Depth=1
	xorl	%edi, %edi
	jmp	LBB32_9
	.p2align	4, 0x90
LBB32_6:                                ## %vector.body.preheader
                                        ##   in Loop: Header=BB32_4 Depth=1
	movq	-56(%rsp), %rdx                 ## 8-byte Reload
	leaq	(%rdx,%r10,4), %r11
	xorl	%r8d, %r8d
	.p2align	4, 0x90
LBB32_7:                                ## %vector.body
                                        ##   Parent Loop BB32_4 Depth=1
                                        ## =>  This Inner Loop Header: Depth=2
	vmovups	-96(%rcx,%r8,4), %ymm4
	vmovups	-64(%rcx,%r8,4), %ymm5
	vmovups	-32(%rcx,%r8,4), %ymm6
	vmovups	(%rcx,%r8,4), %ymm7
	vmovups	-96(%rsi,%r8,4), %ymm8
	vmovups	-64(%rsi,%r8,4), %ymm9
	vmovups	-32(%rsi,%r8,4), %ymm10
	vmovups	(%rsi,%r8,4), %ymm11
	vmulps	-96(%rbp,%r8,4), %ymm8, %ymm8
	vmulps	-64(%rbp,%r8,4), %ymm9, %ymm9
	vmulps	-32(%rbp,%r8,4), %ymm10, %ymm10
	vmulps	(%rbp,%r8,4), %ymm11, %ymm11
	vmulps	-96(%rax,%r8,4), %ymm8, %ymm8
	vmulps	-64(%rax,%r8,4), %ymm9, %ymm9
	vmulps	-32(%rax,%r8,4), %ymm10, %ymm10
	vmulps	(%rax,%r8,4), %ymm11, %ymm11
	vmulps	%ymm3, %ymm8, %ymm8
	vmulps	%ymm3, %ymm9, %ymm9
	vmulps	%ymm3, %ymm10, %ymm10
	vmulps	%ymm3, %ymm11, %ymm11
	vcmpleps	%ymm2, %ymm4, %ymm4
	vandps	%ymm4, %ymm8, %ymm4
	vcmpleps	%ymm2, %ymm5, %ymm5
	vandps	%ymm5, %ymm9, %ymm5
	vcmpleps	%ymm2, %ymm6, %ymm6
	vandps	%ymm6, %ymm10, %ymm6
	vcmpleps	%ymm2, %ymm7, %ymm7
	vandps	%ymm7, %ymm11, %ymm7
	vaddps	-96(%r11,%r8,4), %ymm4, %ymm4
	vaddps	-64(%r11,%r8,4), %ymm5, %ymm5
	vaddps	-32(%r11,%r8,4), %ymm6, %ymm6
	vaddps	(%r11,%r8,4), %ymm7, %ymm7
	vmovups	%ymm4, -96(%r11,%r8,4)
	vmovups	%ymm5, -64(%r11,%r8,4)
	vmovups	%ymm6, -32(%r11,%r8,4)
	vmovups	%ymm7, (%r11,%r8,4)
	addq	$32, %r8
	cmpq	%r8, %r12
	jne	LBB32_7
## %bb.8:                               ## %middle.block
                                        ##   in Loop: Header=BB32_4 Depth=1
	movq	%r12, %rdi
	cmpq	%r14, %r12
	movq	-48(%rsp), %r8                  ## 8-byte Reload
	movl	-20(%rsp), %r11d                ## 4-byte Reload
	je	LBB32_11
LBB32_9:                                ## %"for relu1_0_d_def__.s24.n.ni.us.us.preheader"
                                        ##   in Loop: Header=BB32_4 Depth=1
	leaq	(%r8,%r10,4), %rdx
	.p2align	4, 0x90
LBB32_10:                               ## %"for relu1_0_d_def__.s24.n.ni.us.us"
                                        ##   Parent Loop BB32_4 Depth=1
                                        ## =>  This Inner Loop Header: Depth=2
	vmovss	(%r13,%rdi,4), %xmm4            ## xmm4 = mem[0],zero,zero,zero
	vmulss	(%rbx,%rdi,4), %xmm4, %xmm4
	vmulss	-96(%rax,%rdi,4), %xmm4, %xmm4
	vcmpltss	-96(%rcx,%rdi,4), %xmm1, %xmm5
	vmulss	%xmm0, %xmm4, %xmm4
	vandnps	%xmm4, %xmm5, %xmm4
	vaddss	(%rdx,%rdi,4), %xmm4, %xmm4
	vmovss	%xmm4, (%rdx,%rdi,4)
	incq	%rdi
	cmpq	%rdi, %r14
	jne	LBB32_10
	jmp	LBB32_11
LBB32_12:                               ## %"for relu1_0_d_def__.s24.w.wi.preheader.split"
	vmovd	%ecx, %xmm0
	decl	%r14d
	vpbroadcastd	%xmm0, %ymm0
	vpor	LCPI32_0(%rip), %ymm0, %ymm0
	vmovd	%r14d, %xmm1
	vpbroadcastd	%xmm1, %ymm1
	vpminsd	%ymm0, %ymm1, %ymm2
	vmovd	%edx, %xmm0
	vpbroadcastd	%xmm0, %ymm3
	vpbroadcastd	32(%rdi), %ymm0
	vpaddd	%ymm0, %ymm2, %ymm0
	vextracti128	$1, %ymm0, %xmm1
	vpextrd	$3, %xmm1, %edx
	vpextrd	$1, %xmm1, %esi
	vpaddd	%ymm3, %ymm2, %ymm3
	vextracti128	$1, %ymm3, %xmm4
	vmovd	%xmm4, %ecx
	movslq	%ecx, %rcx
	vmovss	(%r15,%rcx,4), %xmm5            ## xmm5 = mem[0],zero,zero,zero
	vpextrd	$1, %xmm4, %ecx
	movslq	%ecx, %rcx
	vinsertps	$16, (%r15,%rcx,4), %xmm5, %xmm5 ## xmm5 = xmm5[0],mem[0],xmm5[2,3]
	vpextrd	$2, %xmm4, %ecx
	vmovd	%r8d, %xmm6
	movslq	%ecx, %rcx
	vinsertps	$32, (%r15,%rcx,4), %xmm5, %xmm5 ## xmm5 = xmm5[0,1],mem[0],xmm5[3]
	vpbroadcastd	%xmm6, %ymm6
	vpextrd	$3, %xmm4, %ecx
	movslq	%ecx, %rcx
	vinsertps	$48, (%r15,%rcx,4), %xmm5, %xmm4 ## xmm4 = xmm5[0,1,2],mem[0]
	vmovd	%xmm3, %ecx
	movslq	%ecx, %rcx
	vmovss	(%r15,%rcx,4), %xmm5            ## xmm5 = mem[0],zero,zero,zero
	vpextrd	$1, %xmm3, %ecx
	movslq	%ecx, %rcx
	vinsertps	$16, (%r15,%rcx,4), %xmm5, %xmm5 ## xmm5 = xmm5[0],mem[0],xmm5[2,3]
	vpextrd	$2, %xmm3, %ecx
	vpaddd	%ymm6, %ymm2, %ymm2
	movslq	%ecx, %rcx
	vinsertps	$32, (%r15,%rcx,4), %xmm5, %xmm5 ## xmm5 = xmm5[0,1],mem[0],xmm5[3]
	vextracti128	$1, %ymm2, %xmm6
	vpextrd	$3, %xmm3, %ecx
	movslq	%ecx, %rcx
	vinsertps	$48, (%r15,%rcx,4), %xmm5, %xmm3 ## xmm3 = xmm5[0,1,2],mem[0]
	vmovd	%xmm6, %ecx
	movslq	%ecx, %rcx
	vmovss	(%r15,%rcx,4), %xmm5            ## xmm5 = mem[0],zero,zero,zero
	vpextrd	$1, %xmm6, %ecx
	movslq	%ecx, %rcx
	vinsertps	$16, (%r15,%rcx,4), %xmm5, %xmm5 ## xmm5 = xmm5[0],mem[0],xmm5[2,3]
	vpextrd	$2, %xmm6, %ecx
	movslq	%ecx, %rcx
	vinsertps	$32, (%r15,%rcx,4), %xmm5, %xmm5 ## xmm5 = xmm5[0,1],mem[0],xmm5[3]
	vpextrd	$3, %xmm6, %ecx
	movslq	%ecx, %rcx
	vinsertps	$48, (%r15,%rcx,4), %xmm5, %xmm5 ## xmm5 = xmm5[0,1,2],mem[0]
	vmovd	%xmm2, %ecx
	movslq	%ecx, %rcx
	vmovss	(%r15,%rcx,4), %xmm6            ## xmm6 = mem[0],zero,zero,zero
	vpextrd	$1, %xmm2, %ecx
	movslq	%ecx, %rcx
	vinsertps	$16, (%r15,%rcx,4), %xmm6, %xmm6 ## xmm6 = xmm6[0],mem[0],xmm6[2,3]
	vpextrd	$2, %xmm2, %ecx
	movslq	%ecx, %rcx
	vinsertps	$32, (%r15,%rcx,4), %xmm6, %xmm6 ## xmm6 = xmm6[0,1],mem[0],xmm6[3]
	vpextrd	$3, %xmm2, %ecx
	movslq	%ecx, %rcx
	vinsertps	$48, (%r15,%rcx,4), %xmm6, %xmm2 ## xmm2 = xmm6[0,1,2],mem[0]
	vmovd	%xmm1, %ecx
	movslq	%ecx, %rcx
	vmovss	(%r15,%rcx,4), %xmm6            ## xmm6 = mem[0],zero,zero,zero
	movslq	%esi, %rcx
	vinsertps	$16, (%r15,%rcx,4), %xmm6, %xmm6 ## xmm6 = xmm6[0],mem[0],xmm6[2,3]
	movslq	%edx, %rcx
	vpextrd	$2, %xmm1, %edx
	movslq	%edx, %rdx
	vinsertps	$32, (%r15,%rdx,4), %xmm6, %xmm1 ## xmm1 = xmm6[0,1],mem[0],xmm6[3]
	vpextrd	$1, %xmm0, %edx
	vinsertps	$48, (%r15,%rcx,4), %xmm1, %xmm6 ## xmm6 = xmm1[0,1,2],mem[0]
	vmovd	%xmm0, %ecx
	movslq	%ecx, %rcx
	vmovss	(%r15,%rcx,4), %xmm1            ## xmm1 = mem[0],zero,zero,zero
	movslq	%edx, %rcx
	vinsertps	$16, (%r15,%rcx,4), %xmm1, %xmm1 ## xmm1 = xmm1[0],mem[0],xmm1[2,3]
	vpextrd	$2, %xmm0, %ecx
	movslq	%ecx, %rcx
	vinsertps	$32, (%r15,%rcx,4), %xmm1, %xmm1 ## xmm1 = xmm1[0,1],mem[0],xmm1[3]
	vpextrd	$3, %xmm0, %ecx
	movslq	%ecx, %rcx
	vinsertps	$48, (%r15,%rcx,4), %xmm1, %xmm7 ## xmm7 = xmm1[0,1,2],mem[0]
	vinsertf128	$1, %xmm4, %ymm3, %ymm0
	vinsertf128	$1, %xmm5, %ymm2, %ymm1
	vinsertf128	$1, %xmm6, %ymm7, %ymm2
	movl	-24(%rsp), %eax                 ## 4-byte Reload
	movl	%eax, %r11d
	cmpl	$1, %eax
	jne	LBB32_14
## %bb.13:
	xorl	%eax, %eax
	movq	-48(%rsp), %rcx                 ## 8-byte Reload
	jmp	LBB32_16
LBB32_14:                               ## %"for relu1_0_d_def__.s24.w.wi.preheader.split.new"
	movl	%r11d, %r8d
	andl	$2147483646, %r8d               ## imm = 0x7FFFFFFE
	leaq	1(%r10), %rax
	movq	-32(%rsp), %rsi                 ## 8-byte Reload
	imulq	%rsi, %rax
	movq	-56(%rsp), %rcx                 ## 8-byte Reload
	leaq	(%rcx,%rax,4), %r14
	movq	-16(%rsp), %rax                 ## 8-byte Reload
	leaq	(,%rax,4), %rdi
	leaq	(,%rsi,8), %r15
	movl	%r9d, %eax
	shll	$6, %eax
	movl	-40(%rsp), %edx                 ## 4-byte Reload
	shll	$6, %edx
	subl	%edx, %eax
	leal	52(%rax), %edx
	movq	-8(%rsp), %rbx                  ## 8-byte Reload
	imull	%ebx, %edx
	leal	(%rdx,%r13,8), %ebp
	movl	%ebx, %edx
	shll	$6, %edx
	imulq	%r10, %rsi
	leaq	(%rcx,%rsi,4), %rsi
	orl	$20, %eax
	imull	%ebx, %eax
	leal	(%rax,%r13,8), %ebx
	xorl	%eax, %eax
	vbroadcastss	LCPI32_1(%rip), %ymm3   ## ymm3 = [1.0E+0,1.0E+0,1.0E+0,1.0E+0,1.0E+0,1.0E+0,1.0E+0,1.0E+0]
	vcmpltps	%ymm0, %ymm3, %ymm3
	vbroadcastss	LCPI32_2(%rip), %ymm4   ## ymm4 = [1.99999994E-9,1.99999994E-9,1.99999994E-9,1.99999994E-9,1.99999994E-9,1.99999994E-9,1.99999994E-9,1.99999994E-9]
	movq	-48(%rsp), %rcx                 ## 8-byte Reload
	.p2align	4, 0x90
LBB32_15:                               ## %"for relu1_0_d_def__.s24.w.wi"
                                        ## =>This Inner Loop Header: Depth=1
	vmulps	(%rsi,%rdi), %ymm2, %ymm5
	movslq	%ebx, %rbx
	vmulps	%ymm5, %ymm1, %ymm5
	vmulps	%ymm4, %ymm5, %ymm5
	vandnps	%ymm5, %ymm3, %ymm5
	vaddps	(%rcx,%rbx,4), %ymm5, %ymm5
	vmovups	%ymm5, (%rcx,%rbx,4)
	movslq	%ebp, %rbp
	vmulps	(%r14,%rdi), %ymm2, %ymm5
	vmulps	%ymm5, %ymm1, %ymm5
	vmulps	%ymm4, %ymm5, %ymm5
	vandnps	%ymm5, %ymm3, %ymm5
	vaddps	(%rcx,%rbp,4), %ymm5, %ymm5
	vmovups	%ymm5, (%rcx,%rbp,4)
	addq	$2, %rax
	addq	%r15, %rdi
	addl	%edx, %ebp
	addl	%edx, %ebx
	cmpq	%rax, %r8
	jne	LBB32_15
LBB32_16:                               ## %destructor_block.loopexit32.unr-lcssa
	testb	$1, %r11b
	movl	-20(%rsp), %edi                 ## 4-byte Reload
	je	LBB32_18
## %bb.17:                              ## %"for relu1_0_d_def__.s24.w.wi.epil"
	movq	-8(%rsp), %rdx                  ## 8-byte Reload
	leal	(%rdx,%rdx,4), %edx
	subl	-40(%rsp), %r9d                 ## 4-byte Folded Reload
	addl	%r9d, %r9d
	movq	-16(%rsp), %rbp                 ## 8-byte Reload
	leal	(%rbp,%rdx,4), %esi
	vbroadcastss	LCPI32_1(%rip), %ymm3   ## ymm3 = [1.0E+0,1.0E+0,1.0E+0,1.0E+0,1.0E+0,1.0E+0,1.0E+0,1.0E+0]
	leal	(%rax,%r9), %edx
	imull	%edx, %edi
	addl	%esi, %edi
	movslq	%edi, %rdx
	addq	%rax, %r10
	imulq	-32(%rsp), %r10                 ## 8-byte Folded Reload
	addq	%rbp, %r10
	movq	-56(%rsp), %rax                 ## 8-byte Reload
	vmulps	(%rax,%r10,4), %ymm2, %ymm2
	vmulps	%ymm2, %ymm1, %ymm1
	vbroadcastss	LCPI32_2(%rip), %ymm2   ## ymm2 = [1.99999994E-9,1.99999994E-9,1.99999994E-9,1.99999994E-9,1.99999994E-9,1.99999994E-9,1.99999994E-9,1.99999994E-9]
	vmulps	%ymm2, %ymm1, %ymm1
	vcmpnltps	%ymm0, %ymm3, %ymm0
	vandps	%ymm1, %ymm0, %ymm0
	vaddps	(%rcx,%rdx,4), %ymm0, %ymm0
	vmovups	%ymm0, (%rcx,%rdx,4)
LBB32_18:                               ## %destructor_block
	xorl	%eax, %eax
	popq	%rbx
	popq	%r12
	popq	%r13
	popq	%r14
	popq	%r15
	popq	%rbp
	vzeroupper
	retq
                                        ## -- End function
	.section	__TEXT,__const
	.p2align	5                               ## -- Begin function train_cost_model.par_for.relu1_0_d_def__.s25.n.n.n
LCPI33_0:
	.long	0                               ## 0x0
	.long	1                               ## 0x1
	.long	2                               ## 0x2
	.long	3                               ## 0x3
	.long	4                               ## 0x4
	.long	5                               ## 0x5
	.long	6                               ## 0x6
	.long	7                               ## 0x7
	.section	__TEXT,__literal4,4byte_literals
	.p2align	2
LCPI33_1:
	.long	0x3f800000                      ## float 1
LCPI33_2:
	.long	0x3109705f                      ## float 1.99999994E-9
	.section	__TEXT,__text,regular,pure_instructions
	.p2align	4, 0x90
_train_cost_model.par_for.relu1_0_d_def__.s25.n.n.n: ## @train_cost_model.par_for.relu1_0_d_def__.s25.n.n.n
## %bb.0:                               ## %entry
	pushq	%rbp
	pushq	%r15
	pushq	%r14
	pushq	%r13
	pushq	%r12
	pushq	%rbx
	subq	$224, %rsp
	movq	%rdx, %rdi
	movl	12(%rdx), %r15d
	movl	28(%rdx), %r10d
	movl	%esi, %r9d
	sarl	$31, %r9d
	xorl	%ecx, %ecx
	testl	%r10d, %r10d
	sete	%cl
	movl	%ecx, %ebx
	negl	%ebx
	movl	%r10d, %ebp
	sarl	$31, %ebp
	subl	%r9d, %esi
	orl	%r10d, %ebx
	movl	%esi, %eax
	cltd
	idivl	%ebx
	movl	%edx, %r14d
	movl	%ebp, %r11d
	leal	(%r10,%rcx), %ebx
	movl	%esi, %eax
	cltd
	idivl	%ebx
	notl	%r11d
	decl	%ecx
	movl	%r11d, %r8d
	subl	%ebp, %r8d
	andl	%r9d, %r8d
	addl	%eax, %r8d
	andl	%ecx, %r8d
	leal	(%r8,%r8), %eax
	movl	%eax, -112(%rsp)                ## 4-byte Spill
	subl	%eax, %r15d
	cmpl	$2, %r15d
	movl	$2, %edx
	cmovll	%r15d, %edx
	xorl	%eax, %eax
	testl	%r15d, %r15d
	cmovlel	%eax, %edx
	movl	%edx, -88(%rsp)                 ## 4-byte Spill
	jle	LBB33_60
## %bb.1:                               ## %"for relu1_0_d_def__.s25.w.wi.preheader"
	movl	(%rdi), %ebx
	movl	16(%rdi), %esi
	movslq	36(%rdi), %rax
	xorl	%edx, %edx
	movl	44(%rdi), %r12d
	movl	48(%rdi), %r15d
	xorl	%r10d, %ebp
	addl	%r11d, %ebp
	andl	%r9d, %ebp
	addl	%ebp, %r14d
	andl	%ecx, %r14d
	movq	%r14, -72(%rsp)                 ## 8-byte Spill
	leal	(,%r14,8), %ebp
	movl	%r12d, %r9d
	subl	%eax, %r9d
	movl	%r15d, %r10d
	movq	%rax, -96(%rsp)                 ## 8-byte Spill
	subl	%eax, %r10d
	movslq	8(%rdi), %rcx
	movslq	%r8d, %r13
	subq	%rcx, %r13
	addq	%r13, %r13
	movq	%rsi, -80(%rsp)                 ## 8-byte Spill
	movl	%esi, %eax
	shll	$5, %eax
	movl	%eax, -124(%rsp)                ## 4-byte Spill
	movl	%ebx, %esi
	subl	%ebp, %esi
	cmpl	$8, %esi
	movl	$8, %ecx
	cmovll	%esi, %ecx
	testl	%esi, %esi
	cmovgl	%ecx, %edx
	movl	%edx, -120(%rsp)                ## 4-byte Spill
	movl	24(%rdi), %eax
	movl	%eax, -128(%rsp)                ## 4-byte Spill
	movl	%ebp, %eax
	subl	%ebx, %eax
	movl	%eax, %ecx
	sarl	$31, %ecx
	andl	%eax, %ecx
	cmpl	$-8, %ecx
	movl	$-8, %r14d
	cmovgl	%ecx, %r14d
	movl	32(%rdi), %ecx
	movq	56(%rdi), %rax
	movq	%rax, -104(%rsp)                ## 8-byte Spill
	movq	72(%rdi), %rax
	movq	%rax, -64(%rsp)                 ## 8-byte Spill
	movq	88(%rdi), %rax
	movq	%rax, -56(%rsp)                 ## 8-byte Spill
	movslq	4(%rdi), %r11
	leal	8(%rbp), %eax
	movslq	%ebp, %rdx
	cmpl	%ebx, %eax
	jle	LBB33_2
## %bb.4:                               ## %"for relu1_0_d_def__.s25.w.wi.preheader.split.us"
	movq	%rdx, 8(%rsp)                   ## 8-byte Spill
	movq	%r13, -40(%rsp)                 ## 8-byte Spill
	movq	%r11, -48(%rsp)                 ## 8-byte Spill
	movq	-80(%rsp), %rdx                 ## 8-byte Reload
	leal	(%rdx,%rdx,8), %eax
	leal	(%rdx,%rax,2), %r13d
	movl	%r8d, %eax
	subl	%ecx, %eax
	addl	%eax, %eax
	movq	%rax, 56(%rsp)                  ## 8-byte Spill
	movq	-96(%rsp), %r11                 ## 8-byte Reload
	subl	%r11d, %ebx
	addl	%ebx, %r12d
	addl	%ebx, %r15d
	movl	-120(%rsp), %eax                ## 4-byte Reload
	addl	%eax, %ebp
	addl	%eax, %r14d
	movslq	%r12d, %rax
	movq	%rax, 48(%rsp)                  ## 8-byte Spill
	movslq	%r15d, %rax
	movq	%rax, 40(%rsp)                  ## 8-byte Spill
	movslq	%ebx, %rax
	movq	%rax, 32(%rsp)                  ## 8-byte Spill
	testl	%esi, %esi
	movq	%rdx, %rsi
	movl	%r14d, -116(%rsp)               ## 4-byte Spill
	movl	%ecx, %edi
	movq	%r13, (%rsp)                    ## 8-byte Spill
	jle	LBB33_38
## %bb.5:                               ## %"for relu1_0_d_def__.s25.w.wi.us.us.preheader"
	movq	8(%rsp), %rdx                   ## 8-byte Reload
	movl	%edx, %eax
	subl	%r11d, %eax
	addl	%edx, %r10d
	addl	%edx, %r9d
	movslq	%r9d, %rcx
	movq	%rcx, -16(%rsp)                 ## 8-byte Spill
	movslq	%r10d, %rcx
	movq	%rcx, -24(%rsp)                 ## 8-byte Spill
	movslq	%eax, %r12
	movl	-120(%rsp), %eax                ## 4-byte Reload
	movslq	%ebp, %rbp
	movl	%r14d, %r15d
	movslq	-112(%rsp), %r10                ## 4-byte Folded Reload
	movl	-88(%rsp), %ecx                 ## 4-byte Reload
	movq	%rcx, 208(%rsp)                 ## 8-byte Spill
	movq	%rax, 64(%rsp)                  ## 8-byte Spill
                                        ## kill: def $eax killed $eax killed $rax def $rax
	andl	$2147483640, %eax               ## imm = 0x7FFFFFF8
	movq	%rax, 176(%rsp)                 ## 8-byte Spill
	addq	$-8, %rax
	movq	%rax, 168(%rsp)                 ## 8-byte Spill
	movq	%rax, %rcx
	shrq	$3, %rcx
	incq	%rcx
	shll	$6, %r8d
	shll	$6, %edi
	subl	%edi, %r8d
	movl	%r15d, %eax
	andl	$-8, %eax
	orl	$19, %r8d
	imull	%r8d, %esi
	movq	%rdx, %rdi
	subq	%r11, %rdi
	movq	%rdi, 192(%rsp)                 ## 8-byte Spill
	movq	%rax, 152(%rsp)                 ## 8-byte Spill
	addq	$-8, %rax
	movq	%rax, 144(%rsp)                 ## 8-byte Spill
	movq	%rax, %rdi
	shrq	$3, %rdi
	incq	%rdi
	movq	-48(%rsp), %r9                  ## 8-byte Reload
	movq	%r9, %rax
	imulq	-40(%rsp), %rax                 ## 8-byte Folded Reload
	movq	%rsi, -80(%rsp)                 ## 8-byte Spill
	movq	-72(%rsp), %rbx                 ## 8-byte Reload
	leal	(%rsi,%rbx,8), %ebx
	movq	%rcx, %rsi
	movq	%rcx, 160(%rsp)                 ## 8-byte Spill
	andq	$-2, %rcx
	negq	%rcx
	movq	%rcx, 128(%rsp)                 ## 8-byte Spill
	leaq	(%rax,%rdx), %rsi
	addq	%rbp, %rax
	movq	%rdi, %rcx
	movq	%rdi, 136(%rsp)                 ## 8-byte Spill
	andq	$-2, %rdi
	negq	%rdi
	movq	%rdi, 120(%rsp)                 ## 8-byte Spill
	movq	-104(%rsp), %rcx                ## 8-byte Reload
	leaq	(%rcx,%rax,4), %r8
	addq	$32, %r8
	leaq	(%rcx,%rax,4), %r11
	vmovss	LCPI33_2(%rip), %xmm0           ## xmm0 = mem[0],zero,zero,zero
	vmovss	LCPI33_1(%rip), %xmm1           ## xmm1 = mem[0],zero,zero,zero
	vbroadcastss	LCPI33_2(%rip), %ymm2   ## ymm2 = [1.99999994E-9,1.99999994E-9,1.99999994E-9,1.99999994E-9,1.99999994E-9,1.99999994E-9,1.99999994E-9,1.99999994E-9]
	vbroadcastss	LCPI33_1(%rip), %ymm10  ## ymm10 = [1.0E+0,1.0E+0,1.0E+0,1.0E+0,1.0E+0,1.0E+0,1.0E+0,1.0E+0]
	leal	(%rdx,%r13), %eax
	movq	%rax, 88(%rsp)                  ## 8-byte Spill
	movq	%rsi, -32(%rsp)                 ## 8-byte Spill
	leaq	32(%rcx,%rsi,4), %r13
	leaq	(,%r9,4), %rax
	movq	%rax, 200(%rsp)                 ## 8-byte Spill
	movq	-56(%rsp), %rax                 ## 8-byte Reload
	movq	%r12, 96(%rsp)                  ## 8-byte Spill
	leaq	(%rax,%r12,4), %rcx
	movq	%rcx, 112(%rsp)                 ## 8-byte Spill
	leaq	32(%rax), %rax
	movq	%rax, 104(%rsp)                 ## 8-byte Spill
	movq	-64(%rsp), %rax                 ## 8-byte Reload
	leaq	32(%rax), %rcx
	movq	%rcx, 24(%rsp)                  ## 8-byte Spill
	movq	%rbp, 16(%rsp)                  ## 8-byte Spill
	leaq	(%rax,%rbp,4), %rax
	movq	%rax, 184(%rsp)                 ## 8-byte Spill
	movq	%r10, %rax
	movq	%r10, 216(%rsp)                 ## 8-byte Spill
	xorl	%ebp, %ebp
	movl	-124(%rsp), %ecx                ## 4-byte Reload
	movl	-128(%rsp), %edx                ## 4-byte Reload
	movl	-120(%rsp), %r12d               ## 4-byte Reload
	jmp	LBB33_6
	.p2align	4, 0x90
LBB33_21:                               ## %after_bb.us.us
                                        ##   in Loop: Header=BB33_6 Depth=1
	movq	-96(%rsp), %rbp                 ## 8-byte Reload
	incq	%rbp
	movq	200(%rsp), %rax                 ## 8-byte Reload
	addq	%rax, %r13
	movq	-88(%rsp), %r10                 ## 8-byte Reload
	incq	%r10
	movl	-124(%rsp), %ecx                ## 4-byte Reload
	addl	%ecx, %ebx
	addq	%r9, -32(%rsp)                  ## 8-byte Folded Spill
	addq	%rax, %r8
	addl	%ecx, %edi
	movq	%rdi, -80(%rsp)                 ## 8-byte Spill
	addq	%rax, %r11
	cmpq	208(%rsp), %rbp                 ## 8-byte Folded Reload
	movl	-128(%rsp), %edx                ## 4-byte Reload
	je	LBB33_60
LBB33_6:                                ## %"for relu1_0_d_def__.s25.w.wi.us.us"
                                        ## =>This Loop Header: Depth=1
                                        ##     Child Loop BB33_11 Depth 2
                                        ##     Child Loop BB33_16 Depth 2
                                        ##     Child Loop BB33_25 Depth 2
                                        ##     Child Loop BB33_20 Depth 2
	testq	%r10, %r10
	movl	$0, %eax
	cmovgl	%r10d, %eax
	imull	%edx, %eax
	movslq	%eax, %rdi
	movq	216(%rsp), %rax                 ## 8-byte Reload
	addq	%rbp, %rax
	testq	%rax, %rax
	movl	$0, %esi
	cmovlel	%esi, %eax
	imull	%edx, %eax
	movq	56(%rsp), %rdx                  ## 8-byte Reload
	addl	%ebp, %edx
	imull	%ecx, %edx
	movq	%rdx, 80(%rsp)                  ## 8-byte Spill
	movq	-40(%rsp), %rcx                 ## 8-byte Reload
	addq	%rbp, %rcx
	imulq	%r9, %rcx
	movq	%rcx, 72(%rsp)                  ## 8-byte Spill
	movslq	%ebx, %rdx
	cltq
	movq	%rax, -72(%rsp)                 ## 8-byte Spill
	cmpl	$8, %r12d
	movl	%ebx, -112(%rsp)                ## 4-byte Spill
	movq	%r10, -88(%rsp)                 ## 8-byte Spill
	movq	%rbp, -96(%rsp)                 ## 8-byte Spill
	movq	%rdx, -8(%rsp)                  ## 8-byte Spill
	jae	LBB33_8
## %bb.7:                               ##   in Loop: Header=BB33_6 Depth=1
	xorl	%edx, %edx
	movl	-116(%rsp), %r14d               ## 4-byte Reload
	jmp	LBB33_15
	.p2align	4, 0x90
LBB33_8:                                ## %vector.ph70
                                        ##   in Loop: Header=BB33_6 Depth=1
	movq	%r8, %rbp
	cmpq	$0, 168(%rsp)                   ## 8-byte Folded Reload
	je	LBB33_9
## %bb.10:                              ## %vector.body68.preheader
                                        ##   in Loop: Header=BB33_6 Depth=1
	movq	-24(%rsp), %rax                 ## 8-byte Reload
	addq	%rdi, %rax
	movq	-16(%rsp), %rcx                 ## 8-byte Reload
	addq	%rdi, %rcx
	movq	112(%rsp), %rsi                 ## 8-byte Reload
	leaq	(%rsi,%rdi,4), %r9
	movq	104(%rsp), %rsi                 ## 8-byte Reload
	leaq	(%rsi,%rax,4), %r10
	leaq	(%rsi,%rcx,4), %r8
	movq	24(%rsp), %rax                  ## 8-byte Reload
	leaq	(%rax,%rdx,4), %r14
	movq	128(%rsp), %r12                 ## 8-byte Reload
	xorl	%esi, %esi
	.p2align	4, 0x90
LBB33_11:                               ## %vector.body68
                                        ##   Parent Loop BB33_6 Depth=1
                                        ## =>  This Inner Loop Header: Depth=2
	vcmpltps	-32(%r8,%rsi,4), %ymm10, %ymm4
	vmovups	(%r9,%rsi,4), %ymm5
	vmulps	-32(%r13,%rsi,4), %ymm5, %ymm5
	vmulps	-32(%r10,%rsi,4), %ymm5, %ymm5
	vmulps	%ymm2, %ymm5, %ymm5
	vandps	%ymm5, %ymm4, %ymm4
	vaddps	-32(%r14,%rsi,4), %ymm4, %ymm4
	vmovups	%ymm4, -32(%r14,%rsi,4)
	vcmpltps	(%r8,%rsi,4), %ymm10, %ymm4
	vmovups	32(%r9,%rsi,4), %ymm5
	vmulps	(%r13,%rsi,4), %ymm5, %ymm5
	vmulps	(%r10,%rsi,4), %ymm5, %ymm5
	vmulps	%ymm2, %ymm5, %ymm5
	vandps	%ymm5, %ymm4, %ymm4
	vaddps	(%r14,%rsi,4), %ymm4, %ymm4
	vmovups	%ymm4, (%r14,%rsi,4)
	addq	$16, %rsi
	addq	$2, %r12
	jne	LBB33_11
## %bb.12:                              ## %middle.block66.unr-lcssa
                                        ##   in Loop: Header=BB33_6 Depth=1
	testb	$1, 160(%rsp)                   ## 1-byte Folded Reload
	movq	%rbp, %r8
	je	LBB33_14
LBB33_13:                               ## %vector.body68.epil
                                        ##   in Loop: Header=BB33_6 Depth=1
	movq	88(%rsp), %rax                  ## 8-byte Reload
	movq	80(%rsp), %rcx                  ## 8-byte Reload
	addl	%ecx, %eax
	movq	-72(%rsp), %rcx                 ## 8-byte Reload
	addq	%rsi, %rcx
	movq	-16(%rsp), %rdx                 ## 8-byte Reload
	addq	%rcx, %rdx
	movq	%rdi, %rbx
	movq	-56(%rsp), %rdi                 ## 8-byte Reload
	vcmpltps	(%rdi,%rdx,4), %ymm10, %ymm4
	movq	-24(%rsp), %rdx                 ## 8-byte Reload
	addq	%rcx, %rdx
	addq	96(%rsp), %rcx                  ## 8-byte Folded Reload
	vmovups	(%rdi,%rcx,4), %ymm5
	movq	8(%rsp), %rcx                   ## 8-byte Reload
	movq	72(%rsp), %rbp                  ## 8-byte Reload
	addq	%rbp, %rcx
	cltq
	addq	%rsi, %rax
	addq	%rsi, %rcx
	movq	-104(%rsp), %rsi                ## 8-byte Reload
	vmulps	(%rsi,%rcx,4), %ymm5, %ymm5
	vmulps	(%rdi,%rdx,4), %ymm5, %ymm5
	movq	%rbx, %rdi
	vmulps	%ymm2, %ymm5, %ymm5
	vandps	%ymm5, %ymm4, %ymm4
	movq	-64(%rsp), %rcx                 ## 8-byte Reload
	vaddps	(%rcx,%rax,4), %ymm4, %ymm4
	vmovups	%ymm4, (%rcx,%rax,4)
LBB33_14:                               ## %middle.block66
                                        ##   in Loop: Header=BB33_6 Depth=1
	movq	176(%rsp), %rax                 ## 8-byte Reload
	movq	%rax, %rdx
	cmpq	64(%rsp), %rax                  ## 8-byte Folded Reload
	movq	-48(%rsp), %r9                  ## 8-byte Reload
	movl	-116(%rsp), %r14d               ## 4-byte Reload
	movl	-120(%rsp), %r12d               ## 4-byte Reload
	je	LBB33_17
LBB33_15:                               ## %"for relu1_0_d_def__.s25.n.ni.us.us.preheader"
                                        ##   in Loop: Header=BB33_6 Depth=1
	movq	64(%rsp), %rsi                  ## 8-byte Reload
	subq	%rdx, %rsi
	movq	192(%rsp), %rax                 ## 8-byte Reload
	movl	%r12d, %r10d
	movq	%r8, %r12
	leaq	(%rax,%rdx), %r8
	addq	%rdi, %r8
	movq	-24(%rsp), %rax                 ## 8-byte Reload
	addq	%rdx, %rax
	addq	%rdi, %rax
	movq	-16(%rsp), %rcx                 ## 8-byte Reload
	leaq	(%rcx,%rdx), %rbx
	addq	%rdi, %rbx
	movq	-32(%rsp), %rcx                 ## 8-byte Reload
	addq	%rdx, %rcx
	movq	-104(%rsp), %rdi                ## 8-byte Reload
	leaq	(%rdi,%rcx,4), %rbp
	movq	-56(%rsp), %rdi                 ## 8-byte Reload
	leaq	(%rdi,%r8,4), %rcx
	movq	%r12, %r8
	movl	%r10d, %r12d
	leaq	(%rdi,%rax,4), %rax
	leaq	(%rdi,%rbx,4), %rdi
	movq	-8(%rsp), %rbx                  ## 8-byte Reload
	addq	%rdx, %rbx
	movq	-64(%rsp), %rdx                 ## 8-byte Reload
	leaq	(%rdx,%rbx,4), %rdx
	xorl	%ebx, %ebx
	.p2align	4, 0x90
LBB33_16:                               ## %"for relu1_0_d_def__.s25.n.ni.us.us"
                                        ##   Parent Loop BB33_6 Depth=1
                                        ## =>  This Inner Loop Header: Depth=2
	vmovss	(%rcx,%rbx,4), %xmm4            ## xmm4 = mem[0],zero,zero,zero
	vmulss	(%rbp,%rbx,4), %xmm4, %xmm4
	vmulss	(%rax,%rbx,4), %xmm4, %xmm4
	vcmpltss	(%rdi,%rbx,4), %xmm1, %xmm5
	vmulss	%xmm0, %xmm4, %xmm4
	vandps	%xmm4, %xmm5, %xmm4
	vaddss	(%rdx,%rbx,4), %xmm4, %xmm4
	vmovss	%xmm4, (%rdx,%rbx,4)
	incq	%rbx
	cmpq	%rbx, %rsi
	jne	LBB33_16
LBB33_17:                               ## %"end for relu1_0_d_def__.s25.n.ni.loopexit.us.us"
                                        ##   in Loop: Header=BB33_6 Depth=1
	testl	%r14d, %r14d
	movq	-80(%rsp), %rdi                 ## 8-byte Reload
	movl	-112(%rsp), %ebx                ## 4-byte Reload
	jle	LBB33_21
## %bb.18:                              ## %"for relu1_0_d_def__.s25.n.ni.rebased.preheader.us.us"
                                        ##   in Loop: Header=BB33_6 Depth=1
	movslq	%edi, %rbp
	movq	48(%rsp), %rax                  ## 8-byte Reload
	movq	-72(%rsp), %rsi                 ## 8-byte Reload
	addq	%rsi, %rax
	movq	40(%rsp), %rcx                  ## 8-byte Reload
	addq	%rsi, %rcx
	addq	32(%rsp), %rsi                  ## 8-byte Folded Reload
	movq	-56(%rsp), %rdx                 ## 8-byte Reload
	vmovss	-4(%rdx,%rax,4), %xmm4          ## xmm4 = mem[0],zero,zero,zero
	vmovss	-4(%rdx,%rcx,4), %xmm5          ## xmm5 = mem[0],zero,zero,zero
	vmovss	-4(%rdx,%rsi,4), %xmm6          ## xmm6 = mem[0],zero,zero,zero
	cmpl	$8, %r14d
	jae	LBB33_22
## %bb.19:                              ##   in Loop: Header=BB33_6 Depth=1
	xorl	%eax, %eax
	jmp	LBB33_37
	.p2align	4, 0x90
LBB33_22:                               ## %vector.ph53
                                        ##   in Loop: Header=BB33_6 Depth=1
	cmpq	$0, 144(%rsp)                   ## 8-byte Folded Reload
	je	LBB33_23
## %bb.24:                              ## %vector.ph53.new
                                        ##   in Loop: Header=BB33_6 Depth=1
	movq	16(%rsp), %rax                  ## 8-byte Reload
	addq	%rbp, %rax
	movq	24(%rsp), %rcx                  ## 8-byte Reload
	leaq	(%rcx,%rax,4), %rsi
	vmulss	%xmm6, %xmm5, %xmm3
	vbroadcastss	%xmm3, %ymm7
	movq	120(%rsp), %rdx                 ## 8-byte Reload
	xorl	%eax, %eax
	jmp	LBB33_25
	.p2align	4, 0x90
LBB33_30:                               ## %vector.body51
                                        ##   in Loop: Header=BB33_25 Depth=2
	vaddps	(%rsi,%rax,4), %ymm8, %ymm3
	vmovups	%ymm3, (%rsi,%rax,4)
	addq	$16, %rax
	addq	$2, %rdx
	je	LBB33_31
LBB33_25:                               ## %vector.body51
                                        ##   Parent Loop BB33_6 Depth=1
                                        ## =>  This Inner Loop Header: Depth=2
	vucomiss	%xmm1, %xmm4
	vxorps	%xmm8, %xmm8, %xmm8
	ja	LBB33_26
## %bb.27:                              ## %vector.body51
                                        ##   in Loop: Header=BB33_25 Depth=2
	vxorps	%xmm9, %xmm9, %xmm9
	vaddps	-32(%rsi,%rax,4), %ymm9, %ymm3
	vmovups	%ymm3, -32(%rsi,%rax,4)
	jbe	LBB33_30
	jmp	LBB33_29
	.p2align	4, 0x90
LBB33_26:                               ##   in Loop: Header=BB33_25 Depth=2
	vmulps	-32(%r8,%rax,4), %ymm7, %ymm3
	vmulps	%ymm2, %ymm3, %ymm9
	vaddps	-32(%rsi,%rax,4), %ymm9, %ymm3
	vmovups	%ymm3, -32(%rsi,%rax,4)
	jbe	LBB33_30
LBB33_29:                               ##   in Loop: Header=BB33_25 Depth=2
	vmulps	(%r8,%rax,4), %ymm7, %ymm3
	vmulps	%ymm2, %ymm3, %ymm8
	jmp	LBB33_30
LBB33_9:                                ##   in Loop: Header=BB33_6 Depth=1
	xorl	%esi, %esi
	testb	$1, 160(%rsp)                   ## 1-byte Folded Reload
	movq	%rbp, %r8
	jne	LBB33_13
	jmp	LBB33_14
LBB33_23:                               ##   in Loop: Header=BB33_6 Depth=1
	xorl	%eax, %eax
LBB33_31:                               ## %middle.block49.unr-lcssa
                                        ##   in Loop: Header=BB33_6 Depth=1
	testb	$1, 136(%rsp)                   ## 1-byte Folded Reload
	je	LBB33_36
## %bb.32:                              ## %vector.body51.epil
                                        ##   in Loop: Header=BB33_6 Depth=1
	movq	80(%rsp), %rcx                  ## 8-byte Reload
	addl	(%rsp), %ecx                    ## 4-byte Folded Reload
	movslq	%ecx, %rcx
	addq	16(%rsp), %rax                  ## 8-byte Folded Reload
	addq	%rax, %rcx
	vucomiss	%xmm1, %xmm4
	ja	LBB33_33
## %bb.34:                              ## %vector.body51.epil
                                        ##   in Loop: Header=BB33_6 Depth=1
	vxorps	%xmm7, %xmm7, %xmm7
	jmp	LBB33_35
LBB33_33:                               ##   in Loop: Header=BB33_6 Depth=1
	addq	72(%rsp), %rax                  ## 8-byte Folded Reload
	vmulss	%xmm6, %xmm5, %xmm3
	vbroadcastss	%xmm3, %ymm3
	movq	-104(%rsp), %rdx                ## 8-byte Reload
	vmulps	(%rdx,%rax,4), %ymm3, %ymm3
	vmulps	%ymm2, %ymm3, %ymm7
LBB33_35:                               ## %vector.body51.epil
                                        ##   in Loop: Header=BB33_6 Depth=1
	movq	-64(%rsp), %rax                 ## 8-byte Reload
	vaddps	(%rax,%rcx,4), %ymm7, %ymm3
	vmovups	%ymm3, (%rax,%rcx,4)
LBB33_36:                               ## %middle.block49
                                        ##   in Loop: Header=BB33_6 Depth=1
	movq	152(%rsp), %rcx                 ## 8-byte Reload
	movq	%rcx, %rax
	cmpq	%r15, %rcx
	je	LBB33_21
LBB33_37:                               ## %"for relu1_0_d_def__.s25.n.ni.rebased.us.us.preheader"
                                        ##   in Loop: Header=BB33_6 Depth=1
	movq	184(%rsp), %rcx                 ## 8-byte Reload
	leaq	(%rcx,%rbp,4), %rcx
	.p2align	4, 0x90
LBB33_20:                               ## %"for relu1_0_d_def__.s25.n.ni.rebased.us.us"
                                        ##   Parent Loop BB33_6 Depth=1
                                        ## =>  This Inner Loop Header: Depth=2
	vmulss	(%r11,%rax,4), %xmm6, %xmm7
	vmulss	%xmm7, %xmm5, %xmm7
	vmulss	%xmm0, %xmm7, %xmm7
	vcmpltss	%xmm4, %xmm1, %xmm3
	vandps	%xmm7, %xmm3, %xmm3
	vaddss	(%rcx,%rax,4), %xmm3, %xmm3
	vmovss	%xmm3, (%rcx,%rax,4)
	incq	%rax
	cmpq	%rax, %r15
	jne	LBB33_20
	jmp	LBB33_21
LBB33_2:                                ## %"for relu1_0_d_def__.s25.w.wi.preheader20"
	movl	20(%rdi), %r15d
	movl	40(%rdi), %r12d
	decl	%ebx
	vmovd	%edx, %xmm0
	vpbroadcastd	%xmm0, %ymm0
	vpor	LCPI33_0(%rip), %ymm0, %ymm0
	vmovd	%ebx, %xmm1
	vpbroadcastd	%xmm1, %ymm1
	vpminsd	%ymm0, %ymm1, %ymm10
	movslq	-112(%rsp), %rbx                ## 4-byte Folded Reload
	movl	-88(%rsp), %r14d                ## 4-byte Reload
	imulq	%r11, %r13
	addq	%rdx, %r13
	movq	-104(%rsp), %rax                ## 8-byte Reload
	leaq	(%rax,%r13,4), %r13
	shlq	$2, %r11
	shll	$6, %r8d
	shll	$6, %ecx
	subl	%ecx, %r8d
	orl	$19, %r8d
	movq	-80(%rsp), %rax                 ## 8-byte Reload
	imull	%r8d, %eax
	movq	-72(%rsp), %rcx                 ## 8-byte Reload
	leal	(%rax,%rcx,8), %esi
	vbroadcastss	LCPI33_1(%rip), %ymm8   ## ymm8 = [1.0E+0,1.0E+0,1.0E+0,1.0E+0,1.0E+0,1.0E+0,1.0E+0,1.0E+0]
	vbroadcastss	LCPI33_2(%rip), %ymm9   ## ymm9 = [1.99999994E-9,1.99999994E-9,1.99999994E-9,1.99999994E-9,1.99999994E-9,1.99999994E-9,1.99999994E-9,1.99999994E-9]
	movl	-128(%rsp), %r8d                ## 4-byte Reload
	movq	-56(%rsp), %rax                 ## 8-byte Reload
	.p2align	4, 0x90
LBB33_3:                                ## %"for relu1_0_d_def__.s25.w.wi"
                                        ## =>This Inner Loop Header: Depth=1
	testq	%rbx, %rbx
	movl	$0, %ebp
	cmovgl	%ebx, %ebp
	movl	%r8d, %edi
	imull	%ebp, %edi
	leal	(%rdi,%r9), %ecx
	vmovd	%ecx, %xmm3
	vpbroadcastd	%xmm3, %ymm3
	vpaddd	%ymm3, %ymm10, %ymm3
	vmovd	%xmm3, %ecx
	vpextrd	$1, %xmm3, %edx
	movslq	%ecx, %rcx
	vmovss	(%rax,%rcx,4), %xmm4            ## xmm4 = mem[0],zero,zero,zero
	vpextrd	$2, %xmm3, %ecx
	movslq	%edx, %rdx
	movslq	%ecx, %rcx
	vinsertps	$16, (%rax,%rdx,4), %xmm4, %xmm5 ## xmm5 = xmm4[0],mem[0],xmm4[2,3]
	vpextrd	$3, %xmm3, %edx
	vextracti128	$1, %ymm3, %xmm4
	movslq	%edx, %rdx
	vinsertps	$32, (%rax,%rcx,4), %xmm5, %xmm3 ## xmm3 = xmm5[0,1],mem[0],xmm5[3]
	vinsertps	$48, (%rax,%rdx,4), %xmm3, %xmm3 ## xmm3 = xmm3[0,1,2],mem[0]
	vmovd	%xmm4, %ecx
	vpextrd	$1, %xmm4, %edx
	movslq	%ecx, %rcx
	vmovss	(%rax,%rcx,4), %xmm5            ## xmm5 = mem[0],zero,zero,zero
	movslq	%edx, %rcx
	vinsertps	$16, (%rax,%rcx,4), %xmm5, %xmm5 ## xmm5 = xmm5[0],mem[0],xmm5[2,3]
	addl	%r10d, %edi
	vmovd	%edi, %xmm6
	vpextrd	$2, %xmm4, %ecx
	movslq	%ecx, %rcx
	vpbroadcastd	%xmm6, %ymm6
	vinsertps	$32, (%rax,%rcx,4), %xmm5, %xmm5 ## xmm5 = xmm5[0,1],mem[0],xmm5[3]
	vpaddd	%ymm6, %ymm10, %ymm6
	vextracti128	$1, %ymm6, %xmm7
	vmovd	%xmm7, %ecx
	movslq	%ecx, %rcx
	vmovss	(%rax,%rcx,4), %xmm1            ## xmm1 = mem[0],zero,zero,zero
	vpextrd	$1, %xmm7, %ecx
	movslq	%ecx, %rcx
	vinsertps	$16, (%rax,%rcx,4), %xmm1, %xmm1 ## xmm1 = xmm1[0],mem[0],xmm1[2,3]
	vmovd	%xmm6, %ecx
	movslq	%ecx, %rcx
	vmovss	(%rax,%rcx,4), %xmm2            ## xmm2 = mem[0],zero,zero,zero
	vpextrd	$1, %xmm6, %ecx
	movslq	%ecx, %rcx
	vinsertps	$16, (%rax,%rcx,4), %xmm2, %xmm2 ## xmm2 = xmm2[0],mem[0],xmm2[2,3]
	vpextrd	$2, %xmm7, %ecx
	movslq	%ecx, %rcx
	vinsertps	$32, (%rax,%rcx,4), %xmm1, %xmm1 ## xmm1 = xmm1[0,1],mem[0],xmm1[3]
	vpextrd	$2, %xmm6, %ecx
	movslq	%ecx, %rcx
	vinsertps	$32, (%rax,%rcx,4), %xmm2, %xmm2 ## xmm2 = xmm2[0,1],mem[0],xmm2[3]
	vpextrd	$3, %xmm4, %ecx
	movslq	%ecx, %rcx
	vinsertps	$48, (%rax,%rcx,4), %xmm5, %xmm4 ## xmm4 = xmm5[0,1,2],mem[0]
	vpextrd	$3, %xmm6, %ecx
	movslq	%ecx, %rcx
	vinsertps	$48, (%rax,%rcx,4), %xmm2, %xmm5 ## xmm5 = xmm2[0,1,2],mem[0]
	subl	%r15d, %ebp
	imull	%r8d, %ebp
	subl	%r12d, %ebp
	vmovd	%ebp, %xmm2
	vpextrd	$3, %xmm7, %ecx
	movslq	%ecx, %rcx
	vpbroadcastd	%xmm2, %ymm2
	vinsertps	$48, (%rax,%rcx,4), %xmm1, %xmm1 ## xmm1 = xmm1[0,1,2],mem[0]
	vpaddd	%ymm2, %ymm10, %ymm2
	vextracti128	$1, %ymm2, %xmm6
	vmovd	%xmm6, %ecx
	movslq	%ecx, %rcx
	vmovss	(%rax,%rcx,4), %xmm7            ## xmm7 = mem[0],zero,zero,zero
	vpextrd	$1, %xmm6, %ecx
	movslq	%ecx, %rcx
	vinsertps	$16, (%rax,%rcx,4), %xmm7, %xmm7 ## xmm7 = xmm7[0],mem[0],xmm7[2,3]
	vmovd	%xmm2, %ecx
	movslq	%ecx, %rcx
	vmovss	(%rax,%rcx,4), %xmm0            ## xmm0 = mem[0],zero,zero,zero
	vpextrd	$1, %xmm2, %ecx
	movslq	%ecx, %rcx
	vinsertps	$16, (%rax,%rcx,4), %xmm0, %xmm0 ## xmm0 = xmm0[0],mem[0],xmm0[2,3]
	vpextrd	$2, %xmm6, %ecx
	movslq	%ecx, %rcx
	vinsertps	$32, (%rax,%rcx,4), %xmm7, %xmm7 ## xmm7 = xmm7[0,1],mem[0],xmm7[3]
	vpextrd	$2, %xmm2, %ecx
	movslq	%ecx, %rcx
	vinsertps	$32, (%rax,%rcx,4), %xmm0, %xmm0 ## xmm0 = xmm0[0,1],mem[0],xmm0[3]
	vpextrd	$3, %xmm6, %ecx
	movslq	%ecx, %rcx
	vinsertps	$48, (%rax,%rcx,4), %xmm7, %xmm6 ## xmm6 = xmm7[0,1,2],mem[0]
	vpextrd	$3, %xmm2, %ecx
	movslq	%ecx, %rcx
	vinsertps	$48, (%rax,%rcx,4), %xmm0, %xmm0 ## xmm0 = xmm0[0,1,2],mem[0]
	vinsertf128	$1, %xmm4, %ymm3, %ymm2
	vinsertf128	$1, %xmm1, %ymm5, %ymm1
	movslq	%esi, %rsi
	vcmpltps	%ymm2, %ymm8, %ymm2
	vinsertf128	$1, %xmm6, %ymm0, %ymm0
	vmulps	%ymm1, %ymm9, %ymm1
	vmulps	(%r13), %ymm0, %ymm0
	vmulps	%ymm1, %ymm0, %ymm0
	vandps	%ymm0, %ymm2, %ymm0
	movq	-64(%rsp), %rcx                 ## 8-byte Reload
	vaddps	(%rcx,%rsi,4), %ymm0, %ymm0
	vmovups	%ymm0, (%rcx,%rsi,4)
	addq	%r11, %r13
	incq	%rbx
	addl	-124(%rsp), %esi                ## 4-byte Folded Reload
	decq	%r14
	jne	LBB33_3
LBB33_60:                               ## %destructor_block
	xorl	%eax, %eax
	addq	$224, %rsp
	popq	%rbx
	popq	%r12
	popq	%r13
	popq	%r14
	popq	%r15
	popq	%rbp
	vzeroupper
	retq
LBB33_38:                               ## %"for relu1_0_d_def__.s25.w.wi.preheader.split.us.split"
	movl	%edi, -96(%rsp)                 ## 4-byte Spill
	testl	%r14d, %r14d
	movl	-128(%rsp), %ebx                ## 4-byte Reload
	movq	-48(%rsp), %rdx                 ## 8-byte Reload
	jle	LBB33_60
## %bb.39:                              ## %"for relu1_0_d_def__.s25.w.wi.us.us4.preheader"
	movq	%rsi, %r10
	movslq	%ebp, %r9
	movl	-116(%rsp), %ebp                ## 4-byte Reload
	movslq	-112(%rsp), %rax                ## 4-byte Folded Reload
	movq	%rax, -72(%rsp)                 ## 8-byte Spill
	movl	-88(%rsp), %r15d                ## 4-byte Reload
	movl	%ebp, %r12d
	andl	$-8, %r12d
	leaq	-8(%r12), %rax
	movq	%rax, -88(%rsp)                 ## 8-byte Spill
	movq	%rax, %rcx
	shrq	$3, %rcx
	incq	%rcx
	movq	%rdx, %rax
	imulq	-40(%rsp), %rax                 ## 8-byte Folded Reload
	addq	%r9, %rax
	movq	-104(%rsp), %r11                ## 8-byte Reload
	leaq	(%r11,%rax,4), %rdi
	addq	$32, %rdi
	leaq	(,%rdx,4), %r14
	movq	-64(%rsp), %rdx                 ## 8-byte Reload
	leaq	32(%rdx), %rsi
	movq	%rsi, -32(%rsp)                 ## 8-byte Spill
	shll	$6, %r8d
	movl	-96(%rsp), %esi                 ## 4-byte Reload
	shll	$6, %esi
	subl	%esi, %r8d
	orl	$19, %r8d
	imull	%r8d, %r10d
	movq	%rcx, %rsi
	movq	%rcx, -96(%rsp)                 ## 8-byte Spill
	andq	$-2, %rcx
	negq	%rcx
	movq	%rcx, -8(%rsp)                  ## 8-byte Spill
	leaq	(%r11,%rax,4), %r13
	movq	%r9, -112(%rsp)                 ## 8-byte Spill
	leaq	(%rdx,%r9,4), %rax
	movq	%rax, -80(%rsp)                 ## 8-byte Spill
	vmovss	LCPI33_2(%rip), %xmm0           ## xmm0 = mem[0],zero,zero,zero
	vmovss	LCPI33_1(%rip), %xmm1           ## xmm1 = mem[0],zero,zero,zero
	vbroadcastss	LCPI33_2(%rip), %ymm2   ## ymm2 = [1.99999994E-9,1.99999994E-9,1.99999994E-9,1.99999994E-9,1.99999994E-9,1.99999994E-9,1.99999994E-9,1.99999994E-9]
	xorl	%edx, %edx
	jmp	LBB33_40
LBB33_59:                               ## %after_bb.loopexit.us.us17
                                        ##   in Loop: Header=BB33_40 Depth=1
	incq	%rdx
	addq	%r14, %rdi
	addl	-124(%rsp), %r10d               ## 4-byte Folded Reload
	addq	%r14, %r13
	cmpq	%r15, %rdx
	movl	-128(%rsp), %ebx                ## 4-byte Reload
	je	LBB33_60
LBB33_40:                               ## %"for relu1_0_d_def__.s25.w.wi.us.us4"
                                        ## =>This Loop Header: Depth=1
                                        ##     Child Loop BB33_45 Depth 2
                                        ##     Child Loop BB33_58 Depth 2
	movslq	%r10d, %r8
	movq	-72(%rsp), %rax                 ## 8-byte Reload
	leaq	(%rdx,%rax), %rsi
	testq	%rsi, %rsi
	movl	$0, %eax
	cmovlel	%eax, %esi
	imull	%ebx, %esi
	movslq	%esi, %rsi
	movq	48(%rsp), %rax                  ## 8-byte Reload
	leaq	(%rsi,%rax), %rcx
	movq	40(%rsp), %rax                  ## 8-byte Reload
	leaq	(%rsi,%rax), %rbx
	addq	32(%rsp), %rsi                  ## 8-byte Folded Reload
	movq	-56(%rsp), %rax                 ## 8-byte Reload
	vmovss	-4(%rax,%rcx,4), %xmm3          ## xmm3 = mem[0],zero,zero,zero
	vmovss	-4(%rax,%rbx,4), %xmm4          ## xmm4 = mem[0],zero,zero,zero
	vmovss	-4(%rax,%rsi,4), %xmm5          ## xmm5 = mem[0],zero,zero,zero
	cmpl	$8, -116(%rsp)                  ## 4-byte Folded Reload
	jae	LBB33_42
## %bb.41:                              ##   in Loop: Header=BB33_40 Depth=1
	xorl	%esi, %esi
	jmp	LBB33_57
LBB33_42:                               ## %vector.ph
                                        ##   in Loop: Header=BB33_40 Depth=1
	cmpq	$0, -88(%rsp)                   ## 8-byte Folded Reload
	je	LBB33_43
## %bb.44:                              ## %vector.ph.new
                                        ##   in Loop: Header=BB33_40 Depth=1
	movq	-112(%rsp), %rax                ## 8-byte Reload
	leaq	(%rax,%r8), %rcx
	movq	-32(%rsp), %rax                 ## 8-byte Reload
	leaq	(%rax,%rcx,4), %rsi
	vmulss	%xmm5, %xmm4, %xmm6
	vbroadcastss	%xmm6, %ymm6
	movq	-8(%rsp), %r9                   ## 8-byte Reload
	xorl	%r11d, %r11d
	jmp	LBB33_45
LBB33_50:                               ## %vector.body
                                        ##   in Loop: Header=BB33_45 Depth=2
	vaddps	(%rsi,%r11,4), %ymm7, %ymm7
	vmovups	%ymm7, (%rsi,%r11,4)
	addq	$16, %r11
	addq	$2, %r9
	je	LBB33_51
LBB33_45:                               ## %vector.body
                                        ##   Parent Loop BB33_40 Depth=1
                                        ## =>  This Inner Loop Header: Depth=2
	vucomiss	%xmm1, %xmm3
	vxorps	%xmm7, %xmm7, %xmm7
	ja	LBB33_46
## %bb.47:                              ## %vector.body
                                        ##   in Loop: Header=BB33_45 Depth=2
	vxorps	%xmm8, %xmm8, %xmm8
	jmp	LBB33_48
LBB33_46:                               ##   in Loop: Header=BB33_45 Depth=2
	vmulps	-32(%rdi,%r11,4), %ymm6, %ymm8
	vmulps	%ymm2, %ymm8, %ymm8
LBB33_48:                               ## %vector.body
                                        ##   in Loop: Header=BB33_45 Depth=2
	vaddps	-32(%rsi,%r11,4), %ymm8, %ymm8
	vmovups	%ymm8, -32(%rsi,%r11,4)
	jbe	LBB33_50
## %bb.49:                              ##   in Loop: Header=BB33_45 Depth=2
	vmulps	(%rdi,%r11,4), %ymm6, %ymm7
	vmulps	%ymm2, %ymm7, %ymm7
	jmp	LBB33_50
LBB33_43:                               ##   in Loop: Header=BB33_40 Depth=1
	xorl	%r11d, %r11d
LBB33_51:                               ## %middle.block.unr-lcssa
                                        ##   in Loop: Header=BB33_40 Depth=1
	testb	$1, -96(%rsp)                   ## 1-byte Folded Reload
	je	LBB33_56
## %bb.52:                              ## %vector.body.epil
                                        ##   in Loop: Header=BB33_40 Depth=1
	movq	56(%rsp), %rax                  ## 8-byte Reload
	leal	(%rax,%rdx), %ecx
	imull	-124(%rsp), %ecx                ## 4-byte Folded Reload
	addl	(%rsp), %ecx                    ## 4-byte Folded Reload
	movslq	%ecx, %rsi
	addq	-112(%rsp), %r11                ## 8-byte Folded Reload
	addq	%r11, %rsi
	vucomiss	%xmm1, %xmm3
	ja	LBB33_53
## %bb.54:                              ## %vector.body.epil
                                        ##   in Loop: Header=BB33_40 Depth=1
	vxorps	%xmm6, %xmm6, %xmm6
	jmp	LBB33_55
LBB33_53:                               ##   in Loop: Header=BB33_40 Depth=1
	movq	-40(%rsp), %rax                 ## 8-byte Reload
	leaq	(%rdx,%rax), %rcx
	imulq	-48(%rsp), %rcx                 ## 8-byte Folded Reload
	addq	%rcx, %r11
	vmulss	%xmm5, %xmm4, %xmm6
	vbroadcastss	%xmm6, %ymm6
	movq	-104(%rsp), %rax                ## 8-byte Reload
	vmulps	(%rax,%r11,4), %ymm6, %ymm6
	vmulps	%ymm2, %ymm6, %ymm6
LBB33_55:                               ## %vector.body.epil
                                        ##   in Loop: Header=BB33_40 Depth=1
	movq	-64(%rsp), %rcx                 ## 8-byte Reload
	vaddps	(%rcx,%rsi,4), %ymm6, %ymm6
	vmovups	%ymm6, (%rcx,%rsi,4)
LBB33_56:                               ## %middle.block
                                        ##   in Loop: Header=BB33_40 Depth=1
	movq	%r12, %rsi
	cmpq	%rbp, %r12
	je	LBB33_59
LBB33_57:                               ## %"for relu1_0_d_def__.s25.n.ni.rebased.us.us9.preheader"
                                        ##   in Loop: Header=BB33_40 Depth=1
	movq	-80(%rsp), %rax                 ## 8-byte Reload
	leaq	(%rax,%r8,4), %rax
LBB33_58:                               ## %"for relu1_0_d_def__.s25.n.ni.rebased.us.us9"
                                        ##   Parent Loop BB33_40 Depth=1
                                        ## =>  This Inner Loop Header: Depth=2
	vmulss	(%r13,%rsi,4), %xmm5, %xmm6
	vmulss	%xmm6, %xmm4, %xmm6
	vmulss	%xmm0, %xmm6, %xmm6
	vcmpltss	%xmm3, %xmm1, %xmm7
	vandps	%xmm6, %xmm7, %xmm6
	vaddss	(%rax,%rsi,4), %xmm6, %xmm6
	vmovss	%xmm6, (%rax,%rsi,4)
	incq	%rsi
	cmpq	%rsi, %rbp
	jne	LBB33_58
	jmp	LBB33_59
                                        ## -- End function
	.section	__TEXT,__literal4,4byte_literals
	.p2align	2                               ## -- Begin function train_cost_model.par_for.relu1_0_d_def__.s26.n.n.n
LCPI34_0:
	.long	2                               ## 0x2
LCPI34_2:
	.long	0x3f800000                      ## float 1
LCPI34_3:
	.long	0x3109705f                      ## float 1.99999994E-9
	.section	__TEXT,__const
	.p2align	5
LCPI34_1:
	.long	0                               ## 0x0
	.long	1                               ## 0x1
	.long	2                               ## 0x2
	.long	3                               ## 0x3
	.long	4                               ## 0x4
	.long	5                               ## 0x5
	.long	6                               ## 0x6
	.long	7                               ## 0x7
	.section	__TEXT,__text,regular,pure_instructions
	.p2align	4, 0x90
_train_cost_model.par_for.relu1_0_d_def__.s26.n.n.n: ## @train_cost_model.par_for.relu1_0_d_def__.s26.n.n.n
## %bb.0:                               ## %entry
	pushq	%rbp
	pushq	%r15
	pushq	%r14
	pushq	%r13
	pushq	%r12
	pushq	%rbx
	movq	%rdx, %r11
	movl	8(%rdx), %r9d
	movl	24(%rdx), %ecx
	movl	%esi, %r13d
	sarl	$31, %r13d
	xorl	%ebx, %ebx
	testl	%ecx, %ecx
	sete	%bl
	movl	%ebx, %edi
	negl	%edi
	subl	%r13d, %esi
	orl	%ecx, %edi
	movl	%esi, %eax
	cltd
	idivl	%edi
                                        ## kill: def $edx killed $edx def $rdx
	movq	%rdx, -40(%rsp)                 ## 8-byte Spill
	movl	%ecx, %ebp
	sarl	$31, %ebp
	movl	%ebp, %r14d
	notl	%r14d
	leal	(%rcx,%rbx), %edi
	movl	%esi, %eax
	cltd
	idivl	%edi
	decl	%ebx
	movl	%r14d, %r8d
	subl	%ebp, %r8d
	andl	%r13d, %r8d
	addl	%eax, %r8d
	andl	%ebx, %r8d
	leal	(%r8,%r8), %eax
	movl	%r9d, %edx
	subl	%eax, %edx
	movl	$1, %esi
	subl	%eax, %esi
	cmpl	$1, %r9d
	cmovlel	%edx, %esi
	vmovd	%edx, %xmm0
	vpinsrd	$1, %esi, %xmm0, %xmm0
	vpbroadcastd	LCPI34_0(%rip), %xmm1   ## xmm1 = [2,2,2,2]
	vpminsd	%xmm1, %xmm0, %xmm0
	vpxor	%xmm1, %xmm1, %xmm1
	vpmaxsd	%xmm1, %xmm0, %xmm0
	vmovd	%xmm0, %eax
	vpextrd	$1, %xmm0, %edx
	movq	%rdx, -32(%rsp)                 ## 8-byte Spill
	subl	%edx, %eax
	movl	%eax, -52(%rsp)                 ## 4-byte Spill
	jle	LBB34_15
## %bb.1:                               ## %"for relu1_0_d_def__.s26.w.wi.rebased.preheader"
	movl	(%r11), %eax
	movl	12(%r11), %edi
	movslq	20(%r11), %r15
	movl	28(%r11), %edx
	movl	%edx, -56(%rsp)                 ## 4-byte Spill
	movslq	32(%r11), %rdx
	movq	%rdx, -16(%rsp)                 ## 8-byte Spill
	movl	44(%r11), %r12d
	movl	48(%r11), %r9d
	movl	52(%r11), %r10d
	movq	56(%r11), %rdx
	movq	%rdx, -24(%rsp)                 ## 8-byte Spill
	xorl	%ecx, %ebp
	addl	%r14d, %ebp
	andl	%r13d, %ebp
	movq	-40(%rsp), %rcx                 ## 8-byte Reload
	addl	%ebp, %ecx
	andl	%ebx, %ecx
	movq	%rcx, -40(%rsp)                 ## 8-byte Spill
	leal	(,%rcx,8), %esi
	movq	%rdi, -8(%rsp)                  ## 8-byte Spill
	movl	%edi, %ecx
	shll	$5, %ecx
	movl	%ecx, -44(%rsp)                 ## 4-byte Spill
	movl	%eax, %edx
	subl	%esi, %edx
	cmpl	$8, %edx
	movl	$8, %ebp
	cmovll	%edx, %ebp
	xorl	%ecx, %ecx
	testl	%edx, %edx
	cmovgl	%ebp, %ecx
	movl	%ecx, -48(%rsp)                 ## 4-byte Spill
	movq	72(%r11), %r14
	leal	8(%rsi), %ebp
	movslq	%esi, %rsi
	movq	88(%r11), %rbx
	movq	-32(%rsp), %rcx                 ## 8-byte Reload
	leal	(%rcx,%r8,2), %edi
	movslq	40(%r11), %rcx
	movslq	4(%r11), %r13
	cmpl	%eax, %ebp
	jle	LBB34_2
## %bb.4:                               ## %"for relu1_0_d_def__.s26.w.wi.rebased.preheader.split.us"
	testl	%edx, %edx
	jle	LBB34_15
## %bb.5:                               ## %"for relu1_0_d_def__.s26.w.wi.rebased.us.us.preheader"
	movslq	%edi, %rax
	movq	%r15, %rbp
	imulq	%rax, %rbp
	subq	%rcx, %rax
	imulq	%r13, %rax
	addq	%rsi, %rax
	movq	-24(%rsp), %rcx                 ## 8-byte Reload
	leaq	(%rcx,%rax,4), %rdx
	movq	-16(%rsp), %r11                 ## 8-byte Reload
	subl	%r11d, %r12d
	subl	%r11d, %r9d
	subl	%r11d, %r10d
	addl	%esi, %r10d
	addl	%esi, %r9d
	addl	%esi, %r12d
	addq	%rbp, %rsi
	subq	%r11, %rsi
	movslq	%r12d, %rdi
	movslq	%r9d, %rax
	movslq	%r10d, %rcx
	addq	%rbp, %rcx
	addq	%rbp, %rax
	addq	%rdi, %rbp
	leaq	(%rbx,%rcx,4), %rcx
	leaq	(%rbx,%rax,4), %rdi
	leaq	(%rbx,%rsi,4), %rax
	leaq	(%rbx,%rbp,4), %rbx
	movq	-32(%rsp), %rsi                 ## 8-byte Reload
	leal	(%rsi,%r8,2), %esi
	subl	-56(%rsp), %esi                 ## 4-byte Folded Reload
	movl	-48(%rsp), %ebp                 ## 4-byte Reload
	movl	-52(%rsp), %r11d                ## 4-byte Reload
	shll	$5, %esi
	orl	$18, %esi
	movq	-8(%rsp), %r8                   ## 8-byte Reload
	imull	%esi, %r8d
	movl	%ebp, %r10d
	andl	$2147483640, %r10d              ## imm = 0x7FFFFFF8
	shlq	$2, %r13
	shlq	$2, %r15
	movq	-40(%rsp), %rsi                 ## 8-byte Reload
	leal	(%r8,%rsi,8), %r8d
	xorl	%r12d, %r12d
	vmovss	LCPI34_2(%rip), %xmm0           ## xmm0 = mem[0],zero,zero,zero
	vmovss	LCPI34_3(%rip), %xmm1           ## xmm1 = mem[0],zero,zero,zero
	vbroadcastss	LCPI34_2(%rip), %ymm2   ## ymm2 = [1.0E+0,1.0E+0,1.0E+0,1.0E+0,1.0E+0,1.0E+0,1.0E+0,1.0E+0]
	vbroadcastss	LCPI34_3(%rip), %ymm3   ## ymm3 = [1.99999994E-9,1.99999994E-9,1.99999994E-9,1.99999994E-9,1.99999994E-9,1.99999994E-9,1.99999994E-9,1.99999994E-9]
	jmp	LBB34_6
	.p2align	4, 0x90
LBB34_14:                               ## %after_bb.loopexit.us.us
                                        ##   in Loop: Header=BB34_6 Depth=1
	incq	%r12
	addq	%r13, %rdx
	addq	%r15, %rcx
	addq	%r15, %rdi
	addq	%r15, %rax
	addq	%r15, %rbx
	addl	-44(%rsp), %r8d                 ## 4-byte Folded Reload
	cmpq	%r11, %r12
	je	LBB34_15
LBB34_6:                                ## %"for relu1_0_d_def__.s26.w.wi.rebased.us.us"
                                        ## =>This Loop Header: Depth=1
                                        ##     Child Loop BB34_9 Depth 2
                                        ##     Child Loop BB34_11 Depth 2
	movslq	%r8d, %rsi
	leaq	(%r14,%rsi,4), %r9
	cmpl	$8, -48(%rsp)                   ## 4-byte Folded Reload
	jae	LBB34_8
## %bb.7:                               ##   in Loop: Header=BB34_6 Depth=1
	xorl	%esi, %esi
	jmp	LBB34_11
	.p2align	4, 0x90
LBB34_8:                                ## %vector.body.preheader
                                        ##   in Loop: Header=BB34_6 Depth=1
	xorl	%esi, %esi
	.p2align	4, 0x90
LBB34_9:                                ## %vector.body
                                        ##   Parent Loop BB34_6 Depth=1
                                        ## =>  This Inner Loop Header: Depth=2
	vmovups	(%rbx,%rsi,4), %ymm4
	vmovups	(%rax,%rsi,4), %ymm5
	vmulps	(%rdx,%rsi,4), %ymm5, %ymm5
	vmovups	(%rcx,%rsi,4), %ymm6
	vmaxps	%ymm2, %ymm6, %ymm6
	vmulps	(%rdi,%rsi,4), %ymm4, %ymm7
	vmulps	%ymm7, %ymm5, %ymm5
	vmulps	%ymm3, %ymm5, %ymm5
	vdivps	%ymm6, %ymm5, %ymm5
	vcmpleps	%ymm2, %ymm4, %ymm4
	vandps	%ymm5, %ymm4, %ymm4
	vaddps	(%r9,%rsi,4), %ymm4, %ymm4
	vmovups	%ymm4, (%r9,%rsi,4)
	addq	$8, %rsi
	cmpq	%rsi, %r10
	jne	LBB34_9
## %bb.10:                              ## %middle.block
                                        ##   in Loop: Header=BB34_6 Depth=1
	movq	%r10, %rsi
	cmpq	%rbp, %r10
	jne	LBB34_11
	jmp	LBB34_14
	.p2align	4, 0x90
LBB34_13:                               ## %select.end
                                        ##   in Loop: Header=BB34_11 Depth=2
	vaddss	(%r9,%rsi,4), %xmm5, %xmm4
	vmovss	%xmm4, (%r9,%rsi,4)
	incq	%rsi
	cmpq	%rsi, %rbp
	je	LBB34_14
LBB34_11:                               ## %"for relu1_0_d_def__.s26.n.ni.us.us"
                                        ##   Parent Loop BB34_6 Depth=1
                                        ## =>  This Inner Loop Header: Depth=2
	vmovss	(%rbx,%rsi,4), %xmm4            ## xmm4 = mem[0],zero,zero,zero
	vmovss	(%rax,%rsi,4), %xmm5            ## xmm5 = mem[0],zero,zero,zero
	vmulss	(%rdx,%rsi,4), %xmm5, %xmm5
	vmulss	(%rdi,%rsi,4), %xmm4, %xmm6
	vucomiss	%xmm4, %xmm0
	vmulss	%xmm6, %xmm5, %xmm4
	vmulss	%xmm1, %xmm4, %xmm4
	vxorps	%xmm5, %xmm5, %xmm5
	jb	LBB34_13
## %bb.12:                              ## %select.false.sink
                                        ##   in Loop: Header=BB34_11 Depth=2
	vmovss	(%rcx,%rsi,4), %xmm5            ## xmm5 = mem[0],zero,zero,zero
	vmaxss	%xmm0, %xmm5, %xmm5
	vdivss	%xmm5, %xmm4, %xmm5
	jmp	LBB34_13
LBB34_2:                                ## %"for relu1_0_d_def__.s26.w.wi.rebased.preheader4"
	decl	%eax
	vmovd	%eax, %xmm0
	movslq	%edi, %rax
	subq	%rcx, %rax
	vmovd	%esi, %xmm1
	imulq	%r13, %rax
	addq	%rsi, %rax
	movq	-24(%rsp), %rcx                 ## 8-byte Reload
	leaq	(%rcx,%rax,4), %rsi
	movq	-32(%rsp), %rax                 ## 8-byte Reload
	leal	(%rax,%r8,2), %eax
	movl	%eax, %ebp
	subl	16(%r11), %ebp
	imull	%r15d, %ebp
	subl	36(%r11), %ebp
	movl	%r15d, %edx
	imull	%eax, %edx
	subl	-56(%rsp), %eax                 ## 4-byte Folded Reload
	shll	$5, %eax
	orl	$18, %eax
	movq	-8(%rsp), %rcx                  ## 8-byte Reload
	imull	%eax, %ecx
	movq	-40(%rsp), %rax                 ## 8-byte Reload
	leal	(%rcx,%rax,8), %eax
	addl	%edx, %r10d
	addl	%edx, %r9d
	addl	%edx, %r12d
	movq	-16(%rsp), %rcx                 ## 8-byte Reload
	subl	%ecx, %r10d
	subl	%ecx, %r9d
	subl	%ecx, %r12d
	vpbroadcastd	%xmm1, %ymm1
	vpor	LCPI34_1(%rip), %ymm1, %ymm1
	vpbroadcastd	%xmm0, %ymm0
	vpminsd	%ymm1, %ymm0, %ymm11
	movl	-52(%rsp), %edx                 ## 4-byte Reload
	shlq	$2, %r13
	vbroadcastss	LCPI34_2(%rip), %ymm9   ## ymm9 = [1.0E+0,1.0E+0,1.0E+0,1.0E+0,1.0E+0,1.0E+0,1.0E+0,1.0E+0]
	vbroadcastss	LCPI34_3(%rip), %ymm8   ## ymm8 = [1.99999994E-9,1.99999994E-9,1.99999994E-9,1.99999994E-9,1.99999994E-9,1.99999994E-9,1.99999994E-9,1.99999994E-9]
	movl	-44(%rsp), %r8d                 ## 4-byte Reload
	.p2align	4, 0x90
LBB34_3:                                ## %"for relu1_0_d_def__.s26.w.wi.rebased"
                                        ## =>This Inner Loop Header: Depth=1
	vmovd	%r12d, %xmm3
	vpbroadcastd	%xmm3, %ymm3
	vpaddd	%ymm3, %ymm11, %ymm3
	vmovd	%xmm3, %ecx
	movslq	%ecx, %rcx
	vpextrd	$1, %xmm3, %edi
	movslq	%edi, %rdi
	vmovss	(%rbx,%rcx,4), %xmm4            ## xmm4 = mem[0],zero,zero,zero
	vpextrd	$2, %xmm3, %ecx
	vinsertps	$16, (%rbx,%rdi,4), %xmm4, %xmm5 ## xmm5 = xmm4[0],mem[0],xmm4[2,3]
	vpextrd	$3, %xmm3, %edi
	movslq	%ecx, %rcx
	movslq	%edi, %rdi
	vextracti128	$1, %ymm3, %xmm4
	vinsertps	$32, (%rbx,%rcx,4), %xmm5, %xmm3 ## xmm3 = xmm5[0,1],mem[0],xmm5[3]
	vmovd	%xmm4, %ecx
	movslq	%ecx, %rcx
	vmovd	%ebp, %xmm5
	vmovss	(%rbx,%rcx,4), %xmm6            ## xmm6 = mem[0],zero,zero,zero
	vpbroadcastd	%xmm5, %ymm5
	vpaddd	%ymm5, %ymm11, %ymm5
	vinsertps	$48, (%rbx,%rdi,4), %xmm3, %xmm10 ## xmm10 = xmm3[0,1,2],mem[0]
	vmovd	%xmm5, %ecx
	vpextrd	$1, %xmm5, %edi
	movslq	%ecx, %rcx
	vmovss	(%rbx,%rcx,4), %xmm7            ## xmm7 = mem[0],zero,zero,zero
	vpextrd	$2, %xmm5, %ecx
	movslq	%edi, %rdi
	vinsertps	$16, (%rbx,%rdi,4), %xmm7, %xmm7 ## xmm7 = xmm7[0],mem[0],xmm7[2,3]
	vpextrd	$1, %xmm4, %edi
	movslq	%edi, %rdi
	movslq	%ecx, %rcx
	vinsertps	$16, (%rbx,%rdi,4), %xmm6, %xmm2 ## xmm2 = xmm6[0],mem[0],xmm6[2,3]
	vpextrd	$3, %xmm5, %edi
	vextracti128	$1, %ymm5, %xmm6
	movslq	%edi, %rdi
	vinsertps	$32, (%rbx,%rcx,4), %xmm7, %xmm5 ## xmm5 = xmm7[0,1],mem[0],xmm7[3]
	vpextrd	$2, %xmm4, %ecx
	vinsertps	$48, (%rbx,%rdi,4), %xmm5, %xmm12 ## xmm12 = xmm5[0,1,2],mem[0]
	vmovd	%xmm6, %edi
	movslq	%edi, %rdi
	vmovss	(%rbx,%rdi,4), %xmm7            ## xmm7 = mem[0],zero,zero,zero
	vpextrd	$1, %xmm6, %edi
	movslq	%edi, %rdi
	vmovd	%r9d, %xmm1
	vpbroadcastd	%xmm1, %ymm1
	vinsertps	$16, (%rbx,%rdi,4), %xmm7, %xmm7 ## xmm7 = xmm7[0],mem[0],xmm7[2,3]
	vpaddd	%ymm1, %ymm11, %ymm1
	vextracti128	$1, %ymm1, %xmm3
	vmovd	%xmm3, %edi
	movslq	%edi, %rdi
	vmovss	(%rbx,%rdi,4), %xmm0            ## xmm0 = mem[0],zero,zero,zero
	vpextrd	$1, %xmm3, %edi
	movslq	%edi, %rdi
	vinsertps	$16, (%rbx,%rdi,4), %xmm0, %xmm0 ## xmm0 = xmm0[0],mem[0],xmm0[2,3]
	vpextrd	$2, %xmm6, %edi
	movslq	%ecx, %rcx
	movslq	%edi, %rdi
	vinsertps	$32, (%rbx,%rcx,4), %xmm2, %xmm2 ## xmm2 = xmm2[0,1],mem[0],xmm2[3]
	vpextrd	$2, %xmm3, %ecx
	vinsertps	$32, (%rbx,%rdi,4), %xmm7, %xmm7 ## xmm7 = xmm7[0,1],mem[0],xmm7[3]
	vmovd	%xmm1, %edi
	movslq	%ecx, %rcx
	vinsertps	$32, (%rbx,%rcx,4), %xmm0, %xmm0 ## xmm0 = xmm0[0,1],mem[0],xmm0[3]
	movslq	%edi, %rcx
	vpextrd	$1, %xmm1, %edi
	movslq	%edi, %rdi
	vmovss	(%rbx,%rcx,4), %xmm5            ## xmm5 = mem[0],zero,zero,zero
	vpextrd	$2, %xmm1, %ecx
	movslq	%ecx, %rcx
	vinsertps	$16, (%rbx,%rdi,4), %xmm5, %xmm5 ## xmm5 = xmm5[0],mem[0],xmm5[2,3]
	vpextrd	$3, %xmm4, %edi
	vinsertps	$32, (%rbx,%rcx,4), %xmm5, %xmm4 ## xmm4 = xmm5[0,1],mem[0],xmm5[3]
	vpextrd	$3, %xmm6, %ecx
	movslq	%edi, %rdi
	movslq	%ecx, %rcx
	vinsertps	$48, (%rbx,%rdi,4), %xmm2, %xmm2 ## xmm2 = xmm2[0,1,2],mem[0]
	vpextrd	$3, %xmm1, %edi
	vinsertps	$48, (%rbx,%rcx,4), %xmm7, %xmm1 ## xmm1 = xmm7[0,1,2],mem[0]
	movslq	%edi, %rcx
	vpextrd	$3, %xmm3, %edi
	movslq	%edi, %rdi
	vinsertps	$48, (%rbx,%rcx,4), %xmm4, %xmm3 ## xmm3 = xmm4[0,1,2],mem[0]
	vinsertps	$48, (%rbx,%rdi,4), %xmm0, %xmm0 ## xmm0 = xmm0[0,1,2],mem[0]
	vmovd	%r10d, %xmm4
	vpbroadcastd	%xmm4, %ymm4
	vpaddd	%ymm4, %ymm11, %ymm4
	vextracti128	$1, %ymm4, %xmm5
	vmovd	%xmm5, %ecx
	vpextrd	$1, %xmm5, %edi
	movslq	%ecx, %rcx
	vmovss	(%rbx,%rcx,4), %xmm6            ## xmm6 = mem[0],zero,zero,zero
	movslq	%edi, %rcx
	vinsertps	$16, (%rbx,%rcx,4), %xmm6, %xmm6 ## xmm6 = xmm6[0],mem[0],xmm6[2,3]
	vpextrd	$2, %xmm5, %ecx
	movslq	%ecx, %rcx
	vinsertps	$32, (%rbx,%rcx,4), %xmm6, %xmm6 ## xmm6 = xmm6[0,1],mem[0],xmm6[3]
	vmovd	%xmm4, %ecx
	movslq	%ecx, %rcx
	vmovss	(%rbx,%rcx,4), %xmm7            ## xmm7 = mem[0],zero,zero,zero
	vpextrd	$1, %xmm4, %ecx
	movslq	%ecx, %rcx
	vinsertps	$16, (%rbx,%rcx,4), %xmm7, %xmm7 ## xmm7 = xmm7[0],mem[0],xmm7[2,3]
	vpextrd	$3, %xmm5, %ecx
	movslq	%ecx, %rcx
	vinsertps	$48, (%rbx,%rcx,4), %xmm6, %xmm5 ## xmm5 = xmm6[0,1,2],mem[0]
	vpextrd	$2, %xmm4, %ecx
	movslq	%ecx, %rcx
	vinsertps	$32, (%rbx,%rcx,4), %xmm7, %xmm6 ## xmm6 = xmm7[0,1],mem[0],xmm7[3]
	vpextrd	$3, %xmm4, %ecx
	movslq	%ecx, %rcx
	vinsertps	$48, (%rbx,%rcx,4), %xmm6, %xmm4 ## xmm4 = xmm6[0,1,2],mem[0]
	vinsertf128	$1, %xmm2, %ymm10, %ymm2
	vinsertf128	$1, %xmm1, %ymm12, %ymm1
	vinsertf128	$1, %xmm0, %ymm3, %ymm0
	vmulps	(%rsi), %ymm1, %ymm1
	vinsertf128	$1, %xmm5, %ymm4, %ymm3
	vmulps	%ymm0, %ymm2, %ymm0
	vmulps	%ymm0, %ymm8, %ymm0
	vmaxps	%ymm9, %ymm3, %ymm3
	vmulps	%ymm0, %ymm1, %ymm0
	vdivps	%ymm3, %ymm0, %ymm0
	cltq
	vcmpleps	%ymm9, %ymm2, %ymm1
	vandps	%ymm0, %ymm1, %ymm0
	vaddps	(%r14,%rax,4), %ymm0, %ymm0
	vmovups	%ymm0, (%r14,%rax,4)
	addq	%r13, %rsi
	addl	%r15d, %ebp
	addl	%r8d, %eax
	addl	%r15d, %r10d
	addl	%r15d, %r9d
	addl	%r15d, %r12d
	decq	%rdx
	jne	LBB34_3
LBB34_15:                               ## %destructor_block
	xorl	%eax, %eax
	popq	%rbx
	popq	%r12
	popq	%r13
	popq	%r14
	popq	%r15
	popq	%rbp
	vzeroupper
	retq
                                        ## -- End function
	.section	__TEXT,__const
	.p2align	5                               ## -- Begin function train_cost_model.par_for.relu1_0_d_def__.s27.n.n.n
LCPI35_0:
	.long	0                               ## 0x0
	.long	1                               ## 0x1
	.long	2                               ## 0x2
	.long	3                               ## 0x3
	.long	4                               ## 0x4
	.long	5                               ## 0x5
	.long	6                               ## 0x6
	.long	7                               ## 0x7
	.section	__TEXT,__literal4,4byte_literals
	.p2align	2
LCPI35_1:
	.long	0x3f800000                      ## float 1
LCPI35_2:
	.long	0x3109705f                      ## float 1.99999994E-9
	.section	__TEXT,__text,regular,pure_instructions
	.p2align	4, 0x90
_train_cost_model.par_for.relu1_0_d_def__.s27.n.n.n: ## @train_cost_model.par_for.relu1_0_d_def__.s27.n.n.n
## %bb.0:                               ## %entry
	pushq	%rbp
	pushq	%r15
	pushq	%r14
	pushq	%r13
	pushq	%r12
	pushq	%rbx
	movq	%rdx, %rdi
	movl	12(%rdx), %r8d
	movl	20(%rdx), %r10d
	movl	%esi, %r9d
	sarl	$31, %r9d
	xorl	%ecx, %ecx
	testl	%r10d, %r10d
	sete	%cl
	movl	%ecx, %ebx
	negl	%ebx
	movl	%r10d, %ebp
	sarl	$31, %ebp
	subl	%r9d, %esi
	orl	%r10d, %ebx
	movl	%esi, %eax
	cltd
	idivl	%ebx
                                        ## kill: def $edx killed $edx def $rdx
	movq	%rdx, -40(%rsp)                 ## 8-byte Spill
	movl	%ebp, %r14d
	leal	(%r10,%rcx), %ebx
	movl	%esi, %eax
	cltd
	idivl	%ebx
	notl	%r14d
	decl	%ecx
	movl	%r14d, %r15d
	subl	%ebp, %r15d
	andl	%r9d, %r15d
	addl	%eax, %r15d
	andl	%ecx, %r15d
	leal	(%r15,%r15), %eax
	testl	%r8d, %r8d
	movl	$1, %edx
	cmovlel	%r8d, %edx
	subl	%eax, %edx
	cmpl	$2, %edx
	movl	$2, %eax
	cmovll	%edx, %eax
	xorl	%esi, %esi
	testl	%edx, %edx
	movl	$0, -56(%rsp)                   ## 4-byte Folded Spill
	cmovlel	%esi, %eax
	movl	%eax, -44(%rsp)                 ## 4-byte Spill
	jle	LBB35_19
## %bb.1:                               ## %"for relu1_0_d_def__.s27.w.wi.preheader"
	movl	(%rdi), %edx
	movl	16(%rdi), %ebx
	movl	24(%rdi), %eax
	movl	%eax, -60(%rsp)                 ## 4-byte Spill
	movl	28(%rdi), %r12d
	movl	36(%rdi), %esi
	movl	40(%rdi), %r11d
	movl	44(%rdi), %r13d
	movq	48(%rdi), %rax
	movq	%rax, -8(%rsp)                  ## 8-byte Spill
	movq	64(%rdi), %r8
	xorl	%r10d, %ebp
	addl	%r14d, %ebp
	movq	%rbx, %rax
	andl	%r9d, %ebp
	movq	-40(%rsp), %rbx                 ## 8-byte Reload
	addl	%ebp, %ebx
	andl	%ecx, %ebx
	movq	%rbx, %r14
	leal	(,%rbx,8), %ebx
	subl	%r12d, %esi
	subl	%r12d, %r11d
	subl	%r12d, %r13d
	movslq	8(%rdi), %rcx
	movslq	%r15d, %r9
	subq	%rcx, %r9
	addq	%r9, %r9
	movq	%rax, -32(%rsp)                 ## 8-byte Spill
	movl	%eax, %ecx
	shll	$5, %ecx
	movl	%ecx, -40(%rsp)                 ## 4-byte Spill
	movl	%edx, %ecx
	subl	%ebx, %ecx
	cmpl	$8, %ecx
	movl	$8, %ebp
	cmovll	%ecx, %ebp
	movq	80(%rdi), %r10
	testl	%ecx, %ecx
	movl	-56(%rsp), %eax                 ## 4-byte Reload
	cmovgl	%ebp, %eax
	movl	%eax, -56(%rsp)                 ## 4-byte Spill
	movslq	4(%rdi), %rax
	movq	%rax, -16(%rsp)                 ## 8-byte Spill
	leal	8(%rbx), %ebp
	movslq	%ebx, %rax
	movq	%rax, -24(%rsp)                 ## 8-byte Spill
	cmpl	%edx, %ebp
	jle	LBB35_13
## %bb.2:                               ## %"for relu1_0_d_def__.s27.w.wi.preheader.split.us"
	testl	%ecx, %ecx
	jle	LBB35_19
## %bb.3:                               ## %"for relu1_0_d_def__.s27.w.wi.us.us.preheader"
	movq	-24(%rsp), %rax                 ## 8-byte Reload
	movl	%eax, %ecx
	subl	%r12d, %ecx
	addl	%ebx, %r13d
	addl	%ebx, %r11d
	addl	%ebx, %esi
	movq	%r14, %r12
	movq	-16(%rsp), %r14                 ## 8-byte Reload
	imulq	%r14, %r9
	addq	%rax, %r9
	movq	-8(%rsp), %rax                  ## 8-byte Reload
	leaq	(%rax,%r9,4), %rdi
	movslq	%esi, %rsi
	movslq	%ecx, %rbp
	movslq	%r11d, %rcx
	movslq	%r13d, %rax
	leaq	(%r10,%rax,4), %rbx
	leaq	(%r10,%rcx,4), %rcx
	leaq	(%r10,%rbp,4), %rdx
	leaq	(%r10,%rsi,4), %rsi
	shll	$6, %r15d
	movl	-60(%rsp), %eax                 ## 4-byte Reload
	shll	$6, %eax
	subl	%eax, %r15d
	movq	-32(%rsp), %rax                 ## 8-byte Reload
	movl	-56(%rsp), %r11d                ## 4-byte Reload
	movl	-44(%rsp), %r10d                ## 4-byte Reload
	orl	$17, %r15d
	imull	%r15d, %eax
	movl	%r11d, %r9d
	andl	$2147483640, %r9d               ## imm = 0x7FFFFFF8
	shlq	$2, %r14
	leal	(%rax,%r12,8), %r15d
	xorl	%r12d, %r12d
	vmovss	LCPI35_1(%rip), %xmm0           ## xmm0 = mem[0],zero,zero,zero
	vmovss	LCPI35_2(%rip), %xmm1           ## xmm1 = mem[0],zero,zero,zero
	vbroadcastss	LCPI35_1(%rip), %ymm2   ## ymm2 = [1.0E+0,1.0E+0,1.0E+0,1.0E+0,1.0E+0,1.0E+0,1.0E+0,1.0E+0]
	vbroadcastss	LCPI35_2(%rip), %ymm3   ## ymm3 = [1.99999994E-9,1.99999994E-9,1.99999994E-9,1.99999994E-9,1.99999994E-9,1.99999994E-9,1.99999994E-9,1.99999994E-9]
	jmp	LBB35_4
	.p2align	4, 0x90
LBB35_12:                               ## %after_bb.loopexit.us.us
                                        ##   in Loop: Header=BB35_4 Depth=1
	incq	%r12
	addq	%r14, %rdi
	addl	-40(%rsp), %r15d                ## 4-byte Folded Reload
	cmpq	%r10, %r12
	je	LBB35_19
LBB35_4:                                ## %"for relu1_0_d_def__.s27.w.wi.us.us"
                                        ## =>This Loop Header: Depth=1
                                        ##     Child Loop BB35_7 Depth 2
                                        ##     Child Loop BB35_9 Depth 2
	movslq	%r15d, %rax
	leaq	(%r8,%rax,4), %rax
	cmpl	$8, -56(%rsp)                   ## 4-byte Folded Reload
	jae	LBB35_6
## %bb.5:                               ##   in Loop: Header=BB35_4 Depth=1
	xorl	%r13d, %r13d
	jmp	LBB35_9
	.p2align	4, 0x90
LBB35_6:                                ## %vector.body.preheader
                                        ##   in Loop: Header=BB35_4 Depth=1
	xorl	%ebp, %ebp
	.p2align	4, 0x90
LBB35_7:                                ## %vector.body
                                        ##   Parent Loop BB35_4 Depth=1
                                        ## =>  This Inner Loop Header: Depth=2
	vmovups	(%rsi,%rbp,4), %ymm4
	vmovups	(%rdx,%rbp,4), %ymm5
	vmulps	(%rdi,%rbp,4), %ymm5, %ymm5
	vmovups	(%rbx,%rbp,4), %ymm6
	vmaxps	%ymm2, %ymm6, %ymm6
	vmulps	(%rcx,%rbp,4), %ymm4, %ymm7
	vmulps	%ymm7, %ymm5, %ymm5
	vmulps	%ymm3, %ymm5, %ymm5
	vdivps	%ymm6, %ymm5, %ymm5
	vcmpleps	%ymm2, %ymm4, %ymm4
	vandps	%ymm5, %ymm4, %ymm4
	vaddps	(%rax,%rbp,4), %ymm4, %ymm4
	vmovups	%ymm4, (%rax,%rbp,4)
	addq	$8, %rbp
	cmpq	%rbp, %r9
	jne	LBB35_7
## %bb.8:                               ## %middle.block
                                        ##   in Loop: Header=BB35_4 Depth=1
	movq	%r9, %r13
	cmpq	%r11, %r9
	jne	LBB35_9
	jmp	LBB35_12
	.p2align	4, 0x90
LBB35_11:                               ## %select.end
                                        ##   in Loop: Header=BB35_9 Depth=2
	vaddss	(%rax,%r13,4), %xmm5, %xmm4
	vmovss	%xmm4, (%rax,%r13,4)
	incq	%r13
	cmpq	%r13, %r11
	je	LBB35_12
LBB35_9:                                ## %"for relu1_0_d_def__.s27.n.ni.us.us"
                                        ##   Parent Loop BB35_4 Depth=1
                                        ## =>  This Inner Loop Header: Depth=2
	vmovss	(%rsi,%r13,4), %xmm4            ## xmm4 = mem[0],zero,zero,zero
	vmovss	(%rdx,%r13,4), %xmm5            ## xmm5 = mem[0],zero,zero,zero
	vmulss	(%rdi,%r13,4), %xmm5, %xmm5
	vmulss	(%rcx,%r13,4), %xmm4, %xmm6
	vucomiss	%xmm4, %xmm0
	vmulss	%xmm6, %xmm5, %xmm4
	vmulss	%xmm1, %xmm4, %xmm4
	vxorps	%xmm5, %xmm5, %xmm5
	jb	LBB35_11
## %bb.10:                              ## %select.false.sink
                                        ##   in Loop: Header=BB35_9 Depth=2
	vmovss	(%rbx,%r13,4), %xmm5            ## xmm5 = mem[0],zero,zero,zero
	vmaxss	%xmm0, %xmm5, %xmm5
	vdivss	%xmm5, %xmm4, %xmm5
	jmp	LBB35_11
LBB35_13:                               ## %"for relu1_0_d_def__.s27.w.wi.preheader.split"
	decl	%edx
	movq	%rbx, -56(%rsp)                 ## 8-byte Spill
	vmovd	%ebx, %xmm0
	vpbroadcastd	%xmm0, %ymm0
	vpor	LCPI35_0(%rip), %ymm0, %ymm0
	vmovd	%edx, %xmm1
	vpbroadcastd	%xmm1, %ymm1
	vpminsd	%ymm0, %ymm1, %ymm2
	vmovd	%esi, %xmm0
	vpbroadcastd	%xmm0, %ymm0
	vpaddd	%ymm0, %ymm2, %ymm3
	vmovd	%r13d, %xmm0
	vpbroadcastd	%xmm0, %ymm0
	vpaddd	%ymm0, %ymm2, %ymm0
	vextracti128	$1, %ymm0, %xmm1
	vpextrd	$3, %xmm1, %ebp
	vextracti128	$1, %ymm3, %xmm4
	vpextrd	$1, %xmm4, %eax
	vmovd	%xmm4, %ecx
	movslq	%ecx, %rcx
	vmovss	(%r10,%rcx,4), %xmm5            ## xmm5 = mem[0],zero,zero,zero
	cltq
	vinsertps	$16, (%r10,%rax,4), %xmm5, %xmm5 ## xmm5 = xmm5[0],mem[0],xmm5[2,3]
	vpextrd	$2, %xmm4, %eax
	cltq
	vinsertps	$32, (%r10,%rax,4), %xmm5, %xmm5 ## xmm5 = xmm5[0,1],mem[0],xmm5[3]
	vpextrd	$1, %xmm1, %eax
	vpbroadcastd	32(%rdi), %ymm6
	vpextrd	$3, %xmm4, %ecx
	movslq	%ecx, %rcx
	vinsertps	$48, (%r10,%rcx,4), %xmm5, %xmm8 ## xmm8 = xmm5[0,1,2],mem[0]
	vmovd	%xmm3, %ecx
	movslq	%ecx, %rcx
	vmovss	(%r10,%rcx,4), %xmm5            ## xmm5 = mem[0],zero,zero,zero
	vpextrd	$1, %xmm3, %ecx
	movslq	%ecx, %rcx
	vinsertps	$16, (%r10,%rcx,4), %xmm5, %xmm5 ## xmm5 = xmm5[0],mem[0],xmm5[2,3]
	vpextrd	$2, %xmm3, %ecx
	vpaddd	%ymm6, %ymm2, %ymm6
	movslq	%ecx, %rcx
	vinsertps	$32, (%r10,%rcx,4), %xmm5, %xmm5 ## xmm5 = xmm5[0,1],mem[0],xmm5[3]
	vextracti128	$1, %ymm6, %xmm7
	vpextrd	$3, %xmm3, %ecx
	movslq	%ecx, %rcx
	vinsertps	$48, (%r10,%rcx,4), %xmm5, %xmm3 ## xmm3 = xmm5[0,1,2],mem[0]
	vmovd	%xmm7, %ecx
	movslq	%ecx, %rcx
	vmovss	(%r10,%rcx,4), %xmm5            ## xmm5 = mem[0],zero,zero,zero
	vpextrd	$1, %xmm7, %ecx
	movslq	%ecx, %rcx
	vinsertps	$16, (%r10,%rcx,4), %xmm5, %xmm5 ## xmm5 = xmm5[0],mem[0],xmm5[2,3]
	vpextrd	$2, %xmm7, %ecx
	vmovd	%r11d, %xmm4
	movslq	%ecx, %rcx
	vinsertps	$32, (%r10,%rcx,4), %xmm5, %xmm5 ## xmm5 = xmm5[0,1],mem[0],xmm5[3]
	vpbroadcastd	%xmm4, %ymm4
	vpextrd	$3, %xmm7, %ecx
	movslq	%ecx, %rcx
	vinsertps	$48, (%r10,%rcx,4), %xmm5, %xmm5 ## xmm5 = xmm5[0,1,2],mem[0]
	vmovd	%xmm6, %ecx
	movslq	%ecx, %rcx
	vmovss	(%r10,%rcx,4), %xmm7            ## xmm7 = mem[0],zero,zero,zero
	vpextrd	$1, %xmm6, %ecx
	movslq	%ecx, %rcx
	vinsertps	$16, (%r10,%rcx,4), %xmm7, %xmm7 ## xmm7 = xmm7[0],mem[0],xmm7[2,3]
	vpextrd	$2, %xmm6, %ecx
	vpaddd	%ymm4, %ymm2, %ymm2
	movslq	%ecx, %rcx
	vinsertps	$32, (%r10,%rcx,4), %xmm7, %xmm4 ## xmm4 = xmm7[0,1],mem[0],xmm7[3]
	vextracti128	$1, %ymm2, %xmm7
	vpextrd	$3, %xmm6, %ecx
	movslq	%ecx, %rcx
	vinsertps	$48, (%r10,%rcx,4), %xmm4, %xmm4 ## xmm4 = xmm4[0,1,2],mem[0]
	vmovd	%xmm7, %ecx
	movslq	%ecx, %rcx
	vmovss	(%r10,%rcx,4), %xmm6            ## xmm6 = mem[0],zero,zero,zero
	vpextrd	$1, %xmm7, %ecx
	movslq	%ecx, %rcx
	vinsertps	$16, (%r10,%rcx,4), %xmm6, %xmm6 ## xmm6 = xmm6[0],mem[0],xmm6[2,3]
	vpextrd	$2, %xmm7, %ecx
	movslq	%ecx, %rcx
	vinsertps	$32, (%r10,%rcx,4), %xmm6, %xmm6 ## xmm6 = xmm6[0,1],mem[0],xmm6[3]
	vpextrd	$3, %xmm7, %ecx
	movslq	%ecx, %rcx
	vinsertps	$48, (%r10,%rcx,4), %xmm6, %xmm6 ## xmm6 = xmm6[0,1,2],mem[0]
	vmovd	%xmm2, %ecx
	movslq	%ecx, %rcx
	vmovss	(%r10,%rcx,4), %xmm7            ## xmm7 = mem[0],zero,zero,zero
	vpextrd	$1, %xmm2, %ecx
	movslq	%ecx, %rcx
	vinsertps	$16, (%r10,%rcx,4), %xmm7, %xmm7 ## xmm7 = xmm7[0],mem[0],xmm7[2,3]
	vpextrd	$2, %xmm2, %ecx
	movslq	%ecx, %rcx
	vinsertps	$32, (%r10,%rcx,4), %xmm7, %xmm7 ## xmm7 = xmm7[0,1],mem[0],xmm7[3]
	vpextrd	$3, %xmm2, %ecx
	movslq	%ecx, %rcx
	vinsertps	$48, (%r10,%rcx,4), %xmm7, %xmm2 ## xmm2 = xmm7[0,1,2],mem[0]
	vmovd	%xmm1, %ecx
	movslq	%ecx, %rcx
	vmovss	(%r10,%rcx,4), %xmm7            ## xmm7 = mem[0],zero,zero,zero
	cltq
	vinsertps	$16, (%r10,%rax,4), %xmm7, %xmm7 ## xmm7 = xmm7[0],mem[0],xmm7[2,3]
	movslq	%ebp, %rax
	vpextrd	$2, %xmm1, %ecx
	movslq	%ecx, %rcx
	vinsertps	$32, (%r10,%rcx,4), %xmm7, %xmm1 ## xmm1 = xmm7[0,1],mem[0],xmm7[3]
	vpextrd	$1, %xmm0, %ecx
	vinsertps	$48, (%r10,%rax,4), %xmm1, %xmm1 ## xmm1 = xmm1[0,1,2],mem[0]
	vmovd	%xmm0, %eax
	cltq
	vmovss	(%r10,%rax,4), %xmm7            ## xmm7 = mem[0],zero,zero,zero
	movslq	%ecx, %rax
	vinsertps	$16, (%r10,%rax,4), %xmm7, %xmm7 ## xmm7 = xmm7[0],mem[0],xmm7[2,3]
	vpextrd	$2, %xmm0, %eax
	cltq
	vinsertps	$32, (%r10,%rax,4), %xmm7, %xmm7 ## xmm7 = xmm7[0,1],mem[0],xmm7[3]
	vpextrd	$3, %xmm0, %eax
	cltq
	vinsertps	$48, (%r10,%rax,4), %xmm7, %xmm7 ## xmm7 = xmm7[0,1,2],mem[0]
	vinsertf128	$1, %xmm8, %ymm3, %ymm0
	vinsertf128	$1, %xmm5, %ymm4, %ymm3
	vinsertf128	$1, %xmm6, %ymm2, %ymm4
	vinsertf128	$1, %xmm1, %ymm7, %ymm2
	vbroadcastss	LCPI35_1(%rip), %ymm1   ## ymm1 = [1.0E+0,1.0E+0,1.0E+0,1.0E+0,1.0E+0,1.0E+0,1.0E+0,1.0E+0]
	vmaxps	%ymm1, %ymm2, %ymm2
	vmulps	%ymm4, %ymm0, %ymm4
	movl	-44(%rsp), %eax                 ## 4-byte Reload
	movl	%eax, %r10d
	cmpl	$1, %eax
	jne	LBB35_15
## %bb.14:
	xorl	%esi, %esi
	jmp	LBB35_17
LBB35_15:                               ## %"for relu1_0_d_def__.s27.w.wi.preheader.split.new"
	movl	%r10d, %edi
	andl	$2147483646, %edi               ## imm = 0x7FFFFFFE
	leaq	1(%r9), %rax
	movq	-16(%rsp), %rsi                 ## 8-byte Reload
	imulq	%rsi, %rax
	movq	-8(%rsp), %rbp                  ## 8-byte Reload
	movq	%r14, %r13
	leaq	(,%rax,4), %r14
	addq	%rbp, %r14
	movq	-24(%rsp), %rax                 ## 8-byte Reload
	leaq	(,%rax,4), %rcx
	leaq	(,%rsi,8), %r12
	movl	%r15d, %eax
	shll	$6, %eax
	movl	-60(%rsp), %edx                 ## 4-byte Reload
	shll	$6, %edx
	subl	%edx, %eax
	leal	49(%rax), %edx
	movq	-32(%rsp), %r11                 ## 8-byte Reload
	imull	%r11d, %edx
	leal	(%rdx,%r13,8), %edx
	movl	%r11d, %ebx
	shll	$6, %ebx
	imulq	%r9, %rsi
	leaq	(%rbp,%rsi,4), %rbp
	orl	$17, %eax
	imull	%r11d, %eax
	leal	(%rax,%r13,8), %eax
	xorl	%esi, %esi
	vcmpltps	%ymm0, %ymm1, %ymm5
	vbroadcastss	LCPI35_2(%rip), %ymm6   ## ymm6 = [1.99999994E-9,1.99999994E-9,1.99999994E-9,1.99999994E-9,1.99999994E-9,1.99999994E-9,1.99999994E-9,1.99999994E-9]
	.p2align	4, 0x90
LBB35_16:                               ## %"for relu1_0_d_def__.s27.w.wi"
                                        ## =>This Inner Loop Header: Depth=1
	cltq
	vmulps	(%rbp,%rcx), %ymm3, %ymm7
	vmulps	%ymm4, %ymm7, %ymm7
	vmulps	%ymm6, %ymm7, %ymm7
	vdivps	%ymm2, %ymm7, %ymm7
	vandnps	%ymm7, %ymm5, %ymm7
	vaddps	(%r8,%rax,4), %ymm7, %ymm7
	vmovups	%ymm7, (%r8,%rax,4)
	vmulps	(%r14,%rcx), %ymm3, %ymm7
	vmulps	%ymm4, %ymm7, %ymm7
	vmulps	%ymm6, %ymm7, %ymm7
	vdivps	%ymm2, %ymm7, %ymm7
	movslq	%edx, %rdx
	vandnps	%ymm7, %ymm5, %ymm7
	vaddps	(%r8,%rdx,4), %ymm7, %ymm7
	vmovups	%ymm7, (%r8,%rdx,4)
	addq	$2, %rsi
	addq	%r12, %rcx
	addl	%ebx, %edx
	addl	%ebx, %eax
	cmpq	%rsi, %rdi
	jne	LBB35_16
LBB35_17:                               ## %destructor_block.loopexit18.unr-lcssa
	testb	$1, %r10b
	movq	-56(%rsp), %rdx                 ## 8-byte Reload
	je	LBB35_19
## %bb.18:                              ## %"for relu1_0_d_def__.s27.w.wi.epil"
	movq	-32(%rsp), %rcx                 ## 8-byte Reload
	movl	%ecx, %eax
	shll	$4, %eax
	subl	-60(%rsp), %r15d                ## 4-byte Folded Reload
	addl	%ecx, %eax
	addl	%r15d, %r15d
	addl	%eax, %edx
	leal	(%rsi,%r15), %eax
	movl	-40(%rsp), %ecx                 ## 4-byte Reload
	imull	%eax, %ecx
	addq	%rsi, %r9
	imulq	-16(%rsp), %r9                  ## 8-byte Folded Reload
	addq	-24(%rsp), %r9                  ## 8-byte Folded Reload
	movq	-8(%rsp), %rax                  ## 8-byte Reload
	vmulps	(%rax,%r9,4), %ymm3, %ymm3
	vmulps	%ymm4, %ymm3, %ymm3
	vbroadcastss	LCPI35_2(%rip), %ymm4   ## ymm4 = [1.99999994E-9,1.99999994E-9,1.99999994E-9,1.99999994E-9,1.99999994E-9,1.99999994E-9,1.99999994E-9,1.99999994E-9]
	vmulps	%ymm4, %ymm3, %ymm3
	vdivps	%ymm2, %ymm3, %ymm2
	addl	%edx, %ecx
	movslq	%ecx, %rax
	vcmpnltps	%ymm0, %ymm1, %ymm0
	vandps	%ymm2, %ymm0, %ymm0
	vaddps	(%r8,%rax,4), %ymm0, %ymm0
	vmovups	%ymm0, (%r8,%rax,4)
LBB35_19:                               ## %destructor_block
	xorl	%eax, %eax
	popq	%rbx
	popq	%r12
	popq	%r13
	popq	%r14
	popq	%r15
	popq	%rbp
	vzeroupper
	retq
                                        ## -- End function
	.section	__TEXT,__const
	.p2align	5                               ## -- Begin function train_cost_model.par_for.relu1_0_d_def__.s28.n.n.n
LCPI36_0:
	.long	0                               ## 0x0
	.long	1                               ## 0x1
	.long	2                               ## 0x2
	.long	3                               ## 0x3
	.long	4                               ## 0x4
	.long	5                               ## 0x5
	.long	6                               ## 0x6
	.long	7                               ## 0x7
	.section	__TEXT,__literal4,4byte_literals
	.p2align	2
LCPI36_1:
	.long	0x3f800000                      ## float 1
LCPI36_2:
	.long	0x3109705f                      ## float 1.99999994E-9
	.section	__TEXT,__text,regular,pure_instructions
	.p2align	4, 0x90
_train_cost_model.par_for.relu1_0_d_def__.s28.n.n.n: ## @train_cost_model.par_for.relu1_0_d_def__.s28.n.n.n
## %bb.0:                               ## %entry
	pushq	%rbp
	pushq	%r15
	pushq	%r14
	pushq	%r13
	pushq	%r12
	pushq	%rbx
	subq	$168, %rsp
	movq	%rdx, %rdi
	movl	12(%rdx), %r9d
	movl	28(%rdx), %r11d
	movl	%esi, %r8d
	sarl	$31, %r8d
	xorl	%ecx, %ecx
	testl	%r11d, %r11d
	sete	%cl
	movl	%ecx, %ebx
	negl	%ebx
	movl	%r11d, %ebp
	sarl	$31, %ebp
	subl	%r8d, %esi
	orl	%r11d, %ebx
	movl	%esi, %eax
	cltd
	idivl	%ebx
	movl	%edx, %ebx
	movl	%ebp, %r14d
	leal	(%r11,%rcx), %r10d
	movl	%esi, %eax
	cltd
	idivl	%r10d
	notl	%r14d
	decl	%ecx
	movl	%r14d, %edx
	subl	%ebp, %edx
	andl	%r8d, %edx
	addl	%eax, %edx
	andl	%ecx, %edx
	movq	%rdx, %r12
	leal	(%rdx,%rdx), %eax
	movl	%eax, -64(%rsp)                 ## 4-byte Spill
	subl	%eax, %r9d
	cmpl	$2, %r9d
	movl	$2, %edx
	cmovll	%r9d, %edx
	xorl	%eax, %eax
	testl	%r9d, %r9d
	cmovlel	%eax, %edx
	movl	%edx, -96(%rsp)                 ## 4-byte Spill
	jle	LBB36_4
## %bb.1:                               ## %"for relu1_0_d_def__.s28.w.wi.preheader"
	movl	(%rdi), %edx
	movl	16(%rdi), %eax
	movq	%rax, -72(%rsp)                 ## 8-byte Spill
	movl	24(%rdi), %eax
	movl	%eax, -116(%rsp)                ## 4-byte Spill
	movslq	36(%rdi), %rsi
	movl	44(%rdi), %eax
	movq	%rax, -128(%rsp)                ## 8-byte Spill
	movl	48(%rdi), %r9d
	movl	52(%rdi), %r13d
	xorl	%r11d, %ebp
	addl	%r14d, %ebp
	andl	%r8d, %ebp
	addl	%ebp, %ebx
	andl	%ecx, %ebx
	movq	%rdx, %rcx
	movq	%rbx, -80(%rsp)                 ## 8-byte Spill
	leal	(,%rbx,8), %eax
	movl	%r9d, %r15d
	subl	%esi, %r15d
	movl	%r13d, %r14d
	movq	%rsi, -112(%rsp)                ## 8-byte Spill
	subl	%esi, %r14d
	movslq	8(%rdi), %rdx
	movslq	%r12d, %rbx
	subq	%rdx, %rbx
	addq	%rbx, %rbx
	leal	8(%rax), %edx
	movl	%ecx, %esi
	subl	%eax, %esi
	cmpl	$8, %esi
	movl	$8, %ebp
	cmovll	%esi, %ebp
	movl	%esi, -36(%rsp)                 ## 4-byte Spill
	testl	%esi, %esi
	movl	$0, %r11d
	cmovgl	%ebp, %r11d
	movslq	%eax, %rsi
	subl	%ecx, %eax
	movl	%eax, %ebp
	sarl	$31, %ebp
	andl	%eax, %ebp
	cmpl	$-8, %ebp
	movl	$-8, %eax
	cmovgl	%ebp, %eax
	movl	%eax, %ebp
	movq	56(%rdi), %rax
	movq	%rax, -88(%rsp)                 ## 8-byte Spill
	movq	72(%rdi), %rax
	movq	%rax, -104(%rsp)                ## 8-byte Spill
	movq	88(%rdi), %r8
	movl	32(%rdi), %eax
	movslq	4(%rdi), %r10
	cmpl	%ecx, %edx
	jle	LBB36_2
## %bb.5:                               ## %"for relu1_0_d_def__.s28.w.wi.us.preheader"
	movl	%r12d, %edx
	subl	%eax, %edx
	subl	-112(%rsp), %ecx                ## 4-byte Folded Reload
	addl	%ecx, %r9d
	addl	%ecx, %r13d
	addl	%edx, %edx
	movq	%rdx, 16(%rsp)                  ## 8-byte Spill
	addl	%esi, %r14d
	addl	%esi, %r15d
	addl	%r11d, %ebp
	movl	%ebp, -40(%rsp)                 ## 4-byte Spill
	movslq	%r15d, %rdx
	movq	%rdx, -8(%rsp)                  ## 8-byte Spill
	movslq	%r14d, %rdx
	movq	%rdx, -16(%rsp)                 ## 8-byte Spill
	movslq	%ecx, %rdx
	movq	%rdx, 136(%rsp)                 ## 8-byte Spill
	movq	-128(%rsp), %rdi                ## 8-byte Reload
	addl	%edi, %ecx
	movq	%rcx, 152(%rsp)                 ## 8-byte Spill
	movslq	%r9d, %rcx
	movq	%rcx, 128(%rsp)                 ## 8-byte Spill
	movslq	%r13d, %rcx
	movq	%rcx, 120(%rsp)                 ## 8-byte Spill
	leal	(%r11,%rsi), %ecx
	movslq	%ecx, %rdx
	movq	%rsi, %r14
	movq	%r10, %rsi
	movq	%rbx, %rcx
	movq	%rbx, 8(%rsp)                   ## 8-byte Spill
	imulq	%rbx, %rsi
	leaq	(%rsi,%r14), %rcx
	movq	%rcx, -56(%rsp)                 ## 8-byte Spill
	movq	-112(%rsp), %rbx                ## 8-byte Reload
	subq	%rbx, %r14
	movq	-80(%rsp), %rcx                 ## 8-byte Reload
	leal	(%rdi,%rcx,8), %edi
	shll	$2, %r12d
	shll	$2, %eax
	subl	%eax, %r12d
	movl	%edi, %eax
	subl	%ebx, %eax
	movq	%rax, 72(%rsp)                  ## 8-byte Spill
	subl	%ebx, %edi
	movq	%rdi, 88(%rsp)                  ## 8-byte Spill
	movl	%ebp, %r9d
	movslq	-64(%rsp), %rdi                 ## 4-byte Folded Reload
	movl	-96(%rsp), %eax                 ## 4-byte Reload
	movq	%rax, 104(%rsp)                 ## 8-byte Spill
	movl	%r9d, %eax
	andl	$-8, %eax
	movq	%rax, 64(%rsp)                  ## 8-byte Spill
	addq	$-8, %rax
	movq	%rax, 56(%rsp)                  ## 8-byte Spill
	movq	%rax, %rbx
	shrq	$3, %rbx
	incq	%rbx
	movq	%r11, 144(%rsp)                 ## 8-byte Spill
	movl	%r11d, %r13d
	movq	%r13, -24(%rsp)                 ## 8-byte Spill
                                        ## kill: def $r13d killed $r13d killed $r13 def $r13
	andl	$2147483640, %r13d              ## imm = 0x7FFFFFF8
	movq	%r13, 40(%rsp)                  ## 8-byte Spill
	orl	$1, %r12d
	movq	-72(%rsp), %rbp                 ## 8-byte Reload
	imull	%ebp, %r12d
	shll	$4, %r12d
	movq	%r12, -112(%rsp)                ## 8-byte Spill
	leal	(%r12,%rcx,8), %eax
	movl	%eax, -80(%rsp)                 ## 4-byte Spill
	movl	%ebp, %eax
	shll	$5, %eax
	movl	%eax, -44(%rsp)                 ## 4-byte Spill
	addq	%rdx, %rsi
	movq	%rbx, %rax
	movq	%rbx, 48(%rsp)                  ## 8-byte Spill
	andq	$-2, %rbx
	negq	%rbx
	movq	%rbx, 32(%rsp)                  ## 8-byte Spill
	movq	-88(%rsp), %rcx                 ## 8-byte Reload
	leaq	(%rcx,%rsi,4), %rax
	addq	$32, %rax
	movq	%rax, -96(%rsp)                 ## 8-byte Spill
	leaq	(%rcx,%rsi,4), %r15
	vmovss	LCPI36_1(%rip), %xmm0           ## xmm0 = mem[0],zero,zero,zero
	vmovss	LCPI36_2(%rip), %xmm11          ## xmm11 = mem[0],zero,zero,zero
	vbroadcastss	LCPI36_1(%rip), %ymm2   ## ymm2 = [1.0E+0,1.0E+0,1.0E+0,1.0E+0,1.0E+0,1.0E+0,1.0E+0,1.0E+0]
	vbroadcastss	LCPI36_2(%rip), %ymm12  ## ymm12 = [1.99999994E-9,1.99999994E-9,1.99999994E-9,1.99999994E-9,1.99999994E-9,1.99999994E-9,1.99999994E-9,1.99999994E-9]
	leal	(%rbp,%rbp), %eax
	movl	%eax, -48(%rsp)                 ## 4-byte Spill
	movq	-56(%rsp), %rax                 ## 8-byte Reload
	leaq	(%rcx,%rax,4), %r11
	leaq	(,%r10,4), %rax
	movq	%rax, 96(%rsp)                  ## 8-byte Spill
	movq	-104(%rsp), %rcx                ## 8-byte Reload
	leaq	32(%rcx), %rax
	movq	%rax, 24(%rsp)                  ## 8-byte Spill
	movq	%rdx, -32(%rsp)                 ## 8-byte Spill
	leaq	(%rcx,%rdx,4), %rax
	movq	%rax, 80(%rsp)                  ## 8-byte Spill
	movq	%rdi, %rax
	movq	%rdi, 112(%rsp)                 ## 8-byte Spill
	movq	%rdi, %rdx
	xorl	%esi, %esi
	movq	%r14, %rdi
	movq	%r10, 160(%rsp)                 ## 8-byte Spill
	movq	%r14, (%rsp)                    ## 8-byte Spill
	jmp	LBB36_6
	.p2align	4, 0x90
LBB36_18:                               ## %after_bb.us
                                        ##   in Loop: Header=BB36_6 Depth=1
	movq	-128(%rsp), %rsi                ## 8-byte Reload
	incq	%rsi
	movq	96(%rsp), %rcx                  ## 8-byte Reload
	addq	%rcx, %r11
	movq	-64(%rsp), %rdx                 ## 8-byte Reload
	incq	%rdx
	movl	-44(%rsp), %eax                 ## 4-byte Reload
	addl	%eax, -80(%rsp)                 ## 4-byte Folded Spill
	addq	%r10, -56(%rsp)                 ## 8-byte Folded Spill
	addq	%rcx, -96(%rsp)                 ## 8-byte Folded Spill
	addl	%eax, %ebx
	movq	%rbx, -112(%rsp)                ## 8-byte Spill
	addq	%rcx, %r15
	cmpq	104(%rsp), %rsi                 ## 8-byte Folded Reload
	je	LBB36_4
LBB36_6:                                ## %"for relu1_0_d_def__.s28.w.wi.us"
                                        ## =>This Loop Header: Depth=1
                                        ##     Child Loop BB36_35 Depth 2
                                        ##     Child Loop BB36_9 Depth 2
                                        ##     Child Loop BB36_22 Depth 2
                                        ##     Child Loop BB36_15 Depth 2
	testq	%rdx, %rdx
	movl	$0, %eax
	movq	%rdx, -64(%rsp)                 ## 8-byte Spill
	cmovgl	%edx, %eax
	movq	112(%rsp), %rcx                 ## 8-byte Reload
	movq	%rsi, %rdx
	movq	%rsi, -128(%rsp)                ## 8-byte Spill
	leaq	(%rsi,%rcx), %r12
	testq	%r12, %r12
	movl	$0, %ecx
	cmovlel	%ecx, %r12d
	cmpl	$0, -36(%rsp)                   ## 4-byte Folded Reload
	jle	LBB36_12
## %bb.7:                               ## %"for relu1_0_d_def__.s28.n.ni.preheader.us"
                                        ##   in Loop: Header=BB36_6 Depth=1
	imull	-116(%rsp), %eax                ## 4-byte Folded Reload
	movslq	%eax, %r14
	movslq	-80(%rsp), %r13                 ## 4-byte Folded Reload
	cmpl	$8, 144(%rsp)                   ## 4-byte Folded Reload
	jae	LBB36_34
## %bb.8:                               ##   in Loop: Header=BB36_6 Depth=1
	xorl	%ecx, %ecx
	jmp	LBB36_37
	.p2align	4, 0x90
LBB36_34:                               ## %vector.body27.preheader
                                        ##   in Loop: Header=BB36_6 Depth=1
	movq	-16(%rsp), %rax                 ## 8-byte Reload
	leaq	(%rax,%r14), %rcx
	movq	-8(%rsp), %rax                  ## 8-byte Reload
	leaq	(%rax,%r14), %rdx
	movq	(%rsp), %rax                    ## 8-byte Reload
	leaq	(%rax,%r14), %rsi
	movq	72(%rsp), %rax                  ## 8-byte Reload
	leal	(%rax,%r14), %edi
	leaq	(%r8,%rcx,4), %rcx
	leaq	(%r8,%rdx,4), %rdx
	leaq	(%r8,%rsi,4), %rsi
	movslq	%edi, %rdi
	leaq	(%r8,%rdi,4), %rdi
	movq	-104(%rsp), %rax                ## 8-byte Reload
	leaq	(%rax,%r13,4), %rbp
	xorl	%ebx, %ebx
	movq	40(%rsp), %rax                  ## 8-byte Reload
	.p2align	4, 0x90
LBB36_35:                               ## %vector.body27
                                        ##   Parent Loop BB36_6 Depth=1
                                        ## =>  This Inner Loop Header: Depth=2
	vmovups	(%rdi,%rbx,4), %ymm1
	vmovups	(%rsi,%rbx,4), %ymm3
	vmulps	(%r11,%rbx,4), %ymm3, %ymm3
	vmovups	(%rcx,%rbx,4), %ymm4
	vmaxps	%ymm2, %ymm4, %ymm4
	vmulps	(%rdx,%rbx,4), %ymm1, %ymm5
	vmulps	%ymm5, %ymm3, %ymm3
	vmulps	%ymm3, %ymm12, %ymm3
	vdivps	%ymm4, %ymm3, %ymm3
	vcmpltps	%ymm1, %ymm2, %ymm1
	vandps	%ymm3, %ymm1, %ymm1
	vaddps	(%rbp,%rbx,4), %ymm1, %ymm1
	vmovups	%ymm1, (%rbp,%rbx,4)
	addq	$8, %rbx
	cmpq	%rbx, %rax
	jne	LBB36_35
## %bb.36:                              ## %middle.block25
                                        ##   in Loop: Header=BB36_6 Depth=1
	movq	%rax, %rcx
	cmpq	-24(%rsp), %rax                 ## 8-byte Folded Reload
	jne	LBB36_37
LBB36_12:                               ## %"end for relu1_0_d_def__.s28.n.ni.us"
                                        ##   in Loop: Header=BB36_6 Depth=1
	movl	-40(%rsp), %ebp                 ## 4-byte Reload
	testl	%ebp, %ebp
	movq	160(%rsp), %r10                 ## 8-byte Reload
	movq	-112(%rsp), %rbx                ## 8-byte Reload
	jle	LBB36_18
## %bb.13:                              ## %"for relu1_0_d_def__.s28.n.ni.rebased.preheader.us"
                                        ##   in Loop: Header=BB36_6 Depth=1
	movslq	%ebx, %r14
	imull	-116(%rsp), %r12d               ## 4-byte Folded Reload
	movq	152(%rsp), %rax                 ## 8-byte Reload
	addl	%r12d, %eax
	movslq	%r12d, %rcx
	movq	136(%rsp), %rdx                 ## 8-byte Reload
	addq	%rcx, %rdx
	movq	128(%rsp), %rsi                 ## 8-byte Reload
	addq	%rcx, %rsi
	addq	120(%rsp), %rcx                 ## 8-byte Folded Reload
	cltq
	vmovss	-4(%r8,%rax,4), %xmm4           ## xmm4 = mem[0],zero,zero,zero
	vmovss	-4(%r8,%rdx,4), %xmm5           ## xmm5 = mem[0],zero,zero,zero
	vmovss	-4(%r8,%rcx,4), %xmm1           ## xmm1 = mem[0],zero,zero,zero
	vmaxss	%xmm0, %xmm1, %xmm6
	vmulss	-4(%r8,%rsi,4), %xmm4, %xmm7
	cmpl	$8, %ebp
	jae	LBB36_19
## %bb.14:                              ##   in Loop: Header=BB36_6 Depth=1
	xorl	%eax, %eax
	jmp	LBB36_33
	.p2align	4, 0x90
LBB36_37:                               ## %"for relu1_0_d_def__.s28.n.ni.us.preheader"
                                        ##   in Loop: Header=BB36_6 Depth=1
	movq	88(%rsp), %rax                  ## 8-byte Reload
	leal	(%rax,%r14), %edx
	movslq	%edx, %rdx
	movq	-24(%rsp), %r10                 ## 8-byte Reload
	subq	%rcx, %r10
	movq	-16(%rsp), %rax                 ## 8-byte Reload
	leaq	(%rax,%rcx), %rsi
	addq	%r14, %rsi
	movq	-8(%rsp), %rax                  ## 8-byte Reload
	leaq	(%rax,%rcx), %rbx
	addq	%r14, %rbx
	movq	(%rsp), %rax                    ## 8-byte Reload
	addq	%rcx, %rax
	addq	%r14, %rax
	movq	-56(%rsp), %rdi                 ## 8-byte Reload
	addq	%rcx, %rdi
	movq	-88(%rsp), %rbp                 ## 8-byte Reload
	leaq	(%rbp,%rdi,4), %rdi
	leaq	(%r8,%rsi,4), %r14
	leaq	(%r8,%rbx,4), %rbp
	leaq	(%r8,%rax,4), %rsi
	addq	%rcx, %rdx
	leaq	(%r8,%rdx,4), %rbx
	addq	%rcx, %r13
	movq	-104(%rsp), %rax                ## 8-byte Reload
	leaq	(%rax,%r13,4), %rax
	xorl	%ecx, %ecx
	jmp	LBB36_9
	.p2align	4, 0x90
LBB36_11:                               ## %select.end
                                        ##   in Loop: Header=BB36_9 Depth=2
	vaddss	(%rax,%rcx,4), %xmm5, %xmm1
	vmovss	%xmm1, (%rax,%rcx,4)
	incq	%rcx
	cmpq	%rcx, %r10
	je	LBB36_12
LBB36_9:                                ## %"for relu1_0_d_def__.s28.n.ni.us"
                                        ##   Parent Loop BB36_6 Depth=1
                                        ## =>  This Inner Loop Header: Depth=2
	vmovss	(%rbx,%rcx,4), %xmm1            ## xmm1 = mem[0],zero,zero,zero
	vucomiss	%xmm1, %xmm0
	setb	%dl
	vmovss	(%rsi,%rcx,4), %xmm3            ## xmm3 = mem[0],zero,zero,zero
	vmulss	(%rdi,%rcx,4), %xmm3, %xmm3
	vmulss	(%rbp,%rcx,4), %xmm1, %xmm1
	vmulss	%xmm1, %xmm3, %xmm1
	vmulss	%xmm1, %xmm11, %xmm4
	vxorps	%xmm5, %xmm5, %xmm5
	testb	$1, %dl
	je	LBB36_11
## %bb.10:                              ## %select.true.sink
                                        ##   in Loop: Header=BB36_9 Depth=2
	vmovss	(%r14,%rcx,4), %xmm1            ## xmm1 = mem[0],zero,zero,zero
	vmaxss	%xmm0, %xmm1, %xmm1
	vdivss	%xmm1, %xmm4, %xmm5
	jmp	LBB36_11
	.p2align	4, 0x90
LBB36_19:                               ## %vector.ph
                                        ##   in Loop: Header=BB36_6 Depth=1
	vbroadcastss	%xmm6, %ymm8
	cmpq	$0, 56(%rsp)                    ## 8-byte Folded Reload
	je	LBB36_20
## %bb.21:                              ## %vector.ph.new
                                        ##   in Loop: Header=BB36_6 Depth=1
	movq	-32(%rsp), %rax                 ## 8-byte Reload
	addq	%r14, %rax
	movq	24(%rsp), %rcx                  ## 8-byte Reload
	leaq	(%rcx,%rax,4), %rsi
	vmulss	%xmm5, %xmm7, %xmm1
	vbroadcastss	%xmm1, %ymm9
	movq	32(%rsp), %rdx                  ## 8-byte Reload
	xorl	%eax, %eax
	movq	-72(%rsp), %rdi                 ## 8-byte Reload
	movq	-104(%rsp), %rbp                ## 8-byte Reload
	movq	-88(%rsp), %r12                 ## 8-byte Reload
	movq	-96(%rsp), %rcx                 ## 8-byte Reload
	jmp	LBB36_22
	.p2align	4, 0x90
LBB36_27:                               ## %vector.body
                                        ##   in Loop: Header=BB36_22 Depth=2
	vaddps	(%rsi,%rax,4), %ymm10, %ymm1
	vmovups	%ymm1, (%rsi,%rax,4)
	addq	$16, %rax
	addq	$2, %rdx
	je	LBB36_28
LBB36_22:                               ## %vector.body
                                        ##   Parent Loop BB36_6 Depth=1
                                        ## =>  This Inner Loop Header: Depth=2
	vucomiss	%xmm0, %xmm4
	vxorps	%xmm10, %xmm10, %xmm10
	ja	LBB36_23
## %bb.24:                              ## %vector.body
                                        ##   in Loop: Header=BB36_22 Depth=2
	vxorps	%xmm1, %xmm1, %xmm1
	vaddps	-32(%rsi,%rax,4), %ymm1, %ymm1
	vmovups	%ymm1, -32(%rsi,%rax,4)
	jbe	LBB36_27
	jmp	LBB36_26
	.p2align	4, 0x90
LBB36_23:                               ##   in Loop: Header=BB36_22 Depth=2
	vmulps	-32(%rcx,%rax,4), %ymm9, %ymm1
	vmulps	%ymm1, %ymm12, %ymm1
	vdivps	%ymm8, %ymm1, %ymm1
	vaddps	-32(%rsi,%rax,4), %ymm1, %ymm1
	vmovups	%ymm1, -32(%rsi,%rax,4)
	jbe	LBB36_27
LBB36_26:                               ##   in Loop: Header=BB36_22 Depth=2
	vmulps	(%rcx,%rax,4), %ymm9, %ymm1
	vmulps	%ymm1, %ymm12, %ymm1
	vdivps	%ymm8, %ymm1, %ymm10
	jmp	LBB36_27
LBB36_20:                               ##   in Loop: Header=BB36_6 Depth=1
	xorl	%eax, %eax
	movq	-72(%rsp), %rdi                 ## 8-byte Reload
	movq	-104(%rsp), %rbp                ## 8-byte Reload
	movq	-88(%rsp), %r12                 ## 8-byte Reload
LBB36_28:                               ## %middle.block.unr-lcssa
                                        ##   in Loop: Header=BB36_6 Depth=1
	testb	$1, 48(%rsp)                    ## 1-byte Folded Reload
	je	LBB36_32
## %bb.29:                              ## %vector.body.epil
                                        ##   in Loop: Header=BB36_6 Depth=1
	movq	16(%rsp), %rcx                  ## 8-byte Reload
	movq	-128(%rsp), %rdx                ## 8-byte Reload
	addl	%edx, %ecx
	imull	-48(%rsp), %ecx                 ## 4-byte Folded Reload
	addl	%edi, %ecx
	shll	$4, %ecx
	movslq	%ecx, %rcx
	addq	-32(%rsp), %rax                 ## 8-byte Folded Reload
	addq	%rax, %rcx
	vucomiss	%xmm0, %xmm4
	vxorps	%xmm1, %xmm1, %xmm1
	jbe	LBB36_31
## %bb.30:                              ## %select.true.sink132
                                        ##   in Loop: Header=BB36_6 Depth=1
	movq	8(%rsp), %rdx                   ## 8-byte Reload
	movq	-128(%rsp), %rsi                ## 8-byte Reload
	addq	%rsi, %rdx
	imulq	%r10, %rdx
	addq	%rdx, %rax
	vmulss	%xmm5, %xmm7, %xmm1
	vbroadcastss	%xmm1, %ymm1
	vmulps	(%r12,%rax,4), %ymm1, %ymm1
	vmulps	%ymm1, %ymm12, %ymm1
	vdivps	%ymm8, %ymm1, %ymm1
LBB36_31:                               ## %select.end131
                                        ##   in Loop: Header=BB36_6 Depth=1
	vaddps	(%rbp,%rcx,4), %ymm1, %ymm1
	vmovups	%ymm1, (%rbp,%rcx,4)
LBB36_32:                               ## %middle.block
                                        ##   in Loop: Header=BB36_6 Depth=1
	movq	64(%rsp), %rcx                  ## 8-byte Reload
	movq	%rcx, %rax
	cmpq	%r9, %rcx
	je	LBB36_18
LBB36_33:                               ## %"for relu1_0_d_def__.s28.n.ni.rebased.us.preheader"
                                        ##   in Loop: Header=BB36_6 Depth=1
	movq	80(%rsp), %rcx                  ## 8-byte Reload
	leaq	(%rcx,%r14,4), %rcx
	jmp	LBB36_15
	.p2align	4, 0x90
LBB36_17:                               ## %select.end167
                                        ##   in Loop: Header=BB36_15 Depth=2
	vaddss	(%rcx,%rax,4), %xmm1, %xmm1
	vmovss	%xmm1, (%rcx,%rax,4)
	incq	%rax
	cmpq	%rax, %r9
	je	LBB36_18
LBB36_15:                               ## %"for relu1_0_d_def__.s28.n.ni.rebased.us"
                                        ##   Parent Loop BB36_6 Depth=1
                                        ## =>  This Inner Loop Header: Depth=2
	vmulss	(%r15,%rax,4), %xmm5, %xmm1
	vucomiss	%xmm0, %xmm4
	vmulss	%xmm7, %xmm1, %xmm1
	vmulss	%xmm1, %xmm11, %xmm3
	vxorps	%xmm1, %xmm1, %xmm1
	jbe	LBB36_17
## %bb.16:                              ## %select.true.sink168
                                        ##   in Loop: Header=BB36_15 Depth=2
	vdivss	%xmm6, %xmm3, %xmm1
	jmp	LBB36_17
LBB36_2:                                ## %"for relu1_0_d_def__.s28.w.wi.preheader4"
	movl	20(%rdi), %r9d
	movl	40(%rdi), %r11d
	movq	-128(%rsp), %rdx                ## 8-byte Reload
	subl	-112(%rsp), %edx                ## 4-byte Folded Reload
	movq	%rdx, -128(%rsp)                ## 8-byte Spill
	decl	%ecx
	vmovd	%esi, %xmm0
	vpbroadcastd	%xmm0, %ymm0
	vpor	LCPI36_0(%rip), %ymm0, %ymm0
	vmovd	%ecx, %xmm1
	vpbroadcastd	%xmm1, %ymm1
	vpminsd	%ymm0, %ymm1, %ymm11
	movslq	-64(%rsp), %rbp                 ## 4-byte Folded Reload
	movl	-96(%rsp), %r13d                ## 4-byte Reload
	imulq	%r10, %rbx
	addq	%rsi, %rbx
	movq	-88(%rsp), %rcx                 ## 8-byte Reload
	leaq	(%rcx,%rbx,4), %rbx
	shlq	$2, %r10
	shll	$2, %r12d
	shll	$2, %eax
	subl	%eax, %r12d
	orl	$1, %r12d
	movq	-72(%rsp), %rcx                 ## 8-byte Reload
	imull	%ecx, %r12d
	movq	-80(%rsp), %rax                 ## 8-byte Reload
	leal	(%rax,%r12,2), %edx
	shll	$2, %ecx
	vbroadcastss	LCPI36_1(%rip), %ymm9   ## ymm9 = [1.0E+0,1.0E+0,1.0E+0,1.0E+0,1.0E+0,1.0E+0,1.0E+0,1.0E+0]
	vbroadcastss	LCPI36_2(%rip), %ymm8   ## ymm8 = [1.99999994E-9,1.99999994E-9,1.99999994E-9,1.99999994E-9,1.99999994E-9,1.99999994E-9,1.99999994E-9,1.99999994E-9]
	movq	%rcx, %r12
	movl	-116(%rsp), %edi                ## 4-byte Reload
	.p2align	4, 0x90
LBB36_3:                                ## %"for relu1_0_d_def__.s28.w.wi"
                                        ## =>This Inner Loop Header: Depth=1
	testq	%rbp, %rbp
	movl	$0, %esi
	cmovgl	%ebp, %esi
	movl	%edi, %eax
	imull	%esi, %eax
	movq	-128(%rsp), %rcx                ## 8-byte Reload
	addl	%eax, %ecx
	vmovd	%ecx, %xmm3
	vpbroadcastd	%xmm3, %ymm3
	vpaddd	%ymm3, %ymm11, %ymm3
	vmovd	%xmm3, %ecx
	movslq	%ecx, %rcx
	vmovss	(%r8,%rcx,4), %xmm4             ## xmm4 = mem[0],zero,zero,zero
	vpextrd	$1, %xmm3, %ecx
	movslq	%ecx, %rcx
	vinsertps	$16, (%r8,%rcx,4), %xmm4, %xmm4 ## xmm4 = xmm4[0],mem[0],xmm4[2,3]
	vpextrd	$2, %xmm3, %ecx
	movslq	%ecx, %rcx
	vinsertps	$32, (%r8,%rcx,4), %xmm4, %xmm5 ## xmm5 = xmm4[0,1],mem[0],xmm4[3]
	vpextrd	$3, %xmm3, %ecx
	vextracti128	$1, %ymm3, %xmm4
	subl	%r9d, %esi
	imull	%edi, %esi
	subl	%r11d, %esi
	movslq	%ecx, %rcx
	vmovd	%esi, %xmm3
	vmovd	%xmm4, %esi
	movslq	%esi, %rsi
	vmovss	(%r8,%rsi,4), %xmm6             ## xmm6 = mem[0],zero,zero,zero
	vpbroadcastd	%xmm3, %ymm3
	vpaddd	%ymm3, %ymm11, %ymm7
	vinsertps	$48, (%r8,%rcx,4), %xmm5, %xmm10 ## xmm10 = xmm5[0,1,2],mem[0]
	vmovd	%xmm7, %ecx
	vpextrd	$1, %xmm7, %esi
	movslq	%ecx, %rcx
	vmovss	(%r8,%rcx,4), %xmm5             ## xmm5 = mem[0],zero,zero,zero
	vpextrd	$2, %xmm7, %ecx
	movslq	%esi, %rsi
	vinsertps	$16, (%r8,%rsi,4), %xmm5, %xmm5 ## xmm5 = xmm5[0],mem[0],xmm5[2,3]
	vpextrd	$1, %xmm4, %esi
	movslq	%esi, %rsi
	movslq	%ecx, %rcx
	vinsertps	$32, (%r8,%rcx,4), %xmm5, %xmm5 ## xmm5 = xmm5[0,1],mem[0],xmm5[3]
	vpextrd	$2, %xmm4, %ecx
	vinsertps	$16, (%r8,%rsi,4), %xmm6, %xmm2 ## xmm2 = xmm6[0],mem[0],xmm6[2,3]
	vpextrd	$3, %xmm7, %esi
	vextracti128	$1, %ymm7, %xmm6
	movslq	%esi, %rsi
	vinsertps	$48, (%r8,%rsi,4), %xmm5, %xmm12 ## xmm12 = xmm5[0,1,2],mem[0]
	vmovd	%xmm6, %esi
	movslq	%esi, %rsi
	vmovss	(%r8,%rsi,4), %xmm7             ## xmm7 = mem[0],zero,zero,zero
	movslq	%ecx, %rcx
	vinsertps	$32, (%r8,%rcx,4), %xmm2, %xmm2 ## xmm2 = xmm2[0,1],mem[0],xmm2[3]
	leal	(%rax,%r15), %ecx
	vmovd	%ecx, %xmm1
	vpextrd	$1, %xmm6, %ecx
	movslq	%ecx, %rcx
	vpbroadcastd	%xmm1, %ymm1
	vinsertps	$16, (%r8,%rcx,4), %xmm7, %xmm7 ## xmm7 = xmm7[0],mem[0],xmm7[2,3]
	vpaddd	%ymm1, %ymm11, %ymm1
	vextracti128	$1, %ymm1, %xmm3
	vmovd	%xmm3, %ecx
	movslq	%ecx, %rcx
	vmovss	(%r8,%rcx,4), %xmm0             ## xmm0 = mem[0],zero,zero,zero
	vpextrd	$1, %xmm3, %ecx
	movslq	%ecx, %rcx
	vinsertps	$16, (%r8,%rcx,4), %xmm0, %xmm0 ## xmm0 = xmm0[0],mem[0],xmm0[2,3]
	vpextrd	$3, %xmm4, %ecx
	movslq	%ecx, %rcx
	vinsertps	$48, (%r8,%rcx,4), %xmm2, %xmm4 ## xmm4 = xmm2[0,1,2],mem[0]
	vpextrd	$2, %xmm3, %ecx
	movslq	%ecx, %rcx
	vinsertps	$32, (%r8,%rcx,4), %xmm0, %xmm0 ## xmm0 = xmm0[0,1],mem[0],xmm0[3]
	vmovd	%xmm1, %ecx
	movslq	%ecx, %rcx
	vmovss	(%r8,%rcx,4), %xmm2             ## xmm2 = mem[0],zero,zero,zero
	vpextrd	$1, %xmm1, %ecx
	movslq	%ecx, %rcx
	vinsertps	$16, (%r8,%rcx,4), %xmm2, %xmm2 ## xmm2 = xmm2[0],mem[0],xmm2[2,3]
	vpextrd	$2, %xmm6, %ecx
	movslq	%ecx, %rcx
	vinsertps	$32, (%r8,%rcx,4), %xmm7, %xmm5 ## xmm5 = xmm7[0,1],mem[0],xmm7[3]
	vpextrd	$2, %xmm1, %ecx
	movslq	%ecx, %rcx
	vinsertps	$32, (%r8,%rcx,4), %xmm2, %xmm2 ## xmm2 = xmm2[0,1],mem[0],xmm2[3]
	vpextrd	$3, %xmm3, %ecx
	movslq	%ecx, %rcx
	vinsertps	$48, (%r8,%rcx,4), %xmm0, %xmm7 ## xmm7 = xmm0[0,1,2],mem[0]
	vpextrd	$3, %xmm1, %ecx
	movslq	%ecx, %rcx
	vinsertps	$48, (%r8,%rcx,4), %xmm2, %xmm0 ## xmm0 = xmm2[0,1,2],mem[0]
	vpextrd	$3, %xmm6, %ecx
	movslq	%ecx, %rcx
	addl	%r14d, %eax
	vinsertps	$48, (%r8,%rcx,4), %xmm5, %xmm1 ## xmm1 = xmm5[0,1,2],mem[0]
	vmovd	%eax, %xmm2
	vpbroadcastd	%xmm2, %ymm2
	vpaddd	%ymm2, %ymm11, %ymm2
	vextracti128	$1, %ymm2, %xmm3
	vmovd	%xmm3, %eax
	vpextrd	$1, %xmm3, %ecx
	cltq
	vmovss	(%r8,%rax,4), %xmm5             ## xmm5 = mem[0],zero,zero,zero
	movslq	%ecx, %rax
	vinsertps	$16, (%r8,%rax,4), %xmm5, %xmm5 ## xmm5 = xmm5[0],mem[0],xmm5[2,3]
	vpextrd	$2, %xmm3, %eax
	cltq
	vinsertps	$32, (%r8,%rax,4), %xmm5, %xmm5 ## xmm5 = xmm5[0,1],mem[0],xmm5[3]
	vmovd	%xmm2, %eax
	cltq
	vmovss	(%r8,%rax,4), %xmm6             ## xmm6 = mem[0],zero,zero,zero
	vpextrd	$1, %xmm2, %eax
	cltq
	vinsertps	$16, (%r8,%rax,4), %xmm6, %xmm6 ## xmm6 = xmm6[0],mem[0],xmm6[2,3]
	vpextrd	$3, %xmm3, %eax
	cltq
	vinsertps	$48, (%r8,%rax,4), %xmm5, %xmm3 ## xmm3 = xmm5[0,1,2],mem[0]
	vpextrd	$2, %xmm2, %eax
	cltq
	vinsertps	$32, (%r8,%rax,4), %xmm6, %xmm5 ## xmm5 = xmm6[0,1],mem[0],xmm6[3]
	vpextrd	$3, %xmm2, %eax
	vinsertf128	$1, %xmm4, %ymm10, %ymm2
	movslq	%edx, %rdx
	cltq
	vinsertf128	$1, %xmm1, %ymm12, %ymm1
	vinsertps	$48, (%r8,%rax,4), %xmm5, %xmm4 ## xmm4 = xmm5[0,1,2],mem[0]
	movq	%rdx, %rax
	vmulps	(%rbx), %ymm1, %ymm1
	shlq	$5, %rax
	vinsertf128	$1, %xmm7, %ymm0, %ymm0
	vinsertf128	$1, %xmm3, %ymm4, %ymm3
	vmaxps	%ymm9, %ymm3, %ymm3
	vmulps	%ymm0, %ymm2, %ymm0
	vmulps	%ymm0, %ymm8, %ymm0
	vmulps	%ymm0, %ymm1, %ymm0
	vcmpltps	%ymm2, %ymm9, %ymm1
	vdivps	%ymm3, %ymm0, %ymm0
	vandps	%ymm0, %ymm1, %ymm0
	movq	-104(%rsp), %rcx                ## 8-byte Reload
	vaddps	(%rcx,%rax), %ymm0, %ymm0
	vmovaps	%ymm0, (%rcx,%rax)
	addq	%r10, %rbx
	addl	%r12d, %edx
	incq	%rbp
	decq	%r13
	jne	LBB36_3
LBB36_4:                                ## %destructor_block
	xorl	%eax, %eax
	addq	$168, %rsp
	popq	%rbx
	popq	%r12
	popq	%r13
	popq	%r14
	popq	%r15
	popq	%rbp
	vzeroupper
	retq
                                        ## -- End function
	.section	__TEXT,__const
	.p2align	5                               ## -- Begin function train_cost_model.par_for.relu1_0_d_def__.s29.n.n.n
LCPI37_0:
	.long	0                               ## 0x0
	.long	1                               ## 0x1
	.long	2                               ## 0x2
	.long	3                               ## 0x3
	.long	4                               ## 0x4
	.long	5                               ## 0x5
	.long	6                               ## 0x6
	.long	7                               ## 0x7
	.section	__TEXT,__literal4,4byte_literals
	.p2align	2
LCPI37_1:
	.long	0x3f800000                      ## float 1
LCPI37_2:
	.long	0x3089705f                      ## float 9.99999971E-10
	.section	__TEXT,__text,regular,pure_instructions
	.p2align	4, 0x90
_train_cost_model.par_for.relu1_0_d_def__.s29.n.n.n: ## @train_cost_model.par_for.relu1_0_d_def__.s29.n.n.n
## %bb.0:                               ## %entry
	pushq	%rbp
	pushq	%r15
	pushq	%r14
	pushq	%r13
	pushq	%r12
	pushq	%rbx
	subq	$168, %rsp
	movq	%rdx, %rdi
	movl	12(%rdx), %r9d
	movl	24(%rdx), %r10d
	movl	%esi, %r12d
	sarl	$31, %r12d
	xorl	%ecx, %ecx
	testl	%r10d, %r10d
	sete	%cl
	movl	%ecx, %ebx
	negl	%ebx
	movl	%r10d, %ebp
	sarl	$31, %ebp
	subl	%r12d, %esi
	orl	%r10d, %ebx
	movl	%esi, %eax
	cltd
	idivl	%ebx
	movl	%edx, %ebx
	movl	%ebp, %r11d
	leal	(%r10,%rcx), %r14d
	movl	%esi, %eax
	cltd
	idivl	%r14d
	notl	%r11d
	decl	%ecx
	movl	%r11d, %esi
	subl	%ebp, %esi
	andl	%r12d, %esi
	addl	%eax, %esi
	andl	%ecx, %esi
	leal	(%rsi,%rsi), %eax
	movl	%eax, -124(%rsp)                ## 4-byte Spill
	subl	%eax, %r9d
	cmpl	$2, %r9d
	movl	$2, %edx
	cmovll	%r9d, %edx
	xorl	%eax, %eax
	testl	%r9d, %r9d
	cmovlel	%eax, %edx
	movl	%edx, -72(%rsp)                 ## 4-byte Spill
	jle	LBB37_4
## %bb.1:                               ## %"for relu1_0_d_def__.s29.w.wi.preheader"
	movl	(%rdi), %eax
	movl	%eax, -16(%rsp)                 ## 4-byte Spill
	movl	16(%rdi), %r9d
	movl	32(%rdi), %r13d
	movl	40(%rdi), %eax
	movl	44(%rdi), %edx
	movl	48(%rdi), %r15d
	movl	52(%rdi), %r8d
	xorl	%r10d, %ebp
	addl	%r11d, %ebp
	andl	%r12d, %ebp
	addl	%ebp, %ebx
	andl	%ecx, %ebx
	movq	%rbx, -112(%rsp)                ## 8-byte Spill
	leal	(,%rbx,8), %ecx
	movl	%eax, -80(%rsp)                 ## 4-byte Spill
	movl	%eax, %r11d
	subl	%r13d, %r11d
	movl	%edx, -120(%rsp)                ## 4-byte Spill
	movl	%edx, %r14d
	subl	%r13d, %r14d
	movq	%rsi, %r10
	movl	%r15d, -24(%rsp)                ## 4-byte Spill
                                        ## kill: def $r15d killed $r15d def $r15
	subl	%r13d, %r15d
	movl	%r8d, -32(%rsp)                 ## 4-byte Spill
	subl	%r13d, %r8d
	movslq	8(%rdi), %rax
	movslq	%r10d, %rbx
	subq	%rax, %rbx
	addq	%rbx, %rbx
	movq	%r9, -88(%rsp)                  ## 8-byte Spill
	movl	%r9d, %eax
	shll	$5, %eax
	movl	%eax, -100(%rsp)                ## 4-byte Spill
	movl	-16(%rsp), %edx                 ## 4-byte Reload
	movl	%edx, %esi
	subl	%ecx, %esi
	cmpl	$8, %esi
	movl	$8, %eax
	cmovll	%esi, %eax
	movl	%esi, -4(%rsp)                  ## 4-byte Spill
	testl	%esi, %esi
	movl	$0, %r9d
	cmovgl	%eax, %r9d
	leal	8(%rcx), %esi
	movslq	%ecx, %rax
	subl	%edx, %ecx
	movl	%ecx, %ebp
	sarl	$31, %ebp
	andl	%ecx, %ebp
	cmpl	$-8, %ebp
	movl	$-8, %r12d
	cmovgl	%ebp, %r12d
	vmovss	36(%rdi), %xmm14                ## xmm14 = mem[0],zero,zero,zero
	movl	20(%rdi), %ecx
	movl	%ecx, -116(%rsp)                ## 4-byte Spill
	movl	28(%rdi), %ecx
	movl	%ecx, -96(%rsp)                 ## 4-byte Spill
	movq	56(%rdi), %rbp
	movq	%rbp, -40(%rsp)                 ## 8-byte Spill
	movq	72(%rdi), %rcx
	movq	88(%rdi), %rbp
	movslq	4(%rdi), %rdi
	movq	%rdi, -48(%rsp)                 ## 8-byte Spill
	vbroadcastss	%xmm14, %ymm10
	cmpl	%edx, %esi
	movq	%rcx, -64(%rsp)                 ## 8-byte Spill
	movq	%rbp, -56(%rsp)                 ## 8-byte Spill
	jle	LBB37_2
## %bb.5:                               ## %"for relu1_0_d_def__.s29.w.wi.us.preheader"
	subl	%r13d, %edx
	addl	%edx, -80(%rsp)                 ## 4-byte Folded Spill
	addl	%edx, -120(%rsp)                ## 4-byte Folded Spill
	movl	-24(%rsp), %r13d                ## 4-byte Reload
	addl	%edx, %r13d
	addl	-32(%rsp), %edx                 ## 4-byte Folded Reload
	movl	%r10d, %esi
	movl	-96(%rsp), %ecx                 ## 4-byte Reload
	subl	%ecx, %esi
	shll	$6, %r10d
	shll	$6, %ecx
	subl	%ecx, %r10d
	movq	-40(%rsp), %rdi                 ## 8-byte Reload
	movq	-88(%rsp), %rbp                 ## 8-byte Reload
	leal	(%rbp,%rbp,2), %ecx
	movl	%ecx, -8(%rsp)                  ## 4-byte Spill
	addl	%esi, %esi
	movq	%rsi, 32(%rsp)                  ## 8-byte Spill
	addl	%eax, %r8d
	addl	%eax, %r15d
	addl	%eax, %r14d
	addl	%eax, %r11d
	addl	%r9d, %r12d
	movslq	%r11d, %rcx
	movq	%rcx, 16(%rsp)                  ## 8-byte Spill
	movslq	%r14d, %rcx
	movq	%rcx, 8(%rsp)                   ## 8-byte Spill
	movslq	%r15d, %r15
	movslq	%r8d, %r8
	movslq	-80(%rsp), %rcx                 ## 4-byte Folded Reload
	movq	%rcx, -96(%rsp)                 ## 8-byte Spill
	movslq	-120(%rsp), %rcx                ## 4-byte Folded Reload
	movq	%rcx, -16(%rsp)                 ## 8-byte Spill
	movslq	%r13d, %rcx
	movq	%rcx, -24(%rsp)                 ## 8-byte Spill
	movslq	%edx, %rcx
	movq	%rcx, -32(%rsp)                 ## 8-byte Spill
	orl	$3, %r10d
	imull	%ebp, %r10d
	leal	(%r9,%rax), %ecx
	movslq	%ecx, %rsi
	movslq	-124(%rsp), %rbp                ## 4-byte Folded Reload
	movl	-72(%rsp), %ecx                 ## 4-byte Reload
	movq	%rcx, 152(%rsp)                 ## 8-byte Spill
	movl	%r12d, -120(%rsp)               ## 4-byte Spill
	movl	%r12d, %r12d
	movl	%r12d, %ecx
	andl	$-8, %ecx
	movq	%rcx, 80(%rsp)                  ## 8-byte Spill
	addq	$-8, %rcx
	movq	%rcx, 72(%rsp)                  ## 8-byte Spill
	movq	%rbx, %rdx
	movq	%rbx, (%rsp)                    ## 8-byte Spill
	movq	%rcx, %rbx
	shrq	$3, %rbx
	incq	%rbx
	movq	%r9, -88(%rsp)                  ## 8-byte Spill
	movl	%r9d, %r11d
	movl	%r11d, %ecx
	andl	$2147483640, %ecx               ## imm = 0x7FFFFFF8
	movq	%rcx, 56(%rsp)                  ## 8-byte Spill
	movq	-48(%rsp), %r9                  ## 8-byte Reload
	movq	%r9, %rcx
	imulq	%rdx, %rcx
	addq	%rcx, %rax
	leaq	(%rdi,%rax,4), %r14
	movq	-112(%rsp), %rax                ## 8-byte Reload
	leal	(%r10,%rax,8), %eax
	movl	%eax, -124(%rsp)                ## 4-byte Spill
	addq	%rsi, %rcx
	movq	%rbx, %rax
	movq	%rbx, 64(%rsp)                  ## 8-byte Spill
	andq	$-2, %rbx
	negq	%rbx
	movq	%rbx, 48(%rsp)                  ## 8-byte Spill
	movq	%r9, %rax
	leaq	(%rdi,%rcx,4), %r9
	addq	$32, %r9
	leaq	(%rdi,%rcx,4), %r13
	vmovss	LCPI37_1(%rip), %xmm11          ## xmm11 = mem[0],zero,zero,zero
	vmovss	LCPI37_2(%rip), %xmm3           ## xmm3 = mem[0],zero,zero,zero
	vbroadcastss	LCPI37_1(%rip), %ymm4   ## ymm4 = [1.0E+0,1.0E+0,1.0E+0,1.0E+0,1.0E+0,1.0E+0,1.0E+0,1.0E+0]
	vxorps	%xmm9, %xmm9, %xmm9
	vbroadcastss	LCPI37_2(%rip), %ymm6   ## ymm6 = [9.99999971E-10,9.99999971E-10,9.99999971E-10,9.99999971E-10,9.99999971E-10,9.99999971E-10,9.99999971E-10,9.99999971E-10]
	vxorps	%xmm8, %xmm8, %xmm8
	leaq	(,%rax,4), %rax
	movq	%rax, 144(%rsp)                 ## 8-byte Spill
	movq	%r8, 88(%rsp)                   ## 8-byte Spill
	movq	-56(%rsp), %rcx                 ## 8-byte Reload
	leaq	(%rcx,%r8,4), %rax
	movq	%rax, 136(%rsp)                 ## 8-byte Spill
	movq	%r15, 96(%rsp)                  ## 8-byte Spill
	leaq	(%rcx,%r15,4), %rax
	movq	%rax, 128(%rsp)                 ## 8-byte Spill
	movq	8(%rsp), %rax                   ## 8-byte Reload
	leaq	(%rcx,%rax,4), %rax
	movq	%rax, 120(%rsp)                 ## 8-byte Spill
	movq	16(%rsp), %rax                  ## 8-byte Reload
	leaq	(%rcx,%rax,4), %rax
	movq	%rax, 112(%rsp)                 ## 8-byte Spill
	movq	-64(%rsp), %rcx                 ## 8-byte Reload
	leaq	32(%rcx), %rax
	movq	%rax, 40(%rsp)                  ## 8-byte Spill
	movq	%rsi, 24(%rsp)                  ## 8-byte Spill
	leaq	(%rcx,%rsi,4), %rax
	movq	%rax, 104(%rsp)                 ## 8-byte Spill
	movq	%rbp, %rax
	movq	%rbp, 160(%rsp)                 ## 8-byte Spill
	movq	%rbp, %rcx
	xorl	%edx, %edx
	movq	%r10, %r15
	jmp	LBB37_6
	.p2align	4, 0x90
LBB37_18:                               ## %after_bb.us
                                        ##   in Loop: Header=BB37_6 Depth=1
	movq	-112(%rsp), %rdx                ## 8-byte Reload
	incq	%rdx
	movq	144(%rsp), %rax                 ## 8-byte Reload
	addq	%rax, %r14
	movq	-80(%rsp), %rcx                 ## 8-byte Reload
	incq	%rcx
	addl	%r10d, -124(%rsp)               ## 4-byte Folded Spill
	addq	%rax, %r9
	addl	%r10d, %r15d
	addq	%rax, %r13
	cmpq	152(%rsp), %rdx                 ## 8-byte Folded Reload
	je	LBB37_4
LBB37_6:                                ## %"for relu1_0_d_def__.s29.w.wi.us"
                                        ## =>This Loop Header: Depth=1
                                        ##     Child Loop BB37_36 Depth 2
                                        ##     Child Loop BB37_9 Depth 2
                                        ##     Child Loop BB37_22 Depth 2
                                        ##     Child Loop BB37_15 Depth 2
	movq	%r15, -72(%rsp)                 ## 8-byte Spill
	testq	%rcx, %rcx
	movl	$0, %eax
	movq	%rcx, -80(%rsp)                 ## 8-byte Spill
	cmovgl	%ecx, %eax
	movq	160(%rsp), %rcx                 ## 8-byte Reload
	movq	%rdx, %rsi
	movq	%rdx, -112(%rsp)                ## 8-byte Spill
	leaq	(%rdx,%rcx), %rdi
	testq	%rdi, %rdi
	movl	$0, %ecx
	cmovlel	%ecx, %edi
	cmpl	$0, -4(%rsp)                    ## 4-byte Folded Reload
	jle	LBB37_12
## %bb.7:                               ## %"for relu1_0_d_def__.s29.n.ni.preheader.us"
                                        ##   in Loop: Header=BB37_6 Depth=1
	imull	-116(%rsp), %eax                ## 4-byte Folded Reload
	movslq	%eax, %r8
	movslq	-124(%rsp), %rax                ## 4-byte Folded Reload
	movq	-64(%rsp), %rcx                 ## 8-byte Reload
	leaq	(%rcx,%rax,4), %rax
	cmpl	$8, -88(%rsp)                   ## 4-byte Folded Reload
	jae	LBB37_35
## %bb.8:                               ##   in Loop: Header=BB37_6 Depth=1
	xorl	%edx, %edx
	jmp	LBB37_38
	.p2align	4, 0x90
LBB37_35:                               ## %vector.body28.preheader
                                        ##   in Loop: Header=BB37_6 Depth=1
	movq	%rdi, %r15
	movq	88(%rsp), %rcx                  ## 8-byte Reload
	addq	%r8, %rcx
	movq	96(%rsp), %rdx                  ## 8-byte Reload
	leaq	(%rdx,%r8), %rsi
	movq	8(%rsp), %rdx                   ## 8-byte Reload
	leaq	(%rdx,%r8), %rdi
	movq	16(%rsp), %rdx                  ## 8-byte Reload
	leaq	(%rdx,%r8), %r10
	movq	-56(%rsp), %rbx                 ## 8-byte Reload
	leaq	(%rbx,%rcx,4), %rdx
	leaq	(%rbx,%rsi,4), %rbp
	leaq	(%rbx,%rdi,4), %rcx
	leaq	(%rbx,%r10,4), %rdi
	xorl	%esi, %esi
	movq	56(%rsp), %rbx                  ## 8-byte Reload
	.p2align	4, 0x90
LBB37_36:                               ## %vector.body28
                                        ##   Parent Loop BB37_6 Depth=1
                                        ## =>  This Inner Loop Header: Depth=2
	vmovups	(%rdi,%rsi,4), %ymm0
	vmulps	(%rcx,%rsi,4), %ymm0, %ymm0
	vmaxps	%ymm4, %ymm0, %ymm0
	vdivps	%ymm10, %ymm0, %ymm0
	vroundps	$10, %ymm0, %ymm1
	vmaxps	%ymm4, %ymm0, %ymm0
	vmulps	(%r14,%rsi,4), %ymm1, %ymm1
	vmulps	(%rdx,%rsi,4), %ymm1, %ymm1
	vmulps	%ymm6, %ymm1, %ymm1
	vdivps	%ymm0, %ymm1, %ymm0
	vcmpneqps	(%rbp,%rsi,4), %ymm9, %ymm1
	vandps	%ymm0, %ymm1, %ymm0
	vaddps	(%rax,%rsi,4), %ymm0, %ymm0
	vmovups	%ymm0, (%rax,%rsi,4)
	addq	$8, %rsi
	cmpq	%rsi, %rbx
	jne	LBB37_36
## %bb.37:                              ## %middle.block26
                                        ##   in Loop: Header=BB37_6 Depth=1
	movq	%rbx, %rdx
	cmpq	%r11, %rbx
	movq	%r15, %rdi
	jne	LBB37_38
LBB37_12:                               ## %"end for relu1_0_d_def__.s29.n.ni.us"
                                        ##   in Loop: Header=BB37_6 Depth=1
	movl	-120(%rsp), %ebp                ## 4-byte Reload
	testl	%ebp, %ebp
	movl	-100(%rsp), %r10d               ## 4-byte Reload
	movq	-72(%rsp), %r15                 ## 8-byte Reload
	jle	LBB37_18
## %bb.13:                              ## %"for relu1_0_d_def__.s29.n.ni.rebased.preheader.us"
                                        ##   in Loop: Header=BB37_6 Depth=1
	movslq	%r15d, %rax
	imull	-116(%rsp), %edi                ## 4-byte Folded Reload
	movslq	%edi, %rcx
	movq	-96(%rsp), %rdx                 ## 8-byte Reload
	addq	%rcx, %rdx
	movq	-16(%rsp), %rsi                 ## 8-byte Reload
	addq	%rcx, %rsi
	movq	-24(%rsp), %rdi                 ## 8-byte Reload
	addq	%rcx, %rdi
	movq	-56(%rsp), %rbx                 ## 8-byte Reload
	vmovss	-4(%rbx,%rdx,4), %xmm0          ## xmm0 = mem[0],zero,zero,zero
	vmulss	-4(%rbx,%rsi,4), %xmm0, %xmm0
	addq	-32(%rsp), %rcx                 ## 8-byte Folded Reload
	vmaxss	%xmm11, %xmm0, %xmm0
	vdivss	%xmm14, %xmm0, %xmm1
	vmovss	-4(%rbx,%rdi,4), %xmm7          ## xmm7 = mem[0],zero,zero,zero
	vmovss	-4(%rbx,%rcx,4), %xmm5          ## xmm5 = mem[0],zero,zero,zero
	vroundss	$10, %xmm1, %xmm1, %xmm0
	vmaxss	%xmm11, %xmm1, %xmm15
	cmpl	$8, %ebp
	jae	LBB37_19
## %bb.14:                              ##   in Loop: Header=BB37_6 Depth=1
	xorl	%edx, %edx
	jmp	LBB37_34
	.p2align	4, 0x90
LBB37_38:                               ## %"for relu1_0_d_def__.s29.n.ni.us.preheader"
                                        ##   in Loop: Header=BB37_6 Depth=1
	movq	136(%rsp), %rcx                 ## 8-byte Reload
	leaq	(%rcx,%r8,4), %rbp
	movq	128(%rsp), %rcx                 ## 8-byte Reload
	leaq	(%rcx,%r8,4), %r15
	movq	120(%rsp), %rcx                 ## 8-byte Reload
	leaq	(%rcx,%r8,4), %r10
	movq	112(%rsp), %rcx                 ## 8-byte Reload
	leaq	(%rcx,%r8,4), %rsi
	jmp	LBB37_9
	.p2align	4, 0x90
LBB37_11:                               ## %select.end
                                        ##   in Loop: Header=BB37_9 Depth=2
	vaddss	(%rax,%rdx,4), %xmm0, %xmm0
	vmovss	%xmm0, (%rax,%rdx,4)
	incq	%rdx
	cmpq	%rdx, %r11
	je	LBB37_12
LBB37_9:                                ## %"for relu1_0_d_def__.s29.n.ni.us"
                                        ##   Parent Loop BB37_6 Depth=1
                                        ## =>  This Inner Loop Header: Depth=2
	vmovss	(%rsi,%rdx,4), %xmm0            ## xmm0 = mem[0],zero,zero,zero
	vmulss	(%r10,%rdx,4), %xmm0, %xmm0
	vmaxss	%xmm11, %xmm0, %xmm1
	vxorps	%xmm0, %xmm0, %xmm0
	vucomiss	(%r15,%rdx,4), %xmm0
	vdivss	%xmm14, %xmm1, %xmm1
	vroundss	$10, %xmm1, %xmm1, %xmm2
	vmulss	(%r14,%rdx,4), %xmm2, %xmm2
	vmulss	(%rbp,%rdx,4), %xmm2, %xmm2
	vmulss	%xmm3, %xmm2, %xmm2
	je	LBB37_11
## %bb.10:                              ## %select.false.sink
                                        ##   in Loop: Header=BB37_9 Depth=2
	vmaxss	%xmm11, %xmm1, %xmm0
	vdivss	%xmm0, %xmm2, %xmm0
	jmp	LBB37_11
	.p2align	4, 0x90
LBB37_19:                               ## %vector.ph
                                        ##   in Loop: Header=BB37_6 Depth=1
	vbroadcastss	%xmm15, %ymm12
	cmpq	$0, 72(%rsp)                    ## 8-byte Folded Reload
	je	LBB37_20
## %bb.21:                              ## %vector.ph.new
                                        ##   in Loop: Header=BB37_6 Depth=1
	movq	24(%rsp), %rcx                  ## 8-byte Reload
	addq	%rax, %rcx
	movq	40(%rsp), %rdx                  ## 8-byte Reload
	leaq	(%rdx,%rcx,4), %rsi
	vmulss	%xmm0, %xmm5, %xmm1
	vbroadcastss	%xmm1, %ymm13
	movq	48(%rsp), %r8                   ## 8-byte Reload
	xorl	%ebp, %ebp
	movq	(%rsp), %rdi                    ## 8-byte Reload
	jmp	LBB37_22
	.p2align	4, 0x90
LBB37_28:                               ## %vector.body
                                        ##   in Loop: Header=BB37_22 Depth=2
	vaddps	(%rsi,%rbp,4), %ymm1, %ymm1
	vmovups	%ymm1, (%rsi,%rbp,4)
	addq	$16, %rbp
	addq	$2, %r8
	je	LBB37_29
LBB37_22:                               ## %vector.body
                                        ##   Parent Loop BB37_6 Depth=1
                                        ## =>  This Inner Loop Header: Depth=2
	vucomiss	%xmm8, %xmm7
	vmulps	-32(%r9,%rbp,4), %ymm13, %ymm1
	vmulps	%ymm6, %ymm1, %ymm1
	vdivps	%ymm12, %ymm1, %ymm1
	jne	LBB37_25
## %bb.23:                              ## %vector.body
                                        ##   in Loop: Header=BB37_22 Depth=2
	jp	LBB37_25
## %bb.24:                              ## %vector.body
                                        ##   in Loop: Header=BB37_22 Depth=2
	vxorps	%xmm1, %xmm1, %xmm1
LBB37_25:                               ## %vector.body
                                        ##   in Loop: Header=BB37_22 Depth=2
	vaddps	-32(%rsi,%rbp,4), %ymm1, %ymm1
	vmovups	%ymm1, -32(%rsi,%rbp,4)
	vmulps	(%r9,%rbp,4), %ymm13, %ymm1
	vmulps	%ymm6, %ymm1, %ymm1
	vdivps	%ymm12, %ymm1, %ymm1
	jne	LBB37_28
## %bb.26:                              ## %vector.body
                                        ##   in Loop: Header=BB37_22 Depth=2
	jp	LBB37_28
## %bb.27:                              ## %vector.body
                                        ##   in Loop: Header=BB37_22 Depth=2
	vxorps	%xmm1, %xmm1, %xmm1
	jmp	LBB37_28
LBB37_20:                               ##   in Loop: Header=BB37_6 Depth=1
	xorl	%ebp, %ebp
	movq	(%rsp), %rdi                    ## 8-byte Reload
LBB37_29:                               ## %middle.block.unr-lcssa
                                        ##   in Loop: Header=BB37_6 Depth=1
	testb	$1, 64(%rsp)                    ## 1-byte Folded Reload
	movq	-40(%rsp), %r8                  ## 8-byte Reload
	je	LBB37_33
## %bb.30:                              ## %vector.body.epil
                                        ##   in Loop: Header=BB37_6 Depth=1
	movq	32(%rsp), %rcx                  ## 8-byte Reload
	movq	-112(%rsp), %rdx                ## 8-byte Reload
	addl	%edx, %ecx
	imull	%r10d, %ecx
	addl	-8(%rsp), %ecx                  ## 4-byte Folded Reload
	movslq	%ecx, %rdx
	addq	24(%rsp), %rbp                  ## 8-byte Folded Reload
	addq	%rbp, %rdx
	vucomiss	%xmm8, %xmm7
	vxorps	%xmm1, %xmm1, %xmm1
	jne	LBB37_31
	jnp	LBB37_32
LBB37_31:                               ## %select.false.sink143
                                        ##   in Loop: Header=BB37_6 Depth=1
	movq	-112(%rsp), %rcx                ## 8-byte Reload
	addq	%rdi, %rcx
	imulq	-48(%rsp), %rcx                 ## 8-byte Folded Reload
	addq	%rcx, %rbp
	vmulss	%xmm0, %xmm5, %xmm1
	vbroadcastss	%xmm1, %ymm1
	vmulps	(%r8,%rbp,4), %ymm1, %ymm1
	vmulps	%ymm6, %ymm1, %ymm1
	vdivps	%ymm12, %ymm1, %ymm1
LBB37_32:                               ## %select.end142
                                        ##   in Loop: Header=BB37_6 Depth=1
	movq	-64(%rsp), %rcx                 ## 8-byte Reload
	vaddps	(%rcx,%rdx,4), %ymm1, %ymm1
	vmovups	%ymm1, (%rcx,%rdx,4)
LBB37_33:                               ## %middle.block
                                        ##   in Loop: Header=BB37_6 Depth=1
	movq	80(%rsp), %rcx                  ## 8-byte Reload
	movq	%rcx, %rdx
	cmpq	%r12, %rcx
	je	LBB37_18
LBB37_34:                               ## %"for relu1_0_d_def__.s29.n.ni.rebased.us.preheader"
                                        ##   in Loop: Header=BB37_6 Depth=1
	movq	104(%rsp), %rcx                 ## 8-byte Reload
	leaq	(%rcx,%rax,4), %rax
	jmp	LBB37_15
	.p2align	4, 0x90
LBB37_17:                               ## %select.end190
                                        ##   in Loop: Header=BB37_15 Depth=2
	vaddss	(%rax,%rdx,4), %xmm1, %xmm1
	vmovss	%xmm1, (%rax,%rdx,4)
	incq	%rdx
	cmpq	%rdx, %r12
	je	LBB37_18
LBB37_15:                               ## %"for relu1_0_d_def__.s29.n.ni.rebased.us"
                                        ##   Parent Loop BB37_6 Depth=1
                                        ## =>  This Inner Loop Header: Depth=2
	vxorps	%xmm1, %xmm1, %xmm1
	vucomiss	%xmm1, %xmm7
	vmulss	(%r13,%rdx,4), %xmm0, %xmm2
	vmulss	%xmm2, %xmm5, %xmm2
	vmulss	%xmm3, %xmm2, %xmm2
	jne	LBB37_16
	jnp	LBB37_17
LBB37_16:                               ## %select.false.sink191
                                        ##   in Loop: Header=BB37_15 Depth=2
	vdivss	%xmm15, %xmm2, %xmm1
	jmp	LBB37_17
LBB37_2:                                ## %"for relu1_0_d_def__.s29.w.wi.preheader5"
	decl	%edx
	vmovd	%eax, %xmm0
	vpbroadcastd	%xmm0, %ymm0
	vpor	LCPI37_0(%rip), %ymm0, %ymm0
	vmovd	%edx, %xmm1
	vpbroadcastd	%xmm1, %ymm1
	vpminsd	%ymm0, %ymm1, %ymm1
	movslq	-124(%rsp), %rdi                ## 4-byte Folded Reload
	movq	-88(%rsp), %rdx                 ## 8-byte Reload
	movl	-72(%rsp), %r9d                 ## 4-byte Reload
	movq	-48(%rsp), %r12                 ## 8-byte Reload
	imulq	%r12, %rbx
	addq	%rax, %rbx
	movq	-40(%rsp), %rax                 ## 8-byte Reload
	leaq	(%rax,%rbx,4), %rbx
	shlq	$2, %r12
	shll	$6, %r10d
	movl	-96(%rsp), %eax                 ## 4-byte Reload
	shll	$6, %eax
	subl	%eax, %r10d
	orl	$3, %r10d
	imull	%r10d, %edx
	movq	-112(%rsp), %rax                ## 8-byte Reload
	leal	(%rdx,%rax,8), %edx
	vbroadcastss	LCPI37_1(%rip), %ymm12  ## ymm12 = [1.0E+0,1.0E+0,1.0E+0,1.0E+0,1.0E+0,1.0E+0,1.0E+0,1.0E+0]
	vxorps	%xmm9, %xmm9, %xmm9
	vbroadcastss	LCPI37_2(%rip), %ymm11  ## ymm11 = [9.99999971E-10,9.99999971E-10,9.99999971E-10,9.99999971E-10,9.99999971E-10,9.99999971E-10,9.99999971E-10,9.99999971E-10]
	movl	-100(%rsp), %r10d               ## 4-byte Reload
	movq	-56(%rsp), %rbp                 ## 8-byte Reload
	.p2align	4, 0x90
LBB37_3:                                ## %"for relu1_0_d_def__.s29.w.wi"
                                        ## =>This Inner Loop Header: Depth=1
	testq	%rdi, %rdi
	movl	$0, %eax
	cmovgl	%edi, %eax
	imull	-116(%rsp), %eax                ## 4-byte Folded Reload
	leal	(%rax,%r11), %ecx
	vmovd	%ecx, %xmm0
	vpbroadcastd	%xmm0, %ymm0
	vpaddd	%ymm1, %ymm0, %ymm0
	vmovd	%xmm0, %ecx
	movslq	%ecx, %rcx
	vpextrd	$1, %xmm0, %esi
	movslq	%esi, %rsi
	vmovss	(%rbp,%rcx,4), %xmm5            ## xmm5 = mem[0],zero,zero,zero
	vpextrd	$2, %xmm0, %ecx
	vinsertps	$16, (%rbp,%rsi,4), %xmm5, %xmm6 ## xmm6 = xmm5[0],mem[0],xmm5[2,3]
	vpextrd	$3, %xmm0, %esi
	movslq	%ecx, %rcx
	movslq	%esi, %rsi
	vextracti128	$1, %ymm0, %xmm5
	vinsertps	$32, (%rbp,%rcx,4), %xmm6, %xmm0 ## xmm0 = xmm6[0,1],mem[0],xmm6[3]
	vmovd	%xmm5, %ecx
	movslq	%ecx, %rcx
	vinsertps	$48, (%rbp,%rsi,4), %xmm0, %xmm13 ## xmm13 = xmm0[0,1,2],mem[0]
	vmovss	(%rbp,%rcx,4), %xmm6            ## xmm6 = mem[0],zero,zero,zero
	leal	(%rax,%r14), %ecx
	vpextrd	$1, %xmm5, %esi
	vmovd	%ecx, %xmm7
	vpextrd	$2, %xmm5, %ecx
	movslq	%esi, %rsi
	vpbroadcastd	%xmm7, %ymm7
	vpaddd	%ymm1, %ymm7, %ymm7
	vinsertps	$16, (%rbp,%rsi,4), %xmm6, %xmm6 ## xmm6 = xmm6[0],mem[0],xmm6[2,3]
	vextracti128	$1, %ymm7, %xmm3
	vmovd	%xmm3, %esi
	movslq	%esi, %rsi
	movslq	%ecx, %rcx
	vmovss	(%rbp,%rsi,4), %xmm4            ## xmm4 = mem[0],zero,zero,zero
	vpextrd	$1, %xmm3, %esi
	movslq	%esi, %rsi
	vinsertps	$16, (%rbp,%rsi,4), %xmm4, %xmm4 ## xmm4 = xmm4[0],mem[0],xmm4[2,3]
	vinsertps	$32, (%rbp,%rcx,4), %xmm6, %xmm6 ## xmm6 = xmm6[0,1],mem[0],xmm6[3]
	vmovd	%xmm7, %ecx
	vpextrd	$1, %xmm7, %esi
	movslq	%ecx, %rcx
	movslq	%esi, %rsi
	vmovss	(%rbp,%rcx,4), %xmm2            ## xmm2 = mem[0],zero,zero,zero
	vpextrd	$3, %xmm5, %ecx
	vinsertps	$16, (%rbp,%rsi,4), %xmm2, %xmm2 ## xmm2 = xmm2[0],mem[0],xmm2[2,3]
	vpextrd	$2, %xmm3, %esi
	movslq	%ecx, %rcx
	movslq	%esi, %rsi
	vinsertps	$32, (%rbp,%rsi,4), %xmm4, %xmm4 ## xmm4 = xmm4[0,1],mem[0],xmm4[3]
	vpextrd	$2, %xmm7, %esi
	vinsertps	$48, (%rbp,%rcx,4), %xmm6, %xmm5 ## xmm5 = xmm6[0,1,2],mem[0]
	vpextrd	$3, %xmm7, %ecx
	movslq	%esi, %rsi
	vinsertps	$32, (%rbp,%rsi,4), %xmm2, %xmm2 ## xmm2 = xmm2[0,1],mem[0],xmm2[3]
	vpextrd	$3, %xmm3, %esi
	movslq	%ecx, %rcx
	movslq	%esi, %rsi
	vinsertps	$48, (%rbp,%rsi,4), %xmm4, %xmm6 ## xmm6 = xmm4[0,1,2],mem[0]
	vinsertps	$48, (%rbp,%rcx,4), %xmm2, %xmm7 ## xmm7 = xmm2[0,1,2],mem[0]
	leal	(%rax,%r15), %ecx
	vmovd	%ecx, %xmm2
	vpbroadcastd	%xmm2, %ymm2
	vpaddd	%ymm1, %ymm2, %ymm2
	vmovd	%xmm2, %ecx
	movslq	%ecx, %rcx
	vpextrd	$1, %xmm2, %esi
	movslq	%esi, %rsi
	vmovss	(%rbp,%rcx,4), %xmm3            ## xmm3 = mem[0],zero,zero,zero
	vpextrd	$2, %xmm2, %ecx
	vextracti128	$1, %ymm2, %xmm4
	movslq	%ecx, %rcx
	vinsertps	$16, (%rbp,%rsi,4), %xmm3, %xmm3 ## xmm3 = xmm3[0],mem[0],xmm3[2,3]
	vinsertps	$32, (%rbp,%rcx,4), %xmm3, %xmm3 ## xmm3 = xmm3[0,1],mem[0],xmm3[3]
	vmovd	%xmm4, %ecx
	vpextrd	$1, %xmm4, %esi
	movslq	%ecx, %rcx
	vmovss	(%rbp,%rcx,4), %xmm0            ## xmm0 = mem[0],zero,zero,zero
	movslq	%esi, %rcx
	vinsertps	$16, (%rbp,%rcx,4), %xmm0, %xmm0 ## xmm0 = xmm0[0],mem[0],xmm0[2,3]
	vpextrd	$3, %xmm2, %ecx
	movslq	%ecx, %rcx
	vinsertps	$48, (%rbp,%rcx,4), %xmm3, %xmm8 ## xmm8 = xmm3[0,1,2],mem[0]
	vpextrd	$2, %xmm4, %ecx
	movslq	%ecx, %rcx
	vinsertps	$32, (%rbp,%rcx,4), %xmm0, %xmm0 ## xmm0 = xmm0[0,1],mem[0],xmm0[3]
	addl	%r8d, %eax
	vpextrd	$3, %xmm4, %ecx
	vmovd	%eax, %xmm2
	movslq	%ecx, %rax
	vinsertps	$48, (%rbp,%rax,4), %xmm0, %xmm0 ## xmm0 = xmm0[0,1,2],mem[0]
	vpbroadcastd	%xmm2, %ymm2
	vpaddd	%ymm1, %ymm2, %ymm2
	vextracti128	$1, %ymm2, %xmm3
	vmovd	%xmm3, %eax
	cltq
	vmovss	(%rbp,%rax,4), %xmm4            ## xmm4 = mem[0],zero,zero,zero
	vpextrd	$1, %xmm3, %eax
	cltq
	vinsertps	$16, (%rbp,%rax,4), %xmm4, %xmm4 ## xmm4 = xmm4[0],mem[0],xmm4[2,3]
	vinsertf128	$1, %xmm5, %ymm13, %ymm5
	vinsertf128	$1, %xmm6, %ymm7, %ymm6
	vpextrd	$2, %xmm3, %eax
	cltq
	vinsertps	$32, (%rbp,%rax,4), %xmm4, %xmm4 ## xmm4 = xmm4[0,1],mem[0],xmm4[3]
	vmovd	%xmm2, %eax
	cltq
	vmovss	(%rbp,%rax,4), %xmm7            ## xmm7 = mem[0],zero,zero,zero
	vpextrd	$1, %xmm2, %eax
	cltq
	vinsertps	$16, (%rbp,%rax,4), %xmm7, %xmm7 ## xmm7 = xmm7[0],mem[0],xmm7[2,3]
	vpextrd	$3, %xmm3, %eax
	vmulps	%ymm6, %ymm5, %ymm3
	cltq
	vinsertps	$48, (%rbp,%rax,4), %xmm4, %xmm4 ## xmm4 = xmm4[0,1,2],mem[0]
	vmaxps	%ymm12, %ymm3, %ymm3
	vpextrd	$2, %xmm2, %eax
	cltq
	vinsertps	$32, (%rbp,%rax,4), %xmm7, %xmm5 ## xmm5 = xmm7[0,1],mem[0],xmm7[3]
	vpextrd	$3, %xmm2, %eax
	vdivps	%ymm10, %ymm3, %ymm2
	cltq
	vinsertps	$48, (%rbp,%rax,4), %xmm5, %xmm3 ## xmm3 = xmm5[0,1,2],mem[0]
	movslq	%edx, %rdx
	vinsertf128	$1, %xmm0, %ymm8, %ymm0
	vinsertf128	$1, %xmm4, %ymm3, %ymm3
	vmulps	%ymm3, %ymm11, %ymm3
	vroundps	$2, %ymm2, %ymm4
	vmaxps	%ymm12, %ymm2, %ymm2
	vmulps	(%rbx), %ymm4, %ymm4
	vmulps	%ymm3, %ymm4, %ymm3
	vcmpneqps	%ymm0, %ymm9, %ymm0
	vdivps	%ymm2, %ymm3, %ymm2
	vandps	%ymm2, %ymm0, %ymm0
	movq	-64(%rsp), %rax                 ## 8-byte Reload
	vaddps	(%rax,%rdx,4), %ymm0, %ymm0
	vmovups	%ymm0, (%rax,%rdx,4)
	addq	%r12, %rbx
	incq	%rdi
	addl	%r10d, %edx
	decq	%r9
	jne	LBB37_3
LBB37_4:                                ## %destructor_block
	xorl	%eax, %eax
	addq	$168, %rsp
	popq	%rbx
	popq	%r12
	popq	%r13
	popq	%r14
	popq	%r15
	popq	%rbp
	vzeroupper
	retq
                                        ## -- End function
	.section	__TEXT,__const
	.p2align	5                               ## -- Begin function train_cost_model.par_for.relu1_0_d_def__.s30.n.n.n
LCPI38_0:
	.long	0                               ## 0x0
	.long	1                               ## 0x1
	.long	2                               ## 0x2
	.long	3                               ## 0x3
	.long	4                               ## 0x4
	.long	5                               ## 0x5
	.long	6                               ## 0x6
	.long	7                               ## 0x7
LCPI38_3:
	.space	32
	.section	__TEXT,__literal4,4byte_literals
	.p2align	2
LCPI38_1:
	.long	0x3f800000                      ## float 1
LCPI38_2:
	.long	0x3089705f                      ## float 9.99999971E-10
	.section	__TEXT,__text,regular,pure_instructions
	.p2align	4, 0x90
_train_cost_model.par_for.relu1_0_d_def__.s30.n.n.n: ## @train_cost_model.par_for.relu1_0_d_def__.s30.n.n.n
## %bb.0:                               ## %entry
	pushq	%rbp
	pushq	%r15
	pushq	%r14
	pushq	%r13
	pushq	%r12
	pushq	%rbx
	subq	$264, %rsp                      ## imm = 0x108
	movq	%rdx, %rdi
	movl	12(%rdx), %r14d
	movl	24(%rdx), %r9d
	movl	%esi, %r8d
	sarl	$31, %r8d
	xorl	%ecx, %ecx
	testl	%r9d, %r9d
	sete	%cl
	movl	%ecx, %ebp
	negl	%ebp
	movl	%r9d, %ebx
	sarl	$31, %ebx
	subl	%r8d, %esi
	orl	%r9d, %ebp
	movl	%esi, %eax
	cltd
	idivl	%ebp
	movl	%edx, %r13d
	movl	%ebx, %r11d
	leal	(%r9,%rcx), %ebp
	movl	%esi, %eax
	cltd
	idivl	%ebp
	notl	%r11d
	decl	%ecx
	movl	%r11d, %esi
	subl	%ebx, %esi
	andl	%r8d, %esi
	addl	%eax, %esi
	andl	%ecx, %esi
	leal	(%rsi,%rsi), %eax
	movl	%eax, 64(%rsp)                  ## 4-byte Spill
	subl	%eax, %r14d
	cmpl	$2, %r14d
	movl	$2, %edx
	cmovll	%r14d, %edx
	xorl	%eax, %eax
	testl	%r14d, %r14d
	cmovlel	%eax, %edx
	movl	%edx, 32(%rsp)                  ## 4-byte Spill
	jle	LBB38_4
## %bb.1:                               ## %"for relu1_0_d_def__.s30.w.wi.preheader"
	movl	16(%rdi), %ebp
	movl	32(%rdi), %edx
	movl	40(%rdi), %eax
	movl	44(%rdi), %r14d
	movl	48(%rdi), %r15d
	movl	52(%rdi), %r12d
	movl	56(%rdi), %r10d
	xorl	%r9d, %ebx
	addl	%r11d, %ebx
	andl	%r8d, %ebx
	addl	%ebx, %r13d
	andl	%ecx, %r13d
	movl	%eax, -88(%rsp)                 ## 4-byte Spill
                                        ## kill: def $eax killed $eax def $rax
	subl	%edx, %eax
	movq	%rax, -112(%rsp)                ## 8-byte Spill
	movl	%r14d, -96(%rsp)                ## 4-byte Spill
	movl	%r14d, %r8d
	subl	%edx, %r8d
	movl	%r15d, -32(%rsp)                ## 4-byte Spill
	movq	%r13, %rcx
	movl	%r15d, %r13d
	subl	%edx, %r13d
	movl	%r12d, -40(%rsp)                ## 4-byte Spill
	movl	%r12d, %r14d
	subl	%edx, %r14d
	movl	%r10d, -48(%rsp)                ## 4-byte Spill
	movl	%r10d, %r9d
	subl	%edx, %r9d
	movslq	8(%rdi), %rax
	movslq	%esi, %r11
	subq	%rax, %r11
	addq	%r11, %r11
	movq	%rbp, -64(%rsp)                 ## 8-byte Spill
	movl	%ebp, %eax
	shll	$4, %eax
	movl	%eax, -100(%rsp)                ## 4-byte Spill
	movl	(%rdi), %r15d
	movq	%rcx, %r10
	leal	(,%rcx,8), %eax
	movl	%r15d, %ecx
	subl	%eax, %ecx
	cmpl	$8, %ecx
	movl	$8, %ebx
	cmovll	%ecx, %ebx
	movl	%ecx, -20(%rsp)                 ## 4-byte Spill
	testl	%ecx, %ecx
	movl	$0, %ecx
	cmovgl	%ebx, %ecx
	leal	8(%rax), %ebx
	movslq	%eax, %r12
	subl	%r15d, %eax
	movl	%eax, %ebp
	sarl	$31, %ebp
	andl	%eax, %ebp
	cmpl	$-8, %ebp
	movl	$-8, %eax
	cmovgl	%ebp, %eax
	movl	%eax, -120(%rsp)                ## 4-byte Spill
	vmovss	36(%rdi), %xmm11                ## xmm11 = mem[0],zero,zero,zero
	movl	20(%rdi), %eax
	movl	%eax, -116(%rsp)                ## 4-byte Spill
	movq	64(%rdi), %rbp
	movq	80(%rdi), %rax
	movq	%rax, -80(%rsp)                 ## 8-byte Spill
	movq	96(%rdi), %rax
	movq	%rax, -72(%rsp)                 ## 8-byte Spill
	movl	28(%rdi), %eax
	movslq	4(%rdi), %rdi
	vbroadcastss	%xmm11, %ymm10
	cmpl	%r15d, %ebx
	jle	LBB38_2
## %bb.5:                               ## %"for relu1_0_d_def__.s30.w.wi.us.preheader"
	subl	%edx, %r15d
	addl	%r15d, -88(%rsp)                ## 4-byte Folded Spill
	movl	-96(%rsp), %ebx                 ## 4-byte Reload
	addl	%r15d, %ebx
	movq	%rbp, 24(%rsp)                  ## 8-byte Spill
	movl	-32(%rsp), %ebp                 ## 4-byte Reload
	addl	%r15d, %ebp
	movq	%rdi, -56(%rsp)                 ## 8-byte Spill
	movl	-40(%rsp), %edi                 ## 4-byte Reload
	addl	%r15d, %edi
	addl	-48(%rsp), %r15d                ## 4-byte Folded Reload
	movl	%esi, %edx
	subl	%eax, %edx
	shll	$5, %esi
	shll	$5, %eax
	subl	%eax, %esi
	addl	%edx, %edx
	movq	%rdx, 96(%rsp)                  ## 8-byte Spill
	addl	%r12d, %r9d
	addl	%r12d, %r14d
	addl	%r12d, %r13d
	addl	%r12d, %r8d
	movq	-112(%rsp), %rax                ## 8-byte Reload
	addl	%r12d, %eax
	movl	-120(%rsp), %edx                ## 4-byte Reload
	addl	%ecx, %edx
	movl	%edx, -120(%rsp)                ## 4-byte Spill
	cltq
	movq	%rax, (%rsp)                    ## 8-byte Spill
	movslq	%r8d, %rax
	movq	%rax, -8(%rsp)                  ## 8-byte Spill
	movslq	%r13d, %rax
	movq	%rax, -16(%rsp)                 ## 8-byte Spill
	movslq	%r14d, %r10
	movslq	%r9d, %r9
	movslq	-88(%rsp), %rax                 ## 4-byte Folded Reload
	movq	%rax, -40(%rsp)                 ## 8-byte Spill
	movslq	%ebx, %rax
	movq	%rax, -48(%rsp)                 ## 8-byte Spill
	movslq	%ebp, %rax
	movq	%rax, 248(%rsp)                 ## 8-byte Spill
	movslq	%edi, %rax
	movq	%rax, 240(%rsp)                 ## 8-byte Spill
	movslq	%r15d, %rax
	movq	%rax, 232(%rsp)                 ## 8-byte Spill
	leal	(%rcx,%r12), %eax
	movslq	%eax, %r8
	movslq	64(%rsp), %rax                  ## 4-byte Folded Reload
	movq	%rax, -96(%rsp)                 ## 8-byte Spill
	movl	32(%rsp), %eax                  ## 4-byte Reload
	movq	%rax, 224(%rsp)                 ## 8-byte Spill
	movl	%edx, %eax
	movq	%rax, 16(%rsp)                  ## 8-byte Spill
                                        ## kill: def $eax killed $eax killed $rax def $rax
	andl	$-8, %eax
	movq	%rax, 144(%rsp)                 ## 8-byte Spill
	addq	$-8, %rax
	movq	%rax, 136(%rsp)                 ## 8-byte Spill
	movq	%rax, %rdi
	shrq	$3, %rdi
	incq	%rdi
	movq	%rcx, -32(%rsp)                 ## 8-byte Spill
	movl	%ecx, %r15d
	movl	%r15d, %eax
	andl	$2147483640, %eax               ## imm = 0x7FFFFFF8
	movq	%rax, 120(%rsp)                 ## 8-byte Spill
	movq	-56(%rsp), %rdx                 ## 8-byte Reload
	movq	%rdx, %rax
	imulq	%r11, %rax
	movq	%r12, %rcx
	addq	%rax, %rcx
	movq	24(%rsp), %r14                  ## 8-byte Reload
	leaq	(%r14,%rcx,4), %r13
	orl	$1, %esi
	movq	-80(%rsp), %rbx                 ## 8-byte Reload
	movq	-64(%rsp), %rbp                 ## 8-byte Reload
	imull	%ebp, %esi
	leal	(%r12,%rsi,2), %ecx
	movl	%ecx, 64(%rsp)                  ## 4-byte Spill
	movl	%esi, %ecx
	addl	%esi, %ecx
	movl	%ecx, 32(%rsp)                  ## 4-byte Spill
	movl	%ebp, %ecx
	shll	$5, %ecx
	movl	%ecx, -24(%rsp)                 ## 4-byte Spill
	addq	%r8, %rax
	movq	%rdi, %rcx
	movq	%rdi, 128(%rsp)                 ## 8-byte Spill
	andq	$-2, %rdi
	negq	%rdi
	movq	%rdi, 112(%rsp)                 ## 8-byte Spill
	movq	%r14, %rcx
	leaq	(%r14,%rax,4), %r14
	addq	$32, %r14
	leaq	(%rcx,%rax,4), %r12
	vmovss	LCPI38_1(%rip), %xmm14          ## xmm14 = mem[0],zero,zero,zero
	vmovss	LCPI38_2(%rip), %xmm3           ## xmm3 = mem[0],zero,zero,zero
	vbroadcastss	LCPI38_1(%rip), %ymm4   ## ymm4 = [1.0E+0,1.0E+0,1.0E+0,1.0E+0,1.0E+0,1.0E+0,1.0E+0,1.0E+0]
	vxorps	%xmm9, %xmm9, %xmm9
	vbroadcastss	LCPI38_2(%rip), %ymm6   ## ymm6 = [9.99999971E-10,9.99999971E-10,9.99999971E-10,9.99999971E-10,9.99999971E-10,9.99999971E-10,9.99999971E-10,9.99999971E-10]
	vxorps	%xmm8, %xmm8, %xmm8
	leaq	(,%rdx,4), %rax
	movq	%rax, 216(%rsp)                 ## 8-byte Spill
	movq	%r9, 152(%rsp)                  ## 8-byte Spill
	movq	-72(%rsp), %rsi                 ## 8-byte Reload
	leaq	(%rsi,%r9,4), %rax
	movq	%rax, 208(%rsp)                 ## 8-byte Spill
	movq	%r10, 160(%rsp)                 ## 8-byte Spill
	leaq	(%rsi,%r10,4), %rax
	movq	%rax, 200(%rsp)                 ## 8-byte Spill
	movq	-16(%rsp), %rax                 ## 8-byte Reload
	leaq	(%rsi,%rax,4), %rax
	movq	%rax, 192(%rsp)                 ## 8-byte Spill
	movq	-8(%rsp), %rax                  ## 8-byte Reload
	leaq	(%rsi,%rax,4), %rax
	movq	%rax, 184(%rsp)                 ## 8-byte Spill
	movq	(%rsp), %rax                    ## 8-byte Reload
	leaq	(%rsi,%rax,4), %rax
	movq	%rax, 176(%rsp)                 ## 8-byte Spill
	leaq	32(%rbx), %rax
	movq	%rax, 104(%rsp)                 ## 8-byte Spill
	movq	%r8, 8(%rsp)                    ## 8-byte Spill
	leaq	(%rbx,%r8,4), %rax
	movq	%rax, 168(%rsp)                 ## 8-byte Spill
	movq	-96(%rsp), %rdx                 ## 8-byte Reload
	xorl	%esi, %esi
	movq	%r11, %r8
	movl	-120(%rsp), %edi                ## 4-byte Reload
	movq	%r11, 256(%rsp)                 ## 8-byte Spill
	jmp	LBB38_6
	.p2align	4, 0x90
LBB38_18:                               ## %after_bb.us
                                        ##   in Loop: Header=BB38_6 Depth=1
	movq	-112(%rsp), %rsi                ## 8-byte Reload
	incq	%rsi
	movq	216(%rsp), %rcx                 ## 8-byte Reload
	addq	%rcx, %r13
	movq	-88(%rsp), %rdx                 ## 8-byte Reload
	incq	%rdx
	movl	-24(%rsp), %eax                 ## 4-byte Reload
	addl	%eax, 64(%rsp)                  ## 4-byte Folded Spill
	addq	%rcx, %r14
	addl	%eax, 32(%rsp)                  ## 4-byte Folded Spill
	addq	%rcx, %r12
	cmpq	224(%rsp), %rsi                 ## 8-byte Folded Reload
	je	LBB38_4
LBB38_6:                                ## %"for relu1_0_d_def__.s30.w.wi.us"
                                        ## =>This Loop Header: Depth=1
                                        ##     Child Loop BB38_36 Depth 2
                                        ##     Child Loop BB38_9 Depth 2
                                        ##     Child Loop BB38_22 Depth 2
                                        ##     Child Loop BB38_15 Depth 2
	testq	%rdx, %rdx
	movl	$0, %eax
	movq	%rdx, -88(%rsp)                 ## 8-byte Spill
	cmovgl	%edx, %eax
	movq	-96(%rsp), %rcx                 ## 8-byte Reload
	movq	%rsi, -112(%rsp)                ## 8-byte Spill
	leaq	(%rsi,%rcx), %r9
	testq	%r9, %r9
	movl	$0, %ecx
	cmovlel	%ecx, %r9d
	cmpl	$0, -20(%rsp)                   ## 4-byte Folded Reload
	jle	LBB38_12
## %bb.7:                               ## %"for relu1_0_d_def__.s30.n.ni.preheader.us"
                                        ##   in Loop: Header=BB38_6 Depth=1
	imull	-116(%rsp), %eax                ## 4-byte Folded Reload
	movslq	%eax, %r11
	movslq	64(%rsp), %rax                  ## 4-byte Folded Reload
	movq	-80(%rsp), %rcx                 ## 8-byte Reload
	leaq	(%rcx,%rax,4), %rsi
	cmpl	$8, -32(%rsp)                   ## 4-byte Folded Reload
	jae	LBB38_35
## %bb.8:                               ##   in Loop: Header=BB38_6 Depth=1
	xorl	%ebx, %ebx
	jmp	LBB38_38
	.p2align	4, 0x90
LBB38_35:                               ## %vector.body28.preheader
                                        ##   in Loop: Header=BB38_6 Depth=1
	movq	152(%rsp), %rax                 ## 8-byte Reload
	addq	%r11, %rax
	movq	160(%rsp), %rcx                 ## 8-byte Reload
	leaq	(%rcx,%r11), %rdx
	movq	-16(%rsp), %rcx                 ## 8-byte Reload
	leaq	(%rcx,%r11), %rdi
	movq	-8(%rsp), %rcx                  ## 8-byte Reload
	leaq	(%rcx,%r11), %rbx
	movq	(%rsp), %rcx                    ## 8-byte Reload
	leaq	(%rcx,%r11), %r8
	movq	-72(%rsp), %rcx                 ## 8-byte Reload
	leaq	(%rcx,%rax,4), %r10
	leaq	(%rcx,%rdx,4), %rdx
	leaq	(%rcx,%rdi,4), %rbp
	leaq	(%rcx,%rbx,4), %rbx
	leaq	(%rcx,%r8,4), %rdi
	xorl	%ecx, %ecx
	movq	120(%rsp), %rax                 ## 8-byte Reload
	.p2align	4, 0x90
LBB38_36:                               ## %vector.body28
                                        ##   Parent Loop BB38_6 Depth=1
                                        ## =>  This Inner Loop Header: Depth=2
	vmovups	(%rdi,%rcx,4), %ymm0
	vmulps	(%rbx,%rcx,4), %ymm0, %ymm0
	vmaxps	%ymm4, %ymm0, %ymm0
	vdivps	%ymm10, %ymm0, %ymm0
	vroundps	$10, %ymm0, %ymm1
	vmaxps	%ymm4, %ymm0, %ymm0
	vmulps	(%r13,%rcx,4), %ymm1, %ymm1
	vmovups	(%rdx,%rcx,4), %ymm2
	vmulps	(%r10,%rcx,4), %ymm2, %ymm2
	vmulps	%ymm6, %ymm2, %ymm2
	vmulps	%ymm2, %ymm1, %ymm1
	vdivps	%ymm0, %ymm1, %ymm0
	vcmpneqps	(%rbp,%rcx,4), %ymm9, %ymm1
	vandps	%ymm0, %ymm1, %ymm0
	vaddps	(%rsi,%rcx,4), %ymm0, %ymm0
	vmovups	%ymm0, (%rsi,%rcx,4)
	addq	$8, %rcx
	cmpq	%rcx, %rax
	jne	LBB38_36
## %bb.37:                              ## %middle.block26
                                        ##   in Loop: Header=BB38_6 Depth=1
	movq	%rax, %rbx
	cmpq	%r15, %rax
	movl	-120(%rsp), %edi                ## 4-byte Reload
	jne	LBB38_38
LBB38_12:                               ## %"end for relu1_0_d_def__.s30.n.ni.us"
                                        ##   in Loop: Header=BB38_6 Depth=1
	testl	%edi, %edi
	movq	-80(%rsp), %rbx                 ## 8-byte Reload
	movq	256(%rsp), %r8                  ## 8-byte Reload
	movq	24(%rsp), %r10                  ## 8-byte Reload
	jle	LBB38_18
## %bb.13:                              ## %"for relu1_0_d_def__.s30.n.ni.rebased.preheader.us"
                                        ##   in Loop: Header=BB38_6 Depth=1
	movslq	32(%rsp), %r11                  ## 4-byte Folded Reload
	imull	-116(%rsp), %r9d                ## 4-byte Folded Reload
	movslq	%r9d, %rax
	movq	-40(%rsp), %rcx                 ## 8-byte Reload
	addq	%rax, %rcx
	movq	-48(%rsp), %rdx                 ## 8-byte Reload
	addq	%rax, %rdx
	movq	248(%rsp), %rsi                 ## 8-byte Reload
	addq	%rax, %rsi
	movq	240(%rsp), %rdi                 ## 8-byte Reload
	addq	%rax, %rdi
	addq	232(%rsp), %rax                 ## 8-byte Folded Reload
	movq	-72(%rsp), %rbp                 ## 8-byte Reload
	vmovss	-4(%rbp,%rcx,4), %xmm0          ## xmm0 = mem[0],zero,zero,zero
	vmulss	-4(%rbp,%rdx,4), %xmm0, %xmm0
	vmaxss	%xmm14, %xmm0, %xmm0
	vdivss	%xmm11, %xmm0, %xmm0
	vmovss	-4(%rbp,%rsi,4), %xmm7          ## xmm7 = mem[0],zero,zero,zero
	vroundss	$10, %xmm0, %xmm0, %xmm5
	vmaxss	%xmm14, %xmm0, %xmm15
	vmovss	-4(%rbp,%rdi,4), %xmm0          ## xmm0 = mem[0],zero,zero,zero
	movl	-120(%rsp), %edi                ## 4-byte Reload
	vmulss	-4(%rbp,%rax,4), %xmm0, %xmm1
	cmpl	$8, %edi
	jae	LBB38_19
## %bb.14:                              ##   in Loop: Header=BB38_6 Depth=1
	xorl	%eax, %eax
	movq	16(%rsp), %rsi                  ## 8-byte Reload
	jmp	LBB38_34
	.p2align	4, 0x90
LBB38_38:                               ## %"for relu1_0_d_def__.s30.n.ni.us.preheader"
                                        ##   in Loop: Header=BB38_6 Depth=1
	movq	208(%rsp), %rax                 ## 8-byte Reload
	leaq	(%rax,%r11,4), %rdx
	movq	200(%rsp), %rax                 ## 8-byte Reload
	leaq	(%rax,%r11,4), %rbp
	movq	192(%rsp), %rax                 ## 8-byte Reload
	leaq	(%rax,%r11,4), %r8
	movq	184(%rsp), %rax                 ## 8-byte Reload
	leaq	(%rax,%r11,4), %r10
	movq	176(%rsp), %rax                 ## 8-byte Reload
	leaq	(%rax,%r11,4), %rax
	jmp	LBB38_9
	.p2align	4, 0x90
LBB38_11:                               ## %select.end
                                        ##   in Loop: Header=BB38_9 Depth=2
	vaddss	(%rsi,%rbx,4), %xmm0, %xmm0
	vmovss	%xmm0, (%rsi,%rbx,4)
	incq	%rbx
	cmpq	%rbx, %r15
	je	LBB38_12
LBB38_9:                                ## %"for relu1_0_d_def__.s30.n.ni.us"
                                        ##   Parent Loop BB38_6 Depth=1
                                        ## =>  This Inner Loop Header: Depth=2
	vmovss	(%rax,%rbx,4), %xmm0            ## xmm0 = mem[0],zero,zero,zero
	vmulss	(%r10,%rbx,4), %xmm0, %xmm0
	vmaxss	%xmm14, %xmm0, %xmm0
	vdivss	%xmm11, %xmm0, %xmm1
	vxorps	%xmm0, %xmm0, %xmm0
	vucomiss	(%r8,%rbx,4), %xmm0
	vroundss	$10, %xmm1, %xmm1, %xmm2
	vmulss	(%r13,%rbx,4), %xmm2, %xmm2
	vmovss	(%rbp,%rbx,4), %xmm5            ## xmm5 = mem[0],zero,zero,zero
	vmulss	(%rdx,%rbx,4), %xmm5, %xmm5
	vmulss	%xmm5, %xmm2, %xmm2
	vmulss	%xmm3, %xmm2, %xmm2
	je	LBB38_11
## %bb.10:                              ## %select.false.sink
                                        ##   in Loop: Header=BB38_9 Depth=2
	vmaxss	%xmm14, %xmm1, %xmm0
	vdivss	%xmm0, %xmm2, %xmm0
	jmp	LBB38_11
	.p2align	4, 0x90
LBB38_19:                               ## %vector.ph
                                        ##   in Loop: Header=BB38_6 Depth=1
	vbroadcastss	%xmm15, %ymm12
	cmpq	$0, 136(%rsp)                   ## 8-byte Folded Reload
	je	LBB38_20
## %bb.21:                              ## %vector.ph.new
                                        ##   in Loop: Header=BB38_6 Depth=1
	movq	8(%rsp), %rax                   ## 8-byte Reload
	addq	%r11, %rax
	movq	104(%rsp), %rcx                 ## 8-byte Reload
	leaq	(%rcx,%rax,4), %rsi
	vmulss	%xmm5, %xmm1, %xmm0
	vbroadcastss	%xmm0, %ymm13
	movq	112(%rsp), %r9                  ## 8-byte Reload
	xorl	%ebp, %ebp
	movq	-56(%rsp), %rdx                 ## 8-byte Reload
	jmp	LBB38_22
	.p2align	4, 0x90
LBB38_28:                               ## %vector.body
                                        ##   in Loop: Header=BB38_22 Depth=2
	vaddps	(%rsi,%rbp,4), %ymm2, %ymm0
	vmovups	%ymm0, (%rsi,%rbp,4)
	addq	$16, %rbp
	addq	$2, %r9
	je	LBB38_29
LBB38_22:                               ## %vector.body
                                        ##   Parent Loop BB38_6 Depth=1
                                        ## =>  This Inner Loop Header: Depth=2
	vucomiss	%xmm8, %xmm7
	vmulps	-32(%r14,%rbp,4), %ymm13, %ymm0
	vmulps	%ymm6, %ymm0, %ymm0
	vdivps	%ymm12, %ymm0, %ymm2
	jne	LBB38_25
## %bb.23:                              ## %vector.body
                                        ##   in Loop: Header=BB38_22 Depth=2
	jp	LBB38_25
## %bb.24:                              ## %vector.body
                                        ##   in Loop: Header=BB38_22 Depth=2
	vxorps	%xmm2, %xmm2, %xmm2
LBB38_25:                               ## %vector.body
                                        ##   in Loop: Header=BB38_22 Depth=2
	vaddps	-32(%rsi,%rbp,4), %ymm2, %ymm0
	vmovups	%ymm0, -32(%rsi,%rbp,4)
	vmulps	(%r14,%rbp,4), %ymm13, %ymm0
	vmulps	%ymm6, %ymm0, %ymm0
	vdivps	%ymm12, %ymm0, %ymm2
	jne	LBB38_28
## %bb.26:                              ## %vector.body
                                        ##   in Loop: Header=BB38_22 Depth=2
	jp	LBB38_28
## %bb.27:                              ## %vector.body
                                        ##   in Loop: Header=BB38_22 Depth=2
	vxorps	%xmm2, %xmm2, %xmm2
	jmp	LBB38_28
LBB38_20:                               ##   in Loop: Header=BB38_6 Depth=1
	xorl	%ebp, %ebp
	movq	-56(%rsp), %rdx                 ## 8-byte Reload
LBB38_29:                               ## %middle.block.unr-lcssa
                                        ##   in Loop: Header=BB38_6 Depth=1
	testb	$1, 128(%rsp)                   ## 1-byte Folded Reload
	movq	16(%rsp), %rsi                  ## 8-byte Reload
	je	LBB38_33
## %bb.30:                              ## %vector.body.epil
                                        ##   in Loop: Header=BB38_6 Depth=1
	movq	96(%rsp), %rax                  ## 8-byte Reload
	movq	-112(%rsp), %rcx                ## 8-byte Reload
	addl	%ecx, %eax
	imull	-100(%rsp), %eax                ## 4-byte Folded Reload
	addl	-64(%rsp), %eax                 ## 4-byte Folded Reload
	addl	%eax, %eax
	cltq
	addq	8(%rsp), %rbp                   ## 8-byte Folded Reload
	addq	%rbp, %rax
	vucomiss	%xmm8, %xmm7
	vxorps	%xmm2, %xmm2, %xmm2
	jne	LBB38_31
	jnp	LBB38_32
LBB38_31:                               ## %select.false.sink158
                                        ##   in Loop: Header=BB38_6 Depth=1
	addq	%r8, %rcx
	imulq	%rdx, %rcx
	addq	%rcx, %rbp
	vmulss	%xmm5, %xmm1, %xmm0
	vbroadcastss	%xmm0, %ymm0
	vmulps	(%r10,%rbp,4), %ymm0, %ymm0
	vmulps	%ymm6, %ymm0, %ymm0
	vdivps	%ymm12, %ymm0, %ymm2
LBB38_32:                               ## %select.end157
                                        ##   in Loop: Header=BB38_6 Depth=1
	vaddps	(%rbx,%rax,4), %ymm2, %ymm0
	vmovups	%ymm0, (%rbx,%rax,4)
LBB38_33:                               ## %middle.block
                                        ##   in Loop: Header=BB38_6 Depth=1
	movq	144(%rsp), %rcx                 ## 8-byte Reload
	movq	%rcx, %rax
	cmpq	%rsi, %rcx
	je	LBB38_18
LBB38_34:                               ## %"for relu1_0_d_def__.s30.n.ni.rebased.us.preheader"
                                        ##   in Loop: Header=BB38_6 Depth=1
	movq	168(%rsp), %rcx                 ## 8-byte Reload
	leaq	(%rcx,%r11,4), %rdx
	jmp	LBB38_15
	.p2align	4, 0x90
LBB38_17:                               ## %select.end211
                                        ##   in Loop: Header=BB38_15 Depth=2
	vaddss	(%rdx,%rax,4), %xmm2, %xmm0
	vmovss	%xmm0, (%rdx,%rax,4)
	incq	%rax
	cmpq	%rax, %rsi
	je	LBB38_18
LBB38_15:                               ## %"for relu1_0_d_def__.s30.n.ni.rebased.us"
                                        ##   Parent Loop BB38_6 Depth=1
                                        ## =>  This Inner Loop Header: Depth=2
	vxorps	%xmm2, %xmm2, %xmm2
	vucomiss	%xmm2, %xmm7
	vmulss	(%r12,%rax,4), %xmm5, %xmm0
	vmulss	%xmm1, %xmm0, %xmm0
	vmulss	%xmm3, %xmm0, %xmm0
	jne	LBB38_16
	jnp	LBB38_17
LBB38_16:                               ## %select.false.sink212
                                        ##   in Loop: Header=BB38_15 Depth=2
	vdivss	%xmm15, %xmm0, %xmm2
	jmp	LBB38_17
LBB38_2:                                ## %"for relu1_0_d_def__.s30.w.wi.preheader5"
	decl	%r15d
	vmovd	%r12d, %xmm0
	vpbroadcastd	%xmm0, %ymm0
	vpor	LCPI38_0(%rip), %ymm0, %ymm0
	vmovd	%r15d, %xmm1
	vpbroadcastd	%xmm1, %ymm1
	vpminsd	%ymm0, %ymm1, %ymm13
	movslq	64(%rsp), %rdx                  ## 4-byte Folded Reload
	movl	32(%rsp), %r15d                 ## 4-byte Reload
	imulq	%rdi, %r11
	addq	%r12, %r11
	leaq	(,%r11,4), %r12
	addq	%rbp, %r12
	shlq	$2, %rdi
	movq	-64(%rsp), %rcx                 ## 8-byte Reload
	movq	%rdi, %r11
	shll	$5, %esi
	shll	$5, %eax
	subl	%eax, %esi
	orl	$1, %esi
	imull	%esi, %ecx
	leal	(%rcx,%r10,4), %ebx
	vbroadcastss	LCPI38_1(%rip), %ymm0   ## ymm0 = [1.0E+0,1.0E+0,1.0E+0,1.0E+0,1.0E+0,1.0E+0,1.0E+0,1.0E+0]
	vmovups	%ymm0, 64(%rsp)                 ## 32-byte Spill
	vbroadcastss	LCPI38_2(%rip), %ymm0   ## ymm0 = [9.99999971E-10,9.99999971E-10,9.99999971E-10,9.99999971E-10,9.99999971E-10,9.99999971E-10,9.99999971E-10,9.99999971E-10]
	vmovups	%ymm0, 32(%rsp)                 ## 32-byte Spill
	movq	-80(%rsp), %r10                 ## 8-byte Reload
	movq	-72(%rsp), %rbp                 ## 8-byte Reload
	.p2align	4, 0x90
LBB38_3:                                ## %"for relu1_0_d_def__.s30.w.wi"
                                        ## =>This Inner Loop Header: Depth=1
	testq	%rdx, %rdx
	movl	$0, %esi
	cmovgl	%edx, %esi
	imull	-116(%rsp), %esi                ## 4-byte Folded Reload
	movq	-112(%rsp), %rax                ## 8-byte Reload
	addl	%esi, %eax
	vmovd	%eax, %xmm0
	vpbroadcastd	%xmm0, %ymm0
	vpaddd	%ymm0, %ymm13, %ymm0
	vmovd	%xmm0, %eax
	vpextrd	$1, %xmm0, %ecx
	vpextrd	$2, %xmm0, %edi
	cltq
	vmovss	(%rbp,%rax,4), %xmm3            ## xmm3 = mem[0],zero,zero,zero
	leal	(%rsi,%r8), %eax
	movslq	%ecx, %rcx
	vmovd	%eax, %xmm4
	vpextrd	$3, %xmm0, %eax
	movslq	%edi, %rdi
	cltq
	vextracti128	$1, %ymm0, %xmm5
	vinsertps	$16, (%rbp,%rcx,4), %xmm3, %xmm0 ## xmm0 = xmm3[0],mem[0],xmm3[2,3]
	vmovd	%xmm5, %ecx
	movslq	%ecx, %rcx
	vinsertps	$32, (%rbp,%rdi,4), %xmm0, %xmm0 ## xmm0 = xmm0[0,1],mem[0],xmm0[3]
	vpextrd	$1, %xmm5, %edi
	movslq	%edi, %rdi
	vmovss	(%rbp,%rcx,4), %xmm3            ## xmm3 = mem[0],zero,zero,zero
	vpbroadcastd	%xmm4, %ymm4
	vpaddd	%ymm4, %ymm13, %ymm4
	vinsertps	$48, (%rbp,%rax,4), %xmm0, %xmm14 ## xmm14 = xmm0[0,1,2],mem[0]
	vpextrd	$1, %xmm4, %eax
	vinsertps	$16, (%rbp,%rdi,4), %xmm3, %xmm3 ## xmm3 = xmm3[0],mem[0],xmm3[2,3]
	vmovd	%xmm4, %ecx
	vextracti128	$1, %ymm4, %xmm6
	vmovd	%xmm6, %edi
	movslq	%edi, %rdi
	vmovss	(%rbp,%rdi,4), %xmm7            ## xmm7 = mem[0],zero,zero,zero
	vpextrd	$1, %xmm6, %edi
	movslq	%ecx, %rcx
	cltq
	vmovss	(%rbp,%rcx,4), %xmm2            ## xmm2 = mem[0],zero,zero,zero
	vpextrd	$2, %xmm5, %ecx
	vinsertps	$16, (%rbp,%rax,4), %xmm2, %xmm2 ## xmm2 = xmm2[0],mem[0],xmm2[2,3]
	vpextrd	$3, %xmm5, %eax
	movslq	%ecx, %rcx
	movslq	%edi, %rdi
	vinsertps	$16, (%rbp,%rdi,4), %xmm7, %xmm5 ## xmm5 = xmm7[0],mem[0],xmm7[2,3]
	vpextrd	$2, %xmm4, %edi
	vinsertps	$32, (%rbp,%rcx,4), %xmm3, %xmm3 ## xmm3 = xmm3[0,1],mem[0],xmm3[3]
	vpextrd	$2, %xmm6, %ecx
	cltq
	movslq	%edi, %rdi
	movslq	%ecx, %rcx
	vinsertps	$32, (%rbp,%rcx,4), %xmm5, %xmm7 ## xmm7 = xmm5[0,1],mem[0],xmm5[3]
	vpextrd	$3, %xmm4, %ecx
	vinsertps	$48, (%rbp,%rax,4), %xmm3, %xmm15 ## xmm15 = xmm3[0,1,2],mem[0]
	movslq	%ecx, %rax
	vpextrd	$3, %xmm6, %ecx
	movslq	%ecx, %rcx
	vinsertps	$32, (%rbp,%rdi,4), %xmm2, %xmm2 ## xmm2 = xmm2[0,1],mem[0],xmm2[3]
	vinsertps	$48, (%rbp,%rcx,4), %xmm7, %xmm6 ## xmm6 = xmm7[0,1,2],mem[0]
	vinsertps	$48, (%rbp,%rax,4), %xmm2, %xmm7 ## xmm7 = xmm2[0,1,2],mem[0]
	leal	(%rsi,%r13), %eax
	vmovd	%eax, %xmm2
	vpbroadcastd	%xmm2, %ymm2
	vpaddd	%ymm2, %ymm13, %ymm3
	vpextrd	$1, %xmm3, %eax
	vmovd	%xmm3, %ecx
	vpextrd	$2, %xmm3, %edi
	movslq	%ecx, %rcx
	cltq
	vmovss	(%rbp,%rcx,4), %xmm2            ## xmm2 = mem[0],zero,zero,zero
	leal	(%rsi,%r14), %ecx
	movslq	%edi, %rdi
	vmovd	%ecx, %xmm4
	vpextrd	$3, %xmm3, %ecx
	movslq	%ecx, %rcx
	vextracti128	$1, %ymm3, %xmm3
	vpbroadcastd	%xmm4, %ymm4
	vinsertps	$16, (%rbp,%rax,4), %xmm2, %xmm2 ## xmm2 = xmm2[0],mem[0],xmm2[2,3]
	vpaddd	%ymm4, %ymm13, %ymm4
	vmovd	%xmm4, %eax
	vinsertps	$32, (%rbp,%rdi,4), %xmm2, %xmm2 ## xmm2 = xmm2[0,1],mem[0],xmm2[3]
	cltq
	vpextrd	$1, %xmm4, %edi
	movslq	%edi, %rdi
	vmovss	(%rbp,%rax,4), %xmm0            ## xmm0 = mem[0],zero,zero,zero
	vpextrd	$2, %xmm4, %eax
	cltq
	vinsertps	$16, (%rbp,%rdi,4), %xmm0, %xmm0 ## xmm0 = xmm0[0],mem[0],xmm0[2,3]
	vpextrd	$1, %xmm3, %edi
	vinsertps	$32, (%rbp,%rax,4), %xmm0, %xmm0 ## xmm0 = xmm0[0,1],mem[0],xmm0[3]
	vinsertps	$48, (%rbp,%rcx,4), %xmm2, %xmm9 ## xmm9 = xmm2[0,1,2],mem[0]
	vmovd	%xmm3, %eax
	vpextrd	$3, %xmm4, %ecx
	cltq
	vmovss	(%rbp,%rax,4), %xmm2            ## xmm2 = mem[0],zero,zero,zero
	movslq	%ecx, %rax
	vextracti128	$1, %ymm4, %xmm4
	movslq	%edi, %rcx
	vinsertps	$48, (%rbp,%rax,4), %xmm0, %xmm11 ## xmm11 = xmm0[0,1,2],mem[0]
	vinsertps	$16, (%rbp,%rcx,4), %xmm2, %xmm0 ## xmm0 = xmm2[0],mem[0],xmm2[2,3]
	vmovd	%xmm4, %eax
	vpextrd	$1, %xmm4, %ecx
	cltq
	vmovss	(%rbp,%rax,4), %xmm2            ## xmm2 = mem[0],zero,zero,zero
	movslq	%ecx, %rax
	vinsertps	$16, (%rbp,%rax,4), %xmm2, %xmm2 ## xmm2 = xmm2[0],mem[0],xmm2[2,3]
	addl	%r9d, %esi
	vmovd	%esi, %xmm5
	vpextrd	$2, %xmm3, %eax
	cltq
	vpbroadcastd	%xmm5, %ymm5
	vinsertps	$32, (%rbp,%rax,4), %xmm0, %xmm0 ## xmm0 = xmm0[0,1],mem[0],xmm0[3]
	vpaddd	%ymm5, %ymm13, %ymm5
	vextracti128	$1, %ymm5, %xmm8
	vmovd	%xmm8, %eax
	cltq
	vmovss	(%rbp,%rax,4), %xmm12           ## xmm12 = mem[0],zero,zero,zero
	vpextrd	$1, %xmm8, %eax
	cltq
	vinsertps	$16, (%rbp,%rax,4), %xmm12, %xmm1 ## xmm1 = xmm12[0],mem[0],xmm12[2,3]
	vpextrd	$2, %xmm4, %eax
	cltq
	vinsertps	$32, (%rbp,%rax,4), %xmm2, %xmm2 ## xmm2 = xmm2[0,1],mem[0],xmm2[3]
	vpextrd	$2, %xmm8, %eax
	cltq
	vinsertps	$32, (%rbp,%rax,4), %xmm1, %xmm12 ## xmm12 = xmm1[0,1],mem[0],xmm1[3]
	vmovd	%xmm5, %eax
	cltq
	vmovss	(%rbp,%rax,4), %xmm1            ## xmm1 = mem[0],zero,zero,zero
	vpextrd	$1, %xmm5, %eax
	cltq
	vinsertps	$16, (%rbp,%rax,4), %xmm1, %xmm1 ## xmm1 = xmm1[0],mem[0],xmm1[2,3]
	vpextrd	$3, %xmm3, %eax
	cltq
	vinsertps	$48, (%rbp,%rax,4), %xmm0, %xmm0 ## xmm0 = xmm0[0,1,2],mem[0]
	vpextrd	$2, %xmm5, %eax
	cltq
	vinsertps	$32, (%rbp,%rax,4), %xmm1, %xmm1 ## xmm1 = xmm1[0,1],mem[0],xmm1[3]
	vpextrd	$3, %xmm4, %eax
	cltq
	vinsertps	$48, (%rbp,%rax,4), %xmm2, %xmm2 ## xmm2 = xmm2[0,1,2],mem[0]
	vpextrd	$3, %xmm5, %eax
	cltq
	vinsertps	$48, (%rbp,%rax,4), %xmm1, %xmm1 ## xmm1 = xmm1[0,1,2],mem[0]
	vpextrd	$3, %xmm8, %eax
	cltq
	vinsertps	$48, (%rbp,%rax,4), %xmm12, %xmm3 ## xmm3 = xmm12[0,1,2],mem[0]
	vinsertf128	$1, %xmm15, %ymm14, %ymm4
	vinsertf128	$1, %xmm6, %ymm7, %ymm5
	vmulps	%ymm5, %ymm4, %ymm4
	vmovups	64(%rsp), %ymm6                 ## 32-byte Reload
	vmaxps	%ymm6, %ymm4, %ymm4
	vdivps	%ymm10, %ymm4, %ymm4
	vinsertf128	$1, %xmm0, %ymm9, %ymm0
	movslq	%ebx, %rbx
	vinsertf128	$1, %xmm2, %ymm11, %ymm2
	vroundps	$2, %ymm4, %ymm5
	vmulps	(%r12), %ymm5, %ymm5
	vinsertf128	$1, %xmm3, %ymm1, %ymm1
	vmaxps	%ymm6, %ymm4, %ymm3
	vmulps	%ymm1, %ymm2, %ymm1
	vmulps	32(%rsp), %ymm1, %ymm1          ## 32-byte Folded Reload
	vmulps	%ymm1, %ymm5, %ymm1
	vcmpneqps	LCPI38_3(%rip), %ymm0, %ymm0
	vdivps	%ymm3, %ymm1, %ymm1
	vandps	%ymm1, %ymm0, %ymm0
	vaddps	(%r10,%rbx,8), %ymm0, %ymm0
	vmovups	%ymm0, (%r10,%rbx,8)
	addq	%r11, %r12
	incq	%rdx
	addl	-100(%rsp), %ebx                ## 4-byte Folded Reload
	decq	%r15
	jne	LBB38_3
LBB38_4:                                ## %destructor_block
	xorl	%eax, %eax
	addq	$264, %rsp                      ## imm = 0x108
	popq	%rbx
	popq	%r12
	popq	%r13
	popq	%r14
	popq	%r15
	popq	%rbp
	vzeroupper
	retq
                                        ## -- End function
	.section	__TEXT,__const
	.p2align	5                               ## -- Begin function train_cost_model.par_for.relu1_0_d_def__.s31.n.n.n
LCPI39_0:
	.long	0                               ## 0x0
	.long	1                               ## 0x1
	.long	2                               ## 0x2
	.long	3                               ## 0x3
	.long	4                               ## 0x4
	.long	5                               ## 0x5
	.long	6                               ## 0x6
	.long	7                               ## 0x7
	.section	__TEXT,__literal4,4byte_literals
	.p2align	2
LCPI39_1:
	.long	0x3f800000                      ## float 1
LCPI39_2:
	.long	0x3089705f                      ## float 9.99999971E-10
	.section	__TEXT,__text,regular,pure_instructions
	.p2align	4, 0x90
_train_cost_model.par_for.relu1_0_d_def__.s31.n.n.n: ## @train_cost_model.par_for.relu1_0_d_def__.s31.n.n.n
## %bb.0:                               ## %entry
	pushq	%rbp
	pushq	%r15
	pushq	%r14
	pushq	%r13
	pushq	%r12
	pushq	%rbx
	subq	$176, %rsp
	movq	%rdx, %rdi
	movl	12(%rdx), %r11d
	movl	24(%rdx), %r9d
	movl	%esi, %r8d
	sarl	$31, %r8d
	xorl	%ecx, %ecx
	testl	%r9d, %r9d
	sete	%cl
	movl	%ecx, %ebx
	negl	%ebx
	movl	%r9d, %ebp
	sarl	$31, %ebp
	subl	%r8d, %esi
	orl	%r9d, %ebx
	movl	%esi, %eax
	cltd
	idivl	%ebx
	movl	%edx, %ebx
	movl	%ebp, %r10d
	leal	(%r9,%rcx), %r14d
	movl	%esi, %eax
	cltd
	idivl	%r14d
	notl	%r10d
	decl	%ecx
	movl	%r10d, %r15d
	subl	%ebp, %r15d
	andl	%r8d, %r15d
	addl	%eax, %r15d
	andl	%ecx, %r15d
	leal	(%r15,%r15), %eax
	movl	%eax, -96(%rsp)                 ## 4-byte Spill
	subl	%eax, %r11d
	cmpl	$2, %r11d
	movl	$2, %edx
	cmovll	%r11d, %edx
	xorl	%eax, %eax
	testl	%r11d, %r11d
	cmovlel	%eax, %edx
	movl	%edx, -124(%rsp)                ## 4-byte Spill
	jle	LBB39_4
## %bb.1:                               ## %"for relu1_0_d_def__.s31.w.wi.preheader"
	movl	(%rdi), %eax
	movl	16(%rdi), %r14d
	movl	32(%rdi), %r13d
	movl	40(%rdi), %esi
	movl	44(%rdi), %edx
	movl	48(%rdi), %r11d
	movl	52(%rdi), %r12d
	xorl	%r9d, %ebp
	addl	%r10d, %ebp
	andl	%r8d, %ebp
	addl	%ebp, %ebx
	andl	%ecx, %ebx
	movq	%rbx, -120(%rsp)                ## 8-byte Spill
	leal	(,%rbx,8), %ecx
	movl	%esi, -104(%rsp)                ## 4-byte Spill
	movl	%esi, %r8d
	subl	%r13d, %r8d
	movl	%edx, -112(%rsp)                ## 4-byte Spill
	movl	%edx, %r9d
	subl	%r13d, %r9d
	movl	%r11d, -40(%rsp)                ## 4-byte Spill
	movl	%r11d, %r10d
	subl	%r13d, %r10d
	movl	%r12d, -48(%rsp)                ## 4-byte Spill
	movl	%r12d, %r11d
	subl	%r13d, %r11d
	movslq	8(%rdi), %rsi
	movslq	%r15d, %rbp
	subq	%rsi, %rbp
	addq	%rbp, %rbp
	movq	%r14, -72(%rsp)                 ## 8-byte Spill
	movl	%r14d, %esi
	shll	$5, %esi
	movl	%esi, -76(%rsp)                 ## 4-byte Spill
	movl	%eax, %ebx
	subl	%ecx, %ebx
	cmpl	$8, %ebx
	movl	$8, %esi
	cmovll	%ebx, %esi
	movl	%ebx, -20(%rsp)                 ## 4-byte Spill
	testl	%ebx, %ebx
	movl	$0, %edx
	cmovgl	%esi, %edx
	leal	8(%rcx), %esi
	movslq	%ecx, %rbx
	movq	%rbx, -64(%rsp)                 ## 8-byte Spill
	subl	%eax, %ecx
	movl	%ecx, %ebx
	sarl	$31, %ebx
	andl	%ecx, %ebx
	cmpl	$-8, %ebx
	movl	$-8, %r12d
	cmovgl	%ebx, %r12d
	vmovss	36(%rdi), %xmm15                ## xmm15 = mem[0],zero,zero,zero
	movl	20(%rdi), %ecx
	movl	%ecx, -108(%rsp)                ## 4-byte Spill
	movq	56(%rdi), %rcx
	movq	%rcx, -32(%rsp)                 ## 8-byte Spill
	movq	72(%rdi), %rcx
	movq	%rcx, -88(%rsp)                 ## 8-byte Spill
	movq	88(%rdi), %r14
	movl	28(%rdi), %ecx
	movslq	4(%rdi), %rdi
	movq	%rdi, -56(%rsp)                 ## 8-byte Spill
	vbroadcastss	%xmm15, %ymm10
	cmpl	%eax, %esi
	jle	LBB39_2
## %bb.5:                               ## %"for relu1_0_d_def__.s31.w.wi.us.preheader"
	subl	%r13d, %eax
	movq	%rdx, %r13
	movl	%r15d, %edx
	subl	%ecx, %edx
	movl	-104(%rsp), %edi                ## 4-byte Reload
	addl	%eax, %edi
	movl	-112(%rsp), %ebx                ## 4-byte Reload
	addl	%eax, %ebx
	movl	-40(%rsp), %esi                 ## 4-byte Reload
	addl	%eax, %esi
	addl	-48(%rsp), %eax                 ## 4-byte Folded Reload
	addl	%edx, %edx
	movq	%rdx, 24(%rsp)                  ## 8-byte Spill
	movl	%ecx, -104(%rsp)                ## 4-byte Spill
	movq	-64(%rsp), %rcx                 ## 8-byte Reload
	addl	%ecx, %r11d
	addl	%ecx, %r10d
	addl	%ecx, %r9d
	addl	%ecx, %r8d
	addl	%r13d, %r12d
	movslq	%r8d, %rdx
	movq	%rdx, 8(%rsp)                   ## 8-byte Spill
	movslq	%r9d, %rdx
	movq	%rdx, (%rsp)                    ## 8-byte Spill
	movslq	%r10d, %rdx
	movq	%rdx, -8(%rsp)                  ## 8-byte Spill
	movslq	%r11d, %r11
	movslq	%edi, %rdx
	movq	%rdx, -40(%rsp)                 ## 8-byte Spill
	movslq	%ebx, %rdx
	movq	%rdx, -48(%rsp)                 ## 8-byte Spill
	movslq	%esi, %rdx
	movq	%rdx, 168(%rsp)                 ## 8-byte Spill
	cltq
	movq	%rax, 160(%rsp)                 ## 8-byte Spill
	leal	(%rcx,%r13), %eax
	movslq	%eax, %r9
	shll	$6, %r15d
	movl	-104(%rsp), %eax                ## 4-byte Reload
	shll	$6, %eax
	subl	%eax, %r15d
	movl	%r12d, -112(%rsp)               ## 4-byte Spill
	movl	%r12d, %r12d
	movslq	-96(%rsp), %rdi                 ## 4-byte Folded Reload
	movl	-124(%rsp), %eax                ## 4-byte Reload
	movq	%rax, 144(%rsp)                 ## 8-byte Spill
	movl	%r12d, %eax
	andl	$-8, %eax
	movq	%rax, 72(%rsp)                  ## 8-byte Spill
	addq	$-8, %rax
	movq	%rax, 64(%rsp)                  ## 8-byte Spill
	movq	%rax, %rsi
	shrq	$3, %rsi
	incq	%rsi
	movq	%r13, -104(%rsp)                ## 8-byte Spill
	movl	%r13d, %r13d
	movl	%r13d, %eax
	andl	$2147483640, %eax               ## imm = 0x7FFFFFF8
	movq	%rax, 48(%rsp)                  ## 8-byte Spill
	movq	-88(%rsp), %rdx                 ## 8-byte Reload
	movq	-56(%rsp), %rbx                 ## 8-byte Reload
	movq	%rbx, %rax
	movq	%rbp, -16(%rsp)                 ## 8-byte Spill
	imulq	%rbp, %rax
	addq	%rax, %rcx
	movq	-32(%rsp), %r8                  ## 8-byte Reload
	leaq	(%r8,%rcx,4), %r10
	orl	$1, %r15d
	imull	-72(%rsp), %r15d                ## 4-byte Folded Reload
	movq	-120(%rsp), %rbp                ## 8-byte Reload
	leal	(%r15,%rbp,8), %ebp
	movl	%ebp, -124(%rsp)                ## 4-byte Spill
	addq	%r9, %rax
	movq	%rsi, %rbp
	movq	%rsi, 56(%rsp)                  ## 8-byte Spill
	andq	$-2, %rsi
	negq	%rsi
	movq	%rsi, 40(%rsp)                  ## 8-byte Spill
	leaq	(%r8,%rax,4), %rsi
	addq	$32, %rsi
	movq	%rsi, -120(%rsp)                ## 8-byte Spill
	leaq	(%r8,%rax,4), %r8
	vmovss	LCPI39_1(%rip), %xmm11          ## xmm11 = mem[0],zero,zero,zero
	vmovss	LCPI39_2(%rip), %xmm3           ## xmm3 = mem[0],zero,zero,zero
	vbroadcastss	LCPI39_1(%rip), %ymm4   ## ymm4 = [1.0E+0,1.0E+0,1.0E+0,1.0E+0,1.0E+0,1.0E+0,1.0E+0,1.0E+0]
	vxorps	%xmm9, %xmm9, %xmm9
	vbroadcastss	LCPI39_2(%rip), %ymm8   ## ymm8 = [9.99999971E-10,9.99999971E-10,9.99999971E-10,9.99999971E-10,9.99999971E-10,9.99999971E-10,9.99999971E-10,9.99999971E-10]
	leaq	(,%rbx,4), %rax
	movq	%rax, 136(%rsp)                 ## 8-byte Spill
	movq	%r11, 80(%rsp)                  ## 8-byte Spill
	leaq	(%r14,%r11,4), %rax
	movq	%rax, 120(%rsp)                 ## 8-byte Spill
	movq	-8(%rsp), %rax                  ## 8-byte Reload
	leaq	(%r14,%rax,4), %rax
	movq	%rax, 112(%rsp)                 ## 8-byte Spill
	movq	(%rsp), %rax                    ## 8-byte Reload
	leaq	(%r14,%rax,4), %rax
	movq	%rax, 104(%rsp)                 ## 8-byte Spill
	movq	8(%rsp), %rax                   ## 8-byte Reload
	leaq	(%r14,%rax,4), %rax
	movq	%rax, 96(%rsp)                  ## 8-byte Spill
	leaq	32(%rdx), %rax
	movq	%rax, 32(%rsp)                  ## 8-byte Spill
	movq	%r9, 16(%rsp)                   ## 8-byte Spill
	leaq	(%rdx,%r9,4), %rax
	movq	%rax, 88(%rsp)                  ## 8-byte Spill
	movq	%rdi, %rax
	movq	%rdi, 152(%rsp)                 ## 8-byte Spill
	xorl	%esi, %esi
	movq	%r15, %rbx
	jmp	LBB39_6
	.p2align	4, 0x90
LBB39_18:                               ## %after_bb.us
                                        ##   in Loop: Header=BB39_6 Depth=1
	movq	-96(%rsp), %rsi                 ## 8-byte Reload
	incq	%rsi
	movq	136(%rsp), %rax                 ## 8-byte Reload
	addq	%rax, %r10
	movq	-64(%rsp), %rdi                 ## 8-byte Reload
	incq	%rdi
	addl	%r11d, -124(%rsp)               ## 4-byte Folded Spill
	addq	%rax, %rcx
	movq	%rcx, -120(%rsp)                ## 8-byte Spill
	addl	%r11d, %ebx
	addq	%rax, %r8
	cmpq	144(%rsp), %rsi                 ## 8-byte Folded Reload
	je	LBB39_4
LBB39_6:                                ## %"for relu1_0_d_def__.s31.w.wi.us"
                                        ## =>This Loop Header: Depth=1
                                        ##     Child Loop BB39_37 Depth 2
                                        ##     Child Loop BB39_9 Depth 2
                                        ##     Child Loop BB39_22 Depth 2
                                        ##     Child Loop BB39_15 Depth 2
	testq	%rdi, %rdi
	movl	$0, %eax
	movq	%rdi, -64(%rsp)                 ## 8-byte Spill
	cmovgl	%edi, %eax
	movq	152(%rsp), %rcx                 ## 8-byte Reload
	movq	%rsi, -96(%rsp)                 ## 8-byte Spill
	leaq	(%rsi,%rcx), %rdi
	testq	%rdi, %rdi
	movl	$0, %ecx
	cmovlel	%ecx, %edi
	cmpl	$0, -20(%rsp)                   ## 4-byte Folded Reload
	jle	LBB39_12
## %bb.7:                               ## %"for relu1_0_d_def__.s31.n.ni.preheader.us"
                                        ##   in Loop: Header=BB39_6 Depth=1
	imull	-108(%rsp), %eax                ## 4-byte Folded Reload
	movslq	%eax, %r9
	movslq	-124(%rsp), %rax                ## 4-byte Folded Reload
	movq	-88(%rsp), %rcx                 ## 8-byte Reload
	leaq	(%rcx,%rax,4), %rax
	cmpl	$8, -104(%rsp)                  ## 4-byte Folded Reload
	jae	LBB39_36
## %bb.8:                               ##   in Loop: Header=BB39_6 Depth=1
	xorl	%ebp, %ebp
	jmp	LBB39_39
	.p2align	4, 0x90
LBB39_36:                               ## %vector.body28.preheader
                                        ##   in Loop: Header=BB39_6 Depth=1
	movq	%rdi, 128(%rsp)                 ## 8-byte Spill
	movq	%rbx, %r15
	movq	80(%rsp), %rcx                  ## 8-byte Reload
	addq	%r9, %rcx
	movq	-8(%rsp), %rdx                  ## 8-byte Reload
	leaq	(%rdx,%r9), %rdi
	movq	(%rsp), %rdx                    ## 8-byte Reload
	leaq	(%rdx,%r9), %rbx
	movq	8(%rsp), %rdx                   ## 8-byte Reload
	leaq	(%rdx,%r9), %r11
	leaq	(%r14,%rcx,4), %rdx
	leaq	(%r14,%rdi,4), %rbp
	leaq	(%r14,%rbx,4), %rdi
	leaq	(%r14,%r11,4), %rbx
	xorl	%ecx, %ecx
	movq	48(%rsp), %rsi                  ## 8-byte Reload
	.p2align	4, 0x90
LBB39_37:                               ## %vector.body28
                                        ##   Parent Loop BB39_6 Depth=1
                                        ## =>  This Inner Loop Header: Depth=2
	vmovups	(%rbx,%rcx,4), %ymm0
	vmulps	(%rdi,%rcx,4), %ymm0, %ymm0
	vmaxps	%ymm4, %ymm0, %ymm0
	vdivps	%ymm10, %ymm0, %ymm0
	vcmpeqps	(%rbp,%rcx,4), %ymm9, %ymm1
	vroundps	$10, %ymm0, %ymm2
	vmaxps	%ymm4, %ymm0, %ymm0
	vmulps	(%r10,%rcx,4), %ymm2, %ymm2
	vmulps	(%rdx,%rcx,4), %ymm2, %ymm2
	vmulps	%ymm2, %ymm8, %ymm2
	vdivps	%ymm0, %ymm2, %ymm0
	vandps	%ymm0, %ymm1, %ymm0
	vaddps	(%rax,%rcx,4), %ymm0, %ymm0
	vmovups	%ymm0, (%rax,%rcx,4)
	addq	$8, %rcx
	cmpq	%rcx, %rsi
	jne	LBB39_37
## %bb.38:                              ## %middle.block26
                                        ##   in Loop: Header=BB39_6 Depth=1
	movq	%rsi, %rbp
	cmpq	%r13, %rsi
	movq	%r15, %rbx
	movq	128(%rsp), %rdi                 ## 8-byte Reload
	jne	LBB39_39
LBB39_12:                               ## %"end for relu1_0_d_def__.s31.n.ni.us"
                                        ##   in Loop: Header=BB39_6 Depth=1
	movl	-112(%rsp), %ebp                ## 4-byte Reload
	testl	%ebp, %ebp
	movl	-76(%rsp), %r11d                ## 4-byte Reload
	movq	-32(%rsp), %r15                 ## 8-byte Reload
	movq	-120(%rsp), %rcx                ## 8-byte Reload
	jle	LBB39_18
## %bb.13:                              ## %"for relu1_0_d_def__.s31.n.ni.rebased.preheader.us"
                                        ##   in Loop: Header=BB39_6 Depth=1
	movslq	%ebx, %rax
	imull	-108(%rsp), %edi                ## 4-byte Folded Reload
	movslq	%edi, %rcx
	movq	-40(%rsp), %rdx                 ## 8-byte Reload
	addq	%rcx, %rdx
	movq	-48(%rsp), %rsi                 ## 8-byte Reload
	addq	%rcx, %rsi
	movq	168(%rsp), %rdi                 ## 8-byte Reload
	addq	%rcx, %rdi
	vmovss	-4(%r14,%rdx,4), %xmm0          ## xmm0 = mem[0],zero,zero,zero
	vmulss	-4(%r14,%rsi,4), %xmm0, %xmm0
	addq	160(%rsp), %rcx                 ## 8-byte Folded Reload
	vmaxss	%xmm11, %xmm0, %xmm0
	vdivss	%xmm15, %xmm0, %xmm1
	vmovss	-4(%r14,%rdi,4), %xmm7          ## xmm7 = mem[0],zero,zero,zero
	vmovss	-4(%r14,%rcx,4), %xmm5          ## xmm5 = mem[0],zero,zero,zero
	vroundss	$10, %xmm1, %xmm1, %xmm0
	vmaxss	%xmm11, %xmm1, %xmm2
	cmpl	$8, %ebp
	jae	LBB39_19
## %bb.14:                              ##   in Loop: Header=BB39_6 Depth=1
	xorl	%edx, %edx
	movq	-120(%rsp), %rcx                ## 8-byte Reload
	jmp	LBB39_35
	.p2align	4, 0x90
LBB39_39:                               ## %"for relu1_0_d_def__.s31.n.ni.us.preheader"
                                        ##   in Loop: Header=BB39_6 Depth=1
	movq	120(%rsp), %rcx                 ## 8-byte Reload
	leaq	(%rcx,%r9,4), %rdx
	movq	112(%rsp), %rcx                 ## 8-byte Reload
	leaq	(%rcx,%r9,4), %r11
	movq	104(%rsp), %rcx                 ## 8-byte Reload
	leaq	(%rcx,%r9,4), %r15
	movq	96(%rsp), %rcx                  ## 8-byte Reload
	leaq	(%rcx,%r9,4), %rsi
	jmp	LBB39_9
	.p2align	4, 0x90
LBB39_11:                               ## %select.end
                                        ##   in Loop: Header=BB39_9 Depth=2
	vaddss	(%rax,%rbp,4), %xmm0, %xmm0
	vmovss	%xmm0, (%rax,%rbp,4)
	incq	%rbp
	cmpq	%rbp, %r13
	je	LBB39_12
LBB39_9:                                ## %"for relu1_0_d_def__.s31.n.ni.us"
                                        ##   Parent Loop BB39_6 Depth=1
                                        ## =>  This Inner Loop Header: Depth=2
	vmovss	(%rsi,%rbp,4), %xmm0            ## xmm0 = mem[0],zero,zero,zero
	vmulss	(%r15,%rbp,4), %xmm0, %xmm0
	vmaxss	%xmm11, %xmm0, %xmm0
	vdivss	%xmm15, %xmm0, %xmm2
	vxorps	%xmm0, %xmm0, %xmm0
	vucomiss	(%r11,%rbp,4), %xmm0
	sete	%cl
	vroundss	$10, %xmm2, %xmm2, %xmm1
	vmulss	(%r10,%rbp,4), %xmm1, %xmm1
	vmulss	(%rdx,%rbp,4), %xmm1, %xmm1
	vmulss	%xmm3, %xmm1, %xmm1
	testb	$1, %cl
	je	LBB39_11
## %bb.10:                              ## %select.true.sink
                                        ##   in Loop: Header=BB39_9 Depth=2
	vmaxss	%xmm11, %xmm2, %xmm0
	vdivss	%xmm0, %xmm1, %xmm0
	jmp	LBB39_11
	.p2align	4, 0x90
LBB39_19:                               ## %vector.ph
                                        ##   in Loop: Header=BB39_6 Depth=1
	vbroadcastss	%xmm2, %ymm12
	cmpq	$0, 64(%rsp)                    ## 8-byte Folded Reload
	vxorps	%xmm6, %xmm6, %xmm6
	movq	%rbx, %rdi
	je	LBB39_20
## %bb.21:                              ## %vector.ph.new
                                        ##   in Loop: Header=BB39_6 Depth=1
	movq	16(%rsp), %rcx                  ## 8-byte Reload
	addq	%rax, %rcx
	movq	32(%rsp), %rdx                  ## 8-byte Reload
	leaq	(%rdx,%rcx,4), %rsi
	vmulss	%xmm0, %xmm5, %xmm1
	vbroadcastss	%xmm1, %ymm13
	movq	40(%rsp), %rbp                  ## 8-byte Reload
	xorl	%ebx, %ebx
	movq	-72(%rsp), %rdx                 ## 8-byte Reload
	movq	-16(%rsp), %r9                  ## 8-byte Reload
	movq	-120(%rsp), %rcx                ## 8-byte Reload
	jmp	LBB39_22
	.p2align	4, 0x90
LBB39_28:                               ## %vector.body
                                        ##   in Loop: Header=BB39_22 Depth=2
	vaddps	(%rsi,%rbx,4), %ymm14, %ymm1
	vmovups	%ymm1, (%rsi,%rbx,4)
	addq	$16, %rbx
	addq	$2, %rbp
	je	LBB39_29
LBB39_22:                               ## %vector.body
                                        ##   Parent Loop BB39_6 Depth=1
                                        ## =>  This Inner Loop Header: Depth=2
	vucomiss	%xmm6, %xmm7
	vxorps	%xmm14, %xmm14, %xmm14
	vxorps	%xmm1, %xmm1, %xmm1
	jne	LBB39_25
## %bb.23:                              ## %vector.body
                                        ##   in Loop: Header=BB39_22 Depth=2
	vxorps	%xmm1, %xmm1, %xmm1
	jp	LBB39_25
## %bb.24:                              ## %vector.body
                                        ##   in Loop: Header=BB39_22 Depth=2
	vmulps	-32(%rcx,%rbx,4), %ymm13, %ymm1
	vmulps	%ymm1, %ymm8, %ymm1
	vdivps	%ymm12, %ymm1, %ymm1
LBB39_25:                               ## %vector.body
                                        ##   in Loop: Header=BB39_22 Depth=2
	vaddps	-32(%rsi,%rbx,4), %ymm1, %ymm1
	vmovups	%ymm1, -32(%rsi,%rbx,4)
	jne	LBB39_28
## %bb.26:                              ## %vector.body
                                        ##   in Loop: Header=BB39_22 Depth=2
	jp	LBB39_28
## %bb.27:                              ## %vector.body
                                        ##   in Loop: Header=BB39_22 Depth=2
	vmulps	(%rcx,%rbx,4), %ymm13, %ymm1
	vmulps	%ymm1, %ymm8, %ymm1
	vdivps	%ymm12, %ymm1, %ymm14
	jmp	LBB39_28
LBB39_20:                               ##   in Loop: Header=BB39_6 Depth=1
	xorl	%ebx, %ebx
	movq	-72(%rsp), %rdx                 ## 8-byte Reload
	movq	-16(%rsp), %r9                  ## 8-byte Reload
LBB39_29:                               ## %middle.block.unr-lcssa
                                        ##   in Loop: Header=BB39_6 Depth=1
	testb	$1, 56(%rsp)                    ## 1-byte Folded Reload
	je	LBB39_34
## %bb.30:                              ## %vector.body.epil
                                        ##   in Loop: Header=BB39_6 Depth=1
	movq	24(%rsp), %rcx                  ## 8-byte Reload
	movq	-96(%rsp), %rsi                 ## 8-byte Reload
	addl	%esi, %ecx
	imull	%r11d, %ecx
	addl	%edx, %ecx
	movslq	%ecx, %rdx
	addq	16(%rsp), %rbx                  ## 8-byte Folded Reload
	addq	%rbx, %rdx
	vucomiss	%xmm6, %xmm7
	jne	LBB39_31
	jp	LBB39_31
## %bb.32:                              ## %select.true.sink143
                                        ##   in Loop: Header=BB39_6 Depth=1
	leaq	(%rsi,%r9), %rcx
	imulq	-56(%rsp), %rcx                 ## 8-byte Folded Reload
	addq	%rcx, %rbx
	vmulss	%xmm0, %xmm5, %xmm1
	vbroadcastss	%xmm1, %ymm1
	vmulps	(%r15,%rbx,4), %ymm1, %ymm1
	vmulps	%ymm1, %ymm8, %ymm1
	vdivps	%ymm12, %ymm1, %ymm1
	jmp	LBB39_33
LBB39_31:                               ##   in Loop: Header=BB39_6 Depth=1
	vxorps	%xmm1, %xmm1, %xmm1
LBB39_33:                               ## %select.end142
                                        ##   in Loop: Header=BB39_6 Depth=1
	movq	-88(%rsp), %rcx                 ## 8-byte Reload
	vaddps	(%rcx,%rdx,4), %ymm1, %ymm1
	vmovups	%ymm1, (%rcx,%rdx,4)
LBB39_34:                               ## %middle.block
                                        ##   in Loop: Header=BB39_6 Depth=1
	movq	72(%rsp), %rcx                  ## 8-byte Reload
	movq	%rcx, %rdx
	cmpq	%r12, %rcx
	movq	%rdi, %rbx
	movq	-120(%rsp), %rcx                ## 8-byte Reload
	je	LBB39_18
LBB39_35:                               ## %"for relu1_0_d_def__.s31.n.ni.rebased.us.preheader"
                                        ##   in Loop: Header=BB39_6 Depth=1
	movq	88(%rsp), %rsi                  ## 8-byte Reload
	leaq	(%rsi,%rax,4), %rax
	jmp	LBB39_15
	.p2align	4, 0x90
LBB39_17:                               ## %select.end190
                                        ##   in Loop: Header=BB39_15 Depth=2
	vaddss	(%rax,%rdx,4), %xmm1, %xmm1
	vmovss	%xmm1, (%rax,%rdx,4)
	incq	%rdx
	cmpq	%rdx, %r12
	je	LBB39_18
LBB39_15:                               ## %"for relu1_0_d_def__.s31.n.ni.rebased.us"
                                        ##   Parent Loop BB39_6 Depth=1
                                        ## =>  This Inner Loop Header: Depth=2
	vxorps	%xmm1, %xmm1, %xmm1
	vucomiss	%xmm1, %xmm7
	vmulss	(%r8,%rdx,4), %xmm0, %xmm6
	vmulss	%xmm6, %xmm5, %xmm6
	vmulss	%xmm3, %xmm6, %xmm6
	jne	LBB39_17
	jp	LBB39_17
## %bb.16:                              ## %select.true.sink191
                                        ##   in Loop: Header=BB39_15 Depth=2
	vdivss	%xmm2, %xmm6, %xmm1
	jmp	LBB39_17
LBB39_2:                                ## %"for relu1_0_d_def__.s31.w.wi.preheader5"
	decl	%eax
	movl	%ecx, %edx
	movq	-64(%rsp), %rcx                 ## 8-byte Reload
	vmovd	%ecx, %xmm0
	vpbroadcastd	%xmm0, %ymm0
	vpor	LCPI39_0(%rip), %ymm0, %ymm0
	vmovd	%eax, %xmm1
	vpbroadcastd	%xmm1, %ymm1
	vpminsd	%ymm0, %ymm1, %ymm1
	movslq	-96(%rsp), %rdi                 ## 4-byte Folded Reload
	movq	-56(%rsp), %r13                 ## 8-byte Reload
	movl	-124(%rsp), %ebx                ## 4-byte Reload
	imulq	%r13, %rbp
	addq	%rcx, %rbp
	movq	-32(%rsp), %rax                 ## 8-byte Reload
	leaq	(%rax,%rbp,4), %rbp
	shlq	$2, %r13
	shll	$6, %r15d
	shll	$6, %edx
	subl	%edx, %r15d
	orl	$1, %r15d
	movq	-72(%rsp), %rcx                 ## 8-byte Reload
	imull	%r15d, %ecx
	movq	-120(%rsp), %rax                ## 8-byte Reload
	leal	(%rcx,%rax,8), %edx
	vbroadcastss	LCPI39_1(%rip), %ymm12  ## ymm12 = [1.0E+0,1.0E+0,1.0E+0,1.0E+0,1.0E+0,1.0E+0,1.0E+0,1.0E+0]
	vxorps	%xmm9, %xmm9, %xmm9
	vbroadcastss	LCPI39_2(%rip), %ymm11  ## ymm11 = [9.99999971E-10,9.99999971E-10,9.99999971E-10,9.99999971E-10,9.99999971E-10,9.99999971E-10,9.99999971E-10,9.99999971E-10]
	movl	-76(%rsp), %r15d                ## 4-byte Reload
	.p2align	4, 0x90
LBB39_3:                                ## %"for relu1_0_d_def__.s31.w.wi"
                                        ## =>This Inner Loop Header: Depth=1
	testq	%rdi, %rdi
	movl	$0, %eax
	cmovgl	%edi, %eax
	imull	-108(%rsp), %eax                ## 4-byte Folded Reload
	leal	(%rax,%r8), %ecx
	vmovd	%ecx, %xmm0
	vpbroadcastd	%xmm0, %ymm0
	vpaddd	%ymm1, %ymm0, %ymm0
	vmovd	%xmm0, %ecx
	movslq	%ecx, %rcx
	vpextrd	$1, %xmm0, %esi
	movslq	%esi, %rsi
	vmovss	(%r14,%rcx,4), %xmm5            ## xmm5 = mem[0],zero,zero,zero
	vpextrd	$2, %xmm0, %ecx
	vinsertps	$16, (%r14,%rsi,4), %xmm5, %xmm6 ## xmm6 = xmm5[0],mem[0],xmm5[2,3]
	vpextrd	$3, %xmm0, %esi
	movslq	%ecx, %rcx
	movslq	%esi, %rsi
	vextracti128	$1, %ymm0, %xmm5
	vinsertps	$32, (%r14,%rcx,4), %xmm6, %xmm0 ## xmm0 = xmm6[0,1],mem[0],xmm6[3]
	vmovd	%xmm5, %ecx
	movslq	%ecx, %rcx
	vinsertps	$48, (%r14,%rsi,4), %xmm0, %xmm13 ## xmm13 = xmm0[0,1,2],mem[0]
	vmovss	(%r14,%rcx,4), %xmm6            ## xmm6 = mem[0],zero,zero,zero
	leal	(%rax,%r9), %ecx
	vpextrd	$1, %xmm5, %esi
	vmovd	%ecx, %xmm7
	vpextrd	$2, %xmm5, %ecx
	movslq	%esi, %rsi
	vpbroadcastd	%xmm7, %ymm7
	vpaddd	%ymm1, %ymm7, %ymm7
	vinsertps	$16, (%r14,%rsi,4), %xmm6, %xmm6 ## xmm6 = xmm6[0],mem[0],xmm6[2,3]
	vextracti128	$1, %ymm7, %xmm3
	vmovd	%xmm3, %esi
	movslq	%esi, %rsi
	movslq	%ecx, %rcx
	vmovss	(%r14,%rsi,4), %xmm4            ## xmm4 = mem[0],zero,zero,zero
	vpextrd	$1, %xmm3, %esi
	movslq	%esi, %rsi
	vinsertps	$16, (%r14,%rsi,4), %xmm4, %xmm4 ## xmm4 = xmm4[0],mem[0],xmm4[2,3]
	vinsertps	$32, (%r14,%rcx,4), %xmm6, %xmm6 ## xmm6 = xmm6[0,1],mem[0],xmm6[3]
	vmovd	%xmm7, %ecx
	vpextrd	$1, %xmm7, %esi
	movslq	%ecx, %rcx
	movslq	%esi, %rsi
	vmovss	(%r14,%rcx,4), %xmm2            ## xmm2 = mem[0],zero,zero,zero
	vpextrd	$3, %xmm5, %ecx
	vinsertps	$16, (%r14,%rsi,4), %xmm2, %xmm2 ## xmm2 = xmm2[0],mem[0],xmm2[2,3]
	vpextrd	$2, %xmm3, %esi
	movslq	%ecx, %rcx
	movslq	%esi, %rsi
	vinsertps	$32, (%r14,%rsi,4), %xmm4, %xmm4 ## xmm4 = xmm4[0,1],mem[0],xmm4[3]
	vpextrd	$2, %xmm7, %esi
	vinsertps	$48, (%r14,%rcx,4), %xmm6, %xmm5 ## xmm5 = xmm6[0,1,2],mem[0]
	vpextrd	$3, %xmm7, %ecx
	movslq	%esi, %rsi
	vinsertps	$32, (%r14,%rsi,4), %xmm2, %xmm2 ## xmm2 = xmm2[0,1],mem[0],xmm2[3]
	vpextrd	$3, %xmm3, %esi
	movslq	%ecx, %rcx
	movslq	%esi, %rsi
	vinsertps	$48, (%r14,%rsi,4), %xmm4, %xmm6 ## xmm6 = xmm4[0,1,2],mem[0]
	vinsertps	$48, (%r14,%rcx,4), %xmm2, %xmm7 ## xmm7 = xmm2[0,1,2],mem[0]
	leal	(%rax,%r10), %ecx
	vmovd	%ecx, %xmm2
	vpbroadcastd	%xmm2, %ymm2
	vpaddd	%ymm1, %ymm2, %ymm2
	vmovd	%xmm2, %ecx
	movslq	%ecx, %rcx
	vpextrd	$1, %xmm2, %esi
	movslq	%esi, %rsi
	vmovss	(%r14,%rcx,4), %xmm3            ## xmm3 = mem[0],zero,zero,zero
	vpextrd	$2, %xmm2, %ecx
	vextracti128	$1, %ymm2, %xmm4
	movslq	%ecx, %rcx
	vinsertps	$16, (%r14,%rsi,4), %xmm3, %xmm3 ## xmm3 = xmm3[0],mem[0],xmm3[2,3]
	vinsertps	$32, (%r14,%rcx,4), %xmm3, %xmm3 ## xmm3 = xmm3[0,1],mem[0],xmm3[3]
	vmovd	%xmm4, %ecx
	vpextrd	$1, %xmm4, %esi
	movslq	%ecx, %rcx
	vmovss	(%r14,%rcx,4), %xmm0            ## xmm0 = mem[0],zero,zero,zero
	movslq	%esi, %rcx
	vinsertps	$16, (%r14,%rcx,4), %xmm0, %xmm0 ## xmm0 = xmm0[0],mem[0],xmm0[2,3]
	vpextrd	$3, %xmm2, %ecx
	movslq	%ecx, %rcx
	vinsertps	$48, (%r14,%rcx,4), %xmm3, %xmm8 ## xmm8 = xmm3[0,1,2],mem[0]
	vpextrd	$2, %xmm4, %ecx
	movslq	%ecx, %rcx
	vinsertps	$32, (%r14,%rcx,4), %xmm0, %xmm0 ## xmm0 = xmm0[0,1],mem[0],xmm0[3]
	addl	%r11d, %eax
	vpextrd	$3, %xmm4, %ecx
	vmovd	%eax, %xmm2
	movslq	%ecx, %rax
	vinsertps	$48, (%r14,%rax,4), %xmm0, %xmm0 ## xmm0 = xmm0[0,1,2],mem[0]
	vpbroadcastd	%xmm2, %ymm2
	vpaddd	%ymm1, %ymm2, %ymm2
	vextracti128	$1, %ymm2, %xmm3
	vmovd	%xmm3, %eax
	cltq
	vmovss	(%r14,%rax,4), %xmm4            ## xmm4 = mem[0],zero,zero,zero
	vpextrd	$1, %xmm3, %eax
	cltq
	vinsertps	$16, (%r14,%rax,4), %xmm4, %xmm4 ## xmm4 = xmm4[0],mem[0],xmm4[2,3]
	vinsertf128	$1, %xmm5, %ymm13, %ymm5
	vinsertf128	$1, %xmm6, %ymm7, %ymm6
	vpextrd	$2, %xmm3, %eax
	cltq
	vinsertps	$32, (%r14,%rax,4), %xmm4, %xmm4 ## xmm4 = xmm4[0,1],mem[0],xmm4[3]
	vmovd	%xmm2, %eax
	cltq
	vmovss	(%r14,%rax,4), %xmm7            ## xmm7 = mem[0],zero,zero,zero
	vpextrd	$1, %xmm2, %eax
	cltq
	vinsertps	$16, (%r14,%rax,4), %xmm7, %xmm7 ## xmm7 = xmm7[0],mem[0],xmm7[2,3]
	vpextrd	$3, %xmm3, %eax
	vmulps	%ymm6, %ymm5, %ymm3
	cltq
	vinsertps	$48, (%r14,%rax,4), %xmm4, %xmm4 ## xmm4 = xmm4[0,1,2],mem[0]
	vmaxps	%ymm12, %ymm3, %ymm3
	vpextrd	$2, %xmm2, %eax
	cltq
	vinsertps	$32, (%r14,%rax,4), %xmm7, %xmm5 ## xmm5 = xmm7[0,1],mem[0],xmm7[3]
	vpextrd	$3, %xmm2, %eax
	vdivps	%ymm10, %ymm3, %ymm2
	cltq
	vinsertps	$48, (%r14,%rax,4), %xmm5, %xmm3 ## xmm3 = xmm5[0,1,2],mem[0]
	movslq	%edx, %rdx
	vinsertf128	$1, %xmm0, %ymm8, %ymm0
	vinsertf128	$1, %xmm4, %ymm3, %ymm3
	vmulps	%ymm3, %ymm11, %ymm3
	vroundps	$2, %ymm2, %ymm4
	vmaxps	%ymm12, %ymm2, %ymm2
	vmulps	(%rbp), %ymm4, %ymm4
	vmulps	%ymm3, %ymm4, %ymm3
	vcmpeqps	%ymm0, %ymm9, %ymm0
	vdivps	%ymm2, %ymm3, %ymm2
	vandps	%ymm2, %ymm0, %ymm0
	movq	-88(%rsp), %rax                 ## 8-byte Reload
	vaddps	(%rax,%rdx,4), %ymm0, %ymm0
	vmovups	%ymm0, (%rax,%rdx,4)
	addq	%r13, %rbp
	incq	%rdi
	addl	%r15d, %edx
	decq	%rbx
	jne	LBB39_3
LBB39_4:                                ## %destructor_block
	xorl	%eax, %eax
	addq	$176, %rsp
	popq	%rbx
	popq	%r12
	popq	%r13
	popq	%r14
	popq	%r15
	popq	%rbp
	vzeroupper
	retq
                                        ## -- End function
	.section	__TEXT,__const
	.p2align	5                               ## -- Begin function train_cost_model.par_for.relu1_0_d_def__.s32.n.n.n
LCPI40_0:
	.long	0                               ## 0x0
	.long	1                               ## 0x1
	.long	2                               ## 0x2
	.long	3                               ## 0x3
	.long	4                               ## 0x4
	.long	5                               ## 0x5
	.long	6                               ## 0x6
	.long	7                               ## 0x7
LCPI40_3:
	.space	32
	.section	__TEXT,__literal4,4byte_literals
	.p2align	2
LCPI40_1:
	.long	0x3f800000                      ## float 1
LCPI40_2:
	.long	0x3089705f                      ## float 9.99999971E-10
	.section	__TEXT,__text,regular,pure_instructions
	.p2align	4, 0x90
_train_cost_model.par_for.relu1_0_d_def__.s32.n.n.n: ## @train_cost_model.par_for.relu1_0_d_def__.s32.n.n.n
## %bb.0:                               ## %entry
	pushq	%rbp
	pushq	%r15
	pushq	%r14
	pushq	%r13
	pushq	%r12
	pushq	%rbx
	subq	$200, %rsp
	movq	%rdx, %rdi
	movl	12(%rdx), %r9d
	movl	24(%rdx), %r11d
	movl	%esi, %r8d
	sarl	$31, %r8d
	xorl	%ebp, %ebp
	testl	%r11d, %r11d
	sete	%bpl
	movl	%ebp, %ecx
	negl	%ecx
	movl	%r11d, %ebx
	sarl	$31, %ebx
	subl	%r8d, %esi
	orl	%r11d, %ecx
	movl	%esi, %eax
	cltd
	idivl	%ecx
	movl	%edx, %r10d
	movl	%ebx, %r14d
	leal	(%r11,%rbp), %ecx
	movl	%esi, %eax
	cltd
	idivl	%ecx
	notl	%r14d
	decl	%ebp
	movl	%r14d, %ecx
	subl	%ebx, %ecx
	andl	%r8d, %ecx
	addl	%eax, %ecx
	andl	%ebp, %ecx
	movq	%rcx, -40(%rsp)                 ## 8-byte Spill
	leal	(%rcx,%rcx), %r12d
	subl	%r12d, %r9d
	cmpl	$2, %r9d
	movl	$2, %ecx
	cmovll	%r9d, %ecx
	xorl	%eax, %eax
	testl	%r9d, %r9d
	cmovlel	%eax, %ecx
	movl	%ecx, -56(%rsp)                 ## 4-byte Spill
	jle	LBB40_4
## %bb.1:                               ## %"for relu1_0_d_def__.s32.w.wi.preheader"
	movl	(%rdi), %eax
	movl	%eax, -80(%rsp)                 ## 4-byte Spill
	movl	32(%rdi), %r9d
	movl	40(%rdi), %ecx
	movl	44(%rdi), %esi
	movl	48(%rdi), %r15d
	movl	52(%rdi), %r13d
	movl	56(%rdi), %edx
	xorl	%r11d, %ebx
	addl	%r14d, %ebx
	andl	%r8d, %ebx
	addl	%ebx, %r10d
	andl	%ebp, %r10d
	movq	%r10, -112(%rsp)                ## 8-byte Spill
	leal	(,%r10,8), %eax
	movl	%ecx, -64(%rsp)                 ## 4-byte Spill
                                        ## kill: def $ecx killed $ecx def $rcx
	subl	%r9d, %ecx
	movq	%rcx, -104(%rsp)                ## 8-byte Spill
	movl	%esi, -72(%rsp)                 ## 4-byte Spill
	movl	%esi, %ecx
	subl	%r9d, %ecx
	movq	%rcx, -88(%rsp)                 ## 8-byte Spill
	movl	%r15d, -128(%rsp)               ## 4-byte Spill
	movl	%r15d, %r8d
	subl	%r9d, %r8d
	movl	%r13d, -24(%rsp)                ## 4-byte Spill
	movl	%r13d, %r14d
	subl	%r9d, %r14d
	movslq	8(%rdi), %rcx
	movq	-40(%rsp), %r13                 ## 8-byte Reload
	movslq	%r13d, %rbp
	subq	%rcx, %rbp
	movl	%edx, -32(%rsp)                 ## 4-byte Spill
	movl	%edx, %r11d
	movl	-80(%rsp), %edx                 ## 4-byte Reload
	subl	%r9d, %r11d
	addq	%rbp, %rbp
	movl	%edx, %esi
	subl	%eax, %esi
	cmpl	$8, %esi
	movl	$8, %ecx
	cmovll	%esi, %ecx
	movl	%esi, -12(%rsp)                 ## 4-byte Spill
	testl	%esi, %esi
	movl	$0, %esi
	cmovgl	%ecx, %esi
	movq	%rsi, 32(%rsp)                  ## 8-byte Spill
	leal	8(%rax), %ecx
	movslq	%eax, %rbx
	subl	%edx, %eax
	movl	%eax, %esi
	sarl	$31, %esi
	andl	%eax, %esi
	cmpl	$-8, %esi
	movl	$-8, %r10d
	cmovgl	%esi, %r10d
	vmovss	36(%rdi), %xmm11                ## xmm11 = mem[0],zero,zero,zero
	movl	16(%rdi), %eax
	movq	%rax, -120(%rsp)                ## 8-byte Spill
	movl	20(%rdi), %eax
	movl	%eax, -124(%rsp)                ## 4-byte Spill
	movq	64(%rdi), %rsi
	movq	80(%rdi), %rax
	movq	%rax, -96(%rsp)                 ## 8-byte Spill
	movq	96(%rdi), %r15
	movl	28(%rdi), %eax
	movslq	4(%rdi), %rdi
	movq	%rdi, -48(%rsp)                 ## 8-byte Spill
	vbroadcastss	%xmm11, %ymm10
	cmpl	%edx, %ecx
	jle	LBB40_2
## %bb.5:                               ## %"for relu1_0_d_def__.s32.w.wi.us.preheader"
	subl	%r9d, %edx
	addl	%edx, -64(%rsp)                 ## 4-byte Folded Spill
	addl	%edx, -72(%rsp)                 ## 4-byte Folded Spill
	addl	%edx, -128(%rsp)                ## 4-byte Folded Spill
	movq	%rsi, 24(%rsp)                  ## 8-byte Spill
	movl	-24(%rsp), %esi                 ## 4-byte Reload
	addl	%edx, %esi
	addl	-32(%rsp), %edx                 ## 4-byte Folded Reload
	subl	%eax, %r13d
	addl	%eax, %eax
	movslq	%r12d, %rcx
	movq	%rcx, -80(%rsp)                 ## 8-byte Spill
	subl	%eax, %r12d
	addl	%r13d, %r13d
	movq	%r13, -40(%rsp)                 ## 8-byte Spill
	movq	-120(%rsp), %rax                ## 8-byte Reload
	imull	%eax, %r12d
                                        ## kill: def $eax killed $eax killed $rax
	shll	$5, %eax
	movl	%eax, -120(%rsp)                ## 4-byte Spill
	addl	%ebx, %r11d
	addl	%ebx, %r14d
	addl	%ebx, %r8d
	movq	-88(%rsp), %rcx                 ## 8-byte Reload
	addl	%ebx, %ecx
	movq	-104(%rsp), %rax                ## 8-byte Reload
	addl	%ebx, %eax
	movq	%rbp, %rdi
	movq	%rbx, %rbp
	movq	32(%rsp), %rbx                  ## 8-byte Reload
	addl	%ebx, %r10d
	cltq
	movq	%rax, 8(%rsp)                   ## 8-byte Spill
	movslq	%ecx, %rax
	movq	%rax, (%rsp)                    ## 8-byte Spill
	movslq	%r8d, %rax
	movq	%rax, -8(%rsp)                  ## 8-byte Spill
	movslq	%r14d, %r9
	movslq	%r11d, %r11
	movslq	-64(%rsp), %rax                 ## 4-byte Folded Reload
	movq	%rax, -88(%rsp)                 ## 8-byte Spill
	movslq	-72(%rsp), %rax                 ## 4-byte Folded Reload
	movq	%rax, -24(%rsp)                 ## 8-byte Spill
	movslq	-128(%rsp), %rax                ## 4-byte Folded Reload
	movq	%rax, -32(%rsp)                 ## 8-byte Spill
	movslq	%esi, %rax
	movq	%rax, 192(%rsp)                 ## 8-byte Spill
	movslq	%edx, %rax
	movq	%rax, 184(%rsp)                 ## 8-byte Spill
	leal	(%rbx,%rbp), %eax
	movslq	%eax, %rdx
	movl	-56(%rsp), %eax                 ## 4-byte Reload
	movq	%rax, 176(%rsp)                 ## 8-byte Spill
	movl	%r10d, -128(%rsp)               ## 4-byte Spill
	movl	%r10d, %r10d
	movl	%r10d, %eax
	andl	$-8, %eax
	movq	%rax, 88(%rsp)                  ## 8-byte Spill
	addq	$-8, %rax
	movq	%rax, 80(%rsp)                  ## 8-byte Spill
	movq	%rax, %rsi
	shrq	$3, %rsi
	incq	%rsi
	movq	-48(%rsp), %rcx                 ## 8-byte Reload
	movl	%ebx, %r13d
	movl	%r13d, %eax
	andl	$2147483640, %eax               ## imm = 0x7FFFFFF8
	movq	%rax, 64(%rsp)                  ## 8-byte Spill
	movq	%rcx, %rax
	movq	%rdi, 40(%rsp)                  ## 8-byte Spill
	imulq	%rdi, %rax
	addq	%rax, %rbp
	movq	24(%rsp), %rbx                  ## 8-byte Reload
	leaq	(%rbx,%rbp,4), %r14
	shll	$5, %r12d
	movq	-112(%rsp), %rbp                ## 8-byte Reload
	leal	(%r12,%rbp,8), %ebp
	movl	%ebp, -112(%rsp)                ## 4-byte Spill
	addq	%rdx, %rax
	movq	%rsi, %rbp
	movq	%rsi, 72(%rsp)                  ## 8-byte Spill
	andq	$-2, %rsi
	negq	%rsi
	movq	%rsi, 56(%rsp)                  ## 8-byte Spill
	leaq	(%rbx,%rax,4), %rsi
	addq	$32, %rsi
	leaq	(%rbx,%rax,4), %r8
	vmovss	LCPI40_1(%rip), %xmm15          ## xmm15 = mem[0],zero,zero,zero
	vmovss	LCPI40_2(%rip), %xmm3           ## xmm3 = mem[0],zero,zero,zero
	vbroadcastss	LCPI40_1(%rip), %ymm4   ## ymm4 = [1.0E+0,1.0E+0,1.0E+0,1.0E+0,1.0E+0,1.0E+0,1.0E+0,1.0E+0]
	vxorps	%xmm9, %xmm9, %xmm9
	vbroadcastss	LCPI40_2(%rip), %ymm8   ## ymm8 = [9.99999971E-10,9.99999971E-10,9.99999971E-10,9.99999971E-10,9.99999971E-10,9.99999971E-10,9.99999971E-10,9.99999971E-10]
	leaq	(,%rcx,4), %rax
	movq	%rax, 168(%rsp)                 ## 8-byte Spill
	movq	%r11, 96(%rsp)                  ## 8-byte Spill
	leaq	(%r15,%r11,4), %rax
	movq	%rax, 152(%rsp)                 ## 8-byte Spill
	movq	%r9, 104(%rsp)                  ## 8-byte Spill
	leaq	(%r15,%r9,4), %rax
	movq	%rax, 144(%rsp)                 ## 8-byte Spill
	movq	-8(%rsp), %rax                  ## 8-byte Reload
	leaq	(%r15,%rax,4), %rax
	movq	%rax, 136(%rsp)                 ## 8-byte Spill
	movq	(%rsp), %rax                    ## 8-byte Reload
	leaq	(%r15,%rax,4), %rax
	movq	%rax, 128(%rsp)                 ## 8-byte Spill
	movq	8(%rsp), %rax                   ## 8-byte Reload
	leaq	(%r15,%rax,4), %rax
	movq	%rax, 120(%rsp)                 ## 8-byte Spill
	movq	-96(%rsp), %rcx                 ## 8-byte Reload
	leaq	32(%rcx), %rax
	movq	%rax, 48(%rsp)                  ## 8-byte Spill
	movq	%rdx, 16(%rsp)                  ## 8-byte Spill
	leaq	(%rcx,%rdx,4), %rax
	movq	%rax, 112(%rsp)                 ## 8-byte Spill
	movq	-80(%rsp), %rdx                 ## 8-byte Reload
	xorl	%edi, %edi
	jmp	LBB40_6
	.p2align	4, 0x90
LBB40_18:                               ## %after_bb.us
                                        ##   in Loop: Header=BB40_6 Depth=1
	movq	-104(%rsp), %rdi                ## 8-byte Reload
	incq	%rdi
	movq	168(%rsp), %rcx                 ## 8-byte Reload
	addq	%rcx, %r14
	movq	-64(%rsp), %rdx                 ## 8-byte Reload
	incq	%rdx
	movl	-120(%rsp), %eax                ## 4-byte Reload
	addl	%eax, -112(%rsp)                ## 4-byte Folded Spill
	addq	%rcx, %rsi
	addl	%eax, %r12d
	addq	%rcx, %r8
	cmpq	176(%rsp), %rdi                 ## 8-byte Folded Reload
	je	LBB40_4
LBB40_6:                                ## %"for relu1_0_d_def__.s32.w.wi.us"
                                        ## =>This Loop Header: Depth=1
                                        ##     Child Loop BB40_37 Depth 2
                                        ##     Child Loop BB40_9 Depth 2
                                        ##     Child Loop BB40_22 Depth 2
                                        ##     Child Loop BB40_15 Depth 2
	movq	%r12, -56(%rsp)                 ## 8-byte Spill
	testq	%rdx, %rdx
	movl	$0, %eax
	movq	%rdx, -64(%rsp)                 ## 8-byte Spill
	cmovgl	%edx, %eax
	movq	-80(%rsp), %rcx                 ## 8-byte Reload
	movq	%rdi, -104(%rsp)                ## 8-byte Spill
	leaq	(%rdi,%rcx), %rdx
	testq	%rdx, %rdx
	movl	$0, %ecx
	cmovlel	%ecx, %edx
	movq	%rdx, -72(%rsp)                 ## 8-byte Spill
	cmpl	$0, -12(%rsp)                   ## 4-byte Folded Reload
	jle	LBB40_12
## %bb.7:                               ## %"for relu1_0_d_def__.s32.n.ni.preheader.us"
                                        ##   in Loop: Header=BB40_6 Depth=1
	imull	-124(%rsp), %eax                ## 4-byte Folded Reload
	movslq	%eax, %r9
	movslq	-112(%rsp), %rax                ## 4-byte Folded Reload
	movq	-96(%rsp), %rcx                 ## 8-byte Reload
	leaq	(%rcx,%rax,4), %rbp
	cmpl	$8, 32(%rsp)                    ## 4-byte Folded Reload
	jae	LBB40_36
## %bb.8:                               ##   in Loop: Header=BB40_6 Depth=1
	xorl	%r12d, %r12d
	jmp	LBB40_39
	.p2align	4, 0x90
LBB40_36:                               ## %vector.body28.preheader
                                        ##   in Loop: Header=BB40_6 Depth=1
	movq	%rsi, 160(%rsp)                 ## 8-byte Spill
	movq	96(%rsp), %rax                  ## 8-byte Reload
	addq	%r9, %rax
	movq	104(%rsp), %rcx                 ## 8-byte Reload
	leaq	(%rcx,%r9), %rcx
	movq	-8(%rsp), %rdx                  ## 8-byte Reload
	leaq	(%rdx,%r9), %rdx
	movq	(%rsp), %rsi                    ## 8-byte Reload
	addq	%r9, %rsi
	movq	8(%rsp), %rdi                   ## 8-byte Reload
	leaq	(%rdi,%r9), %r11
	leaq	(%r15,%rax,4), %r12
	leaq	(%r15,%rcx,4), %rcx
	leaq	(%r15,%rdx,4), %rdi
	leaq	(%r15,%rsi,4), %rbx
	leaq	(%r15,%r11,4), %rdx
	xorl	%esi, %esi
	movq	64(%rsp), %rax                  ## 8-byte Reload
	.p2align	4, 0x90
LBB40_37:                               ## %vector.body28
                                        ##   Parent Loop BB40_6 Depth=1
                                        ## =>  This Inner Loop Header: Depth=2
	vmovups	(%rdx,%rsi,4), %ymm0
	vmulps	(%rbx,%rsi,4), %ymm0, %ymm0
	vmaxps	%ymm4, %ymm0, %ymm0
	vdivps	%ymm10, %ymm0, %ymm0
	vcmpeqps	(%rdi,%rsi,4), %ymm9, %ymm1
	vroundps	$10, %ymm0, %ymm2
	vmaxps	%ymm4, %ymm0, %ymm0
	vmulps	(%r14,%rsi,4), %ymm2, %ymm2
	vmovups	(%rcx,%rsi,4), %ymm5
	vmulps	(%r12,%rsi,4), %ymm5, %ymm5
	vmulps	%ymm5, %ymm8, %ymm5
	vmulps	%ymm5, %ymm2, %ymm2
	vdivps	%ymm0, %ymm2, %ymm0
	vandps	%ymm0, %ymm1, %ymm0
	vaddps	(%rbp,%rsi,4), %ymm0, %ymm0
	vmovups	%ymm0, (%rbp,%rsi,4)
	addq	$8, %rsi
	cmpq	%rsi, %rax
	jne	LBB40_37
## %bb.38:                              ## %middle.block26
                                        ##   in Loop: Header=BB40_6 Depth=1
	movq	%rax, %r12
	cmpq	%r13, %rax
	movq	160(%rsp), %rsi                 ## 8-byte Reload
	jne	LBB40_39
LBB40_12:                               ## %"end for relu1_0_d_def__.s32.n.ni.us"
                                        ##   in Loop: Header=BB40_6 Depth=1
	movl	-128(%rsp), %ebp                ## 4-byte Reload
	testl	%ebp, %ebp
	movq	-56(%rsp), %r12                 ## 8-byte Reload
	movq	24(%rsp), %rbx                  ## 8-byte Reload
	jle	LBB40_18
## %bb.13:                              ## %"for relu1_0_d_def__.s32.n.ni.rebased.preheader.us"
                                        ##   in Loop: Header=BB40_6 Depth=1
	movq	%rsi, %r11
	movslq	%r12d, %r9
	movq	-72(%rsp), %rax                 ## 8-byte Reload
	imull	-124(%rsp), %eax                ## 4-byte Folded Reload
	cltq
	movq	-88(%rsp), %rcx                 ## 8-byte Reload
	addq	%rax, %rcx
	movq	-24(%rsp), %rdx                 ## 8-byte Reload
	addq	%rax, %rdx
	movq	-32(%rsp), %rsi                 ## 8-byte Reload
	addq	%rax, %rsi
	movq	192(%rsp), %rdi                 ## 8-byte Reload
	addq	%rax, %rdi
	addq	184(%rsp), %rax                 ## 8-byte Folded Reload
	vmovss	-4(%r15,%rcx,4), %xmm0          ## xmm0 = mem[0],zero,zero,zero
	vmulss	-4(%r15,%rdx,4), %xmm0, %xmm0
	vmaxss	%xmm15, %xmm0, %xmm0
	vdivss	%xmm11, %xmm0, %xmm0
	vmovss	-4(%r15,%rsi,4), %xmm7          ## xmm7 = mem[0],zero,zero,zero
	vroundss	$10, %xmm0, %xmm0, %xmm5
	vmaxss	%xmm15, %xmm0, %xmm0
	vmovss	-4(%r15,%rdi,4), %xmm1          ## xmm1 = mem[0],zero,zero,zero
	vmulss	-4(%r15,%rax,4), %xmm1, %xmm1
	cmpl	$8, %ebp
	jae	LBB40_19
## %bb.14:                              ##   in Loop: Header=BB40_6 Depth=1
	xorl	%eax, %eax
	movq	%r11, %rsi
	jmp	LBB40_35
	.p2align	4, 0x90
LBB40_39:                               ## %"for relu1_0_d_def__.s32.n.ni.us.preheader"
                                        ##   in Loop: Header=BB40_6 Depth=1
	movq	152(%rsp), %rax                 ## 8-byte Reload
	leaq	(%rax,%r9,4), %rcx
	movq	144(%rsp), %rax                 ## 8-byte Reload
	leaq	(%rax,%r9,4), %r11
	movq	136(%rsp), %rax                 ## 8-byte Reload
	leaq	(%rax,%r9,4), %rbx
	movq	128(%rsp), %rax                 ## 8-byte Reload
	leaq	(%rax,%r9,4), %rax
	movq	120(%rsp), %rdx                 ## 8-byte Reload
	leaq	(%rdx,%r9,4), %rdi
	jmp	LBB40_9
	.p2align	4, 0x90
LBB40_11:                               ## %select.end
                                        ##   in Loop: Header=BB40_9 Depth=2
	vaddss	(%rbp,%r12,4), %xmm0, %xmm0
	vmovss	%xmm0, (%rbp,%r12,4)
	incq	%r12
	cmpq	%r12, %r13
	je	LBB40_12
LBB40_9:                                ## %"for relu1_0_d_def__.s32.n.ni.us"
                                        ##   Parent Loop BB40_6 Depth=1
                                        ## =>  This Inner Loop Header: Depth=2
	vmovss	(%rdi,%r12,4), %xmm0            ## xmm0 = mem[0],zero,zero,zero
	vmulss	(%rax,%r12,4), %xmm0, %xmm0
	vmaxss	%xmm15, %xmm0, %xmm1
	vxorps	%xmm0, %xmm0, %xmm0
	vucomiss	(%rbx,%r12,4), %xmm0
	vdivss	%xmm11, %xmm1, %xmm1
	vroundss	$10, %xmm1, %xmm1, %xmm2
	vmulss	(%r14,%r12,4), %xmm2, %xmm2
	vmovss	(%r11,%r12,4), %xmm5            ## xmm5 = mem[0],zero,zero,zero
	vmulss	(%rcx,%r12,4), %xmm5, %xmm5
	sete	%dl
	vmulss	%xmm5, %xmm2, %xmm2
	vmulss	%xmm3, %xmm2, %xmm2
	testb	$1, %dl
	je	LBB40_11
## %bb.10:                              ## %select.true.sink
                                        ##   in Loop: Header=BB40_9 Depth=2
	vmaxss	%xmm15, %xmm1, %xmm0
	vdivss	%xmm0, %xmm2, %xmm0
	jmp	LBB40_11
	.p2align	4, 0x90
LBB40_19:                               ## %vector.ph
                                        ##   in Loop: Header=BB40_6 Depth=1
	vbroadcastss	%xmm0, %ymm12
	cmpq	$0, 80(%rsp)                    ## 8-byte Folded Reload
	vxorps	%xmm6, %xmm6, %xmm6
	je	LBB40_20
## %bb.21:                              ## %vector.ph.new
                                        ##   in Loop: Header=BB40_6 Depth=1
	movq	16(%rsp), %rax                  ## 8-byte Reload
	addq	%r9, %rax
	movq	48(%rsp), %rcx                  ## 8-byte Reload
	leaq	(%rcx,%rax,4), %rbp
	vmulss	%xmm5, %xmm1, %xmm2
	vbroadcastss	%xmm2, %ymm13
	movq	56(%rsp), %rcx                  ## 8-byte Reload
	xorl	%edx, %edx
	movq	%r11, %rsi
	jmp	LBB40_22
	.p2align	4, 0x90
LBB40_28:                               ## %vector.body
                                        ##   in Loop: Header=BB40_22 Depth=2
	vaddps	(%rbp,%rdx,4), %ymm14, %ymm2
	vmovups	%ymm2, (%rbp,%rdx,4)
	addq	$16, %rdx
	addq	$2, %rcx
	je	LBB40_29
LBB40_22:                               ## %vector.body
                                        ##   Parent Loop BB40_6 Depth=1
                                        ## =>  This Inner Loop Header: Depth=2
	vucomiss	%xmm6, %xmm7
	vxorps	%xmm14, %xmm14, %xmm14
	vxorps	%xmm2, %xmm2, %xmm2
	jne	LBB40_25
## %bb.23:                              ## %vector.body
                                        ##   in Loop: Header=BB40_22 Depth=2
	vxorps	%xmm2, %xmm2, %xmm2
	jp	LBB40_25
## %bb.24:                              ## %vector.body
                                        ##   in Loop: Header=BB40_22 Depth=2
	vmulps	-32(%rsi,%rdx,4), %ymm13, %ymm2
	vmulps	%ymm2, %ymm8, %ymm2
	vdivps	%ymm12, %ymm2, %ymm2
LBB40_25:                               ## %vector.body
                                        ##   in Loop: Header=BB40_22 Depth=2
	vaddps	-32(%rbp,%rdx,4), %ymm2, %ymm2
	vmovups	%ymm2, -32(%rbp,%rdx,4)
	jne	LBB40_28
## %bb.26:                              ## %vector.body
                                        ##   in Loop: Header=BB40_22 Depth=2
	jp	LBB40_28
## %bb.27:                              ## %vector.body
                                        ##   in Loop: Header=BB40_22 Depth=2
	vmulps	(%rsi,%rdx,4), %ymm13, %ymm2
	vmulps	%ymm2, %ymm8, %ymm2
	vdivps	%ymm12, %ymm2, %ymm14
	jmp	LBB40_28
LBB40_20:                               ##   in Loop: Header=BB40_6 Depth=1
	xorl	%edx, %edx
	movq	%r11, %rsi
LBB40_29:                               ## %middle.block.unr-lcssa
                                        ##   in Loop: Header=BB40_6 Depth=1
	testb	$1, 72(%rsp)                    ## 1-byte Folded Reload
	je	LBB40_34
## %bb.30:                              ## %vector.body.epil
                                        ##   in Loop: Header=BB40_6 Depth=1
	movq	-40(%rsp), %rax                 ## 8-byte Reload
	movq	-104(%rsp), %rdi                ## 8-byte Reload
	addl	%edi, %eax
	imull	-120(%rsp), %eax                ## 4-byte Folded Reload
	cltq
	addq	16(%rsp), %rdx                  ## 8-byte Folded Reload
	addq	%rdx, %rax
	vucomiss	%xmm6, %xmm7
	movq	40(%rsp), %rcx                  ## 8-byte Reload
	jne	LBB40_31
	jp	LBB40_31
## %bb.32:                              ## %select.true.sink157
                                        ##   in Loop: Header=BB40_6 Depth=1
	addq	%rdi, %rcx
	imulq	-48(%rsp), %rcx                 ## 8-byte Folded Reload
	addq	%rcx, %rdx
	vmulss	%xmm5, %xmm1, %xmm2
	vbroadcastss	%xmm2, %ymm2
	vmulps	(%rbx,%rdx,4), %ymm2, %ymm2
	vmulps	%ymm2, %ymm8, %ymm2
	vdivps	%ymm12, %ymm2, %ymm2
	jmp	LBB40_33
LBB40_31:                               ##   in Loop: Header=BB40_6 Depth=1
	vxorps	%xmm2, %xmm2, %xmm2
LBB40_33:                               ## %select.end156
                                        ##   in Loop: Header=BB40_6 Depth=1
	movq	-96(%rsp), %rcx                 ## 8-byte Reload
	vaddps	(%rcx,%rax,4), %ymm2, %ymm2
	vmovups	%ymm2, (%rcx,%rax,4)
LBB40_34:                               ## %middle.block
                                        ##   in Loop: Header=BB40_6 Depth=1
	movq	88(%rsp), %rcx                  ## 8-byte Reload
	movq	%rcx, %rax
	cmpq	%r10, %rcx
	je	LBB40_18
LBB40_35:                               ## %"for relu1_0_d_def__.s32.n.ni.rebased.us.preheader"
                                        ##   in Loop: Header=BB40_6 Depth=1
	movq	112(%rsp), %rcx                 ## 8-byte Reload
	leaq	(%rcx,%r9,4), %rcx
	jmp	LBB40_15
	.p2align	4, 0x90
LBB40_17:                               ## %select.end210
                                        ##   in Loop: Header=BB40_15 Depth=2
	vaddss	(%rcx,%rax,4), %xmm2, %xmm2
	vmovss	%xmm2, (%rcx,%rax,4)
	incq	%rax
	cmpq	%rax, %r10
	je	LBB40_18
LBB40_15:                               ## %"for relu1_0_d_def__.s32.n.ni.rebased.us"
                                        ##   Parent Loop BB40_6 Depth=1
                                        ## =>  This Inner Loop Header: Depth=2
	vxorps	%xmm2, %xmm2, %xmm2
	vucomiss	%xmm2, %xmm7
	vmulss	(%r8,%rax,4), %xmm5, %xmm6
	vmulss	%xmm1, %xmm6, %xmm6
	vmulss	%xmm3, %xmm6, %xmm6
	jne	LBB40_17
	jp	LBB40_17
## %bb.16:                              ## %select.true.sink211
                                        ##   in Loop: Header=BB40_15 Depth=2
	vdivss	%xmm0, %xmm6, %xmm2
	jmp	LBB40_17
LBB40_2:                                ## %"for relu1_0_d_def__.s32.w.wi.preheader5"
	decl	%edx
	movq	-120(%rsp), %r13                ## 8-byte Reload
	leal	(,%r13,4), %r9d
	vmovd	%ebx, %xmm0
	vpbroadcastd	%xmm0, %ymm0
	vpor	LCPI40_0(%rip), %ymm0, %ymm0
	vmovd	%edx, %xmm1
	vpbroadcastd	%xmm1, %ymm1
	vpminsd	%ymm0, %ymm1, %ymm9
	movslq	%r12d, %rdi
	movl	-56(%rsp), %r10d                ## 4-byte Reload
	movq	-48(%rsp), %rdx                 ## 8-byte Reload
	imulq	%rdx, %rbp
	addq	%rbx, %rbp
	leaq	(%rsi,%rbp,4), %rbx
	shlq	$2, %rdx
	movq	%r12, %rcx
	movq	%rdx, %r12
	addl	%eax, %eax
	subl	%eax, %ecx
	imull	%ecx, %r13d
	movq	-112(%rsp), %rax                ## 8-byte Reload
	leal	(%rax,%r13,4), %ecx
	vbroadcastss	LCPI40_1(%rip), %ymm13  ## ymm13 = [1.0E+0,1.0E+0,1.0E+0,1.0E+0,1.0E+0,1.0E+0,1.0E+0,1.0E+0]
	vbroadcastss	LCPI40_2(%rip), %ymm12  ## ymm12 = [9.99999971E-10,9.99999971E-10,9.99999971E-10,9.99999971E-10,9.99999971E-10,9.99999971E-10,9.99999971E-10,9.99999971E-10]
	movq	-88(%rsp), %r13                 ## 8-byte Reload
	.p2align	4, 0x90
LBB40_3:                                ## %"for relu1_0_d_def__.s32.w.wi"
                                        ## =>This Inner Loop Header: Depth=1
	testq	%rdi, %rdi
	movl	$0, %eax
	cmovgl	%edi, %eax
	imull	-124(%rsp), %eax                ## 4-byte Folded Reload
	movq	-104(%rsp), %rdx                ## 8-byte Reload
	addl	%eax, %edx
	vmovd	%edx, %xmm0
	vpbroadcastd	%xmm0, %ymm0
	vpaddd	%ymm0, %ymm9, %ymm0
	vmovd	%xmm0, %edx
	vpextrd	$1, %xmm0, %esi
	vpextrd	$2, %xmm0, %ebp
	movslq	%edx, %rdx
	vmovss	(%r15,%rdx,4), %xmm3            ## xmm3 = mem[0],zero,zero,zero
	leal	(%rax,%r13), %edx
	movslq	%esi, %rsi
	vmovd	%edx, %xmm5
	vpextrd	$3, %xmm0, %edx
	movslq	%ebp, %rbp
	movslq	%edx, %rdx
	vextracti128	$1, %ymm0, %xmm6
	vinsertps	$16, (%r15,%rsi,4), %xmm3, %xmm0 ## xmm0 = xmm3[0],mem[0],xmm3[2,3]
	vmovd	%xmm6, %esi
	movslq	%esi, %rsi
	vinsertps	$32, (%r15,%rbp,4), %xmm0, %xmm0 ## xmm0 = xmm0[0,1],mem[0],xmm0[3]
	vpextrd	$1, %xmm6, %ebp
	movslq	%ebp, %rbp
	vmovss	(%r15,%rsi,4), %xmm3            ## xmm3 = mem[0],zero,zero,zero
	vpbroadcastd	%xmm5, %ymm5
	vpaddd	%ymm5, %ymm9, %ymm5
	vinsertps	$48, (%r15,%rdx,4), %xmm0, %xmm14 ## xmm14 = xmm0[0,1,2],mem[0]
	vpextrd	$1, %xmm5, %edx
	vinsertps	$16, (%r15,%rbp,4), %xmm3, %xmm3 ## xmm3 = xmm3[0],mem[0],xmm3[2,3]
	vmovd	%xmm5, %esi
	vextracti128	$1, %ymm5, %xmm7
	vmovd	%xmm7, %ebp
	movslq	%ebp, %rbp
	vmovss	(%r15,%rbp,4), %xmm4            ## xmm4 = mem[0],zero,zero,zero
	vpextrd	$2, %xmm5, %ebp
	movslq	%esi, %rsi
	vmovss	(%r15,%rsi,4), %xmm2            ## xmm2 = mem[0],zero,zero,zero
	vpextrd	$1, %xmm7, %esi
	movslq	%edx, %rdx
	vinsertps	$16, (%r15,%rdx,4), %xmm2, %xmm2 ## xmm2 = xmm2[0],mem[0],xmm2[2,3]
	vpextrd	$2, %xmm6, %edx
	movslq	%edx, %rdx
	movslq	%esi, %rsi
	vinsertps	$16, (%r15,%rsi,4), %xmm4, %xmm4 ## xmm4 = xmm4[0],mem[0],xmm4[2,3]
	vpextrd	$3, %xmm5, %esi
	vinsertps	$32, (%r15,%rdx,4), %xmm3, %xmm3 ## xmm3 = xmm3[0,1],mem[0],xmm3[3]
	vpextrd	$2, %xmm7, %edx
	movslq	%ebp, %rbp
	vinsertps	$32, (%r15,%rbp,4), %xmm2, %xmm2 ## xmm2 = xmm2[0,1],mem[0],xmm2[3]
	vpextrd	$3, %xmm6, %ebp
	movslq	%edx, %rdx
	vinsertps	$32, (%r15,%rdx,4), %xmm4, %xmm4 ## xmm4 = xmm4[0,1],mem[0],xmm4[3]
	vpextrd	$3, %xmm7, %edx
	movslq	%ebp, %rbp
	movslq	%esi, %rsi
	movslq	%edx, %rdx
	vinsertps	$48, (%r15,%rbp,4), %xmm3, %xmm15 ## xmm15 = xmm3[0,1,2],mem[0]
	vinsertps	$48, (%r15,%rsi,4), %xmm2, %xmm6 ## xmm6 = xmm2[0,1,2],mem[0]
	vinsertps	$48, (%r15,%rdx,4), %xmm4, %xmm7 ## xmm7 = xmm4[0,1,2],mem[0]
	leal	(%rax,%r8), %edx
	vmovd	%edx, %xmm2
	vpbroadcastd	%xmm2, %ymm2
	vpaddd	%ymm2, %ymm9, %ymm2
	vmovd	%xmm2, %edx
	vpextrd	$1, %xmm2, %esi
	movslq	%edx, %rdx
	movslq	%esi, %rsi
	vpextrd	$2, %xmm2, %ebp
	movslq	%ebp, %rbp
	vmovss	(%r15,%rdx,4), %xmm4            ## xmm4 = mem[0],zero,zero,zero
	vpextrd	$3, %xmm2, %edx
	movslq	%edx, %rdx
	vextracti128	$1, %ymm2, %xmm3
	vinsertps	$16, (%r15,%rsi,4), %xmm4, %xmm2 ## xmm2 = xmm4[0],mem[0],xmm4[2,3]
	vmovd	%xmm3, %esi
	vinsertps	$32, (%r15,%rbp,4), %xmm2, %xmm2 ## xmm2 = xmm2[0,1],mem[0],xmm2[3]
	leal	(%rax,%r14), %ebp
	movslq	%esi, %rsi
	vmovd	%ebp, %xmm4
	vpbroadcastd	%xmm4, %ymm4
	vpaddd	%ymm4, %ymm9, %ymm4
	vmovss	(%r15,%rsi,4), %xmm0            ## xmm0 = mem[0],zero,zero,zero
	vmovd	%xmm4, %esi
	movslq	%esi, %rsi
	vinsertps	$48, (%r15,%rdx,4), %xmm2, %xmm8 ## xmm8 = xmm2[0,1,2],mem[0]
	vpextrd	$1, %xmm4, %edx
	vpextrd	$1, %xmm3, %ebp
	vmovss	(%r15,%rsi,4), %xmm2            ## xmm2 = mem[0],zero,zero,zero
	movslq	%edx, %rdx
	vextracti128	$1, %ymm4, %xmm5
	movslq	%ebp, %rsi
	vinsertps	$16, (%r15,%rdx,4), %xmm2, %xmm2 ## xmm2 = xmm2[0],mem[0],xmm2[2,3]
	vinsertps	$16, (%r15,%rsi,4), %xmm0, %xmm0 ## xmm0 = xmm0[0],mem[0],xmm0[2,3]
	vmovd	%xmm5, %edx
	vpextrd	$1, %xmm5, %esi
	movslq	%edx, %rdx
	vmovss	(%r15,%rdx,4), %xmm1            ## xmm1 = mem[0],zero,zero,zero
	movslq	%esi, %rdx
	vinsertps	$16, (%r15,%rdx,4), %xmm1, %xmm1 ## xmm1 = xmm1[0],mem[0],xmm1[2,3]
	vpextrd	$2, %xmm4, %edx
	movslq	%edx, %rdx
	vinsertps	$32, (%r15,%rdx,4), %xmm2, %xmm2 ## xmm2 = xmm2[0,1],mem[0],xmm2[3]
	vpextrd	$2, %xmm3, %edx
	movslq	%edx, %rdx
	vinsertps	$32, (%r15,%rdx,4), %xmm0, %xmm0 ## xmm0 = xmm0[0,1],mem[0],xmm0[3]
	vpextrd	$3, %xmm4, %edx
	movslq	%edx, %rdx
	vinsertps	$48, (%r15,%rdx,4), %xmm2, %xmm11 ## xmm11 = xmm2[0,1,2],mem[0]
	vpextrd	$2, %xmm5, %edx
	movslq	%edx, %rdx
	vinsertps	$32, (%r15,%rdx,4), %xmm1, %xmm1 ## xmm1 = xmm1[0,1],mem[0],xmm1[3]
	vinsertf128	$1, %xmm15, %ymm14, %ymm2
	vinsertf128	$1, %xmm7, %ymm6, %ymm4
	vmulps	%ymm4, %ymm2, %ymm2
	vmaxps	%ymm13, %ymm2, %ymm2
	vdivps	%ymm10, %ymm2, %ymm2
	addl	%r11d, %eax
	vmovd	%eax, %xmm4
	vpextrd	$3, %xmm3, %eax
	cltq
	vpbroadcastd	%xmm4, %ymm3
	vinsertps	$48, (%r15,%rax,4), %xmm0, %xmm0 ## xmm0 = xmm0[0,1,2],mem[0]
	vpaddd	%ymm3, %ymm9, %ymm3
	vextracti128	$1, %ymm3, %xmm4
	vmovd	%xmm4, %eax
	cltq
	vmovss	(%r15,%rax,4), %xmm6            ## xmm6 = mem[0],zero,zero,zero
	vpextrd	$1, %xmm4, %eax
	cltq
	vinsertps	$16, (%r15,%rax,4), %xmm6, %xmm6 ## xmm6 = xmm6[0],mem[0],xmm6[2,3]
	vpextrd	$3, %xmm5, %eax
	cltq
	vinsertps	$48, (%r15,%rax,4), %xmm1, %xmm1 ## xmm1 = xmm1[0,1,2],mem[0]
	vpextrd	$2, %xmm4, %eax
	cltq
	vinsertps	$32, (%r15,%rax,4), %xmm6, %xmm5 ## xmm5 = xmm6[0,1],mem[0],xmm6[3]
	vmovd	%xmm3, %eax
	cltq
	vmovss	(%r15,%rax,4), %xmm6            ## xmm6 = mem[0],zero,zero,zero
	vpextrd	$1, %xmm3, %eax
	cltq
	vinsertps	$16, (%r15,%rax,4), %xmm6, %xmm6 ## xmm6 = xmm6[0],mem[0],xmm6[2,3]
	vpextrd	$3, %xmm4, %eax
	cltq
	vinsertps	$48, (%r15,%rax,4), %xmm5, %xmm4 ## xmm4 = xmm5[0,1,2],mem[0]
	vpextrd	$2, %xmm3, %eax
	cltq
	vinsertps	$32, (%r15,%rax,4), %xmm6, %xmm5 ## xmm5 = xmm6[0,1],mem[0],xmm6[3]
	vpextrd	$3, %xmm3, %eax
	cltq
	vinsertps	$48, (%r15,%rax,4), %xmm5, %xmm3 ## xmm3 = xmm5[0,1,2],mem[0]
	vinsertf128	$1, %xmm0, %ymm8, %ymm0
	movslq	%ecx, %rcx
	movq	%rcx, %rax
	shlq	$5, %rax
	vinsertf128	$1, %xmm1, %ymm11, %ymm1
	vroundps	$2, %ymm2, %ymm5
	vmulps	(%rbx), %ymm5, %ymm5
	vinsertf128	$1, %xmm4, %ymm3, %ymm3
	vmaxps	%ymm13, %ymm2, %ymm2
	vmulps	%ymm3, %ymm1, %ymm1
	vmulps	%ymm1, %ymm12, %ymm1
	vmulps	%ymm1, %ymm5, %ymm1
	vcmpeqps	LCPI40_3(%rip), %ymm0, %ymm0
	vdivps	%ymm2, %ymm1, %ymm1
	vandps	%ymm1, %ymm0, %ymm0
	movq	-96(%rsp), %rdx                 ## 8-byte Reload
	vaddps	(%rdx,%rax), %ymm0, %ymm0
	vmovaps	%ymm0, (%rdx,%rax)
	addq	%r12, %rbx
	addl	%r9d, %ecx
	incq	%rdi
	decq	%r10
	jne	LBB40_3
LBB40_4:                                ## %destructor_block
	xorl	%eax, %eax
	addq	$200, %rsp
	popq	%rbx
	popq	%r12
	popq	%r13
	popq	%r14
	popq	%r15
	popq	%rbp
	vzeroupper
	retq
                                        ## -- End function
	.section	__TEXT,__literal4,4byte_literals
	.p2align	2                               ## -- Begin function train_cost_model.par_for.sum_1_d_def__.s0.n.n
LCPI41_0:
	.long	0x3727c5ac                      ## float 9.99999974E-6
	.section	__TEXT,__const
	.p2align	5
LCPI41_1:
	.long	0                               ## 0x0
	.long	1                               ## 0x1
	.long	2                               ## 0x2
	.long	3                               ## 0x3
	.long	4                               ## 0x4
	.long	5                               ## 0x5
	.long	6                               ## 0x6
	.long	7                               ## 0x7
	.section	__TEXT,__text,regular,pure_instructions
	.p2align	4, 0x90
_train_cost_model.par_for.sum_1_d_def__.s0.n.n: ## @train_cost_model.par_for.sum_1_d_def__.s0.n.n
## %bb.0:                               ## %entry
                                        ## kill: def $esi killed $esi def $rsi
	movl	(%rdx), %edi
	movq	8(%rdx), %r8
	movq	24(%rdx), %r9
	leal	(,%rsi,8), %eax
	cmpl	%esi, 4(%rdx)
	jle	LBB41_2
## %bb.1:                               ## %then_bb
	orl	$1, %eax
	vmovd	%eax, %xmm0
	vpbroadcastd	%xmm0, %ymm0
	vpaddd	LCPI41_1(%rip), %ymm0, %ymm0
	vmovd	%edi, %xmm1
	vpbroadcastd	%xmm1, %ymm1
	vpcmpgtd	%ymm1, %ymm0, %ymm0
	vmovss	(%r8), %xmm1                    ## xmm1 = mem[0],zero,zero,zero
	vmulss	LCPI41_0(%rip), %xmm1, %xmm1
	vbroadcastss	%xmm1, %ymm1
	vpandn	%ymm1, %ymm0, %ymm0
	movslq	%esi, %rax
	shlq	$5, %rax
	vmovdqa	%ymm0, (%r9,%rax)
	xorl	%eax, %eax
	vzeroupper
	retq
LBB41_2:                                ## %next_bb
	subl	%eax, %edi
	jle	LBB41_15
## %bb.3:                               ## %"for sum_1_d_def__.s0.n.ni.preheader"
	cmpl	$8, %edi
	movl	$8, %ecx
	cmovll	%edi, %ecx
	vmovss	(%r8), %xmm0                    ## xmm0 = mem[0],zero,zero,zero
	vmulss	LCPI41_0(%rip), %xmm0, %xmm0
	movslq	%eax, %r8
	cmpl	$32, %ecx
	jae	LBB41_5
## %bb.4:
	xorl	%edx, %edx
	jmp	LBB41_13
LBB41_5:                                ## %vector.ph
	movl	%ecx, %edx
	andl	$-32, %edx
	vbroadcastss	%xmm0, %ymm1
	leaq	-32(%rdx), %rax
	movq	%rax, %rdi
	shrq	$5, %rdi
	incq	%rdi
	movl	%edi, %r10d
	andl	$7, %r10d
	cmpq	$224, %rax
	jae	LBB41_7
## %bb.6:
	xorl	%eax, %eax
	jmp	LBB41_9
LBB41_7:                                ## %vector.ph.new
	leaq	(%r9,%r8,4), %rsi
	addq	$992, %rsi                      ## imm = 0x3E0
	andq	$-8, %rdi
	negq	%rdi
	xorl	%eax, %eax
	.p2align	4, 0x90
LBB41_8:                                ## %vector.body
                                        ## =>This Inner Loop Header: Depth=1
	vmovups	%ymm1, -992(%rsi,%rax,4)
	vmovups	%ymm1, -960(%rsi,%rax,4)
	vmovups	%ymm1, -928(%rsi,%rax,4)
	vmovups	%ymm1, -896(%rsi,%rax,4)
	vmovups	%ymm1, -864(%rsi,%rax,4)
	vmovups	%ymm1, -832(%rsi,%rax,4)
	vmovups	%ymm1, -800(%rsi,%rax,4)
	vmovups	%ymm1, -768(%rsi,%rax,4)
	vmovups	%ymm1, -736(%rsi,%rax,4)
	vmovups	%ymm1, -704(%rsi,%rax,4)
	vmovups	%ymm1, -672(%rsi,%rax,4)
	vmovups	%ymm1, -640(%rsi,%rax,4)
	vmovups	%ymm1, -608(%rsi,%rax,4)
	vmovups	%ymm1, -576(%rsi,%rax,4)
	vmovups	%ymm1, -544(%rsi,%rax,4)
	vmovups	%ymm1, -512(%rsi,%rax,4)
	vmovups	%ymm1, -480(%rsi,%rax,4)
	vmovups	%ymm1, -448(%rsi,%rax,4)
	vmovups	%ymm1, -416(%rsi,%rax,4)
	vmovups	%ymm1, -384(%rsi,%rax,4)
	vmovups	%ymm1, -352(%rsi,%rax,4)
	vmovups	%ymm1, -320(%rsi,%rax,4)
	vmovups	%ymm1, -288(%rsi,%rax,4)
	vmovups	%ymm1, -256(%rsi,%rax,4)
	vmovups	%ymm1, -224(%rsi,%rax,4)
	vmovups	%ymm1, -192(%rsi,%rax,4)
	vmovups	%ymm1, -160(%rsi,%rax,4)
	vmovups	%ymm1, -128(%rsi,%rax,4)
	vmovups	%ymm1, -96(%rsi,%rax,4)
	vmovups	%ymm1, -64(%rsi,%rax,4)
	vmovups	%ymm1, -32(%rsi,%rax,4)
	vmovups	%ymm1, (%rsi,%rax,4)
	addq	$256, %rax                      ## imm = 0x100
	addq	$8, %rdi
	jne	LBB41_8
LBB41_9:                                ## %middle.block.unr-lcssa
	testq	%r10, %r10
	je	LBB41_12
## %bb.10:                              ## %vector.body.epil.preheader
	addq	%r8, %rax
	leaq	(%r9,%rax,4), %rax
	addq	$96, %rax
	shlq	$7, %r10
	xorl	%esi, %esi
	.p2align	4, 0x90
LBB41_11:                               ## %vector.body.epil
                                        ## =>This Inner Loop Header: Depth=1
	vmovups	%ymm1, -96(%rax,%rsi)
	vmovups	%ymm1, -64(%rax,%rsi)
	vmovups	%ymm1, -32(%rax,%rsi)
	vmovups	%ymm1, (%rax,%rsi)
	subq	$-128, %rsi
	cmpq	%rsi, %r10
	jne	LBB41_11
LBB41_12:                               ## %middle.block
	cmpq	%rcx, %rdx
	je	LBB41_15
LBB41_13:                               ## %"for sum_1_d_def__.s0.n.ni.preheader7"
	leaq	(%r9,%r8,4), %rax
	.p2align	4, 0x90
LBB41_14:                               ## %"for sum_1_d_def__.s0.n.ni"
                                        ## =>This Inner Loop Header: Depth=1
	vmovss	%xmm0, (%rax,%rdx,4)
	incq	%rdx
	cmpq	%rdx, %rcx
	jne	LBB41_14
LBB41_15:                               ## %destructor_block
	xorl	%eax, %eax
	vzeroupper
	retq
                                        ## -- End function
	.section	__TEXT,__literal4,4byte_literals
	.p2align	2                               ## -- Begin function train_cost_model.par_for.conv1_stage2_1_d_def__.s0.c.c
LCPI42_0:
	.long	0x80000000                      ## float -0
	.section	__TEXT,__const
	.p2align	5
LCPI42_1:
	.long	0                               ## 0x0
	.long	1                               ## 0x1
	.long	2                               ## 0x2
	.long	3                               ## 0x3
	.long	4                               ## 0x4
	.long	5                               ## 0x5
	.long	6                               ## 0x6
	.long	7                               ## 0x7
	.section	__TEXT,__text,regular,pure_instructions
	.p2align	4, 0x90
_train_cost_model.par_for.conv1_stage2_1_d_def__.s0.c.c: ## @train_cost_model.par_for.conv1_stage2_1_d_def__.s0.c.c
## %bb.0:                               ## %entry
	pushq	%rbp
	pushq	%r15
	pushq	%r14
	pushq	%r13
	pushq	%r12
	pushq	%rbx
	subq	$344, %rsp                      ## imm = 0x158
	movl	%esi, -124(%rsp)                ## 4-byte Spill
	movl	16(%rdx), %eax
	movq	%rax, 128(%rsp)                 ## 8-byte Spill
	testl	%eax, %eax
	jle	LBB42_45
## %bb.1:                               ## %"for conv1_stage2_1_d_def__.s0.w.preheader"
	movslq	(%rdx), %rsi
	movl	4(%rdx), %ebx
	movslq	8(%rdx), %r12
	movslq	12(%rdx), %rbp
	movslq	20(%rdx), %r9
	movl	24(%rdx), %r15d
	movl	28(%rdx), %r13d
	movslq	36(%rdx), %rcx
	movl	%ecx, %r11d
	movq	40(%rdx), %r8
	movq	56(%rdx), %r10
	movq	72(%rdx), %rdi
	movl	-124(%rsp), %eax                ## 4-byte Reload
	addl	%eax, %eax
	movl	%eax, -124(%rsp)                ## 4-byte Spill
	movl	%ebx, -16(%rsp)                 ## 4-byte Spill
                                        ## kill: def $ebx killed $ebx def $rbx
	shll	$5, %ebx
	movq	%rbx, (%rsp)                    ## 8-byte Spill
	movl	%ebp, %ebx
	shll	$5, %ebx
	movl	%ebx, -116(%rsp)                ## 4-byte Spill
	movl	%esi, %ebx
	shll	$5, %ebx
	movl	%ebx, -120(%rsp)                ## 4-byte Spill
	movq	88(%rdx), %r14
	movq	%r12, 112(%rsp)                 ## 8-byte Spill
	leal	-1(%r12), %eax
	movl	%eax, -24(%rsp)                 ## 4-byte Spill
	movq	%r13, 64(%rsp)                  ## 8-byte Spill
	testl	%r13d, %r13d
	movq	%r11, 8(%rsp)                   ## 8-byte Spill
	movq	%r8, -80(%rsp)                  ## 8-byte Spill
	movq	%r14, -112(%rsp)                ## 8-byte Spill
	movq	%rsi, -104(%rsp)                ## 8-byte Spill
	movq	%r9, 72(%rsp)                   ## 8-byte Spill
	movl	%r15d, -20(%rsp)                ## 4-byte Spill
	movq	%rcx, 168(%rsp)                 ## 8-byte Spill
	movq	%r10, -88(%rsp)                 ## 8-byte Spill
	movq	%rdi, -96(%rsp)                 ## 8-byte Spill
	jle	LBB42_34
## %bb.2:                               ## %"for conv1_stage2_1_d_def__.s0.w.us.preheader"
	movslq	32(%rdx), %rax
	movq	%rax, 104(%rsp)                 ## 8-byte Spill
	leal	-1(%rsi), %eax
	vmovd	%eax, %xmm0
	vpbroadcastd	%xmm0, %ymm12
	leal	(,%r11,8), %eax
	movl	%esi, %edx
	subl	%eax, %edx
	movl	%edx, -56(%rsp)                 ## 4-byte Spill
	movl	%r11d, %edx
	andl	$-2, %edx
	leaq	32(%r10), %rax
	movq	%rax, 192(%rsp)                 ## 8-byte Spill
	leaq	32(%rdi), %rax
	movq	%rax, 184(%rsp)                 ## 8-byte Spill
	movl	%r15d, %ebx
	imull	%ebp, %ebx
	shll	$5, %ebx
	negl	%ebx
	leaq	(,%r9,4), %rax
	movq	%r8, %rsi
	subq	%rax, %rsi
	movq	%rsi, 176(%rsp)                 ## 8-byte Spill
	leaq	(,%rcx,8), %rax
	movq	%rax, 280(%rsp)                 ## 8-byte Spill
	shlq	$5, %rcx
	leaq	(%r10,%rcx), %rax
	addq	$32, %rax
	movq	%rax, 264(%rsp)                 ## 8-byte Spill
	leaq	32(%rdi,%rcx), %rax
	movq	%rax, 256(%rsp)                 ## 8-byte Spill
	leaq	32(%r14,%rcx), %rax
	movq	%rax, 248(%rsp)                 ## 8-byte Spill
	leaq	32(%r8), %rax
	movq	%rax, 80(%rsp)                  ## 8-byte Spill
	leal	(,%r11,8), %eax
	subl	%r9d, %eax
	movl	%eax, 120(%rsp)                 ## 4-byte Spill
	addq	%r14, %rcx
	movq	%rcx, 272(%rsp)                 ## 8-byte Spill
	xorl	%eax, %eax
	vxorps	%xmm1, %xmm1, %xmm1
	vpbroadcastd	LCPI42_0(%rip), %ymm2   ## ymm2 = [-0.0E+0,-0.0E+0,-0.0E+0,-0.0E+0,-0.0E+0,-0.0E+0,-0.0E+0,-0.0E+0]
	vbroadcastss	LCPI42_0(%rip), %xmm3   ## xmm3 = [-0.0E+0,-0.0E+0,-0.0E+0,-0.0E+0]
	vmovdqa	LCPI42_1(%rip), %ymm8           ## ymm8 = [0,1,2,3,4,5,6,7]
	xorl	%edi, %edi
	xorl	%r10d, %r10d
	movq	(%rsp), %rcx                    ## 8-byte Reload
	movl	-116(%rsp), %r12d               ## 4-byte Reload
	movl	-120(%rsp), %esi                ## 4-byte Reload
	movq	%rdx, 136(%rsp)                 ## 8-byte Spill
	movq	%r9, %r13
	movq	%rbp, -64(%rsp)                 ## 8-byte Spill
	jmp	LBB42_3
	.p2align	4, 0x90
LBB42_44:                               ## %"end for conv1_stage2_1_d_def__.s0.c.ci.split.us.us"
                                        ##   in Loop: Header=BB42_3 Depth=1
	incq	%r10
	movl	-120(%rsp), %esi                ## 4-byte Reload
	movl	-12(%rsp), %edi                 ## 4-byte Reload
	addl	%esi, %edi
	movl	-116(%rsp), %r12d               ## 4-byte Reload
	movl	-8(%rsp), %ebx                  ## 4-byte Reload
	addl	%r12d, %ebx
	movq	(%rsp), %rcx                    ## 8-byte Reload
	movq	160(%rsp), %rax                 ## 8-byte Reload
	addl	%ecx, %eax
	cmpq	128(%rsp), %r10                 ## 8-byte Folded Reload
	movq	-64(%rsp), %rbp                 ## 8-byte Reload
	movq	72(%rsp), %r13                  ## 8-byte Reload
	movl	-20(%rsp), %r15d                ## 4-byte Reload
	movq	-112(%rsp), %r14                ## 8-byte Reload
	movq	136(%rsp), %rdx                 ## 8-byte Reload
	je	LBB42_45
LBB42_3:                                ## %"for conv1_stage2_1_d_def__.s0.w.us"
                                        ## =>This Loop Header: Depth=1
                                        ##     Child Loop BB42_7 Depth 2
                                        ##     Child Loop BB42_12 Depth 2
                                        ##       Child Loop BB42_27 Depth 3
                                        ##       Child Loop BB42_14 Depth 3
	movslq	%eax, %r9
	movl	%edi, -12(%rsp)                 ## 4-byte Spill
	movslq	%edi, %rax
	movq	%rax, 240(%rsp)                 ## 8-byte Spill
	movl	%ebx, -8(%rsp)                  ## 4-byte Spill
	movslq	%ebx, %rax
	movq	%rax, 232(%rsp)                 ## 8-byte Spill
	movl	%ecx, %r8d
	imull	%r10d, %r8d
	cmpq	112(%rsp), %r10                 ## 8-byte Folded Reload
	setl	%al
	movl	-24(%rsp), %ebx                 ## 4-byte Reload
                                        ## kill: def $ebx killed $ebx def $rbx
	cmovll	%r10d, %ebx
	imull	%ecx, %ebx
                                        ## kill: def $esi killed $esi def $rsi
	imull	%r10d, %esi
	movl	%r15d, %ecx
	movl	%r10d, %r15d
	subl	%ecx, %r15d
	imull	%r12d, %r15d
	movslq	%ebx, %rdi
	subl	%r13d, %ebx
	movslq	%r8d, %rcx
	movq	%rcx, 312(%rsp)                 ## 8-byte Spill
	subl	%r13d, %r8d
	movq	%rbx, %rcx
	movq	%rbx, 304(%rsp)                 ## 8-byte Spill
	movslq	%ebx, %rcx
	movq	%rcx, 224(%rsp)                 ## 8-byte Spill
	vmovd	%eax, %xmm0
	vpbroadcastb	%xmm0, %xmm5
	movl	%r8d, 124(%rsp)                 ## 4-byte Spill
	movslq	%r8d, %rax
	movq	%rax, 216(%rsp)                 ## 8-byte Spill
	movq	%r15, %rax
	movq	%r15, 288(%rsp)                 ## 8-byte Spill
	movslq	%r15d, %rax
	movq	%rax, 152(%rsp)                 ## 8-byte Spill
	movq	%rsi, %rax
	movq	%rsi, 296(%rsp)                 ## 8-byte Spill
	movslq	%esi, %rax
	movq	%rax, 144(%rsp)                 ## 8-byte Spill
	movq	176(%rsp), %rax                 ## 8-byte Reload
	movq	%r9, 160(%rsp)                  ## 8-byte Spill
	leaq	(%rax,%r9,4), %rcx
	movq	%rcx, 208(%rsp)                 ## 8-byte Spill
	movq	%rdi, 16(%rsp)                  ## 8-byte Spill
	leaq	(%rax,%rdi,4), %rax
	movq	%rax, 200(%rsp)                 ## 8-byte Spill
	movl	$0, -4(%rsp)                    ## 4-byte Folded Spill
	movl	-124(%rsp), %r8d                ## 4-byte Reload
	movq	-104(%rsp), %rdi                ## 8-byte Reload
	movl	%r8d, %r15d
	imull	-16(%rsp), %r15d                ## 4-byte Folded Reload
	testl	%r11d, %r11d
	jle	LBB42_5
	.p2align	4, 0x90
LBB42_32:                               ## %"for conv1_stage2_1_d_def__.s0.n.n.preheader.us.us"
                                        ##   in Loop: Header=BB42_3 Depth=1
	movslq	%r8d, %rcx
	movq	%rcx, %rsi
	imulq	%rbp, %rsi
	imulq	%rdi, %rcx
	movslq	%r15d, %rdi
	cmpl	$1, %r11d
	jne	LBB42_6
## %bb.33:                              ##   in Loop: Header=BB42_3 Depth=1
	movq	%rdi, %r13
	movq	%rsi, %r12
	movq	%rcx, %r9
	xorl	%edi, %edi
	jmp	LBB42_8
	.p2align	4, 0x90
LBB42_6:                                ## %"for conv1_stage2_1_d_def__.s0.n.n.us.us.preheader"
                                        ##   in Loop: Header=BB42_3 Depth=1
	movq	240(%rsp), %rax                 ## 8-byte Reload
	movq	%rcx, %r9
	addq	%rcx, %rax
	movq	192(%rsp), %rcx                 ## 8-byte Reload
	leaq	(%rcx,%rax,4), %rcx
	movq	232(%rsp), %rax                 ## 8-byte Reload
	movq	%rsi, %r12
	addq	%rsi, %rax
	movq	184(%rsp), %rsi                 ## 8-byte Reload
	leaq	(%rsi,%rax,4), %rsi
	movq	208(%rsp), %rax                 ## 8-byte Reload
	leaq	(%rax,%rdi,4), %rbp
	movq	200(%rsp), %rax                 ## 8-byte Reload
	movq	%rdi, %r13
	leaq	(%rax,%rdi,4), %rbx
	xorl	%eax, %eax
	xorl	%edi, %edi
	.p2align	4, 0x90
LBB42_7:                                ## %"for conv1_stage2_1_d_def__.s0.n.n.us.us"
                                        ##   Parent Loop BB42_3 Depth=1
                                        ## =>  This Inner Loop Header: Depth=2
	vmovups	(%rbx,%rax), %ymm0
	vcmpleps	%ymm1, %ymm0, %ymm0
	vextractf128	$1, %ymm0, %xmm4
	vpackssdw	%xmm4, %xmm0, %xmm0
	vpand	%xmm5, %xmm0, %xmm0
	vpmovzxwd	%xmm0, %ymm0            ## ymm0 = xmm0[0],zero,xmm0[1],zero,xmm0[2],zero,xmm0[3],zero,xmm0[4],zero,xmm0[5],zero,xmm0[6],zero,xmm0[7],zero
	vpslld	$31, %ymm0, %ymm0
	vpsrad	$31, %ymm0, %ymm0
	vpxor	(%r14,%rax), %ymm2, %ymm4
	vcmpleps	(%rbp,%rax), %ymm1, %ymm6
	vpand	%ymm4, %ymm0, %ymm0
	vandps	-32(%rsi,%rax), %ymm6, %ymm4
	vaddps	%ymm4, %ymm0, %ymm0
	vmovups	%ymm0, -32(%rcx,%rax)
	vmovups	32(%rbx,%rax), %ymm0
	vcmpleps	%ymm1, %ymm0, %ymm0
	vextractf128	$1, %ymm0, %xmm4
	vpackssdw	%xmm4, %xmm0, %xmm0
	vpand	%xmm5, %xmm0, %xmm0
	vpmovzxwd	%xmm0, %ymm0            ## ymm0 = xmm0[0],zero,xmm0[1],zero,xmm0[2],zero,xmm0[3],zero,xmm0[4],zero,xmm0[5],zero,xmm0[6],zero,xmm0[7],zero
	vpslld	$31, %ymm0, %ymm0
	vpsrad	$31, %ymm0, %ymm0
	vpxor	32(%r14,%rax), %ymm2, %ymm4
	vcmpleps	32(%rbp,%rax), %ymm1, %ymm6
	vandps	(%rsi,%rax), %ymm6, %ymm6
	vpand	%ymm4, %ymm0, %ymm0
	vaddps	%ymm6, %ymm0, %ymm0
	vmovups	%ymm0, (%rcx,%rax)
	addq	$2, %rdi
	addq	$64, %rax
	cmpq	%rdi, %rdx
	jne	LBB42_7
LBB42_8:                                ## %"end for conv1_stage2_1_d_def__.s0.n.n.us.us.loopexit.unr-lcssa"
                                        ##   in Loop: Header=BB42_3 Depth=1
	addq	152(%rsp), %r12                 ## 8-byte Folded Reload
	addq	144(%rsp), %r9                  ## 8-byte Folded Reload
	testb	$1, %r11b
	movq	%r9, 32(%rsp)                   ## 8-byte Spill
	movq	%r12, 24(%rsp)                  ## 8-byte Spill
	je	LBB42_10
## %bb.9:                               ## %"for conv1_stage2_1_d_def__.s0.n.n.us.us.epil"
                                        ##   in Loop: Header=BB42_3 Depth=1
	leaq	(,%rdi,8), %rax
	addq	%r13, %rax
	movq	224(%rsp), %rcx                 ## 8-byte Reload
	leaq	(%rax,%rcx), %rcx
	movq	-80(%rsp), %rsi                 ## 8-byte Reload
	vmovups	(%rsi,%rcx,4), %ymm0
	vcmpleps	%ymm1, %ymm0, %ymm0
	vextractf128	$1, %ymm0, %xmm4
	vpackssdw	%xmm4, %xmm0, %xmm0
	vpand	%xmm5, %xmm0, %xmm0
	vpmovzxwd	%xmm0, %ymm0            ## ymm0 = xmm0[0],zero,xmm0[1],zero,xmm0[2],zero,xmm0[3],zero,xmm0[4],zero,xmm0[5],zero,xmm0[6],zero,xmm0[7],zero
	vpslld	$31, %ymm0, %ymm0
	movq	%rdi, %rcx
	shlq	$5, %rcx
	vpxor	(%r14,%rcx), %ymm2, %ymm4
	vpsrad	$31, %ymm0, %ymm0
	vpand	%ymm4, %ymm0, %ymm0
	addq	216(%rsp), %rax                 ## 8-byte Folded Reload
	leaq	(%r12,%rdi,8), %rcx
	vcmpleps	(%rsi,%rax,4), %ymm1, %ymm4
	movq	-96(%rsp), %rax                 ## 8-byte Reload
	vandps	(%rax,%rcx,4), %ymm4, %ymm4
	vaddps	%ymm4, %ymm0, %ymm0
	leaq	(%r9,%rdi,8), %rax
	movq	-88(%rsp), %rcx                 ## 8-byte Reload
	vmovups	%ymm0, (%rcx,%rax,4)
LBB42_10:                               ## %"end for conv1_stage2_1_d_def__.s0.n.n.us.us"
                                        ##   in Loop: Header=BB42_3 Depth=1
	movq	-104(%rsp), %rdi                ## 8-byte Reload
LBB42_11:                               ## %"end for conv1_stage2_1_d_def__.s0.n.n.us.us"
                                        ##   in Loop: Header=BB42_3 Depth=1
	movq	304(%rsp), %rax                 ## 8-byte Reload
	addl	%r15d, %eax
	vmovd	%eax, %xmm0
	vpbroadcastd	%xmm0, %ymm13
	addl	124(%rsp), %r15d                ## 4-byte Folded Reload
	vmovd	%r15d, %xmm0
	vpbroadcastd	%xmm0, %ymm14
	movl	%r8d, %eax
	imull	-64(%rsp), %eax                 ## 4-byte Folded Reload
	movq	288(%rsp), %rcx                 ## 8-byte Reload
	addl	%eax, %ecx
	movl	%edi, %edx
	imull	%r8d, %edx
	movq	296(%rsp), %rsi                 ## 8-byte Reload
	addl	%edx, %esi
	movslq	%ecx, %rcx
	movq	%rcx, 328(%rsp)                 ## 8-byte Spill
	movslq	%esi, %rcx
	movq	%rcx, 320(%rsp)                 ## 8-byte Spill
	addl	-12(%rsp), %edx                 ## 4-byte Folded Reload
	movslq	%edx, %rdx
	movq	264(%rsp), %rcx                 ## 8-byte Reload
	leaq	(%rcx,%rdx,4), %r11
	addl	-8(%rsp), %eax                  ## 4-byte Folded Reload
	movslq	%eax, %rcx
	movq	256(%rsp), %rax                 ## 8-byte Reload
	leaq	(%rax,%rcx,4), %r15
	movq	160(%rsp), %rax                 ## 8-byte Reload
	addq	%r13, %rax
	movq	%rax, 48(%rsp)                  ## 8-byte Spill
	movq	16(%rsp), %rax                  ## 8-byte Reload
	movq	%r13, 336(%rsp)                 ## 8-byte Spill
	addq	%r13, %rax
	movq	%rax, 40(%rsp)                  ## 8-byte Spill
	movq	280(%rsp), %rax                 ## 8-byte Reload
	addq	%rax, %rdx
	movq	%rdx, -32(%rsp)                 ## 8-byte Spill
	addq	%rax, %rcx
	movq	%rcx, -40(%rsp)                 ## 8-byte Spill
	movq	272(%rsp), %rax                 ## 8-byte Reload
	movq	%rax, -48(%rsp)                 ## 8-byte Spill
	movl	-56(%rsp), %r12d                ## 4-byte Reload
	movl	120(%rsp), %eax                 ## 4-byte Reload
	movl	%eax, -72(%rsp)                 ## 4-byte Spill
	movq	248(%rsp), %rax                 ## 8-byte Reload
	xorl	%r13d, %r13d
	jmp	LBB42_12
	.p2align	4, 0x90
LBB42_13:                               ## %then_bb.us.us
                                        ##   in Loop: Header=BB42_12 Depth=2
	movq	104(%rsp), %rcx                 ## 8-byte Reload
	leaq	(%rcx,%r13), %rbx
	leal	(,%rbx,8), %ecx
	vmovd	%ecx, %xmm0
	vpbroadcastd	%xmm0, %ymm0
	vpor	%ymm0, %ymm8, %ymm0
	vpminsd	%ymm0, %ymm12, %ymm10
	vpaddd	%ymm13, %ymm10, %ymm0
	vextracti128	$1, %ymm0, %xmm4
	vmovd	%xmm4, %ecx
	movslq	%ecx, %rcx
	vpextrd	$1, %xmm4, %edx
	movslq	%edx, %rdx
	movq	-80(%rsp), %rsi                 ## 8-byte Reload
	vmovss	(%rsi,%rcx,4), %xmm6            ## xmm6 = mem[0],zero,zero,zero
	vpextrd	$2, %xmm4, %ecx
	vinsertps	$16, (%rsi,%rdx,4), %xmm6, %xmm6 ## xmm6 = xmm6[0],mem[0],xmm6[2,3]
	vmovd	%xmm0, %edx
	movslq	%ecx, %rcx
	vinsertps	$32, (%rsi,%rcx,4), %xmm6, %xmm6 ## xmm6 = xmm6[0,1],mem[0],xmm6[3]
	movslq	%edx, %rcx
	vpextrd	$3, %xmm4, %edx
	movslq	%edx, %rdx
	vinsertps	$48, (%rsi,%rdx,4), %xmm6, %xmm4 ## xmm4 = xmm6[0,1,2],mem[0]
	vpextrd	$1, %xmm0, %edx
	vmovss	(%rsi,%rcx,4), %xmm6            ## xmm6 = mem[0],zero,zero,zero
	movslq	%edx, %rcx
	vinsertps	$16, (%rsi,%rcx,4), %xmm6, %xmm6 ## xmm6 = xmm6[0],mem[0],xmm6[2,3]
	vpextrd	$2, %xmm0, %ecx
	movslq	%ecx, %rcx
	vinsertps	$32, (%rsi,%rcx,4), %xmm6, %xmm6 ## xmm6 = xmm6[0,1],mem[0],xmm6[3]
	vpextrd	$3, %xmm0, %ecx
	movslq	%ecx, %rcx
	vinsertps	$48, (%rsi,%rcx,4), %xmm6, %xmm9 ## xmm9 = xmm6[0,1,2],mem[0]
	movq	%rbx, %rcx
	shlq	$5, %rcx
	movq	-112(%rsp), %rdx                ## 8-byte Reload
	vpxor	(%rdx,%rcx), %ymm2, %ymm11
	vpaddd	%ymm14, %ymm10, %ymm0
	vextracti128	$1, %ymm0, %xmm6
	vmovd	%xmm6, %ecx
	movslq	%ecx, %rcx
	vmovss	(%rsi,%rcx,4), %xmm7            ## xmm7 = mem[0],zero,zero,zero
	vpextrd	$1, %xmm6, %ecx
	movslq	%ecx, %rcx
	vinsertps	$16, (%rsi,%rcx,4), %xmm7, %xmm7 ## xmm7 = xmm7[0],mem[0],xmm7[2,3]
	vpextrd	$2, %xmm6, %ecx
	movslq	%ecx, %rcx
	vinsertps	$32, (%rsi,%rcx,4), %xmm7, %xmm7 ## xmm7 = xmm7[0,1],mem[0],xmm7[3]
	vpextrd	$3, %xmm6, %ecx
	movslq	%ecx, %rcx
	vinsertps	$48, (%rsi,%rcx,4), %xmm7, %xmm6 ## xmm6 = xmm7[0,1,2],mem[0]
	vmovd	%xmm0, %ecx
	movslq	%ecx, %rcx
	vmovss	(%rsi,%rcx,4), %xmm7            ## xmm7 = mem[0],zero,zero,zero
	vpextrd	$1, %xmm0, %ecx
	movslq	%ecx, %rcx
	vinsertps	$16, (%rsi,%rcx,4), %xmm7, %xmm7 ## xmm7 = xmm7[0],mem[0],xmm7[2,3]
	vpextrd	$2, %xmm0, %ecx
	movslq	%ecx, %rcx
	vinsertps	$32, (%rsi,%rcx,4), %xmm7, %xmm7 ## xmm7 = xmm7[0,1],mem[0],xmm7[3]
	vpextrd	$3, %xmm0, %ecx
	movslq	%ecx, %rcx
	vinsertps	$48, (%rsi,%rcx,4), %xmm7, %xmm0 ## xmm0 = xmm7[0,1,2],mem[0]
	movq	24(%rsp), %rcx                  ## 8-byte Reload
	leaq	(%rcx,%rbx,8), %rcx
	vinsertf128	$1, %xmm6, %ymm0, %ymm0
	vcmpleps	%ymm0, %ymm1, %ymm0
	movq	-96(%rsp), %rdx                 ## 8-byte Reload
	vandps	(%rdx,%rcx,4), %ymm0, %ymm0
	movq	32(%rsp), %rcx                  ## 8-byte Reload
	leaq	(%rcx,%rbx,8), %rcx
	vinsertf128	$1, %xmm4, %ymm9, %ymm4
	vcmpleps	%ymm1, %ymm4, %ymm4
	vextractf128	$1, %ymm4, %xmm6
	vpackssdw	%xmm6, %xmm4, %xmm4
	vpand	%xmm5, %xmm4, %xmm4
	vpmovzxwd	%xmm4, %ymm4            ## ymm4 = xmm4[0],zero,xmm4[1],zero,xmm4[2],zero,xmm4[3],zero,xmm4[4],zero,xmm4[5],zero,xmm4[6],zero,xmm4[7],zero
	vpslld	$31, %ymm4, %ymm4
	vpsrad	$31, %ymm4, %ymm4
	vpand	%ymm4, %ymm11, %ymm4
	vaddps	%ymm0, %ymm4, %ymm0
	movq	-88(%rsp), %rdx                 ## 8-byte Reload
	vmovups	%ymm0, (%rdx,%rcx,4)
LBB42_18:                               ## %after_bb.us.us
                                        ##   in Loop: Header=BB42_12 Depth=2
	incq	%r13
	addq	$32, %r11
	addq	$32, %r15
	addq	$32, %rax
	addl	$8, -72(%rsp)                   ## 4-byte Folded Spill
	addl	$-8, %r12d
	addq	$8, -32(%rsp)                   ## 8-byte Folded Spill
	addq	$8, -40(%rsp)                   ## 8-byte Folded Spill
	addq	$32, -48(%rsp)                  ## 8-byte Folded Spill
	cmpq	64(%rsp), %r13                  ## 8-byte Folded Reload
	movq	-104(%rsp), %rdi                ## 8-byte Reload
	je	LBB42_19
LBB42_12:                               ## %"for conv1_stage2_1_d_def__.s0.n.n.rebased.us.us"
                                        ##   Parent Loop BB42_3 Depth=1
                                        ## =>  This Loop Header: Depth=2
                                        ##       Child Loop BB42_27 Depth 3
                                        ##       Child Loop BB42_14 Depth 3
	cmpl	$8, %r12d
	movl	$8, %r14d
	cmovll	%r12d, %r14d
	leal	(,%r13,8), %ecx
	movl	-56(%rsp), %edx                 ## 4-byte Reload
	movl	%edx, %ebx
	subl	%ecx, %ebx
	cmpl	$8, %ebx
	movl	$8, %ecx
	cmovgel	%ecx, %ebx
	movq	168(%rsp), %rcx                 ## 8-byte Reload
	leaq	(%rcx,%r13), %r8
	leal	8(,%r8,8), %ecx
	cmpl	%edi, %ecx
	jle	LBB42_13
## %bb.21:                              ## %next_bb.us.us
                                        ##   in Loop: Header=BB42_12 Depth=2
	leal	(,%r8,8), %edx
	cmpl	%edx, %edi
	jle	LBB42_18
## %bb.22:                              ## %"for conv1_stage2_1_d_def__.s0.n.ni.preheader.us.us"
                                        ##   in Loop: Header=BB42_12 Depth=2
	movslq	-72(%rsp), %rdi                 ## 4-byte Folded Reload
	cmpl	$8, %ebx
	movq	%rdi, 56(%rsp)                  ## 8-byte Spill
	jae	LBB42_24
## %bb.23:                              ##   in Loop: Header=BB42_12 Depth=2
	xorl	%r9d, %r9d
	jmp	LBB42_31
	.p2align	4, 0x90
LBB42_24:                               ## %vector.ph
                                        ##   in Loop: Header=BB42_12 Depth=2
	movl	%ebx, %ecx
	andl	$-8, %ecx
	addq	$-8, %rcx
	movq	%rcx, %rsi
	shrq	$3, %rsi
	incq	%rsi
	testq	%rcx, %rcx
	movl	%edx, 88(%rsp)                  ## 4-byte Spill
	movq	%rsi, 96(%rsp)                  ## 8-byte Spill
	je	LBB42_25
## %bb.26:                              ## %vector.ph.new
                                        ##   in Loop: Header=BB42_12 Depth=2
	movq	48(%rsp), %rcx                  ## 8-byte Reload
	addq	%rdi, %rcx
	movq	80(%rsp), %rsi                  ## 8-byte Reload
	leaq	(%rsi,%rcx,4), %rdx
	movq	40(%rsp), %rcx                  ## 8-byte Reload
	addq	%rdi, %rcx
	leaq	(%rsi,%rcx,4), %rbp
	movl	%r14d, %esi
	andl	$-8, %esi
	addq	$-8, %rsi
	shrq	$3, %rsi
	incq	%rsi
	andq	$-2, %rsi
	negq	%rsi
	xorl	%ecx, %ecx
	.p2align	4, 0x90
LBB42_27:                               ## %vector.body
                                        ##   Parent Loop BB42_3 Depth=1
                                        ##     Parent Loop BB42_12 Depth=2
                                        ## =>    This Inner Loop Header: Depth=3
	vmovups	-32(%rbp,%rcx,4), %ymm0
	vcmpleps	%ymm1, %ymm0, %ymm0
	vextractf128	$1, %ymm0, %xmm4
	vpackssdw	%xmm4, %xmm0, %xmm0
	vpand	%xmm0, %xmm5, %xmm0
	vpmovzxwd	%xmm0, %ymm0            ## ymm0 = xmm0[0],zero,xmm0[1],zero,xmm0[2],zero,xmm0[3],zero,xmm0[4],zero,xmm0[5],zero,xmm0[6],zero,xmm0[7],zero
	vpslld	$31, %ymm0, %ymm0
	vpsrad	$31, %ymm0, %ymm0
	vpxor	-32(%rax,%rcx,4), %ymm2, %ymm4
	vcmpleps	-32(%rdx,%rcx,4), %ymm1, %ymm6
	vandps	-32(%r15,%rcx,4), %ymm6, %ymm6
	vpand	%ymm4, %ymm0, %ymm0
	vaddps	%ymm6, %ymm0, %ymm0
	vmovups	%ymm0, -32(%r11,%rcx,4)
	vmovups	(%rbp,%rcx,4), %ymm0
	vcmpleps	%ymm1, %ymm0, %ymm0
	vextractf128	$1, %ymm0, %xmm4
	vpackssdw	%xmm4, %xmm0, %xmm0
	vpand	%xmm0, %xmm5, %xmm0
	vpmovzxwd	%xmm0, %ymm0            ## ymm0 = xmm0[0],zero,xmm0[1],zero,xmm0[2],zero,xmm0[3],zero,xmm0[4],zero,xmm0[5],zero,xmm0[6],zero,xmm0[7],zero
	vpslld	$31, %ymm0, %ymm0
	vpsrad	$31, %ymm0, %ymm0
	vpxor	(%rax,%rcx,4), %ymm2, %ymm4
	vcmpleps	(%rdx,%rcx,4), %ymm1, %ymm6
	vpand	%ymm4, %ymm0, %ymm0
	vandps	(%r15,%rcx,4), %ymm6, %ymm4
	vaddps	%ymm4, %ymm0, %ymm0
	vmovups	%ymm0, (%r11,%rcx,4)
	addq	$16, %rcx
	addq	$2, %rsi
	jne	LBB42_27
## %bb.28:                              ## %middle.block.unr-lcssa
                                        ##   in Loop: Header=BB42_12 Depth=2
	movl	%ebx, %r9d
	andl	$-8, %r9d
	testb	$1, 96(%rsp)                    ## 1-byte Folded Reload
	je	LBB42_30
LBB42_29:                               ## %vector.body.epil
                                        ##   in Loop: Header=BB42_12 Depth=2
	movl	88(%rsp), %edx                  ## 4-byte Reload
	subl	72(%rsp), %edx                  ## 4-byte Folded Reload
	shlq	$3, %r8
	movslq	%edx, %rdx
	addq	%rcx, %rdx
	addq	336(%rsp), %rdx                 ## 8-byte Folded Reload
	movq	16(%rsp), %rsi                  ## 8-byte Reload
	addq	%rdx, %rsi
	movq	-80(%rsp), %rdi                 ## 8-byte Reload
	vmovups	(%rdi,%rsi,4), %ymm0
	vcmpleps	%ymm1, %ymm0, %ymm0
	vextractf128	$1, %ymm0, %xmm4
	vpackssdw	%xmm4, %xmm0, %xmm0
	vpand	%xmm0, %xmm5, %xmm0
	vpmovzxwd	%xmm0, %ymm0            ## ymm0 = xmm0[0],zero,xmm0[1],zero,xmm0[2],zero,xmm0[3],zero,xmm0[4],zero,xmm0[5],zero,xmm0[6],zero,xmm0[7],zero
	vpslld	$31, %ymm0, %ymm0
	addq	%rcx, %r8
	movq	-112(%rsp), %rcx                ## 8-byte Reload
	vpxor	(%rcx,%r8,4), %ymm2, %ymm4
	vpsrad	$31, %ymm0, %ymm0
	vpand	%ymm4, %ymm0, %ymm0
	addq	312(%rsp), %rdx                 ## 8-byte Folded Reload
	movq	328(%rsp), %rcx                 ## 8-byte Reload
	addq	%r8, %rcx
	vcmpleps	(%rdi,%rdx,4), %ymm1, %ymm4
	movq	-96(%rsp), %rdx                 ## 8-byte Reload
	vandps	(%rdx,%rcx,4), %ymm4, %ymm4
	vaddps	%ymm4, %ymm0, %ymm0
	addq	320(%rsp), %r8                  ## 8-byte Folded Reload
	movq	-88(%rsp), %rcx                 ## 8-byte Reload
	vmovups	%ymm0, (%rcx,%r8,4)
LBB42_30:                               ## %middle.block
                                        ##   in Loop: Header=BB42_12 Depth=2
	cmpq	%rbx, %r9
	je	LBB42_18
LBB42_31:                               ## %"for conv1_stage2_1_d_def__.s0.n.ni.us.us.preheader"
                                        ##   in Loop: Header=BB42_12 Depth=2
	subq	%r9, %r14
	movq	-32(%rsp), %rcx                 ## 8-byte Reload
	addq	%r9, %rcx
	movq	-88(%rsp), %rdx                 ## 8-byte Reload
	leaq	(%rdx,%rcx,4), %rcx
	movq	-40(%rsp), %rdx                 ## 8-byte Reload
	addq	%r9, %rdx
	movq	-96(%rsp), %rsi                 ## 8-byte Reload
	leaq	(%rsi,%rdx,4), %rdx
	movq	-48(%rsp), %rsi                 ## 8-byte Reload
	leaq	(%rsi,%r9,4), %rsi
	movq	48(%rsp), %rdi                  ## 8-byte Reload
	addq	%r9, %rdi
	movq	56(%rsp), %r8                   ## 8-byte Reload
	addq	%r8, %rdi
	movq	-80(%rsp), %rbx                 ## 8-byte Reload
	leaq	(%rbx,%rdi,4), %rbp
	addq	40(%rsp), %r9                   ## 8-byte Folded Reload
	addq	%r8, %r9
	leaq	(%rbx,%r9,4), %rbx
	xorl	%edi, %edi
	jmp	LBB42_14
	.p2align	4, 0x90
LBB42_16:                               ## %"for conv1_stage2_1_d_def__.s0.n.ni.us.us"
                                        ##   in Loop: Header=BB42_14 Depth=3
	vxorps	%xmm0, %xmm0, %xmm0
LBB42_17:                               ## %"for conv1_stage2_1_d_def__.s0.n.ni.us.us"
                                        ##   in Loop: Header=BB42_14 Depth=3
	vmovss	(%rbp,%rdi,4), %xmm6            ## xmm6 = mem[0],zero,zero,zero
	vmovss	(%rdx,%rdi,4), %xmm7            ## xmm7 = mem[0],zero,zero,zero
	vcmpltss	%xmm4, %xmm6, %xmm4
	vandnps	%xmm7, %xmm4, %xmm4
	vaddss	%xmm4, %xmm0, %xmm0
	vmovss	%xmm0, (%rcx,%rdi,4)
	incq	%rdi
	cmpq	%rdi, %r14
	je	LBB42_18
LBB42_14:                               ## %"for conv1_stage2_1_d_def__.s0.n.ni.us.us"
                                        ##   Parent Loop BB42_3 Depth=1
                                        ##     Parent Loop BB42_12 Depth=2
                                        ## =>    This Inner Loop Header: Depth=3
	vxorps	%xmm4, %xmm4, %xmm4
	cmpq	112(%rsp), %r10                 ## 8-byte Folded Reload
	jge	LBB42_16
## %bb.15:                              ##   in Loop: Header=BB42_14 Depth=3
	vmovss	(%rbx,%rdi,4), %xmm0            ## xmm0 = mem[0],zero,zero,zero
	vmovss	(%rsi,%rdi,4), %xmm6            ## xmm6 = mem[0],zero,zero,zero
	vxorps	%xmm3, %xmm6, %xmm6
	vcmpless	%xmm4, %xmm0, %xmm0
	vandps	%xmm6, %xmm0, %xmm0
	jmp	LBB42_17
LBB42_25:                               ##   in Loop: Header=BB42_12 Depth=2
	xorl	%ecx, %ecx
	movl	%ebx, %r9d
	andl	$-8, %r9d
	testb	$1, 96(%rsp)                    ## 1-byte Folded Reload
	jne	LBB42_29
	jmp	LBB42_30
	.p2align	4, 0x90
LBB42_19:                               ## %"end for conv1_stage2_1_d_def__.s0.n.n.rebased.loopexit.us.us"
                                        ##   in Loop: Header=BB42_3 Depth=1
	testb	$1, -4(%rsp)                    ## 1-byte Folded Reload
	movq	8(%rsp), %r11                   ## 8-byte Reload
	jne	LBB42_44
## %bb.20:                              ## %"end for conv1_stage2_1_d_def__.s0.n.n.rebased.loopexit.us.us.for conv1_stage2_1_d_def__.s0.c.ci.us.us_crit_edge"
                                        ##   in Loop: Header=BB42_3 Depth=1
	movl	-124(%rsp), %r8d                ## 4-byte Reload
	orl	$1, %r8d
	movb	$1, %al
	movl	%eax, -4(%rsp)                  ## 4-byte Spill
	movq	-64(%rsp), %rbp                 ## 8-byte Reload
	movq	-112(%rsp), %r14                ## 8-byte Reload
	movq	136(%rsp), %rdx                 ## 8-byte Reload
	movl	%r8d, %r15d
	imull	-16(%rsp), %r15d                ## 4-byte Folded Reload
	testl	%r11d, %r11d
	jg	LBB42_32
LBB42_5:                                ## %"for conv1_stage2_1_d_def__.s0.c.ci.us.us.end for conv1_stage2_1_d_def__.s0.n.n.us.us_crit_edge"
                                        ##   in Loop: Header=BB42_3 Depth=1
	movslq	%r8d, %rax
	movq	%rax, %rcx
	imulq	%rbp, %rcx
	addq	152(%rsp), %rcx                 ## 8-byte Folded Reload
	movq	%rcx, 24(%rsp)                  ## 8-byte Spill
	imulq	%rdi, %rax
	addq	144(%rsp), %rax                 ## 8-byte Folded Reload
	movq	%rax, 32(%rsp)                  ## 8-byte Spill
	movslq	%r15d, %r13
	jmp	LBB42_11
LBB42_34:                               ## %"for conv1_stage2_1_d_def__.s0.w.preheader.split"
	testl	%ecx, %ecx
	movq	(%rsp), %rbx                    ## 8-byte Reload
	jle	LBB42_45
## %bb.35:                              ## %"for conv1_stage2_1_d_def__.s0.w.us21.preheader"
	movq	%r9, %rdi
	movl	%ebp, %r8d
	movl	-124(%rsp), %edx                ## 4-byte Reload
	movslq	%edx, %rax
	movl	-16(%rsp), %esi                 ## 4-byte Reload
	imull	%esi, %edx
	movslq	%edx, %r9
	movq	%rax, %rdx
	imulq	%rbp, %rdx
	movq	%rdx, 64(%rsp)                  ## 8-byte Spill
	movq	%rax, %rcx
	movq	-104(%rsp), %r14                ## 8-byte Reload
	imulq	%r14, %rcx
	movq	%rcx, 56(%rsp)                  ## 8-byte Spill
	orl	$1, %eax
	imull	%eax, %esi
	movslq	%esi, %r10
	movslq	%eax, %rdx
	imulq	%rdx, %rbp
	imulq	%r14, %rdx
                                        ## kill: def $r11d killed $r11d killed $r11 def $r11
	andl	$-2, %r11d
	movq	-88(%rsp), %r14                 ## 8-byte Reload
	leaq	32(%r14), %rax
	movq	%rax, 24(%rsp)                  ## 8-byte Spill
	movq	-96(%rsp), %rax                 ## 8-byte Reload
	leaq	32(%rax), %rcx
	movq	%rcx, 104(%rsp)                 ## 8-byte Spill
	movl	%r15d, %ecx
	imull	%r8d, %ecx
	shll	$5, %ecx
	negl	%ecx
	movl	%ecx, -32(%rsp)                 ## 4-byte Spill
	movq	%r9, 48(%rsp)                   ## 8-byte Spill
	movq	%r9, %rcx
	subq	%rdi, %rcx
	movq	-80(%rsp), %rsi                 ## 8-byte Reload
	leaq	(%rsi,%rcx,4), %rcx
	movq	%rcx, 96(%rsp)                  ## 8-byte Spill
	movq	%rdx, 32(%rsp)                  ## 8-byte Spill
	leaq	32(%r14,%rdx,4), %rcx
	movq	%rcx, 88(%rsp)                  ## 8-byte Spill
	leaq	32(%rax,%rbp,4), %rax
	movq	%rax, 16(%rsp)                  ## 8-byte Spill
	movl	-120(%rsp), %r12d               ## 4-byte Reload
	movl	-116(%rsp), %edx                ## 4-byte Reload
	movq	%r10, 40(%rsp)                  ## 8-byte Spill
	movq	%r10, %rcx
	subq	%rdi, %rcx
	leaq	(%rsi,%rcx,4), %rax
	movq	%rax, 80(%rsp)                  ## 8-byte Spill
	xorl	%eax, %eax
	movq	%rax, -72(%rsp)                 ## 8-byte Spill
	vxorps	%xmm0, %xmm0, %xmm0
	vpbroadcastd	LCPI42_0(%rip), %ymm1   ## ymm1 = [-0.0E+0,-0.0E+0,-0.0E+0,-0.0E+0,-0.0E+0,-0.0E+0,-0.0E+0,-0.0E+0]
	movl	$0, -40(%rsp)                   ## 4-byte Folded Spill
	xorl	%r14d, %r14d
	movq	%rbp, -64(%rsp)                 ## 8-byte Spill
	jmp	LBB42_36
LBB42_50:                               ## %"end for conv1_stage2_1_d_def__.s0.n.n.loopexit.us20.us.1"
                                        ##   in Loop: Header=BB42_36 Depth=1
	incq	%r14
	movl	-120(%rsp), %r12d               ## 4-byte Reload
	addl	%r12d, -40(%rsp)                ## 4-byte Folded Spill
	movl	-116(%rsp), %edx                ## 4-byte Reload
	addl	%edx, -32(%rsp)                 ## 4-byte Folded Spill
	movq	(%rsp), %rbx                    ## 8-byte Reload
	movq	-72(%rsp), %rax                 ## 8-byte Reload
	addl	%ebx, %eax
	movq	%rax, -72(%rsp)                 ## 8-byte Spill
	cmpq	128(%rsp), %r14                 ## 8-byte Folded Reload
	movq	72(%rsp), %rdi                  ## 8-byte Reload
	movl	-20(%rsp), %r15d                ## 4-byte Reload
	je	LBB42_45
LBB42_36:                               ## %"for conv1_stage2_1_d_def__.s0.w.us21"
                                        ## =>This Loop Header: Depth=1
                                        ##     Child Loop BB42_39 Depth 2
                                        ##     Child Loop BB42_47 Depth 2
	movslq	-40(%rsp), %rax                 ## 4-byte Folded Reload
	movq	%rax, -48(%rsp)                 ## 8-byte Spill
	movslq	-32(%rsp), %rax                 ## 4-byte Folded Reload
	movq	%rax, -104(%rsp)                ## 8-byte Spill
	movl	%ebx, %esi
	imull	%r14d, %esi
	cmpq	112(%rsp), %r14                 ## 8-byte Folded Reload
	setl	%r9b
	movl	%r15d, %ebp
	movl	-24(%rsp), %r8d                 ## 4-byte Reload
	cmovll	%r14d, %r8d
	imull	%ebx, %r8d
	imull	%r14d, %r12d
	movl	%r14d, %ecx
	subl	%r15d, %ecx
	imull	%edx, %ecx
	movl	%r8d, %r15d
	subl	%edi, %r15d
	subl	%edi, %esi
	vmovd	%r9d, %xmm2
	movslq	-72(%rsp), %rax                 ## 4-byte Folded Reload
	movq	%rax, -56(%rsp)                 ## 8-byte Spill
	vpbroadcastb	%xmm2, %xmm2
	cmpl	$1, 168(%rsp)                   ## 4-byte Folded Reload
	jne	LBB42_38
## %bb.37:                              ##   in Loop: Header=BB42_36 Depth=1
	xorl	%r9d, %r9d
	jmp	LBB42_40
LBB42_38:                               ## %"for conv1_stage2_1_d_def__.s0.n.n.us14.us.preheader"
                                        ##   in Loop: Header=BB42_36 Depth=1
	movq	56(%rsp), %rax                  ## 8-byte Reload
	movq	-48(%rsp), %rdx                 ## 8-byte Reload
	addq	%rax, %rdx
	movq	64(%rsp), %rax                  ## 8-byte Reload
	movq	-104(%rsp), %rdi                ## 8-byte Reload
	leaq	(%rax,%rdi), %rdi
	movq	24(%rsp), %rax                  ## 8-byte Reload
	leaq	(%rax,%rdx,4), %rbx
	movq	104(%rsp), %rax                 ## 8-byte Reload
	leaq	(%rax,%rdi,4), %rdi
	movq	96(%rsp), %rax                  ## 8-byte Reload
	movq	-56(%rsp), %rdx                 ## 8-byte Reload
	leaq	(%rax,%rdx,4), %rbp
	movslq	%r8d, %rdx
	leaq	(%rax,%rdx,4), %rdx
	xorl	%r13d, %r13d
	xorl	%r9d, %r9d
LBB42_39:                               ## %"for conv1_stage2_1_d_def__.s0.n.n.us14.us"
                                        ##   Parent Loop BB42_36 Depth=1
                                        ## =>  This Inner Loop Header: Depth=2
	vmovups	(%rdx,%r13), %ymm3
	vcmpleps	%ymm0, %ymm3, %ymm3
	vextractf128	$1, %ymm3, %xmm4
	vpackssdw	%xmm4, %xmm3, %xmm3
	vpand	%xmm2, %xmm3, %xmm3
	vpmovzxwd	%xmm3, %ymm3            ## ymm3 = xmm3[0],zero,xmm3[1],zero,xmm3[2],zero,xmm3[3],zero,xmm3[4],zero,xmm3[5],zero,xmm3[6],zero,xmm3[7],zero
	vpslld	$31, %ymm3, %ymm3
	vpsrad	$31, %ymm3, %ymm3
	movq	-112(%rsp), %r10                ## 8-byte Reload
	vpxor	(%r10,%r13), %ymm1, %ymm4
	vcmpleps	(%rbp,%r13), %ymm0, %ymm5
	vpand	%ymm4, %ymm3, %ymm3
	vandps	-32(%rdi,%r13), %ymm5, %ymm4
	vaddps	%ymm4, %ymm3, %ymm3
	vmovups	%ymm3, -32(%rbx,%r13)
	vmovups	32(%rdx,%r13), %ymm3
	vcmpleps	%ymm0, %ymm3, %ymm3
	vextractf128	$1, %ymm3, %xmm4
	vpackssdw	%xmm4, %xmm3, %xmm3
	vpand	%xmm2, %xmm3, %xmm3
	vpmovzxwd	%xmm3, %ymm3            ## ymm3 = xmm3[0],zero,xmm3[1],zero,xmm3[2],zero,xmm3[3],zero,xmm3[4],zero,xmm3[5],zero,xmm3[6],zero,xmm3[7],zero
	vpslld	$31, %ymm3, %ymm3
	vpsrad	$31, %ymm3, %ymm3
	vpxor	32(%r10,%r13), %ymm1, %ymm4
	vcmpleps	32(%rbp,%r13), %ymm0, %ymm5
	vandps	(%rdi,%r13), %ymm5, %ymm5
	vpand	%ymm4, %ymm3, %ymm3
	vaddps	%ymm5, %ymm3, %ymm3
	vmovups	%ymm3, (%rbx,%r13)
	addq	$2, %r9
	addq	$64, %r13
	cmpq	%r9, %r11
	jne	LBB42_39
LBB42_40:                               ## %"end for conv1_stage2_1_d_def__.s0.n.n.loopexit.us20.us.unr-lcssa"
                                        ##   in Loop: Header=BB42_36 Depth=1
	movslq	%ecx, %r13
	movslq	%r12d, %r10
	movslq	%r15d, %r15
	movslq	%esi, %r12
	testb	$1, 8(%rsp)                     ## 1-byte Folded Reload
	je	LBB42_42
## %bb.41:                              ## %"for conv1_stage2_1_d_def__.s0.n.n.us14.us.epil"
                                        ##   in Loop: Header=BB42_36 Depth=1
	movq	48(%rsp), %rax                  ## 8-byte Reload
	leaq	(%rax,%r9,8), %rcx
	leaq	(%rcx,%r15), %rdx
	movq	-80(%rsp), %rsi                 ## 8-byte Reload
	vmovups	(%rsi,%rdx,4), %ymm3
	movq	%r9, %rdx
	shlq	$5, %rdx
	movq	-112(%rsp), %rdi                ## 8-byte Reload
	vpxor	(%rdi,%rdx), %ymm1, %ymm4
	addq	%r12, %rcx
	vcmpleps	(%rsi,%rcx,4), %ymm0, %ymm5
	movq	64(%rsp), %rax                  ## 8-byte Reload
	leaq	(%rax,%r13), %rcx
	leaq	(%rcx,%r9,8), %rcx
	movq	-96(%rsp), %rdx                 ## 8-byte Reload
	vandps	(%rdx,%rcx,4), %ymm5, %ymm5
	movq	56(%rsp), %rax                  ## 8-byte Reload
	leaq	(%rax,%r10), %rcx
	leaq	(%rcx,%r9,8), %rcx
	vcmpleps	%ymm0, %ymm3, %ymm3
	vextractf128	$1, %ymm3, %xmm6
	vpackssdw	%xmm6, %xmm3, %xmm3
	vpand	%xmm2, %xmm3, %xmm3
	vpmovzxwd	%xmm3, %ymm3            ## ymm3 = xmm3[0],zero,xmm3[1],zero,xmm3[2],zero,xmm3[3],zero,xmm3[4],zero,xmm3[5],zero,xmm3[6],zero,xmm3[7],zero
	vpslld	$31, %ymm3, %ymm3
	vpsrad	$31, %ymm3, %ymm3
	vpand	%ymm4, %ymm3, %ymm3
	vaddps	%ymm5, %ymm3, %ymm3
	movq	-88(%rsp), %rdx                 ## 8-byte Reload
	vmovups	%ymm3, (%rdx,%rcx,4)
LBB42_42:                               ## %"end for conv1_stage2_1_d_def__.s0.n.n.loopexit.us20.us"
                                        ##   in Loop: Header=BB42_36 Depth=1
	cmpl	$1, 8(%rsp)                     ## 4-byte Folded Reload
	jne	LBB42_46
## %bb.43:                              ##   in Loop: Header=BB42_36 Depth=1
	xorl	%edx, %edx
	jmp	LBB42_48
LBB42_46:                               ## %"for conv1_stage2_1_d_def__.s0.n.n.us14.us.1.preheader"
                                        ##   in Loop: Header=BB42_36 Depth=1
	movq	88(%rsp), %rax                  ## 8-byte Reload
	movq	-48(%rsp), %rcx                 ## 8-byte Reload
	leaq	(%rax,%rcx,4), %rcx
	movq	16(%rsp), %rax                  ## 8-byte Reload
	movq	-104(%rsp), %rdx                ## 8-byte Reload
	leaq	(%rax,%rdx,4), %rbx
	movq	80(%rsp), %rax                  ## 8-byte Reload
	movq	-56(%rsp), %rdx                 ## 8-byte Reload
	leaq	(%rax,%rdx,4), %rsi
	movslq	%r8d, %rdx
	leaq	(%rax,%rdx,4), %rdi
	xorl	%ebp, %ebp
	xorl	%edx, %edx
LBB42_47:                               ## %"for conv1_stage2_1_d_def__.s0.n.n.us14.us.1"
                                        ##   Parent Loop BB42_36 Depth=1
                                        ## =>  This Inner Loop Header: Depth=2
	vmovups	(%rdi,%rbp), %ymm3
	vcmpleps	%ymm0, %ymm3, %ymm3
	vextractf128	$1, %ymm3, %xmm4
	vpackssdw	%xmm4, %xmm3, %xmm3
	vpand	%xmm2, %xmm3, %xmm3
	vpmovzxwd	%xmm3, %ymm3            ## ymm3 = xmm3[0],zero,xmm3[1],zero,xmm3[2],zero,xmm3[3],zero,xmm3[4],zero,xmm3[5],zero,xmm3[6],zero,xmm3[7],zero
	vpslld	$31, %ymm3, %ymm3
	vpsrad	$31, %ymm3, %ymm3
	movq	-112(%rsp), %rax                ## 8-byte Reload
	vpxor	(%rax,%rbp), %ymm1, %ymm4
	vcmpleps	(%rsi,%rbp), %ymm0, %ymm5
	vpand	%ymm4, %ymm3, %ymm3
	vandps	-32(%rbx,%rbp), %ymm5, %ymm4
	vaddps	%ymm4, %ymm3, %ymm3
	vmovups	%ymm3, -32(%rcx,%rbp)
	vmovups	32(%rdi,%rbp), %ymm3
	vcmpleps	%ymm0, %ymm3, %ymm3
	vextractf128	$1, %ymm3, %xmm4
	vpackssdw	%xmm4, %xmm3, %xmm3
	vpand	%xmm2, %xmm3, %xmm3
	vpmovzxwd	%xmm3, %ymm3            ## ymm3 = xmm3[0],zero,xmm3[1],zero,xmm3[2],zero,xmm3[3],zero,xmm3[4],zero,xmm3[5],zero,xmm3[6],zero,xmm3[7],zero
	vpslld	$31, %ymm3, %ymm3
	vpsrad	$31, %ymm3, %ymm3
	vpxor	32(%rax,%rbp), %ymm1, %ymm4
	vcmpleps	32(%rsi,%rbp), %ymm0, %ymm5
	vandps	(%rbx,%rbp), %ymm5, %ymm5
	vpand	%ymm4, %ymm3, %ymm3
	vaddps	%ymm5, %ymm3, %ymm3
	vmovups	%ymm3, (%rcx,%rbp)
	addq	$2, %rdx
	addq	$64, %rbp
	cmpq	%rdx, %r11
	jne	LBB42_47
LBB42_48:                               ## %"end for conv1_stage2_1_d_def__.s0.n.n.loopexit.us20.us.1.unr-lcssa"
                                        ##   in Loop: Header=BB42_36 Depth=1
	testb	$1, 8(%rsp)                     ## 1-byte Folded Reload
	je	LBB42_50
## %bb.49:                              ## %"for conv1_stage2_1_d_def__.s0.n.n.us14.us.1.epil"
                                        ##   in Loop: Header=BB42_36 Depth=1
	addq	-64(%rsp), %r13                 ## 8-byte Folded Reload
	addq	32(%rsp), %r10                  ## 8-byte Folded Reload
	movq	40(%rsp), %rax                  ## 8-byte Reload
	leaq	(%rax,%rdx,8), %rcx
	addq	%rcx, %r15
	movq	-80(%rsp), %rdi                 ## 8-byte Reload
	vmovups	(%rdi,%r15,4), %ymm3
	vcmpleps	%ymm0, %ymm3, %ymm3
	vextractf128	$1, %ymm3, %xmm4
	vpackssdw	%xmm4, %xmm3, %xmm3
	vpand	%xmm2, %xmm3, %xmm2
	vpmovzxwd	%xmm2, %ymm2            ## ymm2 = xmm2[0],zero,xmm2[1],zero,xmm2[2],zero,xmm2[3],zero,xmm2[4],zero,xmm2[5],zero,xmm2[6],zero,xmm2[7],zero
	vpslld	$31, %ymm2, %ymm2
	movq	%rdx, %rsi
	shlq	$5, %rsi
	movq	-112(%rsp), %rbp                ## 8-byte Reload
	vpxor	(%rbp,%rsi), %ymm1, %ymm3
	vpsrad	$31, %ymm2, %ymm2
	vpand	%ymm3, %ymm2, %ymm2
	addq	%rcx, %r12
	leaq	(,%rdx,8), %rcx
	addq	%r13, %rcx
	vcmpleps	(%rdi,%r12,4), %ymm0, %ymm3
	movq	-96(%rsp), %rsi                 ## 8-byte Reload
	vandps	(%rsi,%rcx,4), %ymm3, %ymm3
	vaddps	%ymm3, %ymm2, %ymm2
	leaq	(%r10,%rdx,8), %rcx
	movq	-88(%rsp), %rdx                 ## 8-byte Reload
	vmovups	%ymm2, (%rdx,%rcx,4)
	jmp	LBB42_50
LBB42_45:                               ## %destructor_block
	xorl	%eax, %eax
	addq	$344, %rsp                      ## imm = 0x158
	popq	%rbx
	popq	%r12
	popq	%r13
	popq	%r14
	popq	%r15
	popq	%rbp
	vzeroupper
	retq
                                        ## -- End function
	.p2align	4, 0x90                         ## -- Begin function train_cost_model.par_for.conv1_stage1_1_d_def__.s0.c
_train_cost_model.par_for.conv1_stage1_1_d_def__.s0.c: ## @train_cost_model.par_for.conv1_stage1_1_d_def__.s0.c
## %bb.0:                               ## %entry
	pushq	%rbp
	pushq	%r15
	pushq	%r14
	pushq	%r13
	pushq	%r12
	pushq	%rbx
	subq	$40, %rsp
	movl	%esi, %r15d
	movl	(%rdx), %r13d
	movl	4(%rdx), %eax
	movq	%rax, 16(%rsp)                  ## 8-byte Spill
	movl	8(%rdx), %ebp
	movq	16(%rdx), %rbx
	imull	%r13d, %r15d
	testl	%ebp, %ebp
	jle	LBB43_2
## %bb.1:                               ## %"for conv1_stage1_1_d_def__.s0.w.w.preheader"
	movslq	%r15d, %rax
	leaq	(%rbx,%rax,4), %rdi
	movq	%rbp, %rdx
	shlq	$5, %rdx
	xorl	%esi, %esi
	callq	_memset
LBB43_2:                                ## %"end for conv1_stage1_1_d_def__.s0.w.w"
	movq	16(%rsp), %rax                  ## 8-byte Reload
	testl	%eax, %eax
	jle	LBB43_14
## %bb.3:                               ## %"for conv1_stage1_1_d_def__.s0.w.w.rebased.preheader"
	leal	(,%rbp,8), %r14d
	movl	%r13d, %ecx
	subl	%r14d, %ecx
	movl	%ecx, 8(%rsp)                   ## 4-byte Spill
	xorl	%r12d, %r12d
	cmpl	$1, %eax
	je	LBB43_11
## %bb.4:                               ## %"for conv1_stage1_1_d_def__.s0.w.w.rebased.preheader.new"
	movl	%r13d, 12(%rsp)                 ## 4-byte Spill
	movq	%rbp, 24(%rsp)                  ## 8-byte Spill
	movq	%r15, 32(%rsp)                  ## 8-byte Spill
	addl	%r15d, %r14d
	movl	%eax, %r13d
	andl	$-2, %r13d
	negq	%r13
	xorl	%r12d, %r12d
	movl	$8, %r15d
	movl	8(%rsp), %eax                   ## 4-byte Reload
	movl	%eax, %ebp
	.p2align	4, 0x90
LBB43_5:                                ## %"for conv1_stage1_1_d_def__.s0.w.w.rebased"
                                        ## =>This Inner Loop Header: Depth=1
	testl	%ebp, %ebp
	jle	LBB43_7
## %bb.6:                               ## %"for conv1_stage1_1_d_def__.s0.w.wi.preheader"
                                        ##   in Loop: Header=BB43_5 Depth=1
	cmpl	$8, %ebp
	movl	$8, %eax
	cmovll	%ebp, %eax
	decl	%eax
	leaq	4(,%rax,4), %rdx
	movslq	%r14d, %rax
	leaq	(%rbx,%rax,4), %rdi
	xorl	%esi, %esi
	callq	_memset
LBB43_7:                                ## %"end for conv1_stage1_1_d_def__.s0.w.wi"
                                        ##   in Loop: Header=BB43_5 Depth=1
	leal	-8(%rbp), %eax
	testl	%eax, %eax
	jle	LBB43_9
## %bb.8:                               ## %"for conv1_stage1_1_d_def__.s0.w.wi.preheader.1"
                                        ##   in Loop: Header=BB43_5 Depth=1
	cmpl	$8, %eax
	cmovgel	%r15d, %eax
	decl	%eax
	leaq	4(,%rax,4), %rdx
	leal	8(%r14), %eax
	cltq
	leaq	(%rbx,%rax,4), %rdi
	xorl	%esi, %esi
	callq	_memset
LBB43_9:                                ## %"end for conv1_stage1_1_d_def__.s0.w.wi.1"
                                        ##   in Loop: Header=BB43_5 Depth=1
	addq	$-2, %r12
	addl	$-16, %ebp
	addl	$16, %r14d
	cmpq	%r12, %r13
	jne	LBB43_5
## %bb.10:                              ## %destructor_block.loopexit.unr-lcssa.loopexit
	negl	%r12d
	movq	32(%rsp), %r15                  ## 8-byte Reload
	movq	24(%rsp), %rbp                  ## 8-byte Reload
	movl	12(%rsp), %r13d                 ## 4-byte Reload
	movq	16(%rsp), %rax                  ## 8-byte Reload
LBB43_11:                               ## %destructor_block.loopexit.unr-lcssa
	testb	$1, %al
	je	LBB43_14
## %bb.12:                              ## %"for conv1_stage1_1_d_def__.s0.w.w.rebased.epil"
	movl	%ebp, %eax
	addl	%r12d, %eax
	shll	$3, %eax
	subl	%eax, %r13d
	testl	%r13d, %r13d
	jle	LBB43_14
## %bb.13:                              ## %"for conv1_stage1_1_d_def__.s0.w.wi.preheader.epil"
	leal	(%r15,%rbp,8), %eax
	leal	(,%r12,8), %ecx
	movl	8(%rsp), %edx                   ## 4-byte Reload
	subl	%ecx, %edx
	cmpl	$8, %edx
	movl	$8, %ecx
	cmovll	%edx, %ecx
	decl	%ecx
	leaq	4(,%rcx,4), %rdx
	leal	(%rax,%r12,8), %eax
	cltq
	leaq	(%rbx,%rax,4), %rdi
	xorl	%esi, %esi
	callq	_memset
LBB43_14:                               ## %destructor_block
	xorl	%eax, %eax
	addq	$40, %rsp
	popq	%rbx
	popq	%r12
	popq	%r13
	popq	%r14
	popq	%r15
	popq	%rbp
	retq
                                        ## -- End function
	.p2align	4, 0x90                         ## -- Begin function train_cost_model.par_for.conv1_stage1_1_d_def__.s1.c
_train_cost_model.par_for.conv1_stage1_1_d_def__.s1.c: ## @train_cost_model.par_for.conv1_stage1_1_d_def__.s1.c
## %bb.0:                               ## %entry
	pushq	%rbp
	movq	%rsp, %rbp
	pushq	%r15
	pushq	%r14
	pushq	%r13
	pushq	%r12
	pushq	%rbx
	andq	$-32, %rsp
	subq	$352, %rsp                      ## imm = 0x160
	movslq	(%rdx), %rax
	movq	%rax, 24(%rsp)                  ## 8-byte Spill
	movl	4(%rdx), %eax
	movl	12(%rdx), %ecx
	movq	%rcx, 240(%rsp)                 ## 8-byte Spill
	movl	16(%rdx), %ecx
	movq	%rcx, 248(%rsp)                 ## 8-byte Spill
	movl	24(%rdx), %ecx
	movq	32(%rdx), %rdi
	movq	%rdi, 8(%rsp)                   ## 8-byte Spill
	movq	48(%rdx), %rdi
	movq	%rdi, 64(%rsp)                  ## 8-byte Spill
	movl	%eax, 20(%rsp)                  ## 4-byte Spill
                                        ## kill: def $eax killed $eax def $rax
	movl	%esi, 16(%rsp)                  ## 4-byte Spill
	imull	%esi, %eax
	movq	%rax, 144(%rsp)                 ## 8-byte Spill
	movq	%rcx, %rax
	movq	%rcx, 152(%rsp)                 ## 8-byte Spill
	testl	%ecx, %ecx
	jle	LBB44_24
## %bb.1:                               ## %"for conv1_stage1_1_d_def__.s1.w.w.preheader"
	movl	8(%rdx), %eax
	movq	%rax, 32(%rsp)                  ## 8-byte Spill
	movl	20(%rdx), %edi
	movq	24(%rsp), %r15                  ## 8-byte Reload
	movq	%r15, %r11
	shlq	$5, %r11
	movl	%r15d, %eax
	shll	$8, %eax
	movl	%eax, 200(%rsp)                 ## 4-byte Spill
	leal	-1(%rdi), %eax
	movl	%eax, 192(%rsp)                 ## 4-byte Spill
	leal	(%rdi,%rdi), %ecx
	movl	%r15d, %eax
	subl	%ecx, %eax
	movl	%eax, 80(%rsp)                  ## 4-byte Spill
	movl	%edi, %eax
	andl	$7, %eax
	movl	%eax, 72(%rsp)                  ## 4-byte Spill
	movl	%edi, %eax
	andl	$-8, %eax
	negl	%eax
	movl	%eax, 184(%rsp)                 ## 4-byte Spill
	movq	64(%rsp), %rax                  ## 8-byte Reload
	addq	$4, %rax
	movq	%rax, 176(%rsp)                 ## 8-byte Spill
	movl	%r15d, %esi
	imull	16(%rsp), %esi                  ## 4-byte Folded Reload
	movq	%r15, %rdx
	shlq	$8, %rdx
	leaq	(%rdx,%rdx,2), %rdx
	movl	$4, %eax
	subq	%rdx, %rax
	movq	%rax, 168(%rsp)                 ## 8-byte Spill
	movl	%r15d, %eax
	subl	%ecx, %eax
	movl	%eax, 160(%rsp)                 ## 4-byte Spill
	shlq	$7, %r15
                                        ## implicit-def: $ymm0
	leal	(%rsi,%rdi,2), %ebx
	xorl	%ecx, %ecx
	jmp	LBB44_2
	.p2align	4, 0x90
LBB44_55:                               ## %"end for conv1_stage1_1_d_def__.s1.r982$x.r982$x.rebased"
                                        ##   in Loop: Header=BB44_2 Depth=1
	movq	224(%rsp), %rcx                 ## 8-byte Reload
	incq	%rcx
	movl	200(%rsp), %eax                 ## 4-byte Reload
	movq	88(%rsp), %rsi                  ## 8-byte Reload
	addl	%eax, %esi
	movl	40(%rsp), %ebx                  ## 4-byte Reload
	addl	%eax, %ebx
	cmpq	152(%rsp), %rcx                 ## 8-byte Folded Reload
	je	LBB44_23
LBB44_2:                                ## %"for conv1_stage1_1_d_def__.s1.w.w"
                                        ## =>This Loop Header: Depth=1
                                        ##     Child Loop BB44_5 Depth 2
                                        ##     Child Loop BB44_7 Depth 2
                                        ##     Child Loop BB44_9 Depth 2
                                        ##     Child Loop BB44_14 Depth 2
                                        ##       Child Loop BB44_17 Depth 3
                                        ##       Child Loop BB44_20 Depth 3
                                        ##     Child Loop BB44_53 Depth 2
                                        ##       Child Loop BB44_57 Depth 3
	leal	(,%rcx,8), %r8d
	movq	144(%rsp), %rax                 ## 8-byte Reload
	leal	(%rax,%rcx,8), %edx
	movslq	%edx, %r13
	testl	%edi, %edi
	movq	%rsi, 88(%rsp)                  ## 8-byte Spill
	movl	%ebx, 40(%rsp)                  ## 4-byte Spill
	movq	%rcx, 224(%rsp)                 ## 8-byte Spill
	movl	%r8d, 208(%rsp)                 ## 4-byte Spill
	jle	LBB44_11
## %bb.3:                               ## %"for conv1_stage1_1_d_def__.s1.r982$x.r982$x.preheader"
                                        ##   in Loop: Header=BB44_2 Depth=1
	movq	8(%rsp), %rax                   ## 8-byte Reload
	vmovups	(%rax,%r13,4), %ymm1
	cmpl	%r8d, 20(%rsp)                  ## 4-byte Folded Reload
	movq	%r13, 216(%rsp)                 ## 8-byte Spill
	jle	LBB44_4
## %bb.8:                               ## %"for conv1_stage1_1_d_def__.s1.r982$x.r982$x.us.preheader"
                                        ##   in Loop: Header=BB44_2 Depth=1
	movslq	%esi, %rdx
	movq	176(%rsp), %rax                 ## 8-byte Reload
	leaq	(%rax,%rdx,4), %r8
	xorl	%esi, %esi
	movq	168(%rsp), %rax                 ## 8-byte Reload
	.p2align	4, 0x90
LBB44_9:                                ## %"for conv1_stage1_1_d_def__.s1.r982$x.r982$x.us"
                                        ##   Parent Loop BB44_2 Depth=1
                                        ## =>  This Inner Loop Header: Depth=2
	leaq	(%r8,%rsi,8), %rbx
	leaq	(%r15,%rbx), %rcx
	addq	$-4, %rcx
	leaq	(%rcx,%r15), %r14
	leaq	(%r14,%r15), %r12
	leaq	(%r12,%r15), %r10
	leaq	(%r10,%r15), %r13
	leaq	(%r13,%r15), %r9
	vmovss	(%r15,%r12), %xmm0              ## xmm0 = mem[0],zero,zero,zero
	vinsertps	$16, (%r15,%r10), %xmm0, %xmm0 ## xmm0 = xmm0[0],mem[0],xmm0[2,3]
	vinsertps	$32, (%r15,%r13), %xmm0, %xmm0 ## xmm0 = xmm0[0,1],mem[0],xmm0[3]
	vinsertps	$48, (%r15,%r9), %xmm0, %xmm0 ## xmm0 = xmm0[0,1,2],mem[0]
	addq	%r15, %r9
	vmovss	-4(%r8,%rsi,8), %xmm2           ## xmm2 = mem[0],zero,zero,zero
	vinsertps	$16, -4(%r15,%rbx), %xmm2, %xmm2 ## xmm2 = xmm2[0],mem[0],xmm2[2,3]
	vinsertps	$32, (%r15,%rcx), %xmm2, %xmm2 ## xmm2 = xmm2[0,1],mem[0],xmm2[3]
	vmovss	(%r8,%rsi,8), %xmm3             ## xmm3 = mem[0],zero,zero,zero
	vinsertps	$48, (%r15,%r14), %xmm2, %xmm2 ## xmm2 = xmm2[0,1,2],mem[0]
	vinsertf128	$1, %xmm0, %ymm2, %ymm0
	vaddps	%ymm0, %ymm1, %ymm1
	leaq	(%r9,%rax), %r10
	leaq	(%r10,%r15), %rbx
	leaq	(%rbx,%r15), %rdx
	leaq	(%rdx,%r15), %rcx
	vmovss	(%r15,%rdx), %xmm0              ## xmm0 = mem[0],zero,zero,zero
	vinsertps	$16, (%r15,%rcx), %xmm0, %xmm0 ## xmm0 = xmm0[0],mem[0],xmm0[2,3]
	addq	%r15, %rcx
	vinsertps	$32, (%r15,%rcx), %xmm0, %xmm0 ## xmm0 = xmm0[0,1],mem[0],xmm0[3]
	addq	%r15, %rcx
	vinsertps	$48, (%r15,%rcx), %xmm0, %xmm0 ## xmm0 = xmm0[0,1,2],mem[0]
	vinsertps	$16, (%rax,%r9), %xmm3, %xmm2 ## xmm2 = xmm3[0],mem[0],xmm3[2,3]
	vinsertps	$32, (%r15,%r10), %xmm2, %xmm2 ## xmm2 = xmm2[0,1],mem[0],xmm2[3]
	vinsertps	$48, (%r15,%rbx), %xmm2, %xmm2 ## xmm2 = xmm2[0,1,2],mem[0]
	vinsertf128	$1, %xmm0, %ymm2, %ymm0
	vaddps	%ymm0, %ymm1, %ymm1
	incq	%rsi
	cmpq	%rsi, %rdi
	jne	LBB44_9
	jmp	LBB44_10
	.p2align	4, 0x90
LBB44_4:                                ## %"for conv1_stage1_1_d_def__.s1.r982$x.r982$x.preheader119"
                                        ##   in Loop: Header=BB44_2 Depth=1
	movl	184(%rsp), %edx                 ## 4-byte Reload
	cmpl	$7, 192(%rsp)                   ## 4-byte Folded Reload
	jb	LBB44_6
	.p2align	4, 0x90
LBB44_5:                                ## %"for conv1_stage1_1_d_def__.s1.r982$x.r982$x"
                                        ##   Parent Loop BB44_2 Depth=1
                                        ## =>  This Inner Loop Header: Depth=2
	vaddps	%ymm0, %ymm1, %ymm1
	vaddps	%ymm0, %ymm1, %ymm1
	vaddps	%ymm0, %ymm1, %ymm1
	vaddps	%ymm0, %ymm1, %ymm1
	vaddps	%ymm0, %ymm1, %ymm1
	vaddps	%ymm0, %ymm1, %ymm1
	vaddps	%ymm0, %ymm1, %ymm1
	vaddps	%ymm0, %ymm1, %ymm1
	vaddps	%ymm0, %ymm1, %ymm1
	vaddps	%ymm0, %ymm1, %ymm1
	vaddps	%ymm0, %ymm1, %ymm1
	vaddps	%ymm0, %ymm1, %ymm1
	vaddps	%ymm0, %ymm1, %ymm1
	vaddps	%ymm0, %ymm1, %ymm1
	vaddps	%ymm0, %ymm1, %ymm1
	vaddps	%ymm0, %ymm1, %ymm1
	addl	$8, %edx
	jne	LBB44_5
LBB44_6:                                ## %"end for conv1_stage1_1_d_def__.s1.r982$x.r982$x.loopexit.loopexit120.unr-lcssa"
                                        ##   in Loop: Header=BB44_2 Depth=1
	movl	72(%rsp), %edx                  ## 4-byte Reload
	testl	%edx, %edx
	je	LBB44_10
	.p2align	4, 0x90
LBB44_7:                                ## %"for conv1_stage1_1_d_def__.s1.r982$x.r982$x.epil"
                                        ##   Parent Loop BB44_2 Depth=1
                                        ## =>  This Inner Loop Header: Depth=2
	vaddps	%ymm0, %ymm1, %ymm1
	vaddps	%ymm0, %ymm1, %ymm1
	decl	%edx
	jne	LBB44_7
LBB44_10:                               ## %"end for conv1_stage1_1_d_def__.s1.r982$x.r982$x.loopexit"
                                        ##   in Loop: Header=BB44_2 Depth=1
	movq	8(%rsp), %rax                   ## 8-byte Reload
	movq	216(%rsp), %r13                 ## 8-byte Reload
	vmovups	%ymm1, (%rax,%r13,4)
LBB44_11:                               ## %"end for conv1_stage1_1_d_def__.s1.r982$x.r982$x"
                                        ##   in Loop: Header=BB44_2 Depth=1
	movq	32(%rsp), %r10                  ## 8-byte Reload
	testl	%r10d, %r10d
	movl	80(%rsp), %r8d                  ## 4-byte Reload
	movl	40(%rsp), %eax                  ## 4-byte Reload
	jle	LBB44_55
## %bb.12:                              ## %"for conv1_stage1_1_d_def__.s1.r982$x.r982$x.rebased.preheader"
                                        ##   in Loop: Header=BB44_2 Depth=1
	movl	208(%rsp), %ecx                 ## 4-byte Reload
	cmpl	%ecx, 20(%rsp)                  ## 4-byte Folded Reload
	jle	LBB44_13
## %bb.52:                              ## %"for conv1_stage1_1_d_def__.s1.r982$x.r982$x.rebased.us.preheader"
                                        ##   in Loop: Header=BB44_2 Depth=1
	movl	%eax, %r8d
	xorl	%r9d, %r9d
	.p2align	4, 0x90
LBB44_53:                               ## %"for conv1_stage1_1_d_def__.s1.r982$x.r982$x.rebased.us"
                                        ##   Parent Loop BB44_2 Depth=1
                                        ## =>  This Loop Header: Depth=2
                                        ##       Child Loop BB44_57 Depth 3
	movq	%rdi, %r12
	leal	(%r9,%rdi), %esi
	addl	%esi, %esi
	movq	24(%rsp), %rax                  ## 8-byte Reload
	movl	%eax, %ebx
	subl	%esi, %ebx
	jle	LBB44_54
## %bb.56:                              ## %"for conv1_stage1_1_d_def__.s1.r982$x.r1029$xi1.preheader.us"
                                        ##   in Loop: Header=BB44_53 Depth=2
	movslq	%r8d, %rsi
	movq	64(%rsp), %rax                  ## 8-byte Reload
	leaq	(%rax,%rsi,4), %rsi
	xorl	%ecx, %ecx
	cmpl	$1, %ebx
	sete	%cl
	movl	$2, %ebx
	subq	%rcx, %rbx
	movq	8(%rsp), %rax                   ## 8-byte Reload
	vmovups	(%rax,%r13,4), %ymm1
	xorl	%r14d, %r14d
	.p2align	4, 0x90
LBB44_57:                               ## %"for conv1_stage1_1_d_def__.s1.r982$x.r1029$xi1.us.us"
                                        ##   Parent Loop BB44_2 Depth=1
                                        ##     Parent Loop BB44_53 Depth=2
                                        ## =>    This Inner Loop Header: Depth=3
	leaq	(%rsi,%r14,4), %rcx
	leaq	(%rcx,%r15), %rdx
	addq	%r15, %rdx
	leaq	(%rdx,%r15), %rax
	addq	%r15, %rax
	leaq	(%rax,%r15), %rdi
	vmovss	(%rdx,%r11,8), %xmm0            ## xmm0 = mem[0],zero,zero,zero
	vinsertps	$16, (%rax,%r11,4), %xmm0, %xmm0 ## xmm0 = xmm0[0],mem[0],xmm0[2,3]
	vinsertps	$32, (%rdi,%r11,4), %xmm0, %xmm0 ## xmm0 = xmm0[0,1],mem[0],xmm0[3]
	addq	%r15, %rdi
	vinsertps	$48, (%rdi,%r11,4), %xmm0, %xmm0 ## xmm0 = xmm0[0,1,2],mem[0]
	vmovss	(%rsi,%r14,4), %xmm2            ## xmm2 = mem[0],zero,zero,zero
	vinsertps	$16, (%rcx,%r11,4), %xmm2, %xmm2 ## xmm2 = xmm2[0],mem[0],xmm2[2,3]
	vinsertps	$32, (%rcx,%r11,8), %xmm2, %xmm2 ## xmm2 = xmm2[0,1],mem[0],xmm2[3]
	vinsertps	$48, (%rdx,%r11,4), %xmm2, %xmm2 ## xmm2 = xmm2[0,1,2],mem[0]
	vinsertf128	$1, %xmm0, %ymm2, %ymm0
	vaddps	%ymm0, %ymm1, %ymm1
	incq	%r14
	cmpq	%r14, %rbx
	jne	LBB44_57
## %bb.58:                              ## %"end for conv1_stage1_1_d_def__.s1.r982$x.r1029$xi2.loopexit.split.us.us"
                                        ##   in Loop: Header=BB44_53 Depth=2
	movq	8(%rsp), %rax                   ## 8-byte Reload
	vmovups	%ymm1, (%rax,%r13,4)
LBB44_54:                               ## %"end for conv1_stage1_1_d_def__.s1.r982$x.r1029$xi2.us"
                                        ##   in Loop: Header=BB44_53 Depth=2
	incq	%r9
	addl	$2, %r8d
	cmpq	%r10, %r9
	movq	%r12, %rdi
	jne	LBB44_53
	jmp	LBB44_55
	.p2align	4, 0x90
LBB44_13:                               ## %"for conv1_stage1_1_d_def__.s1.r982$x.r982$x.rebased.preheader1"
                                        ##   in Loop: Header=BB44_2 Depth=1
	xorl	%ecx, %ecx
	movl	160(%rsp), %r14d                ## 4-byte Reload
	movl	$2, %r9d
	.p2align	4, 0x90
LBB44_14:                               ## %"for conv1_stage1_1_d_def__.s1.r982$x.r982$x.rebased"
                                        ##   Parent Loop BB44_2 Depth=1
                                        ## =>  This Loop Header: Depth=2
                                        ##       Child Loop BB44_17 Depth 3
                                        ##       Child Loop BB44_20 Depth 3
	cmpl	$2, %r14d
	movl	$2, %ebx
	cmovll	%r14d, %ebx
	leal	(%rcx,%rcx), %eax
	movl	%r8d, %esi
	subl	%eax, %esi
	cmpl	$2, %esi
	cmovgel	%r9d, %esi
	leal	(%rdi,%rcx), %eax
	addl	%eax, %eax
	movq	24(%rsp), %rdx                  ## 8-byte Reload
                                        ## kill: def $edx killed $edx killed $rdx
	subl	%eax, %edx
	testl	%edx, %edx
	jle	LBB44_22
## %bb.15:                              ## %"for conv1_stage1_1_d_def__.s1.r982$x.r1029$xi1.preheader"
                                        ##   in Loop: Header=BB44_14 Depth=2
	leal	-1(%rsi), %eax
	movq	8(%rsp), %rdx                   ## 8-byte Reload
	vmovups	(%rdx,%r13,4), %ymm1
	cmpl	$7, %eax
	jb	LBB44_18
## %bb.16:                              ##   in Loop: Header=BB44_14 Depth=2
	movl	%ebx, %edx
	andl	$-8, %edx
	negl	%edx
	.p2align	4, 0x90
LBB44_17:                               ## %"for conv1_stage1_1_d_def__.s1.r982$x.r1029$xi1"
                                        ##   Parent Loop BB44_2 Depth=1
                                        ##     Parent Loop BB44_14 Depth=2
                                        ## =>    This Inner Loop Header: Depth=3
	vaddps	%ymm0, %ymm1, %ymm1
	vaddps	%ymm0, %ymm1, %ymm1
	vaddps	%ymm0, %ymm1, %ymm1
	vaddps	%ymm0, %ymm1, %ymm1
	vaddps	%ymm0, %ymm1, %ymm1
	vaddps	%ymm0, %ymm1, %ymm1
	vaddps	%ymm0, %ymm1, %ymm1
	vaddps	%ymm0, %ymm1, %ymm1
	addl	$8, %edx
	jne	LBB44_17
LBB44_18:                               ## %"end for conv1_stage1_1_d_def__.s1.r982$x.r1029$xi2.loopexit.split.unr-lcssa"
                                        ##   in Loop: Header=BB44_14 Depth=2
	testb	$7, %sil
	je	LBB44_21
## %bb.19:                              ## %"for conv1_stage1_1_d_def__.s1.r982$x.r1029$xi1.epil.preheader"
                                        ##   in Loop: Header=BB44_14 Depth=2
	andl	$7, %ebx
	negl	%ebx
	.p2align	4, 0x90
LBB44_20:                               ## %"for conv1_stage1_1_d_def__.s1.r982$x.r1029$xi1.epil"
                                        ##   Parent Loop BB44_2 Depth=1
                                        ##     Parent Loop BB44_14 Depth=2
                                        ## =>    This Inner Loop Header: Depth=3
	vaddps	%ymm0, %ymm1, %ymm1
	incl	%ebx
	jne	LBB44_20
LBB44_21:                               ## %"end for conv1_stage1_1_d_def__.s1.r982$x.r1029$xi2.loopexit.split"
                                        ##   in Loop: Header=BB44_14 Depth=2
	movq	8(%rsp), %rax                   ## 8-byte Reload
	vmovups	%ymm1, (%rax,%r13,4)
LBB44_22:                               ## %"end for conv1_stage1_1_d_def__.s1.r982$x.r1029$xi2"
                                        ##   in Loop: Header=BB44_14 Depth=2
	incl	%ecx
	addl	$-2, %r14d
	cmpl	%r10d, %ecx
	jne	LBB44_14
	jmp	LBB44_55
LBB44_23:                               ## %"end for conv1_stage1_1_d_def__.s1.w.w.loopexit"
	vmovaps	%ymm0, 96(%rsp)
LBB44_24:                               ## %"end for conv1_stage1_1_d_def__.s1.w.w"
	cmpl	$0, 240(%rsp)                   ## 4-byte Folded Reload
	jle	LBB44_35
## %bb.25:                              ## %"for conv1_stage1_1_d_def__.s1.w.w.rebased.preheader"
	cmpl	$0, 248(%rsp)                   ## 4-byte Folded Reload
	jle	LBB44_35
## %bb.26:                              ## %"for conv1_stage1_1_d_def__.s1.w.w.rebased.us.preheader"
	movq	152(%rsp), %rcx                 ## 8-byte Reload
	leal	(,%rcx,8), %eax
	movl	20(%rsp), %edx                  ## 4-byte Reload
	movl	%eax, 56(%rsp)                  ## 4-byte Spill
	subl	%eax, %edx
	movl	16(%rsp), %eax                  ## 4-byte Reload
	movq	24(%rsp), %rbx                  ## 8-byte Reload
	imull	%ebx, %eax
	movl	%eax, 16(%rsp)                  ## 4-byte Spill
	movq	%rbx, %rax
	shlq	$5, %rax
	movq	%rax, 72(%rsp)                  ## 8-byte Spill
	movq	%rbx, %rsi
	shlq	$9, %rsi
	shlq	$7, %rbx
	movq	8(%rsp), %rax                   ## 8-byte Reload
	addq	$224, %rax
	movq	%rax, 256(%rsp)                 ## 8-byte Spill
	movq	144(%rsp), %rax                 ## 8-byte Reload
	leal	(%rax,%rcx,8), %eax
	movl	%eax, 52(%rsp)                  ## 4-byte Spill
	xorl	%edi, %edi
	movl	%edx, 232(%rsp)                 ## 4-byte Spill
	jmp	LBB44_27
	.p2align	4, 0x90
LBB44_34:                               ## %"end for conv1_stage1_1_d_def__.s1.r982$x.r982$x10.loopexit.us"
                                        ##   in Loop: Header=BB44_27 Depth=1
	movq	264(%rsp), %rdi                 ## 8-byte Reload
	incq	%rdi
	addl	$8, 56(%rsp)                    ## 4-byte Folded Spill
	movl	236(%rsp), %edx                 ## 4-byte Reload
	addl	$-8, %edx
	addl	$8, 52(%rsp)                    ## 4-byte Folded Spill
	cmpq	240(%rsp), %rdi                 ## 8-byte Folded Reload
	je	LBB44_35
LBB44_27:                               ## %"for conv1_stage1_1_d_def__.s1.w.w.rebased.us"
                                        ## =>This Loop Header: Depth=1
                                        ##     Child Loop BB44_29 Depth 2
                                        ##       Child Loop BB44_31 Depth 3
                                        ##         Child Loop BB44_37 Depth 4
                                        ##         Child Loop BB44_40 Depth 4
                                        ##         Child Loop BB44_46 Depth 4
                                        ##         Child Loop BB44_50 Depth 4
	cmpl	$8, %edx
	movl	$8, %r12d
	movl	%edx, 236(%rsp)                 ## 4-byte Spill
	cmovll	%edx, %r12d
	leal	(,%rdi,8), %eax
	movl	232(%rsp), %ecx                 ## 4-byte Reload
                                        ## kill: def $ecx killed $ecx def $rcx
	subl	%eax, %ecx
	cmpl	$8, %ecx
	movl	$8, %eax
	cmovll	%ecx, %eax
	movq	%rax, 80(%rsp)                  ## 8-byte Spill
	cmpl	$8, %ecx
	movl	$8, %eax
	cmovgel	%eax, %ecx
	movq	%rcx, 32(%rsp)                  ## 8-byte Spill
	movq	152(%rsp), %rax                 ## 8-byte Reload
	movq	%rdi, 264(%rsp)                 ## 8-byte Spill
	leal	(%rdi,%rax), %r8d
	shll	$3, %r8d
	cmpl	%r8d, 20(%rsp)                  ## 4-byte Folded Reload
	jle	LBB44_34
## %bb.28:                              ## %"for conv1_stage1_1_d_def__.s1.r982$x.r982$x9.us.us.preheader"
                                        ##   in Loop: Header=BB44_27 Depth=1
	movslq	52(%rsp), %rcx                  ## 4-byte Folded Reload
	movq	8(%rsp), %rdx                   ## 8-byte Reload
	leaq	(%rdx,%rcx,4), %r14
	movq	256(%rsp), %rdx                 ## 8-byte Reload
	leaq	(%rdx,%rcx,4), %r13
	movl	%r12d, %ecx
	andl	$-32, %ecx
	addq	$-32, %rcx
	shrq	$5, %rcx
	incq	%rcx
	andq	$-2, %rcx
	negq	%rcx
	movq	%rcx, 320(%rsp)                 ## 8-byte Spill
	movslq	56(%rsp), %rax                  ## 4-byte Folded Reload
	movl	%r12d, %r11d
	andl	$3, %r11d
	leaq	3(%rax), %rcx
	movq	72(%rsp), %rdi                  ## 8-byte Reload
	imulq	%rdi, %rcx
	movq	64(%rsp), %rdx                  ## 8-byte Reload
	leaq	(%rdx,%rcx,4), %rcx
	movq	%rcx, 296(%rsp)                 ## 8-byte Spill
	leaq	2(%rax), %rcx
	imulq	%rdi, %rcx
	leaq	(%rdx,%rcx,4), %rcx
	movq	%rcx, 288(%rsp)                 ## 8-byte Spill
	leaq	1(%rax), %rcx
	imulq	%rdi, %rcx
	leaq	(%rdx,%rcx,4), %rcx
	movq	%rcx, 280(%rsp)                 ## 8-byte Spill
	movq	%rdi, %rcx
	movq	%rax, 184(%rsp)                 ## 8-byte Spill
	imulq	%rax, %rcx
	leaq	(%rdx,%rcx,4), %rax
	movq	%rax, 272(%rsp)                 ## 8-byte Spill
	movl	%r12d, %r15d
	andl	$-4, %r15d
	movq	80(%rsp), %rcx                  ## 8-byte Reload
	movl	%ecx, %eax
	andl	$-32, %eax
	addq	$-32, %rax
	movq	%rax, 176(%rsp)                 ## 8-byte Spill
	shrq	$5, %rax
	incq	%rax
	movq	%rax, 168(%rsp)                 ## 8-byte Spill
	addl	144(%rsp), %r8d                 ## 4-byte Folded Reload
	movslq	%r8d, %rax
	movq	%rax, 312(%rsp)                 ## 8-byte Spill
	movq	32(%rsp), %rax                  ## 8-byte Reload
                                        ## kill: def $eax killed $eax killed $rax def $rax
	andl	$-32, %eax
	movq	%rax, 160(%rsp)                 ## 8-byte Spill
	leaq	-1(%rcx), %rax
	movq	%rax, 192(%rsp)                 ## 8-byte Spill
	movl	16(%rsp), %eax                  ## 4-byte Reload
	movl	%eax, 60(%rsp)                  ## 4-byte Spill
	xorl	%eax, %eax
	jmp	LBB44_29
	.p2align	4, 0x90
LBB44_33:                               ## %"end for conv1_stage1_1_d_def__.s1.r982$x.r1029$xi13.us.us"
                                        ##   in Loop: Header=BB44_29 Depth=2
	movq	304(%rsp), %rax                 ## 8-byte Reload
	incq	%rax
	addl	$2, 60(%rsp)                    ## 4-byte Folded Spill
	cmpq	248(%rsp), %rax                 ## 8-byte Folded Reload
	je	LBB44_34
LBB44_29:                               ## %"for conv1_stage1_1_d_def__.s1.r982$x.r982$x9.us.us"
                                        ##   Parent Loop BB44_27 Depth=1
                                        ## =>  This Loop Header: Depth=2
                                        ##       Child Loop BB44_31 Depth 3
                                        ##         Child Loop BB44_37 Depth 4
                                        ##         Child Loop BB44_40 Depth 4
                                        ##         Child Loop BB44_46 Depth 4
                                        ##         Child Loop BB44_50 Depth 4
	movq	%rax, 304(%rsp)                 ## 8-byte Spill
	leal	(%rax,%rax), %ecx
	movq	24(%rsp), %rax                  ## 8-byte Reload
                                        ## kill: def $eax killed $eax killed $rax
	subl	%ecx, %eax
	jle	LBB44_33
## %bb.30:                              ## %"for conv1_stage1_1_d_def__.s1.r982$x.r1029$xi12.us.us.us.preheader"
                                        ##   in Loop: Header=BB44_29 Depth=2
	movslq	60(%rsp), %r9                   ## 4-byte Folded Reload
	movq	64(%rsp), %rcx                  ## 8-byte Reload
	leaq	(%rcx,%r9,4), %rcx
	xorl	%edx, %edx
	cmpl	$1, %eax
	sete	%dl
	movl	$2, %eax
	subq	%rdx, %rax
	movq	%rax, 200(%rsp)                 ## 8-byte Spill
	movq	272(%rsp), %r10                 ## 8-byte Reload
	movq	280(%rsp), %rdx                 ## 8-byte Reload
	movq	288(%rsp), %rdi                 ## 8-byte Reload
	movq	296(%rsp), %r8                  ## 8-byte Reload
	xorl	%eax, %eax
	jmp	LBB44_31
	.p2align	4, 0x90
LBB44_51:                               ## %"end for conv1_stage1_1_d_def__.s1.w.wi.loopexit.us.us.us"
                                        ##   in Loop: Header=BB44_31 Depth=3
	movq	208(%rsp), %rax                 ## 8-byte Reload
	incq	%rax
	addq	$4, %r8
	movq	224(%rsp), %rdi                 ## 8-byte Reload
	addq	$4, %rdi
	addq	$4, %rdx
	addq	$4, %r10
	addq	$4, %rcx
	cmpq	200(%rsp), %rax                 ## 8-byte Folded Reload
	je	LBB44_33
LBB44_31:                               ## %"for conv1_stage1_1_d_def__.s1.r982$x.r1029$xi12.us.us.us"
                                        ##   Parent Loop BB44_27 Depth=1
                                        ##     Parent Loop BB44_29 Depth=2
                                        ## =>    This Loop Header: Depth=3
                                        ##         Child Loop BB44_37 Depth 4
                                        ##         Child Loop BB44_40 Depth 4
                                        ##         Child Loop BB44_46 Depth 4
                                        ##         Child Loop BB44_50 Depth 4
	movq	%rax, 208(%rsp)                 ## 8-byte Spill
	cmpq	$3, 192(%rsp)                   ## 8-byte Folded Reload
	movq	%r10, 40(%rsp)                  ## 8-byte Spill
	movq	%rdx, 88(%rsp)                  ## 8-byte Spill
	movq	%rdi, 224(%rsp)                 ## 8-byte Spill
	movq	%r8, 216(%rsp)                  ## 8-byte Spill
	jae	LBB44_36
## %bb.32:                              ##   in Loop: Header=BB44_31 Depth=3
	xorl	%eax, %eax
	jmp	LBB44_38
	.p2align	4, 0x90
LBB44_36:                               ## %"for conv1_stage2_0_d_def__$1.s0.w.wi.us.us.us.preheader"
                                        ##   in Loop: Header=BB44_31 Depth=3
	xorl	%eax, %eax
	.p2align	4, 0x90
LBB44_37:                               ## %"for conv1_stage2_0_d_def__$1.s0.w.wi.us.us.us"
                                        ##   Parent Loop BB44_27 Depth=1
                                        ##     Parent Loop BB44_29 Depth=2
                                        ##       Parent Loop BB44_31 Depth=3
                                        ## =>      This Inner Loop Header: Depth=4
	vmovss	(%r10,%r9,4), %xmm0             ## xmm0 = mem[0],zero,zero,zero
	vmovss	%xmm0, 96(%rsp,%rax,4)
	vmovss	(%rdx,%r9,4), %xmm0             ## xmm0 = mem[0],zero,zero,zero
	vmovss	%xmm0, 100(%rsp,%rax,4)
	vmovss	(%rdi,%r9,4), %xmm0             ## xmm0 = mem[0],zero,zero,zero
	vmovss	%xmm0, 104(%rsp,%rax,4)
	vmovss	(%r8,%r9,4), %xmm0              ## xmm0 = mem[0],zero,zero,zero
	vmovss	%xmm0, 108(%rsp,%rax,4)
	addq	$4, %rax
	addq	%rsi, %r8
	addq	%rsi, %rdi
	addq	%rsi, %rdx
	addq	%rsi, %r10
	cmpq	%rax, %r15
	jne	LBB44_37
LBB44_38:                               ## %"for conv1_stage1_1_d_def__.s1.w.wi.us.us.us.preheader.unr-lcssa"
                                        ##   in Loop: Header=BB44_31 Depth=3
	testb	$3, 80(%rsp)                    ## 1-byte Folded Reload
	je	LBB44_41
## %bb.39:                              ## %"for conv1_stage2_0_d_def__$1.s0.w.wi.us.us.us.epil.preheader"
                                        ##   in Loop: Header=BB44_31 Depth=3
	leaq	(%rsp,%rax,4), %rdx
	addq	$96, %rdx
	addq	184(%rsp), %rax                 ## 8-byte Folded Reload
	imulq	72(%rsp), %rax                  ## 8-byte Folded Reload
	leaq	(%rcx,%rax,4), %rax
	xorl	%edi, %edi
	.p2align	4, 0x90
LBB44_40:                               ## %"for conv1_stage2_0_d_def__$1.s0.w.wi.us.us.us.epil"
                                        ##   Parent Loop BB44_27 Depth=1
                                        ##     Parent Loop BB44_29 Depth=2
                                        ##       Parent Loop BB44_31 Depth=3
                                        ## =>      This Inner Loop Header: Depth=4
	vmovss	(%rax), %xmm0                   ## xmm0 = mem[0],zero,zero,zero
	vmovss	%xmm0, (%rdx,%rdi,4)
	incq	%rdi
	addq	%rbx, %rax
	cmpq	%rdi, %r11
	jne	LBB44_40
LBB44_41:                               ## %"for conv1_stage1_1_d_def__.s1.w.wi.us.us.us.preheader"
                                        ##   in Loop: Header=BB44_31 Depth=3
	cmpl	$32, 32(%rsp)                   ## 4-byte Folded Reload
	movq	216(%rsp), %r8                  ## 8-byte Reload
	jae	LBB44_43
## %bb.42:                              ##   in Loop: Header=BB44_31 Depth=3
	xorl	%eax, %eax
	movq	40(%rsp), %r10                  ## 8-byte Reload
	movq	88(%rsp), %rdx                  ## 8-byte Reload
	jmp	LBB44_50
	.p2align	4, 0x90
LBB44_43:                               ## %vector.ph
                                        ##   in Loop: Header=BB44_31 Depth=3
	cmpq	$0, 176(%rsp)                   ## 8-byte Folded Reload
	je	LBB44_44
## %bb.45:                              ## %vector.body.preheader
                                        ##   in Loop: Header=BB44_31 Depth=3
	movq	320(%rsp), %rdx                 ## 8-byte Reload
	xorl	%eax, %eax
	.p2align	4, 0x90
LBB44_46:                               ## %vector.body
                                        ##   Parent Loop BB44_27 Depth=1
                                        ##     Parent Loop BB44_29 Depth=2
                                        ##       Parent Loop BB44_31 Depth=3
                                        ## =>      This Inner Loop Header: Depth=4
	vmovups	-224(%r13,%rax,4), %ymm0
	vmovups	-192(%r13,%rax,4), %ymm1
	vmovups	-160(%r13,%rax,4), %ymm2
	vmovups	-128(%r13,%rax,4), %ymm3
	vaddps	96(%rsp,%rax,4), %ymm0, %ymm0
	vaddps	128(%rsp,%rax,4), %ymm1, %ymm1
	vaddps	160(%rsp,%rax,4), %ymm2, %ymm2
	vaddps	192(%rsp,%rax,4), %ymm3, %ymm3
	vmovups	%ymm0, -224(%r13,%rax,4)
	vmovups	%ymm1, -192(%r13,%rax,4)
	vmovups	%ymm2, -160(%r13,%rax,4)
	vmovups	%ymm3, -128(%r13,%rax,4)
	vmovups	-96(%r13,%rax,4), %ymm0
	vmovups	-64(%r13,%rax,4), %ymm1
	vmovups	-32(%r13,%rax,4), %ymm2
	vmovups	(%r13,%rax,4), %ymm3
	vaddps	224(%rsp,%rax,4), %ymm0, %ymm0
	vaddps	256(%rsp,%rax,4), %ymm1, %ymm1
	vaddps	288(%rsp,%rax,4), %ymm2, %ymm2
	vaddps	320(%rsp,%rax,4), %ymm3, %ymm3
	vmovups	%ymm0, -96(%r13,%rax,4)
	vmovups	%ymm1, -64(%r13,%rax,4)
	vmovups	%ymm2, -32(%r13,%rax,4)
	vmovups	%ymm3, (%r13,%rax,4)
	addq	$64, %rax
	addq	$2, %rdx
	jne	LBB44_46
## %bb.47:                              ## %middle.block.unr-lcssa
                                        ##   in Loop: Header=BB44_31 Depth=3
	testb	$1, 168(%rsp)                   ## 1-byte Folded Reload
	je	LBB44_49
LBB44_48:                               ## %vector.body.epil
                                        ##   in Loop: Header=BB44_31 Depth=3
	movq	312(%rsp), %rdx                 ## 8-byte Reload
	addq	%rax, %rdx
	movq	8(%rsp), %rdi                   ## 8-byte Reload
	vmovups	(%rdi,%rdx,4), %ymm0
	vmovups	32(%rdi,%rdx,4), %ymm1
	vmovups	64(%rdi,%rdx,4), %ymm2
	vmovups	96(%rdi,%rdx,4), %ymm3
	vaddps	96(%rsp,%rax,4), %ymm0, %ymm0
	vaddps	128(%rsp,%rax,4), %ymm1, %ymm1
	vaddps	160(%rsp,%rax,4), %ymm2, %ymm2
	vaddps	192(%rsp,%rax,4), %ymm3, %ymm3
	vmovups	%ymm0, (%rdi,%rdx,4)
	vmovups	%ymm1, 32(%rdi,%rdx,4)
	vmovups	%ymm2, 64(%rdi,%rdx,4)
	vmovups	%ymm3, 96(%rdi,%rdx,4)
LBB44_49:                               ## %middle.block
                                        ##   in Loop: Header=BB44_31 Depth=3
	movq	160(%rsp), %rdx                 ## 8-byte Reload
	movq	%rdx, %rax
	cmpq	32(%rsp), %rdx                  ## 8-byte Folded Reload
	movq	40(%rsp), %r10                  ## 8-byte Reload
	movq	88(%rsp), %rdx                  ## 8-byte Reload
	je	LBB44_51
	.p2align	4, 0x90
LBB44_50:                               ## %"for conv1_stage1_1_d_def__.s1.w.wi.us.us.us"
                                        ##   Parent Loop BB44_27 Depth=1
                                        ##     Parent Loop BB44_29 Depth=2
                                        ##       Parent Loop BB44_31 Depth=3
                                        ## =>      This Inner Loop Header: Depth=4
	vmovss	(%r14,%rax,4), %xmm0            ## xmm0 = mem[0],zero,zero,zero
	vaddss	96(%rsp,%rax,4), %xmm0, %xmm0
	vmovss	%xmm0, (%r14,%rax,4)
	incq	%rax
	cmpq	%rax, %r12
	jne	LBB44_50
	jmp	LBB44_51
LBB44_44:                               ##   in Loop: Header=BB44_31 Depth=3
	xorl	%eax, %eax
	testb	$1, 168(%rsp)                   ## 1-byte Folded Reload
	jne	LBB44_48
	jmp	LBB44_49
LBB44_35:                               ## %destructor_block
	xorl	%eax, %eax
	leaq	-40(%rbp), %rsp
	popq	%rbx
	popq	%r12
	popq	%r13
	popq	%r14
	popq	%r15
	popq	%rbp
	vzeroupper
	retq
                                        ## -- End function
	.p2align	4, 0x90                         ## -- Begin function train_cost_model.par_for.head1_conv_1_d_def__.s0.w
_train_cost_model.par_for.head1_conv_1_d_def__.s0.w: ## @train_cost_model.par_for.head1_conv_1_d_def__.s0.w
## %bb.0:                               ## %entry
	movq	(%rdx), %rax
	movslq	%esi, %rcx
	shlq	$5, %rcx
	vxorps	%xmm0, %xmm0, %xmm0
	vmovaps	%ymm0, (%rax,%rcx)
	xorl	%eax, %eax
	vzeroupper
	retq
                                        ## -- End function
	.p2align	4, 0x90                         ## -- Begin function train_cost_model.par_for.head1_conv_1_d_def__.s1.w
_train_cost_model.par_for.head1_conv_1_d_def__.s1.w: ## @train_cost_model.par_for.head1_conv_1_d_def__.s1.w
## %bb.0:                               ## %entry
	pushq	%r15
	pushq	%r14
	pushq	%rbx
	movslq	(%rdx), %r14
	movslq	4(%rdx), %r10
	movq	24(%rdx), %r11
	movq	40(%rdx), %r8
	movslq	%esi, %rsi
	leaq	(,%rsi,8), %r9
	movq	%rsi, %rcx
	shlq	$5, %rcx
	shlq	$2, %rsi
	addq	8(%rdx), %rsi
	vmovaps	(%r8,%rcx), %ymm0
	shlq	$2, %r10
	leaq	(,%r14,4), %r15
	xorl	%ecx, %ecx
	.p2align	4, 0x90
LBB46_1:                                ## %"for head1_conv_1_d_def__.s1.r1137$x"
                                        ## =>This Inner Loop Header: Depth=1
	leaq	(%r11,%rcx,4), %rdi
	leaq	(%rdi,%r15), %rbx
	addq	%r15, %rbx
	leaq	(%rbx,%r15), %rax
	addq	%r15, %rax
	leaq	(%rax,%r15), %rdx
	vmovss	(%rbx,%r14,8), %xmm1            ## xmm1 = mem[0],zero,zero,zero
	vinsertps	$16, (%rax,%r14,4), %xmm1, %xmm1 ## xmm1 = xmm1[0],mem[0],xmm1[2,3]
	vinsertps	$32, (%rdx,%r14,4), %xmm1, %xmm1 ## xmm1 = xmm1[0,1],mem[0],xmm1[3]
	addq	%r15, %rdx
	vinsertps	$48, (%rdx,%r14,4), %xmm1, %xmm1 ## xmm1 = xmm1[0,1,2],mem[0]
	vmovss	(%r11,%rcx,4), %xmm2            ## xmm2 = mem[0],zero,zero,zero
	vinsertps	$16, (%rdi,%r14,4), %xmm2, %xmm2 ## xmm2 = xmm2[0],mem[0],xmm2[2,3]
	vinsertps	$32, (%rdi,%r14,8), %xmm2, %xmm2 ## xmm2 = xmm2[0,1],mem[0],xmm2[3]
	vinsertps	$48, (%rbx,%r14,4), %xmm2, %xmm2 ## xmm2 = xmm2[0,1,2],mem[0]
	vinsertf128	$1, %xmm1, %ymm2, %ymm1
	vbroadcastss	(%rsi), %ymm2
	vfmadd231ps	%ymm2, %ymm1, %ymm0     ## ymm0 = (ymm1 * ymm2) + ymm0
	incq	%rcx
	addq	%r10, %rsi
	cmpq	$32, %rcx
	jne	LBB46_1
## %bb.2:                               ## %destructor_block
	vmovaps	%ymm0, (%r8,%r9,4)
	xorl	%eax, %eax
	popq	%rbx
	popq	%r14
	popq	%r15
	vzeroupper
	retq
                                        ## -- End function
	.section	__TEXT,__literal4,4byte_literals
	.p2align	2                               ## -- Begin function train_cost_model.par_for.updated_head1_filter.s1.v235.v235.v235
LCPI47_0:
	.long	0xbfb8aa3b                      ## float -1.44269502
LCPI47_1:
	.long	0x3f317200                      ## float 0.693145751
LCPI47_2:
	.long	0x35bfbe8e                      ## float 1.42860677E-6
LCPI47_3:
	.long	4294967169                      ## 0xffffff81
LCPI47_4:
	.long	128                             ## 0x80
LCPI47_5:
	.long	0xb9a797f3                      ## float -3.19659332E-4
LCPI47_6:
	.long	0xbc0b192a                      ## float -0.00848988629
LCPI47_7:
	.long	0xbe2aae1f                      ## float -0.166679844
LCPI47_8:
	.long	0xbf800000                      ## float -1
LCPI47_9:
	.long	0x3f800000                      ## float 1
LCPI47_10:
	.long	0x3a9c2e66                      ## float 0.00119156833
LCPI47_11:
	.long	0x3d2a66bc                      ## float 0.0416018814
LCPI47_12:
	.long	0x3effffde                      ## float 0.499998987
LCPI47_13:
	.long	0x7f800000                      ## float +Inf
	.section	__TEXT,__text,regular,pure_instructions
	.p2align	4, 0x90
_train_cost_model.par_for.updated_head1_filter.s1.v235.v235.v235: ## @train_cost_model.par_for.updated_head1_filter.s1.v235.v235.v235
## %bb.0:                               ## %entry
	pushq	%rbp
	pushq	%r15
	pushq	%r14
	pushq	%r13
	pushq	%r12
	pushq	%rbx
	subq	$200, %rsp
	movslq	(%rdx), %rax
	movq	%rax, -56(%rsp)                 ## 8-byte Spill
	movslq	4(%rdx), %r14
	movslq	16(%rdx), %r15
	movslq	40(%rdx), %rax
	movq	%rax, -80(%rsp)                 ## 8-byte Spill
	movq	48(%rdx), %r8
	movq	64(%rdx), %rax
	movq	%rax, -64(%rsp)                 ## 8-byte Spill
	movq	80(%rdx), %r11
	movq	96(%rdx), %rbx
	cmpl	$59, %esi
	movq	%rbx, -72(%rsp)                 ## 8-byte Spill
	jg	LBB47_4
## %bb.1:                               ## %then_bb
	movl	20(%rdx), %r9d
	movl	28(%rdx), %r12d
	movl	44(%rdx), %r10d
	movl	%esi, %eax
	sarl	$31, %eax
	movl	%eax, %ecx
	xorl	%esi, %ecx
	imulq	$1717986919, %rcx, %rcx         ## imm = 0x66666667
	shrq	$35, %rcx
	xorl	%eax, %ecx
	leal	(,%rcx,4), %eax
	leal	(%rax,%rax,4), %eax
	subl	%eax, %esi
	addl	%esi, %esi
	movslq	%ecx, %rbp
	addq	%rbp, %rbp
	movslq	%esi, %rdi
	testl	%r9d, %r9d
	jle	LBB47_49
## %bb.2:                               ## %"for updated_head1_filter.s1.v236.v209i.us.preheader"
	movq	%rdi, -104(%rsp)                ## 8-byte Spill
	movl	12(%rdx), %eax
	subl	24(%rdx), %esi
	movl	%ebp, %ecx
	movq	%r14, 80(%rsp)                  ## 8-byte Spill
	imull	%r14d, %ecx
	movl	%ecx, (%rsp)                    ## 4-byte Spill
	movl	%ebp, %r14d
	movl	%r10d, -92(%rsp)                ## 4-byte Spill
	imull	%r10d, %r14d
	movl	%r12d, -36(%rsp)                ## 4-byte Spill
	addl	%r12d, %r14d
	movq	%rbp, 88(%rsp)                  ## 8-byte Spill
	movl	%ebp, %r12d
	movl	%eax, -40(%rsp)                 ## 4-byte Spill
	imull	%eax, %r12d
	addl	%esi, %r12d
	movslq	%r12d, %rcx
	leaq	-1(%r9), %rax
	movl	%r9d, %ebx
	andl	$3, %ebx
	movq	%rax, -48(%rsp)                 ## 8-byte Spill
	cmpq	$3, %rax
	movq	%r9, -88(%rsp)                  ## 8-byte Spill
	movq	%rcx, -32(%rsp)                 ## 8-byte Spill
	jae	LBB47_7
## %bb.3:
	vxorps	%xmm15, %xmm15, %xmm15
	xorl	%ebp, %ebp
	jmp	LBB47_9
LBB47_4:                                ## %next_bb
	movl	8(%rdx), %r12d
	movslq	36(%rdx), %rax
	movq	%rax, -88(%rsp)                 ## 8-byte Spill
	movl	%esi, %eax
	imulq	$1717986919, %rax, %rax         ## imm = 0x66666667
	shrq	$35, %rax
	shll	$2, %eax
	leal	(%rax,%rax,4), %eax
	subl	%eax, %esi
	movslq	%esi, %rax
	leaq	(%rax,%rax), %rcx
	movslq	%ecx, %rdi
	testl	%r12d, %r12d
	jle	LBB47_50
## %bb.5:                               ## %"for updated_head1_filter.s1.v235.v208i1.us.preheader"
	movq	%rdi, -104(%rsp)                ## 8-byte Spill
	movslq	32(%rdx), %rcx
	addq	%r14, %r14
	leaq	(%rcx,%rax,2), %r10
	leaq	-1(%r12), %rax
	movl	%r12d, %r13d
	andl	$3, %r13d
	movq	%rax, -48(%rsp)                 ## 8-byte Spill
	cmpq	$3, %rax
	jae	LBB47_14
## %bb.6:
	vxorps	%xmm15, %xmm15, %xmm15
	xorl	%esi, %esi
	jmp	LBB47_16
LBB47_7:                                ## %"for updated_head1_filter.s1.v236.v209i.us.preheader.new"
	movl	%r9d, %edi
	andl	$-4, %edi
	leaq	(%r15,%r15,2), %rax
	leaq	(%r11,%rax,4), %rdx
	leaq	(,%rcx,4), %rax
	movq	%r15, %r10
	shlq	$4, %r10
	leaq	96(%r8), %rcx
	leaq	(%r11,%r15,8), %r9
	leaq	(%r11,%r15,4), %r13
	vxorps	%xmm15, %xmm15, %xmm15
	xorl	%ebp, %ebp
	.p2align	4, 0x90
LBB47_8:                                ## %"for squashed_head1_filter_0_d_def__.s1.r1260$x.us.us"
                                        ## =>This Inner Loop Header: Depth=1
	vbroadcastss	(%r11,%rax), %ymm0
	vfmadd132ps	-96(%rcx), %ymm15, %ymm0 ## ymm0 = (ymm0 * mem) + ymm15
	vbroadcastss	(%r13,%rax), %ymm1
	vfmadd132ps	-64(%rcx), %ymm0, %ymm1 ## ymm1 = (ymm1 * mem) + ymm0
	vbroadcastss	(%r9,%rax), %ymm0
	vfmadd132ps	-32(%rcx), %ymm1, %ymm0 ## ymm0 = (ymm0 * mem) + ymm1
	vbroadcastss	(%rdx,%rax), %ymm15
	vfmadd132ps	(%rcx), %ymm0, %ymm15   ## ymm15 = (ymm15 * mem) + ymm0
	addq	$4, %rbp
	addq	%r10, %rax
	subq	$-128, %rcx
	cmpq	%rbp, %rdi
	jne	LBB47_8
LBB47_9:                                ## %"consume squashed_head1_filter_0_d_def__.loopexit.us.us.unr-lcssa"
	movslq	(%rsp), %rdi                    ## 4-byte Folded Reload
	movslq	%r14d, %r9
	testq	%rbx, %rbx
	je	LBB47_12
## %bb.10:                              ## %"for squashed_head1_filter_0_d_def__.s1.r1260$x.us.us.epil.preheader"
	movq	%rbp, %rax
	imulq	%r15, %rax
	addq	-32(%rsp), %rax                 ## 8-byte Folded Reload
	leaq	(%r11,%rax,4), %rax
	leaq	(,%r15,4), %rcx
	shlq	$5, %rbp
	addq	%r8, %rbp
	shlq	$5, %rbx
	xorl	%edx, %edx
	.p2align	4, 0x90
LBB47_11:                               ## %"for squashed_head1_filter_0_d_def__.s1.r1260$x.us.us.epil"
                                        ## =>This Inner Loop Header: Depth=1
	vbroadcastss	(%rax), %ymm0
	vfmadd231ps	(%rbp,%rdx), %ymm0, %ymm15 ## ymm15 = (ymm0 * mem) + ymm15
	addq	%rcx, %rax
	addq	$32, %rdx
	cmpq	%rdx, %rbx
	jne	LBB47_11
LBB47_12:                               ## %"consume squashed_head1_filter_0_d_def__.loopexit.us.us"
	movq	-104(%rsp), %rcx                ## 8-byte Reload
	movq	%rcx, %rax
	imulq	-56(%rsp), %rax                 ## 8-byte Folded Reload
	movq	%rax, 72(%rsp)                  ## 8-byte Spill
	movq	%rdi, 48(%rsp)                  ## 8-byte Spill
	addq	%rdi, %rax
	movq	-64(%rsp), %rdx                 ## 8-byte Reload
	vmovups	(%rdx,%rax,4), %ymm11
	vbroadcastss	LCPI47_0(%rip), %ymm0   ## ymm0 = [-1.44269502E+0,-1.44269502E+0,-1.44269502E+0,-1.44269502E+0,-1.44269502E+0,-1.44269502E+0,-1.44269502E+0,-1.44269502E+0]
	vmovups	%ymm0, (%rsp)                   ## 32-byte Spill
	vmulps	%ymm0, %ymm11, %ymm0
	vroundps	$1, %ymm0, %ymm12
	vbroadcastss	LCPI47_1(%rip), %ymm2   ## ymm2 = [6.93145751E-1,6.93145751E-1,6.93145751E-1,6.93145751E-1,6.93145751E-1,6.93145751E-1,6.93145751E-1,6.93145751E-1]
	vfmadd231ps	%ymm2, %ymm12, %ymm11   ## ymm11 = (ymm12 * ymm2) + ymm11
	vbroadcastss	LCPI47_2(%rip), %ymm0   ## ymm0 = [1.42860677E-6,1.42860677E-6,1.42860677E-6,1.42860677E-6,1.42860677E-6,1.42860677E-6,1.42860677E-6,1.42860677E-6]
	vfmadd231ps	%ymm12, %ymm0, %ymm11   ## ymm11 = (ymm0 * ymm12) + ymm11
	vmulps	%ymm11, %ymm11, %ymm13
	vbroadcastss	LCPI47_5(%rip), %ymm8   ## ymm8 = [-3.19659332E-4,-3.19659332E-4,-3.19659332E-4,-3.19659332E-4,-3.19659332E-4,-3.19659332E-4,-3.19659332E-4,-3.19659332E-4]
	vbroadcastss	LCPI47_6(%rip), %ymm1   ## ymm1 = [-8.48988629E-3,-8.48988629E-3,-8.48988629E-3,-8.48988629E-3,-8.48988629E-3,-8.48988629E-3,-8.48988629E-3,-8.48988629E-3]
	vmovaps	%ymm8, %ymm14
	vmovups	%ymm1, -32(%rsp)                ## 32-byte Spill
	vfmadd213ps	%ymm1, %ymm13, %ymm14   ## ymm14 = (ymm13 * ymm14) + ymm1
	vbroadcastss	LCPI47_7(%rip), %ymm1   ## ymm1 = [-1.66679844E-1,-1.66679844E-1,-1.66679844E-1,-1.66679844E-1,-1.66679844E-1,-1.66679844E-1,-1.66679844E-1,-1.66679844E-1]
	vmovups	%ymm1, 160(%rsp)                ## 32-byte Spill
	vfmadd213ps	%ymm1, %ymm13, %ymm14   ## ymm14 = (ymm13 * ymm14) + ymm1
	vbroadcastss	LCPI47_8(%rip), %ymm1   ## ymm1 = [-1.0E+0,-1.0E+0,-1.0E+0,-1.0E+0,-1.0E+0,-1.0E+0,-1.0E+0,-1.0E+0]
	vmovups	%ymm1, 128(%rsp)                ## 32-byte Spill
	vfmadd213ps	%ymm1, %ymm13, %ymm14   ## ymm14 = (ymm13 * ymm14) + ymm1
	vbroadcastss	LCPI47_10(%rip), %ymm9  ## ymm9 = [1.19156833E-3,1.19156833E-3,1.19156833E-3,1.19156833E-3,1.19156833E-3,1.19156833E-3,1.19156833E-3,1.19156833E-3]
	vbroadcastss	LCPI47_11(%rip), %ymm1  ## ymm1 = [4.16018814E-2,4.16018814E-2,4.16018814E-2,4.16018814E-2,4.16018814E-2,4.16018814E-2,4.16018814E-2,4.16018814E-2]
	vmovaps	%ymm9, %ymm4
	vmovups	%ymm1, 96(%rsp)                 ## 32-byte Spill
	vfmadd213ps	%ymm1, %ymm13, %ymm4    ## ymm4 = (ymm13 * ymm4) + ymm1
	vbroadcastss	LCPI47_12(%rip), %ymm10 ## ymm10 = [4.99998987E-1,4.99998987E-1,4.99998987E-1,4.99998987E-1,4.99998987E-1,4.99998987E-1,4.99998987E-1,4.99998987E-1]
	vfmadd213ps	%ymm10, %ymm13, %ymm4   ## ymm4 = (ymm13 * ymm4) + ymm10
	vbroadcastss	LCPI47_9(%rip), %ymm5   ## ymm5 = [1.0E+0,1.0E+0,1.0E+0,1.0E+0,1.0E+0,1.0E+0,1.0E+0,1.0E+0]
	vfmadd213ps	%ymm5, %ymm13, %ymm4    ## ymm4 = (ymm13 * ymm4) + ymm5
	vcvttps2dq	%ymm12, %ymm6
	vfmadd231ps	%ymm14, %ymm11, %ymm4   ## ymm4 = (ymm11 * ymm14) + ymm4
	vpslld	$23, %ymm6, %ymm12
	vpbroadcastd	LCPI47_9(%rip), %ymm11  ## ymm11 = [1.0E+0,1.0E+0,1.0E+0,1.0E+0,1.0E+0,1.0E+0,1.0E+0,1.0E+0]
	vpaddd	%ymm11, %ymm12, %ymm12
	vmulps	%ymm4, %ymm12, %ymm4
	vpbroadcastd	LCPI47_4(%rip), %ymm12  ## ymm12 = [128,128,128,128,128,128,128,128]
	vpcmpgtd	%ymm6, %ymm12, %ymm14
	vbroadcastss	LCPI47_13(%rip), %ymm13 ## ymm13 = [+Inf,+Inf,+Inf,+Inf,+Inf,+Inf,+Inf,+Inf]
	vblendvps	%ymm14, %ymm4, %ymm13, %ymm4
	vpbroadcastd	LCPI47_3(%rip), %ymm14  ## ymm14 = [4294967169,4294967169,4294967169,4294967169,4294967169,4294967169,4294967169,4294967169]
	vpcmpgtd	%ymm14, %ymm6, %ymm6
	vpand	%ymm4, %ymm6, %ymm4
	vmulps	%ymm4, %ymm15, %ymm6
	vaddps	%ymm5, %ymm4, %ymm4
	vmulps	%ymm4, %ymm4, %ymm4
	vdivps	%ymm4, %ymm6, %ymm4
	imulq	-80(%rsp), %rcx                 ## 8-byte Folded Reload
	movq	%rcx, 64(%rsp)                  ## 8-byte Spill
	movq	%r9, 56(%rsp)                   ## 8-byte Spill
	leaq	(%rcx,%r9), %rax
	movq	-72(%rsp), %r13                 ## 8-byte Reload
	vmovups	%ymm4, (%r13,%rax,4)
	incl	%r12d
	movslq	%r12d, %r12
	movq	-88(%rsp), %rdx                 ## 8-byte Reload
	movl	%edx, %r10d
	andl	$3, %r10d
	cmpq	$3, -48(%rsp)                   ## 8-byte Folded Reload
	jae	LBB47_21
## %bb.13:
	vxorps	%xmm15, %xmm15, %xmm15
	xorl	%ebp, %ebp
	jmp	LBB47_23
LBB47_14:                               ## %"for updated_head1_filter.s1.v235.v208i1.us.preheader.new"
	movl	%r12d, %ebx
	andl	$-4, %ebx
	leaq	(%r15,%r15,2), %rax
	leaq	(%r11,%rax,4), %r9
	leaq	(,%r10,4), %rax
	movq	%r15, %rdi
	shlq	$4, %rdi
	leaq	96(%r8), %rcx
	leaq	(%r11,%r15,8), %rbp
	leaq	(%r11,%r15,4), %rdx
	vxorps	%xmm15, %xmm15, %xmm15
	xorl	%esi, %esi
	.p2align	4, 0x90
LBB47_15:                               ## %"for squashed_head1_filter_0_d_def__.s1.r1260$x5.us"
                                        ## =>This Inner Loop Header: Depth=1
	vbroadcastss	(%r11,%rax), %ymm0
	vfmadd132ps	-96(%rcx), %ymm15, %ymm0 ## ymm0 = (ymm0 * mem) + ymm15
	vbroadcastss	(%rdx,%rax), %ymm1
	vfmadd132ps	-64(%rcx), %ymm0, %ymm1 ## ymm1 = (ymm1 * mem) + ymm0
	vbroadcastss	(%rbp,%rax), %ymm0
	vfmadd132ps	-32(%rcx), %ymm1, %ymm0 ## ymm0 = (ymm0 * mem) + ymm1
	vbroadcastss	(%r9,%rax), %ymm15
	vfmadd132ps	(%rcx), %ymm0, %ymm15   ## ymm15 = (ymm15 * mem) + ymm0
	addq	$4, %rsi
	addq	%rdi, %rax
	subq	$-128, %rcx
	cmpq	%rsi, %rbx
	jne	LBB47_15
LBB47_16:                               ## %"consume squashed_head1_filter_0_d_def__8.loopexit.us.unr-lcssa"
	leaq	(%r14,%r14,2), %r9
	testq	%r13, %r13
	je	LBB47_19
## %bb.17:                              ## %"for squashed_head1_filter_0_d_def__.s1.r1260$x5.us.epil.preheader"
	movq	%rsi, %rax
	imulq	%r15, %rax
	addq	%r10, %rax
	leaq	(%r11,%rax,4), %rax
	leaq	(,%r15,4), %rcx
	shlq	$5, %rsi
	addq	%r8, %rsi
	shlq	$5, %r13
	xorl	%edx, %edx
	.p2align	4, 0x90
LBB47_18:                               ## %"for squashed_head1_filter_0_d_def__.s1.r1260$x5.us.epil"
                                        ## =>This Inner Loop Header: Depth=1
	vbroadcastss	(%rax), %ymm0
	vfmadd231ps	(%rsi,%rdx), %ymm0, %ymm15 ## ymm15 = (ymm0 * mem) + ymm15
	addq	%rcx, %rax
	addq	$32, %rdx
	cmpq	%rdx, %r13
	jne	LBB47_18
LBB47_19:                               ## %"consume squashed_head1_filter_0_d_def__8.loopexit.us"
	movq	-104(%rsp), %rcx                ## 8-byte Reload
	movq	%rcx, %rax
	imulq	-56(%rsp), %rax                 ## 8-byte Folded Reload
	addq	%r9, %rax
	movq	-64(%rsp), %rdx                 ## 8-byte Reload
	vmovups	(%rdx,%rax,4), %ymm11
	vbroadcastss	LCPI47_0(%rip), %ymm0   ## ymm0 = [-1.44269502E+0,-1.44269502E+0,-1.44269502E+0,-1.44269502E+0,-1.44269502E+0,-1.44269502E+0,-1.44269502E+0,-1.44269502E+0]
	vmovups	%ymm0, (%rsp)                   ## 32-byte Spill
	vmulps	%ymm0, %ymm11, %ymm0
	vroundps	$1, %ymm0, %ymm12
	vbroadcastss	LCPI47_1(%rip), %ymm2   ## ymm2 = [6.93145751E-1,6.93145751E-1,6.93145751E-1,6.93145751E-1,6.93145751E-1,6.93145751E-1,6.93145751E-1,6.93145751E-1]
	vfmadd231ps	%ymm2, %ymm12, %ymm11   ## ymm11 = (ymm12 * ymm2) + ymm11
	vbroadcastss	LCPI47_2(%rip), %ymm0   ## ymm0 = [1.42860677E-6,1.42860677E-6,1.42860677E-6,1.42860677E-6,1.42860677E-6,1.42860677E-6,1.42860677E-6,1.42860677E-6]
	vfmadd231ps	%ymm12, %ymm0, %ymm11   ## ymm11 = (ymm0 * ymm12) + ymm11
	vmulps	%ymm11, %ymm11, %ymm13
	vbroadcastss	LCPI47_5(%rip), %ymm1   ## ymm1 = [-3.19659332E-4,-3.19659332E-4,-3.19659332E-4,-3.19659332E-4,-3.19659332E-4,-3.19659332E-4,-3.19659332E-4,-3.19659332E-4]
	vbroadcastss	LCPI47_6(%rip), %ymm3   ## ymm3 = [-8.48988629E-3,-8.48988629E-3,-8.48988629E-3,-8.48988629E-3,-8.48988629E-3,-8.48988629E-3,-8.48988629E-3,-8.48988629E-3]
	vmovaps	%ymm1, %ymm14
	vmovups	%ymm3, -32(%rsp)                ## 32-byte Spill
	vfmadd213ps	%ymm3, %ymm13, %ymm14   ## ymm14 = (ymm13 * ymm14) + ymm3
	vbroadcastss	LCPI47_7(%rip), %ymm7   ## ymm7 = [-1.66679844E-1,-1.66679844E-1,-1.66679844E-1,-1.66679844E-1,-1.66679844E-1,-1.66679844E-1,-1.66679844E-1,-1.66679844E-1]
	vfmadd213ps	%ymm7, %ymm13, %ymm14   ## ymm14 = (ymm13 * ymm14) + ymm7
	vbroadcastss	LCPI47_8(%rip), %ymm8   ## ymm8 = [-1.0E+0,-1.0E+0,-1.0E+0,-1.0E+0,-1.0E+0,-1.0E+0,-1.0E+0,-1.0E+0]
	vfmadd213ps	%ymm8, %ymm13, %ymm14   ## ymm14 = (ymm13 * ymm14) + ymm8
	vbroadcastss	LCPI47_10(%rip), %ymm3  ## ymm3 = [1.19156833E-3,1.19156833E-3,1.19156833E-3,1.19156833E-3,1.19156833E-3,1.19156833E-3,1.19156833E-3,1.19156833E-3]
	vbroadcastss	LCPI47_11(%rip), %ymm9  ## ymm9 = [4.16018814E-2,4.16018814E-2,4.16018814E-2,4.16018814E-2,4.16018814E-2,4.16018814E-2,4.16018814E-2,4.16018814E-2]
	vmovaps	%ymm3, %ymm4
	vfmadd213ps	%ymm9, %ymm13, %ymm4    ## ymm4 = (ymm13 * ymm4) + ymm9
	vbroadcastss	LCPI47_12(%rip), %ymm10 ## ymm10 = [4.99998987E-1,4.99998987E-1,4.99998987E-1,4.99998987E-1,4.99998987E-1,4.99998987E-1,4.99998987E-1,4.99998987E-1]
	vfmadd213ps	%ymm10, %ymm13, %ymm4   ## ymm4 = (ymm13 * ymm4) + ymm10
	vbroadcastss	LCPI47_9(%rip), %ymm5   ## ymm5 = [1.0E+0,1.0E+0,1.0E+0,1.0E+0,1.0E+0,1.0E+0,1.0E+0,1.0E+0]
	vfmadd213ps	%ymm5, %ymm13, %ymm4    ## ymm4 = (ymm13 * ymm4) + ymm5
	vcvttps2dq	%ymm12, %ymm6
	vfmadd231ps	%ymm14, %ymm11, %ymm4   ## ymm4 = (ymm11 * ymm14) + ymm4
	vpslld	$23, %ymm6, %ymm12
	vpbroadcastd	LCPI47_9(%rip), %ymm11  ## ymm11 = [1.0E+0,1.0E+0,1.0E+0,1.0E+0,1.0E+0,1.0E+0,1.0E+0,1.0E+0]
	vpaddd	%ymm11, %ymm12, %ymm12
	vmulps	%ymm4, %ymm12, %ymm4
	vpbroadcastd	LCPI47_4(%rip), %ymm12  ## ymm12 = [128,128,128,128,128,128,128,128]
	vpcmpgtd	%ymm6, %ymm12, %ymm14
	vbroadcastss	LCPI47_13(%rip), %ymm13 ## ymm13 = [+Inf,+Inf,+Inf,+Inf,+Inf,+Inf,+Inf,+Inf]
	vblendvps	%ymm14, %ymm4, %ymm13, %ymm4
	vpbroadcastd	LCPI47_3(%rip), %ymm14  ## ymm14 = [4294967169,4294967169,4294967169,4294967169,4294967169,4294967169,4294967169,4294967169]
	vpcmpgtd	%ymm14, %ymm6, %ymm6
	vpand	%ymm4, %ymm6, %ymm4
	vmulps	%ymm4, %ymm15, %ymm6
	vaddps	%ymm5, %ymm4, %ymm4
	vmulps	%ymm4, %ymm4, %ymm4
	vdivps	%ymm4, %ymm6, %ymm4
	movq	%rcx, %rax
	imulq	-80(%rsp), %rax                 ## 8-byte Folded Reload
	movq	-88(%rsp), %r14                 ## 8-byte Reload
	addq	%r14, %rax
	movq	-72(%rsp), %rcx                 ## 8-byte Reload
	vmovups	%ymm4, (%rcx,%rax,4)
	leal	1(%r10), %eax
	movslq	%eax, %r10
	movl	%r12d, %r13d
	andl	$3, %r13d
	cmpq	$3, -48(%rsp)                   ## 8-byte Folded Reload
	jae	LBB47_28
## %bb.20:
	vxorps	%xmm15, %xmm15, %xmm15
	xorl	%ebp, %ebp
	jmp	LBB47_30
LBB47_21:                               ## %"consume squashed_head1_filter_0_d_def__.loopexit.us.us.new"
                                        ## kill: def $edx killed $edx killed $rdx def $rdx
	andl	$-4, %edx
	leaq	(%r15,%r15,2), %rax
	leaq	(%r11,%rax,4), %rdi
	leaq	(,%r12,4), %r9
	movq	%r15, %rax
	shlq	$4, %rax
	leaq	96(%r8), %rcx
	leaq	(%r11,%r15,8), %rbx
	leaq	(%r11,%r15,4), %r14
	vxorps	%xmm15, %xmm15, %xmm15
	xorl	%ebp, %ebp
	.p2align	4, 0x90
LBB47_22:                               ## %"for squashed_head1_filter_0_d_def__.s1.r1260$x.us.us.1"
                                        ## =>This Inner Loop Header: Depth=1
	vbroadcastss	(%r11,%r9), %ymm4
	vfmadd132ps	-96(%rcx), %ymm15, %ymm4 ## ymm4 = (ymm4 * mem) + ymm15
	vbroadcastss	(%r14,%r9), %ymm6
	vfmadd132ps	-64(%rcx), %ymm4, %ymm6 ## ymm6 = (ymm6 * mem) + ymm4
	vbroadcastss	(%rbx,%r9), %ymm4
	vfmadd132ps	-32(%rcx), %ymm6, %ymm4 ## ymm4 = (ymm4 * mem) + ymm6
	vbroadcastss	(%rdi,%r9), %ymm15
	vfmadd132ps	(%rcx), %ymm4, %ymm15   ## ymm15 = (ymm15 * mem) + ymm4
	addq	$4, %rbp
	addq	%rax, %r9
	subq	$-128, %rcx
	cmpq	%rbp, %rdx
	jne	LBB47_22
LBB47_23:                               ## %"consume squashed_head1_filter_0_d_def__.loopexit.us.us.1.unr-lcssa"
	testq	%r10, %r10
	je	LBB47_26
## %bb.24:                              ## %"for squashed_head1_filter_0_d_def__.s1.r1260$x.us.us.1.epil.preheader"
	movq	%rbp, %rax
	imulq	%r15, %rax
	addq	%r12, %rax
	leaq	(%r11,%rax,4), %rax
	leaq	(,%r15,4), %rcx
	shlq	$5, %rbp
	addq	%r8, %rbp
	shlq	$5, %r10
	xorl	%edx, %edx
	.p2align	4, 0x90
LBB47_25:                               ## %"for squashed_head1_filter_0_d_def__.s1.r1260$x.us.us.1.epil"
                                        ## =>This Inner Loop Header: Depth=1
	vbroadcastss	(%rax), %ymm4
	vfmadd231ps	(%rbp,%rdx), %ymm4, %ymm15 ## ymm15 = (ymm4 * mem) + ymm15
	addq	%rcx, %rax
	addq	$32, %rdx
	cmpq	%rdx, %r10
	jne	LBB47_25
LBB47_26:                               ## %"consume squashed_head1_filter_0_d_def__.loopexit.us.us.1"
	movq	-104(%rsp), %rax                ## 8-byte Reload
	orq	$1, %rax
	movq	-56(%rsp), %rcx                 ## 8-byte Reload
	imulq	%rax, %rcx
	movq	%rcx, -56(%rsp)                 ## 8-byte Spill
	movq	48(%rsp), %rdx                  ## 8-byte Reload
	addq	%rcx, %rdx
	movq	-64(%rsp), %rcx                 ## 8-byte Reload
	vmovups	(%rcx,%rdx,4), %ymm4
	vmulps	(%rsp), %ymm4, %ymm6            ## 32-byte Folded Reload
	vroundps	$1, %ymm6, %ymm6
	vfmadd231ps	%ymm2, %ymm6, %ymm4     ## ymm4 = (ymm6 * ymm2) + ymm4
	vfmadd231ps	%ymm6, %ymm0, %ymm4     ## ymm4 = (ymm0 * ymm6) + ymm4
	vmulps	%ymm4, %ymm4, %ymm7
	vmovaps	%ymm8, %ymm1
	vfmadd213ps	-32(%rsp), %ymm7, %ymm1 ## 32-byte Folded Reload
                                        ## ymm1 = (ymm7 * ymm1) + mem
	vfmadd213ps	160(%rsp), %ymm7, %ymm1 ## 32-byte Folded Reload
                                        ## ymm1 = (ymm7 * ymm1) + mem
	vfmadd213ps	128(%rsp), %ymm7, %ymm1 ## 32-byte Folded Reload
                                        ## ymm1 = (ymm7 * ymm1) + mem
	vmovaps	%ymm9, %ymm3
	vfmadd213ps	96(%rsp), %ymm7, %ymm3  ## 32-byte Folded Reload
                                        ## ymm3 = (ymm7 * ymm3) + mem
	vfmadd213ps	%ymm10, %ymm7, %ymm3    ## ymm3 = (ymm7 * ymm3) + ymm10
	vfmadd213ps	%ymm5, %ymm7, %ymm3     ## ymm3 = (ymm7 * ymm3) + ymm5
	vfmadd231ps	%ymm1, %ymm4, %ymm3     ## ymm3 = (ymm4 * ymm1) + ymm3
	vcvttps2dq	%ymm6, %ymm1
	vpslld	$23, %ymm1, %ymm4
	vpaddd	%ymm4, %ymm11, %ymm4
	vmulps	%ymm4, %ymm3, %ymm3
	vpcmpgtd	%ymm1, %ymm12, %ymm4
	vblendvps	%ymm4, %ymm3, %ymm13, %ymm3
	vpcmpgtd	%ymm14, %ymm1, %ymm1
	vpand	%ymm3, %ymm1, %ymm1
	vmulps	%ymm1, %ymm15, %ymm3
	vaddps	%ymm5, %ymm1, %ymm1
	vmulps	%ymm1, %ymm1, %ymm1
	vdivps	%ymm1, %ymm3, %ymm1
	imulq	-80(%rsp), %rax                 ## 8-byte Folded Reload
	movq	%rax, -104(%rsp)                ## 8-byte Spill
	movq	56(%rsp), %rcx                  ## 8-byte Reload
	addq	%rax, %rcx
	vmovups	%ymm1, (%r13,%rcx,4)
	movq	88(%rsp), %rax                  ## 8-byte Reload
	orq	$1, %rax
	movl	-92(%rsp), %ecx                 ## 4-byte Reload
	imull	%eax, %ecx
	addl	-36(%rsp), %ecx                 ## 4-byte Folded Reload
	movl	%ecx, -92(%rsp)                 ## 4-byte Spill
	movq	%rax, %r13
	movl	-40(%rsp), %ecx                 ## 4-byte Reload
	imull	%eax, %ecx
	addl	%ecx, %esi
	movslq	%esi, %r9
	movq	-88(%rsp), %r12                 ## 8-byte Reload
	movl	%r12d, %r10d
	andl	$3, %r10d
	cmpq	$3, -48(%rsp)                   ## 8-byte Folded Reload
	jae	LBB47_34
## %bb.27:
	vxorps	%xmm15, %xmm15, %xmm15
	xorl	%ebx, %ebx
	jmp	LBB47_36
LBB47_28:                               ## %"consume squashed_head1_filter_0_d_def__8.loopexit.us.new"
	andl	$-4, %r12d
	leaq	(%r15,%r15,2), %rax
	leaq	(%r11,%rax,4), %rbx
	leaq	(,%r10,4), %rsi
	movq	%r15, %rax
	shlq	$4, %rax
	leaq	96(%r8), %rcx
	leaq	(%r11,%r15,8), %rdi
	leaq	(%r11,%r15,4), %rdx
	vxorps	%xmm15, %xmm15, %xmm15
	xorl	%ebp, %ebp
	.p2align	4, 0x90
LBB47_29:                               ## %"for squashed_head1_filter_0_d_def__.s1.r1260$x5.us.1"
                                        ## =>This Inner Loop Header: Depth=1
	vbroadcastss	(%r11,%rsi), %ymm4
	vfmadd132ps	-96(%rcx), %ymm15, %ymm4 ## ymm4 = (ymm4 * mem) + ymm15
	vbroadcastss	(%rdx,%rsi), %ymm6
	vfmadd132ps	-64(%rcx), %ymm4, %ymm6 ## ymm6 = (ymm6 * mem) + ymm4
	vbroadcastss	(%rdi,%rsi), %ymm4
	vfmadd132ps	-32(%rcx), %ymm6, %ymm4 ## ymm4 = (ymm4 * mem) + ymm6
	vbroadcastss	(%rbx,%rsi), %ymm15
	vfmadd132ps	(%rcx), %ymm4, %ymm15   ## ymm15 = (ymm15 * mem) + ymm4
	addq	$4, %rbp
	addq	%rax, %rsi
	subq	$-128, %rcx
	cmpq	%rbp, %r12
	jne	LBB47_29
LBB47_30:                               ## %"consume squashed_head1_filter_0_d_def__8.loopexit.us.1.unr-lcssa"
	testq	%r13, %r13
	je	LBB47_33
## %bb.31:                              ## %"for squashed_head1_filter_0_d_def__.s1.r1260$x5.us.1.epil.preheader"
	movq	%rbp, %rax
	imulq	%r15, %rax
	addq	%r10, %rax
	leaq	(%r11,%rax,4), %rax
	shlq	$2, %r15
	shlq	$5, %rbp
	addq	%rbp, %r8
	shlq	$5, %r13
	xorl	%ecx, %ecx
	.p2align	4, 0x90
LBB47_32:                               ## %"for squashed_head1_filter_0_d_def__.s1.r1260$x5.us.1.epil"
                                        ## =>This Inner Loop Header: Depth=1
	vbroadcastss	(%rax), %ymm4
	vfmadd231ps	(%r8,%rcx), %ymm4, %ymm15 ## ymm15 = (ymm4 * mem) + ymm15
	addq	%r15, %rax
	addq	$32, %rcx
	cmpq	%rcx, %r13
	jne	LBB47_32
LBB47_33:                               ## %"consume squashed_head1_filter_0_d_def__8.loopexit.us.1"
	movq	-104(%rsp), %rdi                ## 8-byte Reload
	orq	$1, %rdi
	movq	-56(%rsp), %rax                 ## 8-byte Reload
	imulq	%rdi, %rax
	addq	%r9, %rax
	movq	-64(%rsp), %rcx                 ## 8-byte Reload
	vmovups	(%rcx,%rax,4), %ymm4
	vmulps	(%rsp), %ymm4, %ymm6            ## 32-byte Folded Reload
	vroundps	$1, %ymm6, %ymm6
	vfmadd213ps	%ymm4, %ymm6, %ymm2     ## ymm2 = (ymm6 * ymm2) + ymm4
	vfmadd213ps	%ymm2, %ymm6, %ymm0     ## ymm0 = (ymm6 * ymm0) + ymm2
	vmulps	%ymm0, %ymm0, %ymm2
	vfmadd213ps	-32(%rsp), %ymm2, %ymm1 ## 32-byte Folded Reload
                                        ## ymm1 = (ymm2 * ymm1) + mem
	vfmadd213ps	%ymm7, %ymm2, %ymm1     ## ymm1 = (ymm2 * ymm1) + ymm7
	vfmadd213ps	%ymm8, %ymm2, %ymm1     ## ymm1 = (ymm2 * ymm1) + ymm8
	vfmadd213ps	%ymm9, %ymm2, %ymm3     ## ymm3 = (ymm2 * ymm3) + ymm9
	vfmadd213ps	%ymm10, %ymm2, %ymm3    ## ymm3 = (ymm2 * ymm3) + ymm10
	vfmadd213ps	%ymm5, %ymm2, %ymm3     ## ymm3 = (ymm2 * ymm3) + ymm5
	vfmadd231ps	%ymm1, %ymm0, %ymm3     ## ymm3 = (ymm0 * ymm1) + ymm3
	vcvttps2dq	%ymm6, %ymm0
	vpslld	$23, %ymm0, %ymm1
	vpaddd	%ymm1, %ymm11, %ymm1
	vmulps	%ymm1, %ymm3, %ymm1
	vpcmpgtd	%ymm0, %ymm12, %ymm2
	vblendvps	%ymm2, %ymm1, %ymm13, %ymm1
	vpcmpgtd	%ymm14, %ymm0, %ymm0
	vpand	%ymm1, %ymm0, %ymm0
	vaddps	%ymm5, %ymm0, %ymm1
	vmulps	%ymm0, %ymm15, %ymm0
	vmulps	%ymm1, %ymm1, %ymm1
	vdivps	%ymm1, %ymm0, %ymm0
	imulq	-80(%rsp), %rdi                 ## 8-byte Folded Reload
	addq	%r14, %rdi
	jmp	LBB47_47
LBB47_34:                               ## %"consume squashed_head1_filter_0_d_def__.loopexit.us.us.1.new"
                                        ## kill: def $r12d killed $r12d killed $r12 def $r12
	andl	$-4, %r12d
	leaq	(%r15,%r15,2), %rax
	leaq	(%r11,%rax,4), %r14
	leaq	(,%r9,4), %rax
	movq	%r15, %rcx
	shlq	$4, %rcx
	leaq	96(%r8), %rdx
	leaq	(%r11,%r15,8), %rbp
	leaq	(%r11,%r15,4), %rdi
	vxorps	%xmm15, %xmm15, %xmm15
	xorl	%ebx, %ebx
	.p2align	4, 0x90
LBB47_35:                               ## %"for squashed_head1_filter_0_d_def__.s1.r1260$x.us.us.131"
                                        ## =>This Inner Loop Header: Depth=1
	vbroadcastss	(%r11,%rax), %ymm1
	vfmadd132ps	-96(%rdx), %ymm15, %ymm1 ## ymm1 = (ymm1 * mem) + ymm15
	vbroadcastss	(%rdi,%rax), %ymm3
	vfmadd132ps	-64(%rdx), %ymm1, %ymm3 ## ymm3 = (ymm3 * mem) + ymm1
	vbroadcastss	(%rbp,%rax), %ymm1
	vfmadd132ps	-32(%rdx), %ymm3, %ymm1 ## ymm1 = (ymm1 * mem) + ymm3
	vbroadcastss	(%r14,%rax), %ymm15
	vfmadd132ps	(%rdx), %ymm1, %ymm15   ## ymm15 = (ymm15 * mem) + ymm1
	addq	$4, %rbx
	addq	%rcx, %rax
	subq	$-128, %rdx
	cmpq	%rbx, %r12
	jne	LBB47_35
LBB47_36:                               ## %"consume squashed_head1_filter_0_d_def__.loopexit.us.us.137.unr-lcssa"
	movq	%r13, %r14
	imulq	80(%rsp), %r14                  ## 8-byte Folded Reload
	movslq	-92(%rsp), %r12                 ## 4-byte Folded Reload
	testq	%r10, %r10
	je	LBB47_39
## %bb.37:                              ## %"for squashed_head1_filter_0_d_def__.s1.r1260$x.us.us.131.epil.preheader"
	movq	%rbx, %rax
	imulq	%r15, %rax
	addq	%r9, %rax
	leaq	(%r11,%rax,4), %rax
	leaq	(,%r15,4), %rcx
	shlq	$5, %rbx
	addq	%r8, %rbx
	shlq	$5, %r10
	xorl	%edx, %edx
	.p2align	4, 0x90
LBB47_38:                               ## %"for squashed_head1_filter_0_d_def__.s1.r1260$x.us.us.131.epil"
                                        ## =>This Inner Loop Header: Depth=1
	vbroadcastss	(%rax), %ymm1
	vfmadd231ps	(%rbx,%rdx), %ymm1, %ymm15 ## ymm15 = (ymm1 * mem) + ymm15
	addq	%rcx, %rax
	addq	$32, %rdx
	cmpq	%rdx, %r10
	jne	LBB47_38
LBB47_39:                               ## %"consume squashed_head1_filter_0_d_def__.loopexit.us.us.137"
	movq	72(%rsp), %rcx                  ## 8-byte Reload
	addq	%r14, %rcx
	movq	-64(%rsp), %rax                 ## 8-byte Reload
	vmovups	(%rax,%rcx,4), %ymm1
	vmulps	(%rsp), %ymm1, %ymm3            ## 32-byte Folded Reload
	vroundps	$1, %ymm3, %ymm3
	vfmadd231ps	%ymm2, %ymm3, %ymm1     ## ymm1 = (ymm3 * ymm2) + ymm1
	vfmadd231ps	%ymm3, %ymm0, %ymm1     ## ymm1 = (ymm0 * ymm3) + ymm1
	vmulps	%ymm1, %ymm1, %ymm4
	vmovaps	%ymm8, %ymm6
	vfmadd213ps	-32(%rsp), %ymm4, %ymm6 ## 32-byte Folded Reload
                                        ## ymm6 = (ymm4 * ymm6) + mem
	vfmadd213ps	160(%rsp), %ymm4, %ymm6 ## 32-byte Folded Reload
                                        ## ymm6 = (ymm4 * ymm6) + mem
	vfmadd213ps	128(%rsp), %ymm4, %ymm6 ## 32-byte Folded Reload
                                        ## ymm6 = (ymm4 * ymm6) + mem
	vmovaps	%ymm9, %ymm7
	vfmadd213ps	96(%rsp), %ymm4, %ymm7  ## 32-byte Folded Reload
                                        ## ymm7 = (ymm4 * ymm7) + mem
	vfmadd213ps	%ymm10, %ymm4, %ymm7    ## ymm7 = (ymm4 * ymm7) + ymm10
	vfmadd213ps	%ymm5, %ymm4, %ymm7     ## ymm7 = (ymm4 * ymm7) + ymm5
	vfmadd231ps	%ymm6, %ymm1, %ymm7     ## ymm7 = (ymm1 * ymm6) + ymm7
	vcvttps2dq	%ymm3, %ymm1
	vpslld	$23, %ymm1, %ymm3
	vpaddd	%ymm3, %ymm11, %ymm3
	vmulps	%ymm3, %ymm7, %ymm3
	vpcmpgtd	%ymm1, %ymm12, %ymm4
	vblendvps	%ymm4, %ymm3, %ymm13, %ymm3
	vpcmpgtd	%ymm14, %ymm1, %ymm1
	vpand	%ymm3, %ymm1, %ymm1
	vmulps	%ymm1, %ymm15, %ymm3
	vaddps	%ymm5, %ymm1, %ymm1
	vmulps	%ymm1, %ymm1, %ymm1
	vdivps	%ymm1, %ymm3, %ymm1
	movq	64(%rsp), %rcx                  ## 8-byte Reload
	addq	%r12, %rcx
	movq	-72(%rsp), %rax                 ## 8-byte Reload
	vmovups	%ymm1, (%rax,%rcx,4)
	incl	%esi
	movslq	%esi, %r9
	movq	-88(%rsp), %rbx                 ## 8-byte Reload
	movl	%ebx, %r10d
	andl	$3, %r10d
	cmpq	$3, -48(%rsp)                   ## 8-byte Folded Reload
	jae	LBB47_41
## %bb.40:
	vxorps	%xmm15, %xmm15, %xmm15
	xorl	%ebp, %ebp
	jmp	LBB47_43
LBB47_41:                               ## %"consume squashed_head1_filter_0_d_def__.loopexit.us.us.137.new"
	andl	$-4, %ebx
	leaq	(%r15,%r15,2), %rax
	leaq	(%r11,%rax,4), %r13
	leaq	(,%r9,4), %rax
	movq	%r15, %rcx
	shlq	$4, %rcx
	leaq	96(%r8), %rdi
	leaq	(%r11,%r15,8), %rsi
	leaq	(%r11,%r15,4), %rdx
	vxorps	%xmm15, %xmm15, %xmm15
	xorl	%ebp, %ebp
	.p2align	4, 0x90
LBB47_42:                               ## %"for squashed_head1_filter_0_d_def__.s1.r1260$x.us.us.1.1"
                                        ## =>This Inner Loop Header: Depth=1
	vbroadcastss	(%r11,%rax), %ymm1
	vfmadd132ps	-96(%rdi), %ymm15, %ymm1 ## ymm1 = (ymm1 * mem) + ymm15
	vbroadcastss	(%rdx,%rax), %ymm3
	vfmadd132ps	-64(%rdi), %ymm1, %ymm3 ## ymm3 = (ymm3 * mem) + ymm1
	vbroadcastss	(%rsi,%rax), %ymm1
	vfmadd132ps	-32(%rdi), %ymm3, %ymm1 ## ymm1 = (ymm1 * mem) + ymm3
	vbroadcastss	(%r13,%rax), %ymm15
	vfmadd132ps	(%rdi), %ymm1, %ymm15   ## ymm15 = (ymm15 * mem) + ymm1
	addq	$4, %rbp
	addq	%rcx, %rax
	subq	$-128, %rdi
	cmpq	%rbp, %rbx
	jne	LBB47_42
LBB47_43:                               ## %"consume squashed_head1_filter_0_d_def__.loopexit.us.us.1.1.unr-lcssa"
	testq	%r10, %r10
	je	LBB47_46
## %bb.44:                              ## %"for squashed_head1_filter_0_d_def__.s1.r1260$x.us.us.1.1.epil.preheader"
	movq	%rbp, %rax
	imulq	%r15, %rax
	addq	%r9, %rax
	leaq	(%r11,%rax,4), %rax
	shlq	$2, %r15
	shlq	$5, %rbp
	addq	%rbp, %r8
	shlq	$5, %r10
	xorl	%ecx, %ecx
	.p2align	4, 0x90
LBB47_45:                               ## %"for squashed_head1_filter_0_d_def__.s1.r1260$x.us.us.1.1.epil"
                                        ## =>This Inner Loop Header: Depth=1
	vbroadcastss	(%rax), %ymm1
	vfmadd231ps	(%r8,%rcx), %ymm1, %ymm15 ## ymm15 = (ymm1 * mem) + ymm15
	addq	%r15, %rax
	addq	$32, %rcx
	cmpq	%rcx, %r10
	jne	LBB47_45
LBB47_46:                               ## %"consume squashed_head1_filter_0_d_def__.loopexit.us.us.1.1"
	movq	-56(%rsp), %rax                 ## 8-byte Reload
	addq	%r14, %rax
	movq	-64(%rsp), %rcx                 ## 8-byte Reload
	vmovups	(%rcx,%rax,4), %ymm1
	vmulps	(%rsp), %ymm1, %ymm3            ## 32-byte Folded Reload
	vroundps	$1, %ymm3, %ymm3
	vfmadd213ps	%ymm1, %ymm3, %ymm2     ## ymm2 = (ymm3 * ymm2) + ymm1
	vfmadd213ps	%ymm2, %ymm3, %ymm0     ## ymm0 = (ymm3 * ymm0) + ymm2
	vmulps	%ymm0, %ymm0, %ymm1
	vfmadd213ps	-32(%rsp), %ymm1, %ymm8 ## 32-byte Folded Reload
                                        ## ymm8 = (ymm1 * ymm8) + mem
	vfmadd213ps	160(%rsp), %ymm1, %ymm8 ## 32-byte Folded Reload
                                        ## ymm8 = (ymm1 * ymm8) + mem
	vfmadd213ps	128(%rsp), %ymm1, %ymm8 ## 32-byte Folded Reload
                                        ## ymm8 = (ymm1 * ymm8) + mem
	vfmadd213ps	96(%rsp), %ymm1, %ymm9  ## 32-byte Folded Reload
                                        ## ymm9 = (ymm1 * ymm9) + mem
	vfmadd213ps	%ymm10, %ymm1, %ymm9    ## ymm9 = (ymm1 * ymm9) + ymm10
	vfmadd213ps	%ymm5, %ymm1, %ymm9     ## ymm9 = (ymm1 * ymm9) + ymm5
	vfmadd231ps	%ymm8, %ymm0, %ymm9     ## ymm9 = (ymm0 * ymm8) + ymm9
	vcvttps2dq	%ymm3, %ymm0
	vpslld	$23, %ymm0, %ymm1
	vpaddd	%ymm1, %ymm11, %ymm1
	vmulps	%ymm1, %ymm9, %ymm1
	vpcmpgtd	%ymm0, %ymm12, %ymm2
	vblendvps	%ymm2, %ymm1, %ymm13, %ymm1
	vpcmpgtd	%ymm14, %ymm0, %ymm0
	vpand	%ymm1, %ymm0, %ymm0
	vaddps	%ymm5, %ymm0, %ymm1
	vmulps	%ymm0, %ymm15, %ymm0
	vmulps	%ymm1, %ymm1, %ymm1
	vdivps	%ymm1, %ymm0, %ymm0
	movq	-104(%rsp), %rdi                ## 8-byte Reload
	addq	%r12, %rdi
LBB47_47:                               ## %destructor_block
	movq	-72(%rsp), %rbx                 ## 8-byte Reload
LBB47_48:                               ## %destructor_block
	vmovups	%ymm0, (%rbx,%rdi,4)
	xorl	%eax, %eax
	addq	$200, %rsp
	popq	%rbx
	popq	%r12
	popq	%r13
	popq	%r14
	popq	%r15
	popq	%rbp
	vzeroupper
	retq
LBB47_49:                               ## %"for updated_head1_filter.s1.v236.v209i.preheader"
	movl	%ebp, %eax
	imull	%r10d, %eax
	addl	%r12d, %eax
	cltq
	movq	%rdi, %rcx
	movq	-80(%rsp), %rsi                 ## 8-byte Reload
	imulq	%rsi, %rcx
	leaq	(%rcx,%rax), %rdx
	vxorps	%xmm0, %xmm0, %xmm0
	vmovups	%ymm0, (%rbx,%rdx,4)
	orq	$1, %rdi
	imulq	%rsi, %rdi
	addq	%rdi, %rax
	vmovups	%ymm0, (%rbx,%rax,4)
	orl	$1, %ebp
	imull	%r10d, %ebp
	addl	%r12d, %ebp
	movslq	%ebp, %rax
	addq	%rax, %rcx
	vmovups	%ymm0, (%rbx,%rcx,4)
	addq	%rax, %rdi
	jmp	LBB47_48
LBB47_50:                               ## %"for updated_head1_filter.s1.v235.v208i1"
	movq	%rdi, %rax
	movq	-80(%rsp), %rcx                 ## 8-byte Reload
	imulq	%rcx, %rax
	movq	-88(%rsp), %rdx                 ## 8-byte Reload
	addq	%rdx, %rax
	vxorps	%xmm0, %xmm0, %xmm0
	movq	-72(%rsp), %rbx                 ## 8-byte Reload
	vmovups	%ymm0, (%rbx,%rax,4)
	orq	$1, %rdi
	imulq	%rcx, %rdi
	addq	%rdx, %rdi
	jmp	LBB47_48
                                        ## -- End function
	.section	__TEXT,__literal4,4byte_literals
	.p2align	2                               ## -- Begin function train_cost_model.par_for.updated_head1_filter.s2.v235.v235.v235
LCPI48_0:
	.long	0x3f666666                      ## float 0.899999976
LCPI48_1:
	.long	0x3dcccccd                      ## float 0.100000001
	.section	__TEXT,__text,regular,pure_instructions
	.p2align	4, 0x90
_train_cost_model.par_for.updated_head1_filter.s2.v235.v235.v235: ## @train_cost_model.par_for.updated_head1_filter.s2.v235.v235.v235
## %bb.0:                               ## %entry
	pushq	%r14
	pushq	%rbx
	movslq	(%rdx), %r8
	movslq	4(%rdx), %r9
	movslq	8(%rdx), %r11
	movq	16(%rdx), %rcx
	cmpl	$59, %esi
	jg	LBB48_2
## %bb.1:                               ## %then_bb
	movl	%esi, %edx
	sarl	$31, %edx
	movl	%edx, %edi
	xorl	%esi, %edi
	imulq	$1717986919, %rdi, %rdi         ## imm = 0x66666667
	shrq	$35, %rdi
	xorl	%edx, %edi
	leal	(%rdi,%rdi), %eax
	leal	(,%rdi,4), %edx
	leal	(%rdx,%rdx,4), %edx
	subl	%edx, %esi
	addl	%esi, %esi
	leaq	(%r11,%r11,2), %r10
	movslq	%esi, %rdx
	imull	%r9d, %eax
	cltq
	movq	%rdx, %rsi
	imulq	%r8, %rsi
	leaq	(%rsi,%rax), %rbx
	vbroadcastss	LCPI48_0(%rip), %ymm0   ## ymm0 = [8.99999976E-1,8.99999976E-1,8.99999976E-1,8.99999976E-1,8.99999976E-1,8.99999976E-1,8.99999976E-1,8.99999976E-1]
	leaq	(%r10,%rbx), %r14
	addq	%r11, %rbx
	vbroadcastss	LCPI48_1(%rip), %ymm1   ## ymm1 = [1.00000001E-1,1.00000001E-1,1.00000001E-1,1.00000001E-1,1.00000001E-1,1.00000001E-1,1.00000001E-1,1.00000001E-1]
	vmulps	(%rcx,%r14,4), %ymm1, %ymm2
	vfmadd231ps	(%rcx,%rbx,4), %ymm0, %ymm2 ## ymm2 = (ymm0 * mem) + ymm2
	vmovups	%ymm2, (%rcx,%rbx,4)
	orq	$1, %rdx
	imulq	%r8, %rdx
	addq	%rdx, %rax
	leaq	(%r10,%rax), %rbx
	addq	%r11, %rax
	vmulps	(%rcx,%rbx,4), %ymm1, %ymm2
	vfmadd231ps	(%rcx,%rax,4), %ymm0, %ymm2 ## ymm2 = (ymm0 * mem) + ymm2
	vmovups	%ymm2, (%rcx,%rax,4)
	leal	(%rdi,%rdi), %eax
	incl	%eax
	imull	%r9d, %eax
	cltq
	addq	%rax, %rsi
	leaq	(%r10,%rsi), %rdi
	addq	%r11, %rsi
	vmulps	(%rcx,%rdi,4), %ymm1, %ymm1
	vfmadd231ps	(%rcx,%rsi,4), %ymm0, %ymm1 ## ymm1 = (ymm0 * mem) + ymm1
	vmovups	%ymm1, (%rcx,%rsi,4)
	addq	%rax, %rdx
	addq	%rdx, %r11
	addq	%r10, %rdx
	jmp	LBB48_3
LBB48_2:                                ## %next_bb
	movl	%esi, %eax
	imulq	$1717986919, %rax, %rax         ## imm = 0x66666667
	shrq	$35, %rax
	shll	$2, %eax
	leal	(%rax,%rax,4), %eax
	subl	%eax, %esi
	addl	%esi, %esi
	addq	%r9, %r9
	leaq	(%r9,%r9,2), %rax
	leaq	(%r11,%r11,2), %rdi
	movslq	%esi, %rdx
	movq	%rdx, %rsi
	imulq	%r8, %rsi
	addq	%rax, %rsi
	vbroadcastss	LCPI48_0(%rip), %ymm0   ## ymm0 = [8.99999976E-1,8.99999976E-1,8.99999976E-1,8.99999976E-1,8.99999976E-1,8.99999976E-1,8.99999976E-1,8.99999976E-1]
	leaq	(%rdi,%rsi), %rbx
	addq	%r11, %rsi
	vbroadcastss	LCPI48_1(%rip), %ymm1   ## ymm1 = [1.00000001E-1,1.00000001E-1,1.00000001E-1,1.00000001E-1,1.00000001E-1,1.00000001E-1,1.00000001E-1,1.00000001E-1]
	vmulps	(%rcx,%rbx,4), %ymm1, %ymm1
	vfmadd231ps	(%rcx,%rsi,4), %ymm0, %ymm1 ## ymm1 = (ymm0 * mem) + ymm1
	vmovups	%ymm1, (%rcx,%rsi,4)
	orq	$1, %rdx
	imulq	%r8, %rdx
	addq	%rax, %rdx
	addq	%rdx, %r11
	addq	%rdi, %rdx
LBB48_3:                                ## %destructor_block
	vbroadcastss	LCPI48_0(%rip), %ymm0   ## ymm0 = [8.99999976E-1,8.99999976E-1,8.99999976E-1,8.99999976E-1,8.99999976E-1,8.99999976E-1,8.99999976E-1,8.99999976E-1]
	vbroadcastss	LCPI48_1(%rip), %ymm1   ## ymm1 = [1.00000001E-1,1.00000001E-1,1.00000001E-1,1.00000001E-1,1.00000001E-1,1.00000001E-1,1.00000001E-1,1.00000001E-1]
	vmulps	(%rcx,%rdx,4), %ymm1, %ymm1
	vfmadd231ps	(%rcx,%r11,4), %ymm0, %ymm1 ## ymm1 = (ymm0 * mem) + ymm1
	vmovups	%ymm1, (%rcx,%r11,4)
	xorl	%eax, %eax
	popq	%rbx
	popq	%r14
	vzeroupper
	retq
                                        ## -- End function
	.section	__TEXT,__literal4,4byte_literals
	.p2align	2                               ## -- Begin function train_cost_model.par_for.updated_head1_filter.s3.v235.v235.v235
LCPI49_0:
	.long	0x3f7fbe77                      ## float 0.999000012
LCPI49_1:
	.long	0x3a83126f                      ## float 0.00100000005
	.section	__TEXT,__text,regular,pure_instructions
	.p2align	4, 0x90
_train_cost_model.par_for.updated_head1_filter.s3.v235.v235.v235: ## @train_cost_model.par_for.updated_head1_filter.s3.v235.v235.v235
## %bb.0:                               ## %entry
	pushq	%r14
	pushq	%rbx
	movslq	(%rdx), %r11
	movslq	4(%rdx), %r8
	movslq	8(%rdx), %r9
	movslq	12(%rdx), %r10
	movq	16(%rdx), %rax
	cmpl	$59, %esi
	jg	LBB49_2
## %bb.1:                               ## %then_bb
	movl	%esi, %edx
	sarl	$31, %edx
	movl	%edx, %ecx
	xorl	%esi, %ecx
	imulq	$1717986919, %rcx, %r14         ## imm = 0x66666667
	shrq	$35, %r14
	xorl	%edx, %r14d
	leal	(%r14,%r14), %edi
	leal	(,%r14,4), %edx
	leal	(%rdx,%rdx,4), %edx
	subl	%edx, %esi
	addl	%esi, %esi
	movslq	%esi, %rdx
	imull	%r10d, %edi
	movslq	%edi, %rdi
	movq	%rdx, %rsi
	imulq	%r9, %rsi
	leaq	(%rsi,%rdi), %rbx
	leaq	(%rbx,%r11), %rcx
	vmovups	(%rax,%rcx,4), %ymm1
	addq	%r8, %rbx
	vbroadcastss	LCPI49_0(%rip), %ymm0   ## ymm0 = [9.99000012E-1,9.99000012E-1,9.99000012E-1,9.99000012E-1,9.99000012E-1,9.99000012E-1,9.99000012E-1,9.99000012E-1]
	vmulps	%ymm1, %ymm1, %ymm1
	vbroadcastss	LCPI49_1(%rip), %ymm2   ## ymm2 = [1.00000005E-3,1.00000005E-3,1.00000005E-3,1.00000005E-3,1.00000005E-3,1.00000005E-3,1.00000005E-3,1.00000005E-3]
	vmulps	%ymm2, %ymm1, %ymm1
	vfmadd231ps	(%rax,%rbx,4), %ymm0, %ymm1 ## ymm1 = (ymm0 * mem) + ymm1
	vmovups	%ymm1, (%rax,%rbx,4)
	orq	$1, %rdx
	imulq	%r9, %rdx
	addq	%rdx, %rdi
	leaq	(%rdi,%r11), %rcx
	vmovups	(%rax,%rcx,4), %ymm1
	addq	%r8, %rdi
	vmulps	%ymm1, %ymm1, %ymm1
	vmulps	%ymm2, %ymm1, %ymm1
	vfmadd231ps	(%rax,%rdi,4), %ymm0, %ymm1 ## ymm1 = (ymm0 * mem) + ymm1
	vmovups	%ymm1, (%rax,%rdi,4)
	leal	(%r14,%r14), %ecx
	incl	%ecx
	imull	%r10d, %ecx
	movslq	%ecx, %rcx
	addq	%rcx, %rsi
	leaq	(%rsi,%r11), %rdi
	vmovups	(%rax,%rdi,4), %ymm1
	addq	%r8, %rsi
	vmulps	%ymm1, %ymm1, %ymm1
	vmulps	%ymm2, %ymm1, %ymm1
	vfmadd231ps	(%rax,%rsi,4), %ymm0, %ymm1 ## ymm1 = (ymm0 * mem) + ymm1
	vmovups	%ymm1, (%rax,%rsi,4)
	jmp	LBB49_3
LBB49_2:                                ## %next_bb
	movl	%esi, %ecx
	imulq	$1717986919, %rcx, %rcx         ## imm = 0x66666667
	shrq	$35, %rcx
	shll	$2, %ecx
	leal	(%rcx,%rcx,4), %ecx
	subl	%ecx, %esi
	addl	%esi, %esi
	addq	%r10, %r10
	leaq	(%r10,%r10,2), %rcx
	movslq	%esi, %rdx
	movq	%rdx, %rsi
	imulq	%r9, %rsi
	addq	%rcx, %rsi
	leaq	(%rsi,%r11), %rdi
	vmovups	(%rax,%rdi,4), %ymm0
	addq	%r8, %rsi
	vbroadcastss	LCPI49_0(%rip), %ymm1   ## ymm1 = [9.99000012E-1,9.99000012E-1,9.99000012E-1,9.99000012E-1,9.99000012E-1,9.99000012E-1,9.99000012E-1,9.99000012E-1]
	vmulps	%ymm0, %ymm0, %ymm0
	vbroadcastss	LCPI49_1(%rip), %ymm2   ## ymm2 = [1.00000005E-3,1.00000005E-3,1.00000005E-3,1.00000005E-3,1.00000005E-3,1.00000005E-3,1.00000005E-3,1.00000005E-3]
	vmulps	%ymm2, %ymm0, %ymm0
	vfmadd231ps	(%rax,%rsi,4), %ymm1, %ymm0 ## ymm0 = (ymm1 * mem) + ymm0
	vmovups	%ymm0, (%rax,%rsi,4)
	orq	$1, %rdx
	imulq	%r9, %rdx
LBB49_3:                                ## %destructor_block
	addq	%rcx, %rdx
	addq	%rdx, %r11
	vmovups	(%rax,%r11,4), %ymm0
	addq	%r8, %rdx
	vmulps	%ymm0, %ymm0, %ymm0
	vbroadcastss	LCPI49_0(%rip), %ymm1   ## ymm1 = [9.99000012E-1,9.99000012E-1,9.99000012E-1,9.99000012E-1,9.99000012E-1,9.99000012E-1,9.99000012E-1,9.99000012E-1]
	vbroadcastss	LCPI49_1(%rip), %ymm2   ## ymm2 = [1.00000005E-3,1.00000005E-3,1.00000005E-3,1.00000005E-3,1.00000005E-3,1.00000005E-3,1.00000005E-3,1.00000005E-3]
	vmulps	%ymm2, %ymm0, %ymm0
	vfmadd231ps	(%rax,%rdx,4), %ymm1, %ymm0 ## ymm0 = (ymm1 * mem) + ymm0
	vmovups	%ymm0, (%rax,%rdx,4)
	xorl	%eax, %eax
	popq	%rbx
	popq	%r14
	vzeroupper
	retq
                                        ## -- End function
	.section	__TEXT,__literal4,4byte_literals
	.p2align	2                               ## -- Begin function train_cost_model.par_for.updated_head1_filter.s4.v235.v235.v235
LCPI50_0:
	.long	0xb727c5ac                      ## float -9.99999974E-6
LCPI50_1:
	.long	0x3727c5ac                      ## float 9.99999974E-6
	.section	__TEXT,__text,regular,pure_instructions
	.p2align	4, 0x90
_train_cost_model.par_for.updated_head1_filter.s4.v235.v235.v235: ## @train_cost_model.par_for.updated_head1_filter.s4.v235.v235.v235
## %bb.0:                               ## %entry
	pushq	%rbp
	pushq	%r15
	pushq	%r14
	pushq	%r13
	pushq	%r12
	pushq	%rbx
	movslq	(%rdx), %r9
	movslq	4(%rdx), %r10
	vmovss	8(%rdx), %xmm1                  ## xmm1 = mem[0],zero,zero,zero
	vmovss	12(%rdx), %xmm2                 ## xmm2 = mem[0],zero,zero,zero
	vmovss	16(%rdx), %xmm0                 ## xmm0 = mem[0],zero,zero,zero
	movslq	20(%rdx), %r13
	movslq	24(%rdx), %r11
	movslq	28(%rdx), %r14
	movq	32(%rdx), %r8
	movq	48(%rdx), %rdx
	cmpl	$59, %esi
	jg	LBB50_2
## %bb.1:                               ## %then_bb
	movl	%esi, %ecx
	sarl	$31, %ecx
	movl	%ecx, %edi
	xorl	%esi, %edi
	imulq	$1717986919, %rdi, %r15         ## imm = 0x66666667
	shrq	$35, %r15
	xorl	%ecx, %r15d
	leal	(%r15,%r15), %ecx
	leal	(,%r15,4), %edi
	leal	(%rdi,%rdi,4), %edi
	subl	%edi, %esi
	addl	%esi, %esi
	movslq	%esi, %rbx
	vmulss	%xmm2, %xmm1, %xmm1
	vbroadcastss	%xmm1, %ymm1
	vbroadcastss	%xmm0, %ymm0
	movl	%ecx, %edi
	imull	%r11d, %edi
	imull	%r10d, %ecx
	movslq	%ecx, %r12
	movslq	%edi, %rcx
	movq	%rbx, %rsi
	imulq	%r13, %rsi
	leaq	(%rsi,%rcx), %rax
	movq	%rbx, %rdi
	imulq	%r9, %rdi
	leaq	(%rax,%r14), %rbp
	vmulps	(%rdx,%rbp,4), %ymm1, %ymm3
	leaq	(%rax,%r14,2), %rbp
	vmulps	(%rdx,%rbp,4), %ymm0, %ymm2
	leaq	(%rdi,%r12), %rbp
	vsqrtps	%ymm2, %ymm4
	vbroadcastss	LCPI50_1(%rip), %ymm2   ## ymm2 = [9.99999974E-6,9.99999974E-6,9.99999974E-6,9.99999974E-6,9.99999974E-6,9.99999974E-6,9.99999974E-6,9.99999974E-6]
	vaddps	%ymm2, %ymm4, %ymm4
	vdivps	%ymm4, %ymm3, %ymm3
	vmovups	(%r8,%rbp,4), %ymm4
	vsubps	%ymm3, %ymm4, %ymm3
	vmovups	%ymm3, (%rdx,%rax,4)
	orq	$1, %rbx
	imulq	%rbx, %r13
	addq	%r13, %rcx
	imulq	%r9, %rbx
	addq	%rbx, %r12
	leaq	(%rcx,%r14), %rax
	vmulps	(%rdx,%rax,4), %ymm1, %ymm3
	vmovups	(%r8,%r12,4), %ymm4
	leaq	(%rcx,%r14,2), %rax
	vmulps	(%rdx,%rax,4), %ymm0, %ymm5
	vsqrtps	%ymm5, %ymm5
	vaddps	%ymm2, %ymm5, %ymm5
	vdivps	%ymm5, %ymm3, %ymm3
	vsubps	%ymm3, %ymm4, %ymm3
	vmovups	%ymm3, (%rdx,%rcx,4)
	leal	(%r15,%r15), %eax
	incl	%eax
	imull	%eax, %r11d
	imull	%r10d, %eax
	cltq
	movslq	%r11d, %rcx
	addq	%rcx, %rsi
	addq	%rax, %rdi
	leaq	(%rsi,%r14), %rbp
	vmulps	(%rdx,%rbp,4), %ymm1, %ymm3
	vmovups	(%r8,%rdi,4), %ymm4
	leaq	(%rsi,%r14,2), %rdi
	vmulps	(%rdx,%rdi,4), %ymm0, %ymm5
	vsqrtps	%ymm5, %ymm5
	vaddps	%ymm2, %ymm5, %ymm2
	vdivps	%ymm2, %ymm3, %ymm2
	vsubps	%ymm2, %ymm4, %ymm2
	vmovups	%ymm2, (%rdx,%rsi,4)
	addq	%rcx, %r13
	addq	%rax, %rbx
	jmp	LBB50_3
LBB50_2:                                ## %next_bb
	movl	%esi, %eax
	imulq	$1717986919, %rax, %rax         ## imm = 0x66666667
	shrq	$35, %rax
	shll	$2, %eax
	leal	(%rax,%rax,4), %eax
	subl	%eax, %esi
	addl	%esi, %esi
	addq	%r11, %r11
	leaq	(%r11,%r11,2), %rax
	addq	%r10, %r10
	leaq	(%r10,%r10,2), %rcx
	movslq	%esi, %rbx
	vmulss	%xmm2, %xmm1, %xmm1
	vbroadcastss	%xmm1, %ymm1
	movq	%rbx, %rdi
	imulq	%r13, %rdi
	addq	%rax, %rdi
	leaq	(%rdi,%r14), %rbp
	vmulps	(%rdx,%rbp,4), %ymm1, %ymm2
	vbroadcastss	%xmm0, %ymm0
	movq	%rbx, %rsi
	leaq	(%rdi,%r14,2), %rbp
	vmulps	(%rdx,%rbp,4), %ymm0, %ymm3
	vsqrtps	%ymm3, %ymm3
	vbroadcastss	LCPI50_0(%rip), %ymm4   ## ymm4 = [-9.99999974E-6,-9.99999974E-6,-9.99999974E-6,-9.99999974E-6,-9.99999974E-6,-9.99999974E-6,-9.99999974E-6,-9.99999974E-6]
	vsubps	%ymm3, %ymm4, %ymm3
	vdivps	%ymm3, %ymm2, %ymm2
	imulq	%r9, %rsi
	addq	%rcx, %rsi
	vaddps	(%r8,%rsi,4), %ymm2, %ymm2
	vmovups	%ymm2, (%rdx,%rdi,4)
	orq	$1, %rbx
	imulq	%rbx, %r13
	addq	%rax, %r13
	imulq	%r9, %rbx
	addq	%rcx, %rbx
LBB50_3:                                ## %destructor_block
	leaq	(%r14,%r13), %rax
	vmulps	(%rdx,%rax,4), %ymm1, %ymm1
	leaq	(,%r14,2), %rcx
	addq	%r13, %rcx
	vmulps	(%rdx,%rcx,4), %ymm0, %ymm0
	vsqrtps	%ymm0, %ymm0
	vbroadcastss	LCPI50_0(%rip), %ymm2   ## ymm2 = [-9.99999974E-6,-9.99999974E-6,-9.99999974E-6,-9.99999974E-6,-9.99999974E-6,-9.99999974E-6,-9.99999974E-6,-9.99999974E-6]
	vsubps	%ymm0, %ymm2, %ymm0
	vdivps	%ymm0, %ymm1, %ymm0
	vaddps	(%r8,%rbx,4), %ymm0, %ymm0
	vmovups	%ymm0, (%rdx,%r13,4)
	xorl	%eax, %eax
	popq	%rbx
	popq	%r12
	popq	%r13
	popq	%r14
	popq	%r15
	popq	%rbp
	vzeroupper
	retq
                                        ## -- End function
	.p2align	4, 0x90                         ## -- Begin function train_cost_model.par_for.head2_conv_1_d_def__.s0.n.n
_train_cost_model.par_for.head2_conv_1_d_def__.s0.n.n: ## @train_cost_model.par_for.head2_conv_1_d_def__.s0.n.n
## %bb.0:                               ## %entry
	pushq	%rbp
	movq	%rsp, %rbp
	pushq	%r15
	pushq	%r14
	pushq	%r13
	pushq	%r12
	pushq	%rbx
	andq	$-32, %rsp
	subq	$448, %rsp                      ## imm = 0x1C0
                                        ## kill: def $esi killed $esi def $rsi
	movl	(%rdx), %r8d
	movslq	4(%rdx), %r14
	movslq	8(%rdx), %rbx
	movslq	12(%rdx), %r10
	movl	24(%rdx), %r15d
	movq	32(%rdx), %r9
	movq	48(%rdx), %rdi
	movq	%rdi, 152(%rsp)                 ## 8-byte Spill
	movq	64(%rdx), %rdi
	movq	%rdi, 200(%rsp)                 ## 8-byte Spill
	movq	80(%rdx), %rax
	movq	%rax, 80(%rsp)                  ## 8-byte Spill
	leal	(%rsi,%rsi,4), %edi
	movslq	%edi, %rax
	cmpl	%esi, 16(%rdx)
	movl	%r8d, 12(%rsp)                  ## 4-byte Spill
	movq	%rbx, 128(%rsp)                 ## 8-byte Spill
	movq	%r9, 136(%rsp)                  ## 8-byte Spill
	jle	LBB51_8
## %bb.1:                               ## %then_bb
	movl	20(%rdx), %edx
	movq	%rdx, 40(%rsp)                  ## 8-byte Spill
	testl	%edx, %edx
	jle	LBB51_58
## %bb.2:                               ## %"for head2_conv_1_d_def__.s0.w.preheader"
	movl	%r8d, %edi
	movq	%rax, %rdx
	subl	%edx, %edi
	subl	%r15d, %eax
	movl	%eax, 168(%rsp)                 ## 4-byte Spill
	cmpl	$5, %edi
	movl	$5, %r12d
	movl	%edi, 176(%rsp)                 ## 4-byte Spill
	cmovll	%edi, %r12d
	leal	(,%rbx,8), %eax
	movl	%eax, 160(%rsp)                 ## 4-byte Spill
	leaq	(%r10,%r10,2), %rax
	leal	-1(%r12), %ecx
	shlq	$5, %rcx
	addq	$32, %rcx
	movq	%rcx, 16(%rsp)                  ## 8-byte Spill
	movq	%rax, %rcx
	imulq	%rdx, %rcx
	movq	%rcx, 24(%rsp)                  ## 8-byte Spill
	leaq	1(%rdx), %rcx
	imulq	%rax, %rcx
	movq	%rcx, 72(%rsp)                  ## 8-byte Spill
	leaq	2(%rdx), %rcx
	imulq	%rax, %rcx
	movq	%rcx, 64(%rsp)                  ## 8-byte Spill
	leaq	3(%rdx), %rcx
	imulq	%rax, %rcx
	movq	%rcx, 56(%rsp)                  ## 8-byte Spill
	addq	$4, %rdx
	imulq	%rax, %rdx
	movq	%rdx, 184(%rsp)                 ## 8-byte Spill
	leaq	-1(%r12), %rax
	movq	%rax, 104(%rsp)                 ## 8-byte Spill
	movl	%r12d, %eax
	andl	$3, %eax
	andl	$-4, %r12d
	leal	(%rsi,%rsi,4), %ecx
	movl	%ecx, 88(%rsp)                  ## 4-byte Spill
	shll	$5, %r8d
	movl	%r8d, 8(%rsp)                   ## 4-byte Spill
	leaq	12(%r9), %rcx
	movq	%rcx, 96(%rsp)                  ## 8-byte Spill
	movq	%rax, 144(%rsp)                 ## 8-byte Spill
	leaq	(,%rax,4), %r13
	xorl	%ecx, %ecx
	jmp	LBB51_3
	.p2align	4, 0x90
LBB51_21:                               ## %"end for head2_conv_1_d_def__.s0.c.c"
                                        ##   in Loop: Header=BB51_3 Depth=1
	movq	48(%rsp), %rcx                  ## 8-byte Reload
	incq	%rcx
	movl	8(%rsp), %eax                   ## 4-byte Reload
	addl	%eax, 88(%rsp)                  ## 4-byte Folded Spill
	cmpq	40(%rsp), %rcx                  ## 8-byte Folded Reload
	je	LBB51_58
LBB51_3:                                ## %"for head2_conv_1_d_def__.s0.w"
                                        ## =>This Loop Header: Depth=1
                                        ##     Child Loop BB51_4 Depth 2
                                        ##       Child Loop BB51_6 Depth 3
                                        ##         Child Loop BB51_15 Depth 4
                                        ##         Child Loop BB51_18 Depth 4
	movq	%rcx, 48(%rsp)                  ## 8-byte Spill
	leal	(%rcx,%rcx,2), %eax
	movq	%rax, 120(%rsp)                 ## 8-byte Spill
	xorl	%eax, %eax
	movq	%rax, 32(%rsp)                  ## 8-byte Spill
	jmp	LBB51_4
	.p2align	4, 0x90
LBB51_20:                               ## %"consume head2_relu_0_d_def__"
                                        ##   in Loop: Header=BB51_4 Depth=2
	movq	120(%rsp), %rdx                 ## 8-byte Reload
	movq	32(%rsp), %r15                  ## 8-byte Reload
	leal	(%r15,%rdx), %eax
	imull	160(%rsp), %eax                 ## 4-byte Folded Reload
	addl	168(%rsp), %eax                 ## 4-byte Folded Reload
	movslq	%eax, %r8
	movq	200(%rsp), %r11                 ## 8-byte Reload
	leaq	(%r11,%r8,4), %r9
	movq	128(%rsp), %rcx                 ## 8-byte Reload
	leaq	(%r9,%rcx,4), %rsi
	leaq	(%rsi,%rcx,4), %rdi
	leaq	(%rdi,%rcx,4), %rax
	leaq	(%rax,%rcx,4), %rbx
	vmovss	(%rax,%rcx,4), %xmm0            ## xmm0 = mem[0],zero,zero,zero
	vinsertps	$16, (%rbx,%rcx,4), %xmm0, %xmm0 ## xmm0 = xmm0[0],mem[0],xmm0[2,3]
	leaq	(%r15,%rdx), %rax
	leaq	(%rbx,%rcx,4), %rbx
	vinsertps	$32, (%rbx,%rcx,4), %xmm0, %xmm0 ## xmm0 = xmm0[0,1],mem[0],xmm0[3]
	leaq	(%rbx,%rcx,4), %rbx
	vinsertps	$48, (%rbx,%rcx,4), %xmm0, %xmm0 ## xmm0 = xmm0[0,1,2],mem[0]
	vmovss	(%r11,%r8,4), %xmm1             ## xmm1 = mem[0],zero,zero,zero
	vinsertps	$16, (%r9,%rcx,4), %xmm1, %xmm1 ## xmm1 = xmm1[0],mem[0],xmm1[2,3]
	vinsertps	$32, (%rsi,%rcx,4), %xmm1, %xmm1 ## xmm1 = xmm1[0,1],mem[0],xmm1[3]
	vinsertps	$48, (%rdi,%rcx,4), %xmm1, %xmm1 ## xmm1 = xmm1[0,1,2],mem[0]
	vinsertf128	$1, %xmm0, %ymm1, %ymm0
	vxorps	%xmm2, %xmm2, %xmm2
	vcmpleps	%ymm0, %ymm2, %ymm0
	vandps	256(%rsp), %ymm0, %ymm0
	movq	24(%rsp), %rdi                  ## 8-byte Reload
	addq	%rax, %rdi
	shlq	$5, %rdi
	movq	80(%rsp), %r10                  ## 8-byte Reload
	vmovaps	%ymm0, (%r10,%rdi)
	leaq	(%rsi,%rcx,4), %rdi
	addq	$4, %rdi
	leaq	(%rdi,%rcx,4), %rbx
	vmovss	(%rbx,%rcx,4), %xmm0            ## xmm0 = mem[0],zero,zero,zero
	leaq	(%rbx,%rcx,4), %rbx
	vinsertps	$16, (%rbx,%rcx,4), %xmm0, %xmm0 ## xmm0 = xmm0[0],mem[0],xmm0[2,3]
	leaq	(%rbx,%rcx,4), %rbx
	vinsertps	$32, (%rbx,%rcx,4), %xmm0, %xmm0 ## xmm0 = xmm0[0,1],mem[0],xmm0[3]
	leaq	(%rbx,%rcx,4), %rbx
	vinsertps	$48, (%rbx,%rcx,4), %xmm0, %xmm0 ## xmm0 = xmm0[0,1,2],mem[0]
	vmovss	4(%r11,%r8,4), %xmm1            ## xmm1 = mem[0],zero,zero,zero
	vinsertps	$16, 4(%r9,%rcx,4), %xmm1, %xmm1 ## xmm1 = xmm1[0],mem[0],xmm1[2,3]
	vinsertps	$32, 4(%rsi,%rcx,4), %xmm1, %xmm1 ## xmm1 = xmm1[0,1],mem[0],xmm1[3]
	vinsertps	$48, (%rdi,%rcx,4), %xmm1, %xmm1 ## xmm1 = xmm1[0,1,2],mem[0]
	vinsertf128	$1, %xmm0, %ymm1, %ymm0
	vcmpleps	%ymm0, %ymm2, %ymm0
	vandps	288(%rsp), %ymm0, %ymm0
	movq	72(%rsp), %rdi                  ## 8-byte Reload
	addq	%rax, %rdi
	shlq	$5, %rdi
	vmovaps	%ymm0, (%r10,%rdi)
	leaq	(%rsi,%rcx,4), %rdi
	addq	$8, %rdi
	leaq	(%rdi,%rcx,4), %rbx
	vmovss	(%rbx,%rcx,4), %xmm0            ## xmm0 = mem[0],zero,zero,zero
	leaq	(%rbx,%rcx,4), %rbx
	vinsertps	$16, (%rbx,%rcx,4), %xmm0, %xmm0 ## xmm0 = xmm0[0],mem[0],xmm0[2,3]
	leaq	(%rbx,%rcx,4), %rbx
	vinsertps	$32, (%rbx,%rcx,4), %xmm0, %xmm0 ## xmm0 = xmm0[0,1],mem[0],xmm0[3]
	leaq	(%rbx,%rcx,4), %rbx
	vinsertps	$48, (%rbx,%rcx,4), %xmm0, %xmm0 ## xmm0 = xmm0[0,1,2],mem[0]
	vmovss	8(%r11,%r8,4), %xmm1            ## xmm1 = mem[0],zero,zero,zero
	vinsertps	$16, 8(%r9,%rcx,4), %xmm1, %xmm1 ## xmm1 = xmm1[0],mem[0],xmm1[2,3]
	vinsertps	$32, 8(%rsi,%rcx,4), %xmm1, %xmm1 ## xmm1 = xmm1[0,1],mem[0],xmm1[3]
	vinsertps	$48, (%rdi,%rcx,4), %xmm1, %xmm1 ## xmm1 = xmm1[0,1,2],mem[0]
	vinsertf128	$1, %xmm0, %ymm1, %ymm0
	vcmpleps	%ymm0, %ymm2, %ymm0
	vandps	320(%rsp), %ymm0, %ymm0
	movq	64(%rsp), %rdi                  ## 8-byte Reload
	addq	%rax, %rdi
	shlq	$5, %rdi
	vmovaps	%ymm0, (%r10,%rdi)
	leaq	(%rsi,%rcx,4), %rdi
	addq	$12, %rdi
	leaq	(%rdi,%rcx,4), %rbx
	vmovss	(%rbx,%rcx,4), %xmm0            ## xmm0 = mem[0],zero,zero,zero
	leaq	(%rbx,%rcx,4), %rbx
	vinsertps	$16, (%rbx,%rcx,4), %xmm0, %xmm0 ## xmm0 = xmm0[0],mem[0],xmm0[2,3]
	leaq	(%rbx,%rcx,4), %rbx
	vinsertps	$32, (%rbx,%rcx,4), %xmm0, %xmm0 ## xmm0 = xmm0[0,1],mem[0],xmm0[3]
	leaq	(%rbx,%rcx,4), %rbx
	vinsertps	$48, (%rbx,%rcx,4), %xmm0, %xmm0 ## xmm0 = xmm0[0,1,2],mem[0]
	vmovss	12(%r11,%r8,4), %xmm1           ## xmm1 = mem[0],zero,zero,zero
	vinsertps	$16, 12(%r9,%rcx,4), %xmm1, %xmm1 ## xmm1 = xmm1[0],mem[0],xmm1[2,3]
	vinsertps	$32, 12(%rsi,%rcx,4), %xmm1, %xmm1 ## xmm1 = xmm1[0,1],mem[0],xmm1[3]
	vinsertps	$48, (%rdi,%rcx,4), %xmm1, %xmm1 ## xmm1 = xmm1[0,1,2],mem[0]
	vinsertf128	$1, %xmm0, %ymm1, %ymm0
	vcmpleps	%ymm0, %ymm2, %ymm0
	vandps	352(%rsp), %ymm0, %ymm0
	movq	56(%rsp), %rdi                  ## 8-byte Reload
	addq	%rax, %rdi
	shlq	$5, %rdi
	vmovaps	%ymm0, (%r10,%rdi)
	leaq	(%rsi,%rcx,4), %rdi
	addq	$16, %rdi
	leaq	(%rdi,%rcx,4), %rbx
	vmovss	(%rbx,%rcx,4), %xmm0            ## xmm0 = mem[0],zero,zero,zero
	leaq	(%rbx,%rcx,4), %rbx
	vinsertps	$16, (%rbx,%rcx,4), %xmm0, %xmm0 ## xmm0 = xmm0[0],mem[0],xmm0[2,3]
	leaq	(%rbx,%rcx,4), %rbx
	vinsertps	$32, (%rbx,%rcx,4), %xmm0, %xmm0 ## xmm0 = xmm0[0,1],mem[0],xmm0[3]
	leaq	(%rbx,%rcx,4), %rbx
	vinsertps	$48, (%rbx,%rcx,4), %xmm0, %xmm0 ## xmm0 = xmm0[0,1,2],mem[0]
	vmovss	16(%r11,%r8,4), %xmm1           ## xmm1 = mem[0],zero,zero,zero
	vinsertps	$16, 16(%r9,%rcx,4), %xmm1, %xmm1 ## xmm1 = xmm1[0],mem[0],xmm1[2,3]
	vinsertps	$32, 16(%rsi,%rcx,4), %xmm1, %xmm1 ## xmm1 = xmm1[0,1],mem[0],xmm1[3]
	vinsertps	$48, (%rdi,%rcx,4), %xmm1, %xmm1 ## xmm1 = xmm1[0,1,2],mem[0]
	vinsertf128	$1, %xmm0, %ymm1, %ymm0
	vcmpleps	%ymm0, %ymm2, %ymm0
	vandps	384(%rsp), %ymm0, %ymm0
	addq	184(%rsp), %rax                 ## 8-byte Folded Reload
	shlq	$5, %rax
	vmovaps	%ymm0, (%r10,%rax)
	incq	%r15
	movq	%r15, %rax
	movq	%r15, 32(%rsp)                  ## 8-byte Spill
	cmpq	$3, %r15
	je	LBB51_21
LBB51_4:                                ## %"for head2_conv_1_d_def__.s0.c.c"
                                        ##   Parent Loop BB51_3 Depth=1
                                        ## =>  This Loop Header: Depth=2
                                        ##       Child Loop BB51_6 Depth 3
                                        ##         Child Loop BB51_15 Depth 4
                                        ##         Child Loop BB51_18 Depth 4
	cmpl	$0, 176(%rsp)                   ## 4-byte Folded Reload
	jle	LBB51_20
## %bb.5:                               ## %"for head2_relu_0_d_def__.s1.r940$x.us.preheader"
                                        ##   in Loop: Header=BB51_4 Depth=2
	leaq	256(%rsp), %rdi
	xorl	%esi, %esi
	movq	16(%rsp), %rdx                  ## 8-byte Reload
	vzeroupper
	callq	_memset
	movq	136(%rsp), %r11                 ## 8-byte Reload
	movl	12(%rsp), %r10d                 ## 4-byte Reload
	movq	32(%rsp), %rax                  ## 8-byte Reload
	leaq	8(,%rax,8), %r8
	imulq	%r14, %r8
	movl	88(%rsp), %r9d                  ## 4-byte Reload
	xorl	%edx, %edx
	jmp	LBB51_6
	.p2align	4, 0x90
LBB51_19:                               ## %"end for head2_relu_0_d_def__.s1.n.rebased.loopexit.us"
                                        ##   in Loop: Header=BB51_6 Depth=3
	incq	%rdx
	addl	%r10d, %r9d
	cmpq	$32, %rdx
	je	LBB51_20
LBB51_6:                                ## %"for head2_relu_0_d_def__.s1.r940$x.us"
                                        ##   Parent Loop BB51_3 Depth=1
                                        ##     Parent Loop BB51_4 Depth=2
                                        ## =>    This Loop Header: Depth=3
                                        ##         Child Loop BB51_15 Depth 4
                                        ##         Child Loop BB51_18 Depth 4
	leaq	(%rdx,%r8), %rax
	movq	152(%rsp), %r15                 ## 8-byte Reload
	leaq	(%r15,%rax,4), %rsi
	leaq	(%rsi,%r14,4), %rdi
	leaq	(%rdi,%r14,4), %rbx
	leaq	(%rbx,%r14,4), %rcx
	vmovss	(%rcx,%r14,4), %xmm0            ## xmm0 = mem[0],zero,zero,zero
	leaq	(%rcx,%r14,4), %rcx
	vinsertps	$16, (%rcx,%r14,4), %xmm0, %xmm0 ## xmm0 = xmm0[0],mem[0],xmm0[2,3]
	leaq	(%rcx,%r14,4), %rcx
	vinsertps	$32, (%rcx,%r14,4), %xmm0, %xmm0 ## xmm0 = xmm0[0,1],mem[0],xmm0[3]
	leaq	(%rcx,%r14,4), %rcx
	vinsertps	$48, (%rcx,%r14,4), %xmm0, %xmm0 ## xmm0 = xmm0[0,1,2],mem[0]
	vmovss	(%r15,%rax,4), %xmm1            ## xmm1 = mem[0],zero,zero,zero
	vinsertps	$16, (%rsi,%r14,4), %xmm1, %xmm1 ## xmm1 = xmm1[0],mem[0],xmm1[2,3]
	vinsertps	$32, (%rdi,%r14,4), %xmm1, %xmm1 ## xmm1 = xmm1[0,1],mem[0],xmm1[3]
	vinsertps	$48, (%rbx,%r14,4), %xmm1, %xmm1 ## xmm1 = xmm1[0,1,2],mem[0]
	movslq	%r9d, %rsi
	vinsertf128	$1, %xmm0, %ymm1, %ymm0
	cmpq	$3, 104(%rsp)                   ## 8-byte Folded Reload
	jae	LBB51_14
## %bb.7:                               ##   in Loop: Header=BB51_6 Depth=3
	xorl	%edi, %edi
	jmp	LBB51_16
	.p2align	4, 0x90
LBB51_14:                               ## %"for head2_relu_0_d_def__.s1.n.rebased.us.preheader"
                                        ##   in Loop: Header=BB51_6 Depth=3
	movq	96(%rsp), %rax                  ## 8-byte Reload
	leaq	(%rax,%rsi,4), %rax
	leaq	352(%rsp), %rbx
	xorl	%edi, %edi
	.p2align	4, 0x90
LBB51_15:                               ## %"for head2_relu_0_d_def__.s1.n.rebased.us"
                                        ##   Parent Loop BB51_3 Depth=1
                                        ##     Parent Loop BB51_4 Depth=2
                                        ##       Parent Loop BB51_6 Depth=3
                                        ## =>      This Inner Loop Header: Depth=4
	vbroadcastss	-12(%rax,%rdi,4), %ymm1
	vfmadd213ps	-96(%rbx), %ymm0, %ymm1 ## ymm1 = (ymm0 * ymm1) + mem
	vmovaps	%ymm1, -96(%rbx)
	vbroadcastss	-8(%rax,%rdi,4), %ymm1
	vfmadd213ps	-64(%rbx), %ymm0, %ymm1 ## ymm1 = (ymm0 * ymm1) + mem
	vmovaps	%ymm1, -64(%rbx)
	vbroadcastss	-4(%rax,%rdi,4), %ymm1
	vfmadd213ps	-32(%rbx), %ymm0, %ymm1 ## ymm1 = (ymm0 * ymm1) + mem
	vmovaps	%ymm1, -32(%rbx)
	vbroadcastss	(%rax,%rdi,4), %ymm1
	vfmadd213ps	(%rbx), %ymm0, %ymm1    ## ymm1 = (ymm0 * ymm1) + mem
	vmovaps	%ymm1, (%rbx)
	addq	$4, %rdi
	subq	$-128, %rbx
	cmpq	%rdi, %r12
	jne	LBB51_15
LBB51_16:                               ## %"end for head2_relu_0_d_def__.s1.n.rebased.loopexit.us.unr-lcssa"
                                        ##   in Loop: Header=BB51_6 Depth=3
	cmpq	$0, 144(%rsp)                   ## 8-byte Folded Reload
	je	LBB51_19
## %bb.17:                              ## %"for head2_relu_0_d_def__.s1.n.rebased.us.epil.preheader"
                                        ##   in Loop: Header=BB51_6 Depth=3
	addq	%rdi, %rsi
	leaq	(%r11,%rsi,4), %rax
	shlq	$5, %rdi
	leaq	(%rsp,%rdi), %rsi
	addq	$256, %rsi                      ## imm = 0x100
	xorl	%edi, %edi
	.p2align	4, 0x90
LBB51_18:                               ## %"for head2_relu_0_d_def__.s1.n.rebased.us.epil"
                                        ##   Parent Loop BB51_3 Depth=1
                                        ##     Parent Loop BB51_4 Depth=2
                                        ##       Parent Loop BB51_6 Depth=3
                                        ## =>      This Inner Loop Header: Depth=4
	vbroadcastss	(%rax,%rdi), %ymm1
	vfmadd213ps	(%rsi,%rdi,8), %ymm0, %ymm1 ## ymm1 = (ymm0 * ymm1) + mem
	vmovaps	%ymm1, (%rsi,%rdi,8)
	addq	$4, %rdi
	cmpq	%rdi, %r13
	jne	LBB51_18
	jmp	LBB51_19
LBB51_8:                                ## %next_bb
	testl	%r10d, %r10d
	jle	LBB51_58
## %bb.9:                               ## %"for head2_conv_1_d_def__.s0.w1.preheader"
	movl	%r8d, %ecx
	subl	%eax, %ecx
	cmpl	$5, %ecx
	movl	$5, %edx
	cmovll	%ecx, %edx
	xorl	%edi, %edi
	movq	%rdi, 72(%rsp)                  ## 8-byte Spill
	movl	%ecx, 16(%rsp)                  ## 4-byte Spill
	testl	%ecx, %ecx
	movl	$0, %r11d
	cmovgl	%edx, %r11d
	movl	%r10d, %r10d
	leal	(%rsi,%rsi,4), %esi
	imulq	%r10, %rax
	leaq	(%rax,%rax,2), %rcx
	shlq	$5, %rcx
	movq	80(%rsp), %rdi                  ## 8-byte Reload
	leaq	(%rdi,%rcx), %rax
	movq	%rax, 224(%rsp)                 ## 8-byte Spill
	movl	%esi, %eax
	subl	%r15d, %eax
	movl	%eax, 8(%rsp)                   ## 4-byte Spill
	leaq	(%rdi,%rcx), %rax
	addq	$32, %rax
	movq	%rax, 216(%rsp)                 ## 8-byte Spill
	leal	(%rsi,%rbx,8), %eax
	subl	%r15d, %eax
	movl	%eax, 116(%rsp)                 ## 4-byte Spill
	leaq	(%rdi,%rcx), %rax
	addq	$64, %rax
	movq	%rax, 208(%rsp)                 ## 8-byte Spill
	movl	%ebx, %eax
	shll	$4, %eax
	movq	%rsi, 24(%rsp)                  ## 8-byte Spill
	addl	%esi, %eax
	subl	%r15d, %eax
	movl	%eax, 112(%rsp)                 ## 4-byte Spill
	leal	-1(%rdx), %eax
	shlq	$5, %rax
	addq	$32, %rax
	movq	%rax, 56(%rsp)                  ## 8-byte Spill
	movq	%r14, %rax
	shlq	$4, %rax
	movq	%rax, 184(%rsp)                 ## 8-byte Spill
	movl	%edx, %ecx
	andl	$3, %ecx
	movl	%edx, %r13d
	andl	$-4, %r13d
	shll	$5, %r8d
	movl	%r8d, 196(%rsp)                 ## 4-byte Spill
	movq	%r10, 232(%rsp)                 ## 8-byte Spill
	movq	%r10, %rax
	shlq	$5, %rax
	leaq	(%rax,%rax,2), %rsi
	leal	(,%rbx,8), %eax
	leal	(%rax,%rax,2), %eax
	movl	%eax, 192(%rsp)                 ## 4-byte Spill
	leaq	(,%r14,8), %rax
	movq	%rax, 168(%rsp)                 ## 8-byte Spill
	leaq	(%rax,%rax,2), %rax
	movq	%rax, 160(%rsp)                 ## 8-byte Spill
	movq	%rdx, 144(%rsp)                 ## 8-byte Spill
	leaq	-1(%rdx), %rax
	movq	%rax, 104(%rsp)                 ## 8-byte Spill
	leaq	12(%r9), %rax
	movq	%rax, 96(%rsp)                  ## 8-byte Spill
	movq	%rsi, %r9
	movq	%rcx, 176(%rsp)                 ## 8-byte Spill
	leaq	(,%rcx,4), %r12
	leaq	(,%rbx,4), %rax
	movq	%rax, 40(%rsp)                  ## 8-byte Spill
	xorl	%ecx, %ecx
	movq	%r11, 64(%rsp)                  ## 8-byte Spill
	movq	%rsi, 48(%rsp)                  ## 8-byte Spill
	.p2align	4, 0x90
LBB51_10:                               ## %"for head2_conv_1_d_def__.s0.w1"
                                        ## =>This Loop Header: Depth=1
                                        ##     Child Loop BB51_12 Depth 2
                                        ##       Child Loop BB51_23 Depth 3
                                        ##       Child Loop BB51_26 Depth 3
                                        ##     Child Loop BB51_30 Depth 2
                                        ##     Child Loop BB51_33 Depth 2
                                        ##       Child Loop BB51_36 Depth 3
                                        ##       Child Loop BB51_39 Depth 3
                                        ##     Child Loop BB51_43 Depth 2
                                        ##     Child Loop BB51_46 Depth 2
                                        ##       Child Loop BB51_49 Depth 3
                                        ##       Child Loop BB51_52 Depth 3
                                        ##     Child Loop BB51_56 Depth 2
	movq	%rcx, 248(%rsp)                 ## 8-byte Spill
	cmpl	$0, 16(%rsp)                    ## 4-byte Folded Reload
	jle	LBB51_57
## %bb.11:                              ## %"for head2_relu_0_d_def__.s1.r940$x11.us.preheader"
                                        ##   in Loop: Header=BB51_10 Depth=1
	movl	72(%rsp), %r15d                 ## 4-byte Reload
	shlq	$5, %r15
	movq	208(%rsp), %rax                 ## 8-byte Reload
	leaq	(%rax,%r15), %rbx
	movq	216(%rsp), %rax                 ## 8-byte Reload
	addq	%r15, %rax
	movq	%rax, 240(%rsp)                 ## 8-byte Spill
	addq	224(%rsp), %r15                 ## 8-byte Folded Reload
	movslq	112(%rsp), %rax                 ## 4-byte Folded Reload
	movq	200(%rsp), %rcx                 ## 8-byte Reload
	leaq	(%rcx,%rax,4), %rax
	movq	%rax, 32(%rsp)                  ## 8-byte Spill
	movslq	116(%rsp), %rax                 ## 4-byte Folded Reload
	leaq	(%rcx,%rax,4), %rax
	movq	%rax, 88(%rsp)                  ## 8-byte Spill
	movslq	8(%rsp), %rax                   ## 4-byte Folded Reload
	leaq	(%rcx,%rax,4), %rax
	movq	%rax, 120(%rsp)                 ## 8-byte Spill
	leaq	256(%rsp), %rdi
	xorl	%esi, %esi
	movq	56(%rsp), %rdx                  ## 8-byte Reload
	vzeroupper
	callq	_memset
	movl	12(%rsp), %r11d                 ## 4-byte Reload
	movq	24(%rsp), %rax                  ## 8-byte Reload
	movl	%eax, %r9d
	xorl	%r10d, %r10d
	movq	%rbx, 80(%rsp)                  ## 8-byte Spill
	jmp	LBB51_12
	.p2align	4, 0x90
LBB51_27:                               ## %"end for head2_relu_0_d_def__.s1.n.rebased15.loopexit.us"
                                        ##   in Loop: Header=BB51_12 Depth=2
	incq	%r10
	addl	%r11d, %r9d
	cmpq	$32, %r10
	je	LBB51_28
LBB51_12:                               ## %"for head2_relu_0_d_def__.s1.r940$x11.us"
                                        ##   Parent Loop BB51_10 Depth=1
                                        ## =>  This Loop Header: Depth=2
                                        ##       Child Loop BB51_23 Depth 3
                                        ##       Child Loop BB51_26 Depth 3
	movq	168(%rsp), %rax                 ## 8-byte Reload
	leaq	(%r10,%rax), %rdx
	movq	152(%rsp), %rbx                 ## 8-byte Reload
	leaq	(%rbx,%rdx,4), %rsi
	leaq	(%rsi,%r14,4), %rdi
	leaq	(%rdi,%r14,4), %rax
	leaq	(%rax,%r14,4), %rcx
	vmovss	(%rcx,%r14,4), %xmm0            ## xmm0 = mem[0],zero,zero,zero
	leaq	(%rcx,%r14,4), %rcx
	vinsertps	$16, (%rcx,%r14,4), %xmm0, %xmm0 ## xmm0 = xmm0[0],mem[0],xmm0[2,3]
	leaq	(%rcx,%r14,4), %rcx
	vinsertps	$32, (%rcx,%r14,4), %xmm0, %xmm0 ## xmm0 = xmm0[0,1],mem[0],xmm0[3]
	leaq	(%rcx,%r14,4), %rcx
	vinsertps	$48, (%rcx,%r14,4), %xmm0, %xmm0 ## xmm0 = xmm0[0,1,2],mem[0]
	vmovss	(%rbx,%rdx,4), %xmm1            ## xmm1 = mem[0],zero,zero,zero
	vinsertps	$16, (%rsi,%r14,4), %xmm1, %xmm1 ## xmm1 = xmm1[0],mem[0],xmm1[2,3]
	vinsertps	$32, (%rdi,%r14,4), %xmm1, %xmm1 ## xmm1 = xmm1[0,1],mem[0],xmm1[3]
	vinsertps	$48, (%rax,%r14,4), %xmm1, %xmm1 ## xmm1 = xmm1[0,1,2],mem[0]
	movslq	%r9d, %r8
	vinsertf128	$1, %xmm0, %ymm1, %ymm0
	cmpq	$3, 104(%rsp)                   ## 8-byte Folded Reload
	jae	LBB51_22
## %bb.13:                              ##   in Loop: Header=BB51_12 Depth=2
	xorl	%esi, %esi
	jmp	LBB51_24
	.p2align	4, 0x90
LBB51_22:                               ## %"for head2_relu_0_d_def__.s1.n.rebased14.us.preheader"
                                        ##   in Loop: Header=BB51_12 Depth=2
	movq	96(%rsp), %rax                  ## 8-byte Reload
	leaq	(%rax,%r8,4), %rdi
	leaq	352(%rsp), %rdx
	xorl	%esi, %esi
	.p2align	4, 0x90
LBB51_23:                               ## %"for head2_relu_0_d_def__.s1.n.rebased14.us"
                                        ##   Parent Loop BB51_10 Depth=1
                                        ##     Parent Loop BB51_12 Depth=2
                                        ## =>    This Inner Loop Header: Depth=3
	vbroadcastss	-12(%rdi,%rsi,4), %ymm1
	vfmadd213ps	-96(%rdx), %ymm0, %ymm1 ## ymm1 = (ymm0 * ymm1) + mem
	vmovaps	%ymm1, -96(%rdx)
	vbroadcastss	-8(%rdi,%rsi,4), %ymm1
	vfmadd213ps	-64(%rdx), %ymm0, %ymm1 ## ymm1 = (ymm0 * ymm1) + mem
	vmovaps	%ymm1, -64(%rdx)
	vbroadcastss	-4(%rdi,%rsi,4), %ymm1
	vfmadd213ps	-32(%rdx), %ymm0, %ymm1 ## ymm1 = (ymm0 * ymm1) + mem
	vmovaps	%ymm1, -32(%rdx)
	vbroadcastss	(%rdi,%rsi,4), %ymm1
	vfmadd213ps	(%rdx), %ymm0, %ymm1    ## ymm1 = (ymm0 * ymm1) + mem
	vmovaps	%ymm1, (%rdx)
	addq	$4, %rsi
	subq	$-128, %rdx
	cmpq	%rsi, %r13
	jne	LBB51_23
LBB51_24:                               ## %"end for head2_relu_0_d_def__.s1.n.rebased15.loopexit.us.unr-lcssa"
                                        ##   in Loop: Header=BB51_12 Depth=2
	cmpq	$0, 176(%rsp)                   ## 8-byte Folded Reload
	je	LBB51_27
## %bb.25:                              ## %"for head2_relu_0_d_def__.s1.n.rebased14.us.epil.preheader"
                                        ##   in Loop: Header=BB51_12 Depth=2
	addq	%rsi, %r8
	movq	136(%rsp), %rax                 ## 8-byte Reload
	leaq	(%rax,%r8,4), %rdx
	shlq	$5, %rsi
	addq	%rsp, %rsi
	addq	$256, %rsi                      ## imm = 0x100
	xorl	%edi, %edi
	.p2align	4, 0x90
LBB51_26:                               ## %"for head2_relu_0_d_def__.s1.n.rebased14.us.epil"
                                        ##   Parent Loop BB51_10 Depth=1
                                        ##     Parent Loop BB51_12 Depth=2
                                        ## =>    This Inner Loop Header: Depth=3
	vbroadcastss	(%rdx,%rdi), %ymm1
	vfmadd213ps	(%rsi,%rdi,8), %ymm0, %ymm1 ## ymm1 = (ymm0 * ymm1) + mem
	vmovaps	%ymm1, (%rsi,%rdi,8)
	addq	$4, %rdi
	cmpq	%rdi, %r12
	jne	LBB51_26
	jmp	LBB51_27
	.p2align	4, 0x90
LBB51_28:                               ## %"consume head2_relu_0_d_def__17"
                                        ##   in Loop: Header=BB51_10 Depth=1
	cmpl	$0, 16(%rsp)                    ## 4-byte Folded Reload
	movq	128(%rsp), %rdx                 ## 8-byte Reload
	movq	64(%rsp), %r11                  ## 8-byte Reload
	movq	48(%rsp), %r9                   ## 8-byte Reload
	vxorps	%xmm2, %xmm2, %xmm2
	movq	40(%rsp), %r10                  ## 8-byte Reload
	jle	LBB51_57
## %bb.29:                              ## %"for head2_conv_1_d_def__.s0.n.ni18.preheader"
                                        ##   in Loop: Header=BB51_10 Depth=1
	leaq	256(%rsp), %rax
	xorl	%ecx, %ecx
	movq	120(%rsp), %rdi                 ## 8-byte Reload
	.p2align	4, 0x90
LBB51_30:                               ## %"for head2_conv_1_d_def__.s0.n.ni18"
                                        ##   Parent Loop BB51_10 Depth=1
                                        ## =>  This Inner Loop Header: Depth=2
	leaq	(%rdi,%rcx,4), %r8
	leaq	(%r8,%r10), %rsi
	addq	%r10, %rsi
	leaq	(%rsi,%r10), %rdi
	addq	%r10, %rdi
	leaq	(%rdi,%r10), %rbx
	vmovss	(%rsi,%rdx,8), %xmm0            ## xmm0 = mem[0],zero,zero,zero
	vinsertps	$16, (%rdi,%rdx,4), %xmm0, %xmm0 ## xmm0 = xmm0[0],mem[0],xmm0[2,3]
	movq	120(%rsp), %rdi                 ## 8-byte Reload
	vinsertps	$32, (%rbx,%rdx,4), %xmm0, %xmm0 ## xmm0 = xmm0[0,1],mem[0],xmm0[3]
	addq	%r10, %rbx
	vinsertps	$48, (%rbx,%rdx,4), %xmm0, %xmm0 ## xmm0 = xmm0[0,1,2],mem[0]
	vmovss	(%rdi,%rcx,4), %xmm1            ## xmm1 = mem[0],zero,zero,zero
	vinsertps	$16, (%r8,%rdx,4), %xmm1, %xmm1 ## xmm1 = xmm1[0],mem[0],xmm1[2,3]
	vinsertps	$32, (%r8,%rdx,8), %xmm1, %xmm1 ## xmm1 = xmm1[0,1],mem[0],xmm1[3]
	vinsertps	$48, (%rsi,%rdx,4), %xmm1, %xmm1 ## xmm1 = xmm1[0,1,2],mem[0]
	vinsertf128	$1, %xmm0, %ymm1, %ymm0
	vcmpleps	%ymm0, %ymm2, %ymm0
	vandps	(%rax), %ymm0, %ymm0
	vmovaps	%ymm0, (%r15)
	incq	%rcx
	addq	%r9, %r15
	addq	$32, %rax
	cmpq	%rcx, %r11
	jne	LBB51_30
## %bb.31:                              ## %"end for head2_conv_1_d_def__.s0.n.ni19"
                                        ##   in Loop: Header=BB51_10 Depth=1
	cmpl	$0, 16(%rsp)                    ## 4-byte Folded Reload
	jle	LBB51_57
## %bb.32:                              ## %"for head2_relu_0_d_def__.s1.r940$x11.us.preheader.1"
                                        ##   in Loop: Header=BB51_10 Depth=1
	leaq	256(%rsp), %rdi
	xorl	%esi, %esi
	movq	56(%rsp), %rdx                  ## 8-byte Reload
	vzeroupper
	callq	_memset
	movq	136(%rsp), %r11                 ## 8-byte Reload
	movl	12(%rsp), %r10d                 ## 4-byte Reload
	movq	24(%rsp), %rax                  ## 8-byte Reload
	movl	%eax, %r9d
	xorl	%r15d, %r15d
	jmp	LBB51_33
	.p2align	4, 0x90
LBB51_40:                               ## %"end for head2_relu_0_d_def__.s1.n.rebased15.loopexit.us.1"
                                        ##   in Loop: Header=BB51_33 Depth=2
	incq	%r15
	addl	%r10d, %r9d
	cmpq	$32, %r15
	je	LBB51_41
LBB51_33:                               ## %"for head2_relu_0_d_def__.s1.r940$x11.us.1"
                                        ##   Parent Loop BB51_10 Depth=1
                                        ## =>  This Loop Header: Depth=2
                                        ##       Child Loop BB51_36 Depth 3
                                        ##       Child Loop BB51_39 Depth 3
	movq	184(%rsp), %rax                 ## 8-byte Reload
	leaq	(%r15,%rax), %rdx
	movq	152(%rsp), %rcx                 ## 8-byte Reload
	leaq	(%rcx,%rdx,4), %rsi
	leaq	(%rsi,%r14,4), %rdi
	leaq	(%rdi,%r14,4), %rbx
	leaq	(%rbx,%r14,4), %rax
	vmovss	(%rax,%r14,4), %xmm0            ## xmm0 = mem[0],zero,zero,zero
	leaq	(%rax,%r14,4), %rax
	vinsertps	$16, (%rax,%r14,4), %xmm0, %xmm0 ## xmm0 = xmm0[0],mem[0],xmm0[2,3]
	leaq	(%rax,%r14,4), %rax
	vinsertps	$32, (%rax,%r14,4), %xmm0, %xmm0 ## xmm0 = xmm0[0,1],mem[0],xmm0[3]
	leaq	(%rax,%r14,4), %rax
	vinsertps	$48, (%rax,%r14,4), %xmm0, %xmm0 ## xmm0 = xmm0[0,1,2],mem[0]
	vmovss	(%rcx,%rdx,4), %xmm1            ## xmm1 = mem[0],zero,zero,zero
	vinsertps	$16, (%rsi,%r14,4), %xmm1, %xmm1 ## xmm1 = xmm1[0],mem[0],xmm1[2,3]
	vinsertps	$32, (%rdi,%r14,4), %xmm1, %xmm1 ## xmm1 = xmm1[0,1],mem[0],xmm1[3]
	vinsertps	$48, (%rbx,%r14,4), %xmm1, %xmm1 ## xmm1 = xmm1[0,1,2],mem[0]
	movslq	%r9d, %r8
	vinsertf128	$1, %xmm0, %ymm1, %ymm0
	cmpq	$3, 104(%rsp)                   ## 8-byte Folded Reload
	jae	LBB51_35
## %bb.34:                              ##   in Loop: Header=BB51_33 Depth=2
	xorl	%esi, %esi
	jmp	LBB51_37
	.p2align	4, 0x90
LBB51_35:                               ## %"for head2_relu_0_d_def__.s1.n.rebased14.us.1.preheader"
                                        ##   in Loop: Header=BB51_33 Depth=2
	movq	96(%rsp), %rax                  ## 8-byte Reload
	leaq	(%rax,%r8,4), %rdi
	leaq	352(%rsp), %rdx
	xorl	%esi, %esi
	.p2align	4, 0x90
LBB51_36:                               ## %"for head2_relu_0_d_def__.s1.n.rebased14.us.1"
                                        ##   Parent Loop BB51_10 Depth=1
                                        ##     Parent Loop BB51_33 Depth=2
                                        ## =>    This Inner Loop Header: Depth=3
	vbroadcastss	-12(%rdi,%rsi,4), %ymm1
	vfmadd213ps	-96(%rdx), %ymm0, %ymm1 ## ymm1 = (ymm0 * ymm1) + mem
	vmovaps	%ymm1, -96(%rdx)
	vbroadcastss	-8(%rdi,%rsi,4), %ymm1
	vfmadd213ps	-64(%rdx), %ymm0, %ymm1 ## ymm1 = (ymm0 * ymm1) + mem
	vmovaps	%ymm1, -64(%rdx)
	vbroadcastss	-4(%rdi,%rsi,4), %ymm1
	vfmadd213ps	-32(%rdx), %ymm0, %ymm1 ## ymm1 = (ymm0 * ymm1) + mem
	vmovaps	%ymm1, -32(%rdx)
	vbroadcastss	(%rdi,%rsi,4), %ymm1
	vfmadd213ps	(%rdx), %ymm0, %ymm1    ## ymm1 = (ymm0 * ymm1) + mem
	vmovaps	%ymm1, (%rdx)
	addq	$4, %rsi
	subq	$-128, %rdx
	cmpq	%rsi, %r13
	jne	LBB51_36
LBB51_37:                               ## %"end for head2_relu_0_d_def__.s1.n.rebased15.loopexit.us.1.unr-lcssa"
                                        ##   in Loop: Header=BB51_33 Depth=2
	testb	$3, 144(%rsp)                   ## 1-byte Folded Reload
	je	LBB51_40
## %bb.38:                              ## %"for head2_relu_0_d_def__.s1.n.rebased14.us.1.epil.preheader"
                                        ##   in Loop: Header=BB51_33 Depth=2
	addq	%rsi, %r8
	leaq	(%r11,%r8,4), %rdx
	shlq	$5, %rsi
	addq	%rsp, %rsi
	addq	$256, %rsi                      ## imm = 0x100
	xorl	%edi, %edi
	.p2align	4, 0x90
LBB51_39:                               ## %"for head2_relu_0_d_def__.s1.n.rebased14.us.1.epil"
                                        ##   Parent Loop BB51_10 Depth=1
                                        ##     Parent Loop BB51_33 Depth=2
                                        ## =>    This Inner Loop Header: Depth=3
	vbroadcastss	(%rdx,%rdi), %ymm1
	vfmadd213ps	(%rsi,%rdi,8), %ymm0, %ymm1 ## ymm1 = (ymm0 * ymm1) + mem
	vmovaps	%ymm1, (%rsi,%rdi,8)
	addq	$4, %rdi
	cmpq	%rdi, %r12
	jne	LBB51_39
	jmp	LBB51_40
	.p2align	4, 0x90
LBB51_41:                               ## %"consume head2_relu_0_d_def__17.1"
                                        ##   in Loop: Header=BB51_10 Depth=1
	cmpl	$0, 16(%rsp)                    ## 4-byte Folded Reload
	movq	128(%rsp), %r15                 ## 8-byte Reload
	movq	64(%rsp), %r11                  ## 8-byte Reload
	movq	48(%rsp), %r9                   ## 8-byte Reload
	vxorps	%xmm2, %xmm2, %xmm2
	movq	40(%rsp), %r10                  ## 8-byte Reload
	jle	LBB51_57
## %bb.42:                              ## %"for head2_conv_1_d_def__.s0.n.ni18.preheader.1"
                                        ##   in Loop: Header=BB51_10 Depth=1
	leaq	256(%rsp), %rax
	xorl	%ecx, %ecx
	movq	240(%rsp), %rdx                 ## 8-byte Reload
	movq	88(%rsp), %rdi                  ## 8-byte Reload
	.p2align	4, 0x90
LBB51_43:                               ## %"for head2_conv_1_d_def__.s0.n.ni18.1"
                                        ##   Parent Loop BB51_10 Depth=1
                                        ## =>  This Inner Loop Header: Depth=2
	leaq	(%rdi,%rcx,4), %r8
	leaq	(%r8,%r10), %rsi
	addq	%r10, %rsi
	leaq	(%rsi,%r10), %rdi
	addq	%r10, %rdi
	leaq	(%rdi,%r10), %rbx
	vmovss	(%rsi,%r15,8), %xmm0            ## xmm0 = mem[0],zero,zero,zero
	vinsertps	$16, (%rdi,%r15,4), %xmm0, %xmm0 ## xmm0 = xmm0[0],mem[0],xmm0[2,3]
	movq	88(%rsp), %rdi                  ## 8-byte Reload
	vinsertps	$32, (%rbx,%r15,4), %xmm0, %xmm0 ## xmm0 = xmm0[0,1],mem[0],xmm0[3]
	addq	%r10, %rbx
	vinsertps	$48, (%rbx,%r15,4), %xmm0, %xmm0 ## xmm0 = xmm0[0,1,2],mem[0]
	vmovss	(%rdi,%rcx,4), %xmm1            ## xmm1 = mem[0],zero,zero,zero
	vinsertps	$16, (%r8,%r15,4), %xmm1, %xmm1 ## xmm1 = xmm1[0],mem[0],xmm1[2,3]
	vinsertps	$32, (%r8,%r15,8), %xmm1, %xmm1 ## xmm1 = xmm1[0,1],mem[0],xmm1[3]
	vinsertps	$48, (%rsi,%r15,4), %xmm1, %xmm1 ## xmm1 = xmm1[0,1,2],mem[0]
	vinsertf128	$1, %xmm0, %ymm1, %ymm0
	vcmpleps	%ymm0, %ymm2, %ymm0
	vandps	(%rax), %ymm0, %ymm0
	vmovaps	%ymm0, (%rdx)
	incq	%rcx
	addq	%r9, %rdx
	addq	$32, %rax
	cmpq	%rcx, %r11
	jne	LBB51_43
## %bb.44:                              ## %"end for head2_conv_1_d_def__.s0.n.ni19.1"
                                        ##   in Loop: Header=BB51_10 Depth=1
	cmpl	$0, 16(%rsp)                    ## 4-byte Folded Reload
	jle	LBB51_57
## %bb.45:                              ## %"for head2_relu_0_d_def__.s1.r940$x11.us.preheader.2"
                                        ##   in Loop: Header=BB51_10 Depth=1
	leaq	256(%rsp), %rdi
	xorl	%esi, %esi
	movq	56(%rsp), %rdx                  ## 8-byte Reload
	vzeroupper
	callq	_memset
	movq	136(%rsp), %r11                 ## 8-byte Reload
	movl	12(%rsp), %r10d                 ## 4-byte Reload
	movq	24(%rsp), %rax                  ## 8-byte Reload
	movl	%eax, %r9d
	xorl	%r15d, %r15d
	jmp	LBB51_46
	.p2align	4, 0x90
LBB51_53:                               ## %"end for head2_relu_0_d_def__.s1.n.rebased15.loopexit.us.2"
                                        ##   in Loop: Header=BB51_46 Depth=2
	incq	%r15
	addl	%r10d, %r9d
	cmpq	$32, %r15
	je	LBB51_54
LBB51_46:                               ## %"for head2_relu_0_d_def__.s1.r940$x11.us.2"
                                        ##   Parent Loop BB51_10 Depth=1
                                        ## =>  This Loop Header: Depth=2
                                        ##       Child Loop BB51_49 Depth 3
                                        ##       Child Loop BB51_52 Depth 3
	movq	160(%rsp), %rax                 ## 8-byte Reload
	leaq	(%r15,%rax), %rdx
	movq	152(%rsp), %rcx                 ## 8-byte Reload
	leaq	(%rcx,%rdx,4), %rsi
	leaq	(%rsi,%r14,4), %rdi
	leaq	(%rdi,%r14,4), %rbx
	leaq	(%rbx,%r14,4), %rax
	vmovss	(%rax,%r14,4), %xmm0            ## xmm0 = mem[0],zero,zero,zero
	leaq	(%rax,%r14,4), %rax
	vinsertps	$16, (%rax,%r14,4), %xmm0, %xmm0 ## xmm0 = xmm0[0],mem[0],xmm0[2,3]
	leaq	(%rax,%r14,4), %rax
	vinsertps	$32, (%rax,%r14,4), %xmm0, %xmm0 ## xmm0 = xmm0[0,1],mem[0],xmm0[3]
	leaq	(%rax,%r14,4), %rax
	vinsertps	$48, (%rax,%r14,4), %xmm0, %xmm0 ## xmm0 = xmm0[0,1,2],mem[0]
	vmovss	(%rcx,%rdx,4), %xmm1            ## xmm1 = mem[0],zero,zero,zero
	vinsertps	$16, (%rsi,%r14,4), %xmm1, %xmm1 ## xmm1 = xmm1[0],mem[0],xmm1[2,3]
	vinsertps	$32, (%rdi,%r14,4), %xmm1, %xmm1 ## xmm1 = xmm1[0,1],mem[0],xmm1[3]
	vinsertps	$48, (%rbx,%r14,4), %xmm1, %xmm1 ## xmm1 = xmm1[0,1,2],mem[0]
	movslq	%r9d, %r8
	vinsertf128	$1, %xmm0, %ymm1, %ymm0
	cmpq	$3, 104(%rsp)                   ## 8-byte Folded Reload
	jae	LBB51_48
## %bb.47:                              ##   in Loop: Header=BB51_46 Depth=2
	xorl	%esi, %esi
	movq	80(%rsp), %rbx                  ## 8-byte Reload
	jmp	LBB51_50
	.p2align	4, 0x90
LBB51_48:                               ## %"for head2_relu_0_d_def__.s1.n.rebased14.us.2.preheader"
                                        ##   in Loop: Header=BB51_46 Depth=2
	movq	96(%rsp), %rax                  ## 8-byte Reload
	leaq	(%rax,%r8,4), %rdi
	leaq	352(%rsp), %rdx
	xorl	%esi, %esi
	movq	80(%rsp), %rbx                  ## 8-byte Reload
	.p2align	4, 0x90
LBB51_49:                               ## %"for head2_relu_0_d_def__.s1.n.rebased14.us.2"
                                        ##   Parent Loop BB51_10 Depth=1
                                        ##     Parent Loop BB51_46 Depth=2
                                        ## =>    This Inner Loop Header: Depth=3
	vbroadcastss	-12(%rdi,%rsi,4), %ymm1
	vfmadd213ps	-96(%rdx), %ymm0, %ymm1 ## ymm1 = (ymm0 * ymm1) + mem
	vmovaps	%ymm1, -96(%rdx)
	vbroadcastss	-8(%rdi,%rsi,4), %ymm1
	vfmadd213ps	-64(%rdx), %ymm0, %ymm1 ## ymm1 = (ymm0 * ymm1) + mem
	vmovaps	%ymm1, -64(%rdx)
	vbroadcastss	-4(%rdi,%rsi,4), %ymm1
	vfmadd213ps	-32(%rdx), %ymm0, %ymm1 ## ymm1 = (ymm0 * ymm1) + mem
	vmovaps	%ymm1, -32(%rdx)
	vbroadcastss	(%rdi,%rsi,4), %ymm1
	vfmadd213ps	(%rdx), %ymm0, %ymm1    ## ymm1 = (ymm0 * ymm1) + mem
	vmovaps	%ymm1, (%rdx)
	addq	$4, %rsi
	subq	$-128, %rdx
	cmpq	%rsi, %r13
	jne	LBB51_49
LBB51_50:                               ## %"end for head2_relu_0_d_def__.s1.n.rebased15.loopexit.us.2.unr-lcssa"
                                        ##   in Loop: Header=BB51_46 Depth=2
	testb	$3, 144(%rsp)                   ## 1-byte Folded Reload
	je	LBB51_53
## %bb.51:                              ## %"for head2_relu_0_d_def__.s1.n.rebased14.us.2.epil.preheader"
                                        ##   in Loop: Header=BB51_46 Depth=2
	addq	%rsi, %r8
	leaq	(%r11,%r8,4), %rdx
	shlq	$5, %rsi
	addq	%rsp, %rsi
	addq	$256, %rsi                      ## imm = 0x100
	xorl	%edi, %edi
	.p2align	4, 0x90
LBB51_52:                               ## %"for head2_relu_0_d_def__.s1.n.rebased14.us.2.epil"
                                        ##   Parent Loop BB51_10 Depth=1
                                        ##     Parent Loop BB51_46 Depth=2
                                        ## =>    This Inner Loop Header: Depth=3
	vbroadcastss	(%rdx,%rdi), %ymm1
	vfmadd213ps	(%rsi,%rdi,8), %ymm0, %ymm1 ## ymm1 = (ymm0 * ymm1) + mem
	vmovaps	%ymm1, (%rsi,%rdi,8)
	addq	$4, %rdi
	cmpq	%rdi, %r12
	jne	LBB51_52
	jmp	LBB51_53
	.p2align	4, 0x90
LBB51_54:                               ## %"consume head2_relu_0_d_def__17.2"
                                        ##   in Loop: Header=BB51_10 Depth=1
	cmpl	$0, 16(%rsp)                    ## 4-byte Folded Reload
	movq	128(%rsp), %rdx                 ## 8-byte Reload
	movq	64(%rsp), %r11                  ## 8-byte Reload
	movq	48(%rsp), %r9                   ## 8-byte Reload
	vxorps	%xmm2, %xmm2, %xmm2
	movq	40(%rsp), %r10                  ## 8-byte Reload
	movq	32(%rsp), %rdi                  ## 8-byte Reload
	jle	LBB51_57
## %bb.55:                              ## %"for head2_conv_1_d_def__.s0.n.ni18.preheader.2"
                                        ##   in Loop: Header=BB51_10 Depth=1
	leaq	256(%rsp), %rax
	xorl	%ecx, %ecx
	.p2align	4, 0x90
LBB51_56:                               ## %"for head2_conv_1_d_def__.s0.n.ni18.2"
                                        ##   Parent Loop BB51_10 Depth=1
                                        ## =>  This Inner Loop Header: Depth=2
	leaq	(%rdi,%rcx,4), %r8
	leaq	(%r8,%r10), %rsi
	addq	%r10, %rsi
	leaq	(%rsi,%r10), %rdi
	addq	%r10, %rdi
	movq	%rbx, %r15
	leaq	(%rdi,%r10), %rbx
	vmovss	(%rsi,%rdx,8), %xmm0            ## xmm0 = mem[0],zero,zero,zero
	vinsertps	$16, (%rdi,%rdx,4), %xmm0, %xmm0 ## xmm0 = xmm0[0],mem[0],xmm0[2,3]
	movq	32(%rsp), %rdi                  ## 8-byte Reload
	vinsertps	$32, (%rbx,%rdx,4), %xmm0, %xmm0 ## xmm0 = xmm0[0,1],mem[0],xmm0[3]
	addq	%r10, %rbx
	vinsertps	$48, (%rbx,%rdx,4), %xmm0, %xmm0 ## xmm0 = xmm0[0,1,2],mem[0]
	movq	%r15, %rbx
	vmovss	(%rdi,%rcx,4), %xmm1            ## xmm1 = mem[0],zero,zero,zero
	vinsertps	$16, (%r8,%rdx,4), %xmm1, %xmm1 ## xmm1 = xmm1[0],mem[0],xmm1[2,3]
	vinsertps	$32, (%r8,%rdx,8), %xmm1, %xmm1 ## xmm1 = xmm1[0,1],mem[0],xmm1[3]
	vinsertps	$48, (%rsi,%rdx,4), %xmm1, %xmm1 ## xmm1 = xmm1[0,1,2],mem[0]
	vinsertf128	$1, %xmm0, %ymm1, %ymm0
	vcmpleps	%ymm0, %ymm2, %ymm0
	vandps	(%rax), %ymm0, %ymm0
	vmovaps	%ymm0, (%r15)
	incq	%rcx
	addq	%r9, %rbx
	addq	$32, %rax
	cmpq	%rcx, %r11
	jne	LBB51_56
LBB51_57:                               ## %"end for head2_conv_1_d_def__.s0.n.ni19.2"
                                        ##   in Loop: Header=BB51_10 Depth=1
	movq	248(%rsp), %rcx                 ## 8-byte Reload
	incq	%rcx
	movq	24(%rsp), %rax                  ## 8-byte Reload
	addl	196(%rsp), %eax                 ## 4-byte Folded Reload
	movq	%rax, 24(%rsp)                  ## 8-byte Spill
	movq	72(%rsp), %rax                  ## 8-byte Reload
	addl	$3, %eax
	movq	%rax, 72(%rsp)                  ## 8-byte Spill
	movl	192(%rsp), %eax                 ## 4-byte Reload
	addl	%eax, 8(%rsp)                   ## 4-byte Folded Spill
	addl	%eax, 116(%rsp)                 ## 4-byte Folded Spill
	addl	%eax, 112(%rsp)                 ## 4-byte Folded Spill
	cmpq	232(%rsp), %rcx                 ## 8-byte Folded Reload
	jne	LBB51_10
LBB51_58:                               ## %destructor_block
	xorl	%eax, %eax
	leaq	-40(%rbp), %rsp
	popq	%rbx
	popq	%r12
	popq	%r13
	popq	%r14
	popq	%r15
	popq	%rbp
	vzeroupper
	retq
                                        ## -- End function
	.p2align	4, 0x90                         ## -- Begin function train_cost_model.par_for.updated_head2_filter.s1.v240.v240.v240
_train_cost_model.par_for.updated_head2_filter.s1.v240.v240.v240: ## @train_cost_model.par_for.updated_head2_filter.s1.v240.v240.v240
## %bb.0:                               ## %entry
	pushq	%rbp
	pushq	%r15
	pushq	%r14
	pushq	%r13
	pushq	%r12
	pushq	%rbx
	movl	(%rdx), %r8d
	movl	8(%rdx), %r9d
	movslq	4(%rdx), %rbp
	movq	32(%rdx), %rax
	movq	%rax, -120(%rsp)                ## 8-byte Spill
	movq	48(%rdx), %r10
	movq	64(%rdx), %rax
	movq	%rax, -96(%rsp)                 ## 8-byte Spill
	cmpl	$56, %esi
	movq	%r8, -48(%rsp)                  ## 8-byte Spill
	jg	LBB52_17
## %bb.1:                               ## %then_bb
	movslq	28(%rdx), %rax
	movq	%rax, -104(%rsp)                ## 8-byte Spill
	movl	%esi, %edi
	movslq	20(%rdx), %rax
	movl	%esi, %ecx
	sarl	$31, %ecx
	movl	%ecx, %esi
	xorl	%edi, %esi
	imulq	$1431655766, %rsi, %rbx         ## imm = 0x55555556
	shrq	$32, %rbx
	xorl	%ecx, %ebx
	leal	(%rbx,%rbx,2), %ecx
	subl	%ecx, %edi
	addl	%ebx, %ebx
	movl	%edi, -124(%rsp)                ## 4-byte Spill
	movslq	%edi, %rcx
	leaq	(%rax,%rcx,8), %rax
	movq	%rax, -112(%rsp)                ## 8-byte Spill
	movslq	%ebx, %rsi
	testl	%r8d, %r8d
	jle	LBB52_4
## %bb.2:                               ## %then_bb.split.us
	movq	%rsi, -72(%rsp)                 ## 8-byte Spill
	testl	%r9d, %r9d
	jle	LBB52_3
## %bb.5:                               ## %"for updated_head2_filter.s1.v241.v214i.us.us.preheader"
	movl	24(%rdx), %ecx
	movq	%rbx, -32(%rsp)                 ## 8-byte Spill
	movl	%ebx, %eax
	imull	%ebp, %eax
	movl	%ecx, -36(%rsp)                 ## 4-byte Spill
	subl	%ecx, %eax
	cltq
	leaq	-1(%r9), %rcx
	movq	%rcx, -80(%rsp)                 ## 8-byte Spill
	movl	%r9d, %esi
	andl	$3, %esi
	movl	%r9d, %r8d
	andl	$-4, %r8d
	imulq	$468, %rbp, %r14                ## imm = 0x1D4
	movq	%r10, -24(%rsp)                 ## 8-byte Spill
	leaq	(%r10,%rax,4), %r13
	imulq	$624, %rbp, %rbx                ## imm = 0x270
	movq	%r9, -8(%rsp)                   ## 8-byte Spill
	leal	(%r9,%r9,2), %eax
	movl	%eax, -64(%rsp)                 ## 4-byte Spill
	imulq	$312, %rbp, %rcx                ## imm = 0x138
	imulq	$156, %rbp, %rdx
	movq	%rbp, -16(%rsp)                 ## 8-byte Spill
	imulq	$39, %rbp, %rax
	movq	%rax, -88(%rsp)                 ## 8-byte Spill
	movq	%rsi, -56(%rsp)                 ## 8-byte Spill
	movq	%rsi, %rax
	shlq	$5, %rax
	leaq	(%rax,%rax,2), %r11
	vxorps	%xmm0, %xmm0, %xmm0
	xorl	%r10d, %r10d
	movl	-124(%rsp), %esi                ## 4-byte Reload
	jmp	LBB52_6
	.p2align	4, 0x90
LBB52_13:                               ## %"end for head2_filter_im_0_d_def__.s1.r1046$x.loopexit.us.us.us"
                                        ##   in Loop: Header=BB52_6 Depth=1
	incq	%r10
	addq	$4, %r13
	addl	-64(%rsp), %esi                 ## 4-byte Folded Reload
	cmpq	-48(%rsp), %r10                 ## 8-byte Folded Reload
	je	LBB52_14
LBB52_6:                                ## %"for head2_filter_im_0_d_def__.s1.r1046$y.us.us.us"
                                        ## =>This Loop Header: Depth=1
                                        ##     Child Loop BB52_9 Depth 2
                                        ##     Child Loop BB52_12 Depth 2
	movslq	%esi, %r9
	cmpq	$3, -80(%rsp)                   ## 8-byte Folded Reload
	jae	LBB52_8
## %bb.7:                               ##   in Loop: Header=BB52_6 Depth=1
	xorl	%r15d, %r15d
	jmp	LBB52_10
	.p2align	4, 0x90
LBB52_8:                                ## %"for head2_filter_im_0_d_def__.s1.r1046$x.us.us.us.preheader"
                                        ##   in Loop: Header=BB52_6 Depth=1
	movq	%r9, %r12
	shlq	$5, %r12
	addq	-120(%rsp), %r12                ## 8-byte Folded Reload
	movq	%r13, %rbp
	xorl	%r15d, %r15d
	.p2align	4, 0x90
LBB52_9:                                ## %"for head2_filter_im_0_d_def__.s1.r1046$x.us.us.us"
                                        ##   Parent Loop BB52_6 Depth=1
                                        ## =>  This Inner Loop Header: Depth=2
	vbroadcastss	(%rbp), %ymm1
	vfmadd132ps	(%r12), %ymm0, %ymm1    ## ymm1 = (ymm1 * mem) + ymm0
	vbroadcastss	(%rbp,%rdx), %ymm0
	vfmadd132ps	96(%r12), %ymm1, %ymm0  ## ymm0 = (ymm0 * mem) + ymm1
	vbroadcastss	(%rbp,%rcx), %ymm1
	vfmadd132ps	192(%r12), %ymm0, %ymm1 ## ymm1 = (ymm1 * mem) + ymm0
	vbroadcastss	(%rbp,%r14), %ymm0
	vfmadd132ps	288(%r12), %ymm1, %ymm0 ## ymm0 = (ymm0 * mem) + ymm1
	addq	$4, %r15
	addq	%rbx, %rbp
	addq	$384, %r12                      ## imm = 0x180
	cmpq	%r15, %r8
	jne	LBB52_9
LBB52_10:                               ## %"end for head2_filter_im_0_d_def__.s1.r1046$x.loopexit.us.us.us.unr-lcssa"
                                        ##   in Loop: Header=BB52_6 Depth=1
	cmpq	$0, -56(%rsp)                   ## 8-byte Folded Reload
	je	LBB52_13
## %bb.11:                              ## %"for head2_filter_im_0_d_def__.s1.r1046$x.us.us.us.epil.preheader"
                                        ##   in Loop: Header=BB52_6 Depth=1
	shlq	$3, %r9
	movq	-88(%rsp), %rax                 ## 8-byte Reload
	imulq	%r15, %rax
	leaq	(,%rax,4), %rax
	addq	%r13, %rax
	leaq	(%r15,%r15,2), %rdi
	leaq	(%r9,%rdi,8), %rdi
	movq	-120(%rsp), %rbp                ## 8-byte Reload
	leaq	(%rbp,%rdi,4), %rbp
	xorl	%edi, %edi
	.p2align	4, 0x90
LBB52_12:                               ## %"for head2_filter_im_0_d_def__.s1.r1046$x.us.us.us.epil"
                                        ##   Parent Loop BB52_6 Depth=1
                                        ## =>  This Inner Loop Header: Depth=2
	vbroadcastss	(%rax), %ymm1
	vfmadd231ps	(%rbp,%rdi), %ymm1, %ymm0 ## ymm0 = (ymm1 * mem) + ymm0
	addq	%rdx, %rax
	addq	$96, %rdi
	cmpq	%rdi, %r11
	jne	LBB52_12
	jmp	LBB52_13
LBB52_17:                               ## %"produce head2_filter_im_0_d_def__1"
	movslq	16(%rdx), %rax
	movq	%rax, -64(%rsp)                 ## 8-byte Spill
	movl	%esi, %ecx
	movl	%esi, %eax
	imulq	$1431655766, %rax, %rsi         ## imm = 0x55555556
	shrq	$32, %rsi
	leal	(%rsi,%rsi,2), %eax
	movl	%ecx, %r11d
	subl	%eax, %ecx
	movl	%ecx, -124(%rsp)                ## 4-byte Spill
	vxorps	%xmm0, %xmm0, %xmm0
	testl	%r8d, %r8d
	jle	LBB52_28
## %bb.18:                              ## %"for head2_filter_im_0_d_def__.s1.r1046$y2.preheader"
	testl	%r9d, %r9d
	jle	LBB52_28
## %bb.19:                              ## %"for head2_filter_im_0_d_def__.s1.r1046$y2.us.preheader"
	movslq	12(%rdx), %rax
	leaq	-1(%r9), %rcx
	movq	%rcx, -80(%rsp)                 ## 8-byte Spill
	movl	%r9d, %r12d
	andl	$3, %r12d
	movl	%r9d, %r14d
	andl	$-4, %r14d
	imulq	$468, %rbp, %rcx                ## imm = 0x1D4
	leaq	(%r10,%rax,4), %r15
	imulq	$624, %rbp, %rdx                ## imm = 0x270
	leal	(%rsi,%rsi,2), %eax
	subl	%eax, %r11d
	leal	(%r9,%r9,2), %eax
	movl	%eax, -88(%rsp)                 ## 4-byte Spill
	imulq	$312, %rbp, %rdi                ## imm = 0x138
	imulq	$156, %rbp, %rbx
	imulq	$39, %rbp, %rax
	movq	%rax, -56(%rsp)                 ## 8-byte Spill
	movq	%r12, %rax
	shlq	$5, %rax
	leaq	(%rax,%rax,2), %r10
	vxorps	%xmm0, %xmm0, %xmm0
	xorl	%r8d, %r8d
	jmp	LBB52_20
	.p2align	4, 0x90
LBB52_27:                               ## %"end for head2_filter_im_0_d_def__.s1.r1046$x6.loopexit.us"
                                        ##   in Loop: Header=BB52_20 Depth=1
	incq	%r8
	addq	$4, %r15
	addl	-88(%rsp), %r11d                ## 4-byte Folded Reload
	cmpq	-48(%rsp), %r8                  ## 8-byte Folded Reload
	je	LBB52_28
LBB52_20:                               ## %"for head2_filter_im_0_d_def__.s1.r1046$y2.us"
                                        ## =>This Loop Header: Depth=1
                                        ##     Child Loop BB52_23 Depth 2
                                        ##     Child Loop BB52_26 Depth 2
	movslq	%r11d, %r9
	cmpq	$3, -80(%rsp)                   ## 8-byte Folded Reload
	jae	LBB52_22
## %bb.21:                              ##   in Loop: Header=BB52_20 Depth=1
	xorl	%r13d, %r13d
	jmp	LBB52_24
	.p2align	4, 0x90
LBB52_22:                               ## %"for head2_filter_im_0_d_def__.s1.r1046$x5.us.preheader"
                                        ##   in Loop: Header=BB52_20 Depth=1
	movq	%r9, %rsi
	shlq	$5, %rsi
	addq	-120(%rsp), %rsi                ## 8-byte Folded Reload
	movq	%r15, %rbp
	xorl	%r13d, %r13d
	.p2align	4, 0x90
LBB52_23:                               ## %"for head2_filter_im_0_d_def__.s1.r1046$x5.us"
                                        ##   Parent Loop BB52_20 Depth=1
                                        ## =>  This Inner Loop Header: Depth=2
	vbroadcastss	(%rbp), %ymm1
	vfmadd132ps	(%rsi), %ymm0, %ymm1    ## ymm1 = (ymm1 * mem) + ymm0
	vbroadcastss	(%rbp,%rbx), %ymm0
	vfmadd132ps	96(%rsi), %ymm1, %ymm0  ## ymm0 = (ymm0 * mem) + ymm1
	vbroadcastss	(%rbp,%rdi), %ymm1
	vfmadd132ps	192(%rsi), %ymm0, %ymm1 ## ymm1 = (ymm1 * mem) + ymm0
	vbroadcastss	(%rbp,%rcx), %ymm0
	vfmadd132ps	288(%rsi), %ymm1, %ymm0 ## ymm0 = (ymm0 * mem) + ymm1
	addq	$4, %r13
	addq	%rdx, %rbp
	addq	$384, %rsi                      ## imm = 0x180
	cmpq	%r13, %r14
	jne	LBB52_23
LBB52_24:                               ## %"end for head2_filter_im_0_d_def__.s1.r1046$x6.loopexit.us.unr-lcssa"
                                        ##   in Loop: Header=BB52_20 Depth=1
	testq	%r12, %r12
	je	LBB52_27
## %bb.25:                              ## %"for head2_filter_im_0_d_def__.s1.r1046$x5.us.epil.preheader"
                                        ##   in Loop: Header=BB52_20 Depth=1
	shlq	$3, %r9
	movq	-56(%rsp), %rax                 ## 8-byte Reload
	imulq	%r13, %rax
	leaq	(%r15,%rax,4), %rsi
	leaq	(,%r13,2), %rax
	addq	%r13, %rax
	leaq	(%r9,%rax,8), %rax
	movq	-120(%rsp), %rbp                ## 8-byte Reload
	leaq	(%rbp,%rax,4), %rbp
	xorl	%eax, %eax
	.p2align	4, 0x90
LBB52_26:                               ## %"for head2_filter_im_0_d_def__.s1.r1046$x5.us.epil"
                                        ##   Parent Loop BB52_20 Depth=1
                                        ## =>  This Inner Loop Header: Depth=2
	vbroadcastss	(%rsi), %ymm1
	vfmadd231ps	(%rbp,%rax), %ymm1, %ymm0 ## ymm0 = (ymm1 * mem) + ymm0
	addq	%rbx, %rsi
	addq	$96, %rax
	cmpq	%rax, %r10
	jne	LBB52_26
	jmp	LBB52_27
LBB52_28:                               ## %"consume head2_filter_im_0_d_def__8"
	movslq	-124(%rsp), %rax                ## 4-byte Folded Reload
	movq	-64(%rsp), %rcx                 ## 8-byte Reload
	leaq	(%rcx,%rax,8), %rsi
	jmp	LBB52_29
LBB52_14:                               ## %"consume head2_filter_im_0_d_def__.loopexit.split.us.us.us"
	movq	-72(%rsp), %rax                 ## 8-byte Reload
	imulq	-104(%rsp), %rax                ## 8-byte Folded Reload
	addq	-112(%rsp), %rax                ## 8-byte Folded Reload
	movq	-96(%rsp), %rsi                 ## 8-byte Reload
	vmovups	%ymm0, (%rsi,%rax,4)
	movq	-32(%rsp), %rax                 ## 8-byte Reload
	orl	$1, %eax
	imull	-16(%rsp), %eax                 ## 4-byte Folded Reload
	subl	-36(%rsp), %eax                 ## 4-byte Folded Reload
	cltq
	movq	-8(%rsp), %rsi                  ## 8-byte Reload
	movl	%esi, %r8d
	andl	$3, %r8d
	movl	%esi, %r9d
	andl	$-4, %r9d
	movq	-24(%rsp), %rdi                 ## 8-byte Reload
	leaq	(%rdi,%rax,4), %r13
	leal	(%rsi,%rsi,2), %r10d
	movq	%r8, %rax
	shlq	$5, %rax
	leaq	(%rax,%rax,2), %rbp
	vxorps	%xmm0, %xmm0, %xmm0
	xorl	%r11d, %r11d
	movl	-124(%rsp), %eax                ## 4-byte Reload
	jmp	LBB52_15
	.p2align	4, 0x90
LBB52_35:                               ## %"end for head2_filter_im_0_d_def__.s1.r1046$x.loopexit.us.us.us.1"
                                        ##   in Loop: Header=BB52_15 Depth=1
	incq	%r11
	addq	$4, %r13
	movl	%r12d, %eax
	addl	%r10d, %eax
	cmpq	-48(%rsp), %r11                 ## 8-byte Folded Reload
	je	LBB52_36
LBB52_15:                               ## %"for head2_filter_im_0_d_def__.s1.r1046$y.us.us.us.1"
                                        ## =>This Loop Header: Depth=1
                                        ##     Child Loop BB52_31 Depth 2
                                        ##     Child Loop BB52_34 Depth 2
	movl	%eax, %r12d
	movslq	%eax, %r15
	cmpq	$3, -80(%rsp)                   ## 8-byte Folded Reload
	jae	LBB52_30
## %bb.16:                              ##   in Loop: Header=BB52_15 Depth=1
	xorl	%esi, %esi
	jmp	LBB52_32
	.p2align	4, 0x90
LBB52_30:                               ## %"for head2_filter_im_0_d_def__.s1.r1046$x.us.us.us.1.preheader"
                                        ##   in Loop: Header=BB52_15 Depth=1
	movq	%r15, %rdi
	shlq	$5, %rdi
	addq	-120(%rsp), %rdi                ## 8-byte Folded Reload
	movq	%r13, %rax
	xorl	%esi, %esi
	.p2align	4, 0x90
LBB52_31:                               ## %"for head2_filter_im_0_d_def__.s1.r1046$x.us.us.us.1"
                                        ##   Parent Loop BB52_15 Depth=1
                                        ## =>  This Inner Loop Header: Depth=2
	vbroadcastss	(%rax), %ymm1
	vfmadd132ps	(%rdi), %ymm0, %ymm1    ## ymm1 = (ymm1 * mem) + ymm0
	vbroadcastss	(%rax,%rdx), %ymm0
	vfmadd132ps	96(%rdi), %ymm1, %ymm0  ## ymm0 = (ymm0 * mem) + ymm1
	vbroadcastss	(%rax,%rcx), %ymm1
	vfmadd132ps	192(%rdi), %ymm0, %ymm1 ## ymm1 = (ymm1 * mem) + ymm0
	vbroadcastss	(%rax,%r14), %ymm0
	vfmadd132ps	288(%rdi), %ymm1, %ymm0 ## ymm0 = (ymm0 * mem) + ymm1
	addq	$4, %rsi
	addq	%rbx, %rax
	addq	$384, %rdi                      ## imm = 0x180
	cmpq	%rsi, %r9
	jne	LBB52_31
LBB52_32:                               ## %"end for head2_filter_im_0_d_def__.s1.r1046$x.loopexit.us.us.us.1.unr-lcssa"
                                        ##   in Loop: Header=BB52_15 Depth=1
	testq	%r8, %r8
	je	LBB52_35
## %bb.33:                              ## %"for head2_filter_im_0_d_def__.s1.r1046$x.us.us.us.1.epil.preheader"
                                        ##   in Loop: Header=BB52_15 Depth=1
	shlq	$3, %r15
	movq	-88(%rsp), %rax                 ## 8-byte Reload
	imulq	%rsi, %rax
	leaq	(,%rax,4), %rax
	addq	%r13, %rax
	leaq	(%rsi,%rsi,2), %rsi
	leaq	(%r15,%rsi,8), %rsi
	movq	-120(%rsp), %rdi                ## 8-byte Reload
	leaq	(%rdi,%rsi,4), %rsi
	xorl	%edi, %edi
	.p2align	4, 0x90
LBB52_34:                               ## %"for head2_filter_im_0_d_def__.s1.r1046$x.us.us.us.1.epil"
                                        ##   Parent Loop BB52_15 Depth=1
                                        ## =>  This Inner Loop Header: Depth=2
	vbroadcastss	(%rax), %ymm1
	vfmadd231ps	(%rsi,%rdi), %ymm1, %ymm0 ## ymm0 = (ymm1 * mem) + ymm0
	addq	%rdx, %rax
	addq	$96, %rdi
	cmpq	%rdi, %rbp
	jne	LBB52_34
	jmp	LBB52_35
LBB52_36:                               ## %"consume head2_filter_im_0_d_def__.loopexit.split.us.us.us.1"
	movq	-72(%rsp), %rsi                 ## 8-byte Reload
	orq	$1, %rsi
	imulq	-104(%rsp), %rsi                ## 8-byte Folded Reload
	addq	-112(%rsp), %rsi                ## 8-byte Folded Reload
LBB52_29:                               ## %destructor_block
	movq	-96(%rsp), %rax                 ## 8-byte Reload
	vmovups	%ymm0, (%rax,%rsi,4)
	xorl	%eax, %eax
	popq	%rbx
	popq	%r12
	popq	%r13
	popq	%r14
	popq	%r15
	popq	%rbp
	vzeroupper
	retq
LBB52_3:                                ## %"for updated_head2_filter.s1.v241.v214i.us.preheader"
	movq	-72(%rsp), %rsi                 ## 8-byte Reload
LBB52_4:                                ## %"for updated_head2_filter.s1.v241.v214i"
	movq	%rsi, %rax
	movq	-104(%rsp), %rdx                ## 8-byte Reload
	imulq	%rdx, %rax
	movq	-112(%rsp), %rdi                ## 8-byte Reload
	addq	%rdi, %rax
	vxorps	%xmm0, %xmm0, %xmm0
	movq	-96(%rsp), %rcx                 ## 8-byte Reload
	vmovups	%ymm0, (%rcx,%rax,4)
	orq	$1, %rsi
	imulq	%rdx, %rsi
	addq	%rdi, %rsi
	jmp	LBB52_29
                                        ## -- End function
	.section	__TEXT,__literal4,4byte_literals
	.p2align	2                               ## -- Begin function train_cost_model.par_for.updated_head2_filter.s2.v240.v240.v240
LCPI53_0:
	.long	0x3f666666                      ## float 0.899999976
LCPI53_1:
	.long	0x3dcccccd                      ## float 0.100000001
	.section	__TEXT,__text,regular,pure_instructions
	.p2align	4, 0x90
_train_cost_model.par_for.updated_head2_filter.s2.v240.v240.v240: ## @train_cost_model.par_for.updated_head2_filter.s2.v240.v240.v240
## %bb.0:                               ## %entry
                                        ## kill: def $esi killed $esi def $rsi
	movslq	(%rdx), %r8
	movslq	4(%rdx), %rcx
	movq	8(%rdx), %rax
	cmpl	$56, %esi
	jg	LBB53_2
## %bb.1:                               ## %then_bb
	movl	%esi, %edx
	sarl	$31, %edx
	movl	%edx, %edi
	xorl	%esi, %edi
	imulq	$1431655766, %rdi, %rdi         ## imm = 0x55555556
	shrq	$32, %rdi
	xorl	%edx, %edi
	leal	(%rdi,%rdi,2), %edx
                                        ## kill: def $edi killed $edi killed $rdi
	addl	%edi, %edi
	subl	%edx, %esi
	shll	$3, %esi
	leaq	(%rcx,%rcx,2), %r9
	movslq	%edi, %rdx
	movslq	%esi, %r10
	movq	%rdx, %rdi
	imulq	%r8, %rdi
	addq	%r10, %rdi
	vbroadcastss	LCPI53_0(%rip), %ymm0   ## ymm0 = [8.99999976E-1,8.99999976E-1,8.99999976E-1,8.99999976E-1,8.99999976E-1,8.99999976E-1,8.99999976E-1,8.99999976E-1]
	leaq	(%r9,%rdi), %rsi
	addq	%rcx, %rdi
	vbroadcastss	LCPI53_1(%rip), %ymm1   ## ymm1 = [1.00000001E-1,1.00000001E-1,1.00000001E-1,1.00000001E-1,1.00000001E-1,1.00000001E-1,1.00000001E-1,1.00000001E-1]
	vmulps	(%rax,%rsi,4), %ymm1, %ymm1
	vfmadd231ps	(%rax,%rdi,4), %ymm0, %ymm1 ## ymm1 = (ymm0 * mem) + ymm1
	vmovups	%ymm1, (%rax,%rdi,4)
	orq	$1, %rdx
	imulq	%r8, %rdx
	addq	%r10, %rdx
	addq	%rdx, %rcx
	addq	%r9, %rdx
	jmp	LBB53_3
LBB53_2:                                ## %next_bb
	imull	$38, %r8d, %edx
	movl	%esi, %edi
	imulq	$1431655766, %rdi, %rdi         ## imm = 0x55555556
	shrq	$32, %rdi
	leal	(%rdi,%rdi,2), %edi
	subl	%edi, %esi
	leal	(%rdx,%rsi,8), %edx
	movslq	%edx, %rsi
	leaq	(%rcx,%rcx,2), %rdx
	addq	%rsi, %rcx
	addq	%rsi, %rdx
LBB53_3:                                ## %destructor_block
	vbroadcastss	LCPI53_0(%rip), %ymm0   ## ymm0 = [8.99999976E-1,8.99999976E-1,8.99999976E-1,8.99999976E-1,8.99999976E-1,8.99999976E-1,8.99999976E-1,8.99999976E-1]
	vbroadcastss	LCPI53_1(%rip), %ymm1   ## ymm1 = [1.00000001E-1,1.00000001E-1,1.00000001E-1,1.00000001E-1,1.00000001E-1,1.00000001E-1,1.00000001E-1,1.00000001E-1]
	vmulps	(%rax,%rdx,4), %ymm1, %ymm1
	vfmadd231ps	(%rax,%rcx,4), %ymm0, %ymm1 ## ymm1 = (ymm0 * mem) + ymm1
	vmovups	%ymm1, (%rax,%rcx,4)
	xorl	%eax, %eax
	vzeroupper
	retq
                                        ## -- End function
	.section	__TEXT,__literal4,4byte_literals
	.p2align	2                               ## -- Begin function train_cost_model.par_for.updated_head2_filter.s3.v240.v240.v240
LCPI54_0:
	.long	0x3f7fbe77                      ## float 0.999000012
LCPI54_1:
	.long	0x3a83126f                      ## float 0.00100000005
	.section	__TEXT,__text,regular,pure_instructions
	.p2align	4, 0x90
_train_cost_model.par_for.updated_head2_filter.s3.v240.v240.v240: ## @train_cost_model.par_for.updated_head2_filter.s3.v240.v240.v240
## %bb.0:                               ## %entry
                                        ## kill: def $esi killed $esi def $rsi
	movslq	(%rdx), %rdi
	movslq	4(%rdx), %r8
	movslq	8(%rdx), %r9
	movq	16(%rdx), %rax
	cmpl	$56, %esi
	jg	LBB54_2
## %bb.1:                               ## %then_bb
	movl	%esi, %edx
	sarl	$31, %edx
	movl	%edx, %ecx
	xorl	%esi, %ecx
	imulq	$1431655766, %rcx, %rcx         ## imm = 0x55555556
	shrq	$32, %rcx
	xorl	%edx, %ecx
	leal	(%rcx,%rcx,2), %edx
                                        ## kill: def $ecx killed $ecx killed $rcx
	addl	%ecx, %ecx
	subl	%edx, %esi
	shll	$3, %esi
	movslq	%ecx, %rdx
	movslq	%esi, %r10
	movq	%rdx, %rsi
	imulq	%r9, %rsi
	addq	%r10, %rsi
	leaq	(%rsi,%rdi), %rcx
	vmovups	(%rax,%rcx,4), %ymm0
	addq	%r8, %rsi
	vbroadcastss	LCPI54_0(%rip), %ymm1   ## ymm1 = [9.99000012E-1,9.99000012E-1,9.99000012E-1,9.99000012E-1,9.99000012E-1,9.99000012E-1,9.99000012E-1,9.99000012E-1]
	vmulps	%ymm0, %ymm0, %ymm0
	vbroadcastss	LCPI54_1(%rip), %ymm2   ## ymm2 = [1.00000005E-3,1.00000005E-3,1.00000005E-3,1.00000005E-3,1.00000005E-3,1.00000005E-3,1.00000005E-3,1.00000005E-3]
	vmulps	%ymm2, %ymm0, %ymm0
	vfmadd231ps	(%rax,%rsi,4), %ymm1, %ymm0 ## ymm0 = (ymm1 * mem) + ymm0
	vmovups	%ymm0, (%rax,%rsi,4)
	orq	$1, %rdx
	imulq	%r9, %rdx
	addq	%r10, %rdx
	addq	%rdx, %rdi
	vmovups	(%rax,%rdi,4), %ymm0
	addq	%r8, %rdx
	jmp	LBB54_3
LBB54_2:                                ## %next_bb
	imull	$38, %r9d, %ecx
	movl	%esi, %edx
	imulq	$1431655766, %rdx, %rdx         ## imm = 0x55555556
	shrq	$32, %rdx
	leal	(%rdx,%rdx,2), %edx
	subl	%edx, %esi
	leal	(%rcx,%rsi,8), %ecx
	movslq	%ecx, %rcx
	addq	%rcx, %rdi
	vmovups	(%rax,%rdi,4), %ymm0
	addl	%r8d, %ecx
	movslq	%ecx, %rdx
LBB54_3:                                ## %destructor_block
	vmulps	%ymm0, %ymm0, %ymm0
	vbroadcastss	LCPI54_0(%rip), %ymm1   ## ymm1 = [9.99000012E-1,9.99000012E-1,9.99000012E-1,9.99000012E-1,9.99000012E-1,9.99000012E-1,9.99000012E-1,9.99000012E-1]
	vbroadcastss	LCPI54_1(%rip), %ymm2   ## ymm2 = [1.00000005E-3,1.00000005E-3,1.00000005E-3,1.00000005E-3,1.00000005E-3,1.00000005E-3,1.00000005E-3,1.00000005E-3]
	vmulps	%ymm2, %ymm0, %ymm0
	vfmadd231ps	(%rax,%rdx,4), %ymm1, %ymm0 ## ymm0 = (ymm1 * mem) + ymm0
	vmovups	%ymm0, (%rax,%rdx,4)
	xorl	%eax, %eax
	vzeroupper
	retq
                                        ## -- End function
	.section	__TEXT,__literal4,4byte_literals
	.p2align	2                               ## -- Begin function train_cost_model.par_for.updated_head2_filter.s4.v240.v240.v240
LCPI55_0:
	.long	0xb727c5ac                      ## float -9.99999974E-6
	.section	__TEXT,__text,regular,pure_instructions
	.p2align	4, 0x90
_train_cost_model.par_for.updated_head2_filter.s4.v240.v240.v240: ## @train_cost_model.par_for.updated_head2_filter.s4.v240.v240.v240
## %bb.0:                               ## %entry
	pushq	%rbx
                                        ## kill: def $esi killed $esi def $rsi
	movslq	(%rdx), %r9
	vmovss	4(%rdx), %xmm1                  ## xmm1 = mem[0],zero,zero,zero
	vmovss	8(%rdx), %xmm2                  ## xmm2 = mem[0],zero,zero,zero
	vmovss	12(%rdx), %xmm0                 ## xmm0 = mem[0],zero,zero,zero
	movslq	16(%rdx), %rdi
	movslq	20(%rdx), %r10
	movq	24(%rdx), %r8
	movq	40(%rdx), %r11
	cmpl	$56, %esi
	jg	LBB55_2
## %bb.1:                               ## %then_bb
	movl	%esi, %edx
	sarl	$31, %edx
	movl	%edx, %eax
	xorl	%esi, %eax
	imulq	$1431655766, %rax, %rax         ## imm = 0x55555556
	shrq	$32, %rax
	xorl	%edx, %eax
	leal	(%rax,%rax,2), %edx
                                        ## kill: def $eax killed $eax killed $rax
	addl	%eax, %eax
	subl	%edx, %esi
	shll	$3, %esi
	movslq	%eax, %rcx
	movslq	%esi, %rax
	vmulss	%xmm2, %xmm1, %xmm1
	vbroadcastss	%xmm1, %ymm1
	vbroadcastss	%xmm0, %ymm0
	movq	%rcx, %rsi
	imulq	%rdi, %rsi
	addq	%rax, %rsi
	movq	%rcx, %rdx
	imulq	%r9, %rdx
	leaq	(%rsi,%r10), %rbx
	vmulps	(%r11,%rbx,4), %ymm1, %ymm2
	addq	%rax, %rdx
	leaq	(%rsi,%r10,2), %rbx
	vmulps	(%r11,%rbx,4), %ymm0, %ymm3
	vsqrtps	%ymm3, %ymm3
	vbroadcastss	LCPI55_0(%rip), %ymm4   ## ymm4 = [-9.99999974E-6,-9.99999974E-6,-9.99999974E-6,-9.99999974E-6,-9.99999974E-6,-9.99999974E-6,-9.99999974E-6,-9.99999974E-6]
	vsubps	%ymm3, %ymm4, %ymm3
	vdivps	%ymm3, %ymm2, %ymm2
	vaddps	(%r8,%rdx,4), %ymm2, %ymm2
	vmovups	%ymm2, (%r11,%rsi,4)
	orq	$1, %rcx
	imulq	%rcx, %rdi
	addq	%rax, %rdi
	imulq	%r9, %rcx
	leaq	(%rdi,%r10), %rdx
	vmulps	(%r11,%rdx,4), %ymm1, %ymm1
	addq	%rax, %rcx
	leaq	(%rdi,%r10,2), %rax
	jmp	LBB55_3
LBB55_2:                                ## %next_bb
	imull	$38, %edi, %eax
	movl	%esi, %ecx
	imulq	$1431655766, %rcx, %rcx         ## imm = 0x55555556
	shrq	$32, %rcx
	leal	(%rcx,%rcx,2), %ecx
	subl	%ecx, %esi
	leal	(%rax,%rsi,8), %eax
	imulq	$38, %r9, %rcx
	movslq	%esi, %rdx
	leaq	(%rcx,%rdx,8), %rcx
	movslq	%eax, %rdi
	leaq	(%rdi,%r10), %rax
	vmulss	%xmm2, %xmm1, %xmm1
	vbroadcastss	%xmm1, %ymm1
	vmulps	(%r11,%rax,4), %ymm1, %ymm1
	leaq	(%rdi,%r10,2), %rax
	vbroadcastss	%xmm0, %ymm0
LBB55_3:                                ## %destructor_block
	vmulps	(%r11,%rax,4), %ymm0, %ymm0
	vsqrtps	%ymm0, %ymm0
	vbroadcastss	LCPI55_0(%rip), %ymm2   ## ymm2 = [-9.99999974E-6,-9.99999974E-6,-9.99999974E-6,-9.99999974E-6,-9.99999974E-6,-9.99999974E-6,-9.99999974E-6,-9.99999974E-6]
	vsubps	%ymm0, %ymm2, %ymm0
	vdivps	%ymm0, %ymm1, %ymm0
	vaddps	(%r8,%rcx,4), %ymm0, %ymm0
	vmovups	%ymm0, (%r11,%rdi,4)
	xorl	%eax, %eax
	popq	%rbx
	vzeroupper
	retq
                                        ## -- End function
	.p2align	4, 0x90                         ## -- Begin function train_cost_model.par_for.updated_head2_bias.s1.v243.v243
_train_cost_model.par_for.updated_head2_bias.s1.v243.v243: ## @train_cost_model.par_for.updated_head2_bias.s1.v243.v243
## %bb.0:                               ## %entry
	pushq	%rbp
	pushq	%r15
	pushq	%r14
	pushq	%r13
	pushq	%r12
	pushq	%rbx
	movl	(%rdx), %r10d
	movslq	8(%rdx), %r8
	movq	32(%rdx), %r9
	vxorps	%xmm0, %xmm0, %xmm0
	testl	%r10d, %r10d
	jle	LBB56_11
## %bb.1:                               ## %entry
	movl	4(%rdx), %eax
	testl	%eax, %eax
	jle	LBB56_11
## %bb.2:                               ## %"for head2_bias_im_0_d_def__.s1.r1067$y.us.preheader"
	movq	16(%rdx), %r15
	movq	%rax, %r11
	decq	%r11
	movl	%eax, %r12d
	andl	$7, %r12d
	movl	%eax, %edi
	andl	$-8, %edi
	leal	(%rax,%rax,2), %r14d
	movq	%r12, %rax
	shlq	$5, %rax
	leaq	(%rax,%rax,2), %rbx
	vxorps	%xmm0, %xmm0, %xmm0
	xorl	%eax, %eax
	movl	%esi, %ebp
	jmp	LBB56_3
	.p2align	4, 0x90
LBB56_10:                               ## %"end for head2_bias_im_0_d_def__.s1.r1067$x.loopexit.us"
                                        ##   in Loop: Header=BB56_3 Depth=1
	incq	%rax
	addl	%r14d, %ebp
	cmpq	%r10, %rax
	je	LBB56_11
LBB56_3:                                ## %"for head2_bias_im_0_d_def__.s1.r1067$y.us"
                                        ## =>This Loop Header: Depth=1
                                        ##     Child Loop BB56_6 Depth 2
                                        ##     Child Loop BB56_9 Depth 2
	movslq	%ebp, %r13
	cmpq	$7, %r11
	jae	LBB56_5
## %bb.4:                               ##   in Loop: Header=BB56_3 Depth=1
	xorl	%ecx, %ecx
	jmp	LBB56_7
	.p2align	4, 0x90
LBB56_5:                                ## %"for head2_bias_im_0_d_def__.s1.r1067$x.us.preheader"
                                        ##   in Loop: Header=BB56_3 Depth=1
	movq	%r13, %rdx
	shlq	$5, %rdx
	addq	%r15, %rdx
	xorl	%ecx, %ecx
	.p2align	4, 0x90
LBB56_6:                                ## %"for head2_bias_im_0_d_def__.s1.r1067$x.us"
                                        ##   Parent Loop BB56_3 Depth=1
                                        ## =>  This Inner Loop Header: Depth=2
	vaddps	(%rdx), %ymm0, %ymm0
	vaddps	96(%rdx), %ymm0, %ymm0
	vaddps	192(%rdx), %ymm0, %ymm0
	vaddps	288(%rdx), %ymm0, %ymm0
	vaddps	384(%rdx), %ymm0, %ymm0
	vaddps	480(%rdx), %ymm0, %ymm0
	vaddps	576(%rdx), %ymm0, %ymm0
	vaddps	672(%rdx), %ymm0, %ymm0
	addq	$8, %rcx
	addq	$768, %rdx                      ## imm = 0x300
	cmpq	%rcx, %rdi
	jne	LBB56_6
LBB56_7:                                ## %"end for head2_bias_im_0_d_def__.s1.r1067$x.loopexit.us.unr-lcssa"
                                        ##   in Loop: Header=BB56_3 Depth=1
	testq	%r12, %r12
	je	LBB56_10
## %bb.8:                               ## %"for head2_bias_im_0_d_def__.s1.r1067$x.us.epil.preheader"
                                        ##   in Loop: Header=BB56_3 Depth=1
	shlq	$3, %r13
	leaq	(%rcx,%rcx,2), %rcx
	leaq	(,%rcx,8), %rcx
	addq	%r13, %rcx
	leaq	(%r15,%rcx,4), %rcx
	xorl	%edx, %edx
	.p2align	4, 0x90
LBB56_9:                                ## %"for head2_bias_im_0_d_def__.s1.r1067$x.us.epil"
                                        ##   Parent Loop BB56_3 Depth=1
                                        ## =>  This Inner Loop Header: Depth=2
	vaddps	(%rcx,%rdx), %ymm0, %ymm0
	addq	$96, %rdx
	cmpq	%rdx, %rbx
	jne	LBB56_9
	jmp	LBB56_10
LBB56_11:                               ## %"consume head2_bias_im_0_d_def__"
	movslq	%esi, %rax
	leaq	(%r8,%rax,8), %rax
	vmovups	%ymm0, (%r9,%rax,4)
	xorl	%eax, %eax
	popq	%rbx
	popq	%r12
	popq	%r13
	popq	%r14
	popq	%r15
	popq	%rbp
	vzeroupper
	retq
                                        ## -- End function
	.section	__TEXT,__literal4,4byte_literals
	.p2align	2                               ## -- Begin function train_cost_model.par_for.updated_head2_bias.s2.v243.v243
LCPI57_0:
	.long	0x3f666666                      ## float 0.899999976
LCPI57_1:
	.long	0x3dcccccd                      ## float 0.100000001
	.section	__TEXT,__text,regular,pure_instructions
	.p2align	4, 0x90
_train_cost_model.par_for.updated_head2_bias.s2.v243.v243: ## @train_cost_model.par_for.updated_head2_bias.s2.v243.v243
## %bb.0:                               ## %entry
	movslq	(%rdx), %rax
	movq	8(%rdx), %rcx
	movslq	%esi, %rdx
	leaq	(%rax,%rdx,8), %rsi
	vbroadcastss	LCPI57_0(%rip), %ymm0   ## ymm0 = [8.99999976E-1,8.99999976E-1,8.99999976E-1,8.99999976E-1,8.99999976E-1,8.99999976E-1,8.99999976E-1,8.99999976E-1]
	leaq	(%rax,%rax,2), %rax
	leaq	(%rax,%rdx,8), %rax
	vbroadcastss	LCPI57_1(%rip), %ymm1   ## ymm1 = [1.00000001E-1,1.00000001E-1,1.00000001E-1,1.00000001E-1,1.00000001E-1,1.00000001E-1,1.00000001E-1,1.00000001E-1]
	vmulps	(%rcx,%rax,4), %ymm1, %ymm1
	vfmadd231ps	(%rcx,%rsi,4), %ymm0, %ymm1 ## ymm1 = (ymm0 * mem) + ymm1
	vmovups	%ymm1, (%rcx,%rsi,4)
	xorl	%eax, %eax
	vzeroupper
	retq
                                        ## -- End function
	.section	__TEXT,__literal4,4byte_literals
	.p2align	2                               ## -- Begin function train_cost_model.par_for.updated_head2_bias.s3.v243.v243
LCPI58_0:
	.long	0x3f7fbe77                      ## float 0.999000012
LCPI58_1:
	.long	0x3a83126f                      ## float 0.00100000005
	.section	__TEXT,__text,regular,pure_instructions
	.p2align	4, 0x90
_train_cost_model.par_for.updated_head2_bias.s3.v243.v243: ## @train_cost_model.par_for.updated_head2_bias.s3.v243.v243
## %bb.0:                               ## %entry
	movslq	(%rdx), %rax
	movq	8(%rdx), %rcx
	leaq	(%rax,%rax,2), %rdx
	movslq	%esi, %rsi
	leaq	(%rdx,%rsi,8), %rdx
	vmovups	(%rcx,%rdx,4), %ymm0
	leaq	(%rax,%rsi,4), %rax
	vbroadcastss	LCPI58_0(%rip), %ymm1   ## ymm1 = [9.99000012E-1,9.99000012E-1,9.99000012E-1,9.99000012E-1,9.99000012E-1,9.99000012E-1,9.99000012E-1,9.99000012E-1]
	vmulps	%ymm0, %ymm0, %ymm0
	vbroadcastss	LCPI58_1(%rip), %ymm2   ## ymm2 = [1.00000005E-3,1.00000005E-3,1.00000005E-3,1.00000005E-3,1.00000005E-3,1.00000005E-3,1.00000005E-3,1.00000005E-3]
	vmulps	%ymm2, %ymm0, %ymm0
	vfmadd231ps	(%rcx,%rax,8), %ymm1, %ymm0 ## ymm0 = (ymm1 * mem) + ymm0
	vmovups	%ymm0, (%rcx,%rax,8)
	xorl	%eax, %eax
	vzeroupper
	retq
                                        ## -- End function
	.section	__TEXT,__literal4,4byte_literals
	.p2align	2                               ## -- Begin function train_cost_model.par_for.updated_head2_bias.s4.v243.v243
LCPI59_0:
	.long	0xb727c5ac                      ## float -9.99999974E-6
	.section	__TEXT,__text,regular,pure_instructions
	.p2align	4, 0x90
_train_cost_model.par_for.updated_head2_bias.s4.v243.v243: ## @train_cost_model.par_for.updated_head2_bias.s4.v243.v243
## %bb.0:                               ## %entry
	vmovss	(%rdx), %xmm0                   ## xmm0 = mem[0],zero,zero,zero
	movslq	12(%rdx), %rax
	movq	32(%rdx), %rcx
	movslq	%esi, %rsi
	vmulss	4(%rdx), %xmm0, %xmm0
	leaq	(%rax,%rsi,8), %rdi
	vbroadcastss	%xmm0, %ymm0
	vmulps	(%rcx,%rdi,4), %ymm0, %ymm0
	leaq	(%rax,%rsi,4), %rax
	vbroadcastss	8(%rdx), %ymm1
	vmulps	(%rcx,%rax,8), %ymm1, %ymm1
	vsqrtps	%ymm1, %ymm1
	vbroadcastss	LCPI59_0(%rip), %ymm2   ## ymm2 = [-9.99999974E-6,-9.99999974E-6,-9.99999974E-6,-9.99999974E-6,-9.99999974E-6,-9.99999974E-6,-9.99999974E-6,-9.99999974E-6]
	vsubps	%ymm1, %ymm2, %ymm1
	vdivps	%ymm1, %ymm0, %ymm0
	movq	16(%rdx), %rax
	shlq	$5, %rsi
	vaddps	(%rax,%rsi), %ymm0, %ymm0
	vmovups	%ymm0, (%rcx,%rsi)
	xorl	%eax, %eax
	vzeroupper
	retq
                                        ## -- End function
	.p2align	4, 0x90                         ## -- Begin function train_cost_model.par_for.filter1_im_0_d_def__.s0.v12
_train_cost_model.par_for.filter1_im_0_d_def__.s0.v12: ## @train_cost_model.par_for.filter1_im_0_d_def__.s0.v12
## %bb.0:                               ## %entry
	movq	(%rdx), %rax
	shll	$2, %esi
	movslq	%esi, %rcx
	shlq	$5, %rcx
	vxorps	%xmm0, %xmm0, %xmm0
	vmovaps	%ymm0, 96(%rax,%rcx)
	vmovaps	%ymm0, 64(%rax,%rcx)
	vmovaps	%ymm0, 32(%rax,%rcx)
	vmovaps	%ymm0, (%rax,%rcx)
	xorl	%eax, %eax
	vzeroupper
	retq
                                        ## -- End function
	.p2align	4, 0x90                         ## -- Begin function train_cost_model.par_for.filter1_im_0_d_def__.s1.v12
_train_cost_model.par_for.filter1_im_0_d_def__.s1.v12: ## @train_cost_model.par_for.filter1_im_0_d_def__.s1.v12
## %bb.0:                               ## %entry
	pushq	%rbp
	pushq	%r15
	pushq	%r14
	pushq	%r13
	pushq	%r12
	pushq	%rbx
	subq	$88, %rsp
	movslq	(%rdx), %rdi
	testq	%rdi, %rdi
	jle	LBB61_13
## %bb.1:                               ## %entry
	movl	8(%rdx), %eax
	testl	%eax, %eax
	jle	LBB61_13
## %bb.2:                               ## %"for filter1_im_0_d_def__.s1.r962$z.us.us.preheader"
	movq	%rax, %rcx
	movq	%rdi, %r8
	shlq	$5, %r8
	movslq	4(%rdx), %r9
	movl	12(%rdx), %ebp
	movq	16(%rdx), %rax
	movq	48(%rdx), %r10
	movl	%edi, %ebx
	imull	%esi, %ebx
	shll	$5, %esi
	movq	32(%rdx), %r11
	movslq	%ebx, %rbx
	movslq	%esi, %rdx
	movq	%rdx, -80(%rsp)                 ## 8-byte Spill
	leaq	-1(%rcx), %rdx
	movq	%rdx, 56(%rsp)                  ## 8-byte Spill
	movl	%ecx, %edx
	andl	$3, %edx
	movq	%rdx, -112(%rsp)                ## 8-byte Spill
	andl	$-4, %ecx
	movq	%rcx, -32(%rsp)                 ## 8-byte Spill
	leaq	(%r9,%r9,8), %rdx
	shlq	$5, %rdx
	addq	%r10, %rdx
	movq	%rdx, -88(%rsp)                 ## 8-byte Spill
	negl	%ebp
	movq	%r9, %rdx
	shlq	$7, %rdx
	leaq	(%rdx,%rdx,2), %rdx
	leaq	(%rdi,%rdi,2), %rsi
	shlq	$5, %rsi
	addq	%rbx, %rsi
	leaq	(%rax,%rsi,4), %rsi
	movq	%rsi, -8(%rsp)                  ## 8-byte Spill
	movq	%rdi, %rsi
	shlq	$6, %rsi
	addq	%rbx, %rsi
	leaq	(%rax,%rsi,4), %rsi
	movq	%rsi, -16(%rsp)                 ## 8-byte Spill
	movq	%r8, 40(%rsp)                   ## 8-byte Spill
	leaq	(%r8,%rbx), %rsi
	leaq	(%rax,%rsi,4), %rsi
	movq	%rsi, -24(%rsp)                 ## 8-byte Spill
	leaq	(%rax,%rbx,4), %rax
	movq	%rax, -120(%rsp)                ## 8-byte Spill
	movq	%rdi, %rax
	shlq	$9, %rax
	leaq	(%r9,%r9,2), %rsi
	shlq	$6, %rsi
	addq	%r10, %rsi
	movq	%rsi, -96(%rsp)                 ## 8-byte Spill
	leaq	(,%r9,8), %rsi
	leaq	(%rsi,%rsi,2), %rbx
	movq	%r9, -56(%rsp)                  ## 8-byte Spill
	movq	%r9, %rsi
	shlq	$5, %rsi
	leaq	(%rsi,%rsi,2), %r8
	movl	%edi, %esi
	movq	%rsi, 48(%rsp)                  ## 8-byte Spill
	shlq	$7, %rdi
	movq	%r10, -64(%rsp)                 ## 8-byte Spill
	movq	%rbx, 32(%rsp)                  ## 8-byte Spill
	leaq	(%r10,%rbx,4), %rsi
	movq	%rsi, -104(%rsp)                ## 8-byte Spill
	xorl	%ebx, %ebx
	movq	%r11, -72(%rsp)                 ## 8-byte Spill
	jmp	LBB61_3
	.p2align	4, 0x90
LBB61_12:                               ## %"end for filter1_im_0_d_def__.s1.r962$y.loopexit.split.us.us.us"
                                        ##   in Loop: Header=BB61_3 Depth=1
	movq	-72(%rsp), %r11                 ## 8-byte Reload
	movq	-48(%rsp), %rsi                 ## 8-byte Reload
	vmovss	%xmm0, 32(%r11,%rsi,4)
	movq	-40(%rsp), %rbx                 ## 8-byte Reload
	incq	%rbx
	movl	-124(%rsp), %ebp                ## 4-byte Reload
	addl	-56(%rsp), %ebp                 ## 4-byte Folded Reload
	cmpq	$24, %rbx
	je	LBB61_13
LBB61_3:                                ## %"for filter1_im_0_d_def__.s1.r962$z.us.us"
                                        ## =>This Loop Header: Depth=1
                                        ##     Child Loop BB61_4 Depth 2
                                        ##       Child Loop BB61_7 Depth 3
                                        ##       Child Loop BB61_10 Depth 3
	movl	%ebp, -124(%rsp)                ## 4-byte Spill
	movslq	%ebp, %rsi
	movq	-64(%rsp), %rbp                 ## 8-byte Reload
	leaq	(,%rsi,4), %r9
	addq	%rbp, %r9
	movq	-88(%rsp), %rbp                 ## 8-byte Reload
	leaq	(%rbp,%rsi,4), %rbp
	movq	%rbp, 16(%rsp)                  ## 8-byte Spill
	movq	-96(%rsp), %rbp                 ## 8-byte Reload
	leaq	(%rbp,%rsi,4), %rbp
	movq	%rbp, 8(%rsp)                   ## 8-byte Spill
	movq	-104(%rsp), %rbp                ## 8-byte Reload
	leaq	(%rbp,%rsi,4), %rsi
	movq	%rsi, (%rsp)                    ## 8-byte Spill
	movq	-80(%rsp), %rsi                 ## 8-byte Reload
	movq	%rbx, -40(%rsp)                 ## 8-byte Spill
	addq	%rbx, %rsi
	movq	%rsi, -48(%rsp)                 ## 8-byte Spill
	vmovss	32(%r11,%rsi,4), %xmm0          ## xmm0 = mem[0],zero,zero,zero
	movq	-120(%rsp), %rcx                ## 8-byte Reload
	movq	%r9, 24(%rsp)                   ## 8-byte Spill
	xorl	%r12d, %r12d
	xorl	%esi, %esi
	jmp	LBB61_4
	.p2align	4, 0x90
LBB61_11:                               ## %"end for filter1_im_0_d_def__.s1.r962$x.loopexit.us.us.us"
                                        ##   in Loop: Header=BB61_4 Depth=2
	movq	64(%rsp), %rsi                  ## 8-byte Reload
	incq	%rsi
	addq	$4, %r12
	addq	$4, %r9
	addq	$4, %rcx
	cmpq	48(%rsp), %rsi                  ## 8-byte Folded Reload
	je	LBB61_12
LBB61_4:                                ## %"for filter1_im_0_d_def__.s1.r962$y.us.us.us"
                                        ##   Parent Loop BB61_3 Depth=1
                                        ## =>  This Loop Header: Depth=2
                                        ##       Child Loop BB61_7 Depth 3
                                        ##       Child Loop BB61_10 Depth 3
	cmpq	$3, 56(%rsp)                    ## 8-byte Folded Reload
	movq	%rcx, 80(%rsp)                  ## 8-byte Spill
	movq	%r9, 72(%rsp)                   ## 8-byte Spill
	movq	%rsi, 64(%rsp)                  ## 8-byte Spill
	jae	LBB61_6
## %bb.5:                               ##   in Loop: Header=BB61_4 Depth=2
	xorl	%r10d, %r10d
	jmp	LBB61_8
	.p2align	4, 0x90
LBB61_6:                                ## %"for filter1_im_0_d_def__.s1.r962$x.us.us.us.preheader"
                                        ##   in Loop: Header=BB61_4 Depth=2
	movq	-120(%rsp), %r14                ## 8-byte Reload
	movq	24(%rsp), %rbp                  ## 8-byte Reload
	movq	-24(%rsp), %rsi                 ## 8-byte Reload
	movq	(%rsp), %r15                    ## 8-byte Reload
	movq	-16(%rsp), %rbx                 ## 8-byte Reload
	movq	8(%rsp), %r11                   ## 8-byte Reload
	movq	-8(%rsp), %r9                   ## 8-byte Reload
	movq	16(%rsp), %r13                  ## 8-byte Reload
	xorl	%r10d, %r10d
	movq	-32(%rsp), %rcx                 ## 8-byte Reload
	.p2align	4, 0x90
LBB61_7:                                ## %"for filter1_im_0_d_def__.s1.r962$x.us.us.us"
                                        ##   Parent Loop BB61_3 Depth=1
                                        ##     Parent Loop BB61_4 Depth=2
                                        ## =>    This Inner Loop Header: Depth=3
	vmovss	(%r14,%r12), %xmm1              ## xmm1 = mem[0],zero,zero,zero
	vfmadd132ss	(%rbp,%r12), %xmm0, %xmm1 ## xmm1 = (xmm1 * mem) + xmm0
	vmovss	(%rsi,%r12), %xmm0              ## xmm0 = mem[0],zero,zero,zero
	vfmadd132ss	(%r15,%r12), %xmm1, %xmm0 ## xmm0 = (xmm0 * mem) + xmm1
	vmovss	(%rbx,%r12), %xmm1              ## xmm1 = mem[0],zero,zero,zero
	vfmadd132ss	(%r11,%r12), %xmm0, %xmm1 ## xmm1 = (xmm1 * mem) + xmm0
	vmovss	(%r9,%r12), %xmm0               ## xmm0 = mem[0],zero,zero,zero
	vfmadd132ss	(%r13,%r12), %xmm1, %xmm0 ## xmm0 = (xmm0 * mem) + xmm1
	addq	$4, %r10
	addq	%rdx, %r13
	addq	%rax, %r9
	addq	%rdx, %r11
	addq	%rax, %rbx
	addq	%rdx, %r15
	addq	%rax, %rsi
	addq	%rdx, %rbp
	addq	%rax, %r14
	cmpq	%r10, %rcx
	jne	LBB61_7
LBB61_8:                                ## %"end for filter1_im_0_d_def__.s1.r962$x.loopexit.us.us.us.unr-lcssa"
                                        ##   in Loop: Header=BB61_4 Depth=2
	cmpq	$0, -112(%rsp)                  ## 8-byte Folded Reload
	movq	80(%rsp), %rcx                  ## 8-byte Reload
	movq	72(%rsp), %r9                   ## 8-byte Reload
	je	LBB61_11
## %bb.9:                               ## %"for filter1_im_0_d_def__.s1.r962$x.us.us.us.epil.preheader"
                                        ##   in Loop: Header=BB61_4 Depth=2
	movq	32(%rsp), %rsi                  ## 8-byte Reload
	imulq	%r10, %rsi
	leaq	(%r9,%rsi,4), %rsi
	imulq	40(%rsp), %r10                  ## 8-byte Folded Reload
	leaq	(%rcx,%r10,4), %rbp
	movq	-112(%rsp), %rbx                ## 8-byte Reload
	.p2align	4, 0x90
LBB61_10:                               ## %"for filter1_im_0_d_def__.s1.r962$x.us.us.us.epil"
                                        ##   Parent Loop BB61_3 Depth=1
                                        ##     Parent Loop BB61_4 Depth=2
                                        ## =>    This Inner Loop Header: Depth=3
	vmovss	(%rbp), %xmm1                   ## xmm1 = mem[0],zero,zero,zero
	vfmadd231ss	(%rsi), %xmm1, %xmm0    ## xmm0 = (xmm1 * mem) + xmm0
	addq	%r8, %rsi
	addq	%rdi, %rbp
	decq	%rbx
	jne	LBB61_10
	jmp	LBB61_11
LBB61_13:                               ## %destructor_block
	xorl	%eax, %eax
	addq	$88, %rsp
	popq	%rbx
	popq	%r12
	popq	%r13
	popq	%r14
	popq	%r15
	popq	%rbp
	retq
                                        ## -- End function
	.p2align	4, 0x90                         ## -- Begin function train_cost_model.par_for.filter1_im_0_d_def__.s2.v12
_train_cost_model.par_for.filter1_im_0_d_def__.s2.v12: ## @train_cost_model.par_for.filter1_im_0_d_def__.s2.v12
## %bb.0:                               ## %entry
	pushq	%r15
	pushq	%r14
	pushq	%rbx
	movl	(%rdx), %edi
	testl	%edi, %edi
	jle	LBB62_58
## %bb.1:                               ## %"for filter1_im_0_d_def__.s2.r1158$y.us.preheader"
	movq	8(%rdx), %r8
	movq	24(%rdx), %r14
	movq	40(%rdx), %r11
	movl	%edi, %eax
	imull	%esi, %eax
	movslq	%eax, %r9
	shll	$5, %esi
	movslq	%esi, %r10
	vmovss	(%r14,%r10,4), %xmm1            ## xmm1 = mem[0],zero,zero,zero
	cmpl	$17, %edi
	jae	LBB62_3
## %bb.2:
	xorl	%edx, %edx
	jmp	LBB62_6
LBB62_3:                                ## %vector.ph
	movl	%edi, %eax
	andl	$15, %eax
	testq	%rax, %rax
	movl	$16, %ecx
	cmovneq	%rax, %rcx
	movq	%rdi, %rdx
	subq	%rcx, %rdx
	vxorps	%xmm0, %xmm0, %xmm0
	vblendps	$1, %xmm1, %xmm0, %xmm1         ## xmm1 = xmm1[0],xmm0[1,2,3]
	leaq	384(%r11), %rax
	leaq	(%r8,%r9,4), %rsi
	addq	$48, %rsi
	xorl	%ecx, %ecx
	vxorps	%xmm2, %xmm2, %xmm2
	vxorps	%xmm3, %xmm3, %xmm3
	.p2align	4, 0x90
LBB62_4:                                ## %vector.body
                                        ## =>This Inner Loop Header: Depth=1
	vmovups	-288(%rax), %xmm4
	vunpcklpd	-320(%rax), %xmm4, %xmm4 ## xmm4 = xmm4[0],mem[0]
	vmovups	-384(%rax), %xmm5
	vunpcklps	-352(%rax), %xmm5, %xmm5 ## xmm5 = xmm5[0],mem[0],xmm5[1],mem[1]
	vmovups	-160(%rax), %xmm6
	vunpcklpd	-192(%rax), %xmm6, %xmm6 ## xmm6 = xmm6[0],mem[0]
	vmovups	-256(%rax), %xmm7
	vunpcklps	-224(%rax), %xmm7, %xmm7 ## xmm7 = xmm7[0],mem[0],xmm7[1],mem[1]
	vshufps	$36, %xmm4, %xmm5, %xmm8        ## xmm8 = xmm5[0,1],xmm4[2,0]
	vmovups	-32(%rax), %xmm5
	vunpcklpd	-64(%rax), %xmm5, %xmm5 ## xmm5 = xmm5[0],mem[0]
	vshufps	$36, %xmm6, %xmm7, %xmm6        ## xmm6 = xmm7[0,1],xmm6[2,0]
	vmovups	-128(%rax), %xmm7
	vunpcklps	-96(%rax), %xmm7, %xmm7 ## xmm7 = xmm7[0],mem[0],xmm7[1],mem[1]
	vshufps	$36, %xmm5, %xmm7, %xmm5        ## xmm5 = xmm7[0,1],xmm5[2,0]
	vmovups	96(%rax), %xmm7
	vunpcklpd	64(%rax), %xmm7, %xmm7  ## xmm7 = xmm7[0],mem[0]
	vmovups	(%rax), %xmm4
	vunpcklps	32(%rax), %xmm4, %xmm4  ## xmm4 = xmm4[0],mem[0],xmm4[1],mem[1]
	vshufps	$36, %xmm7, %xmm4, %xmm4        ## xmm4 = xmm4[0,1],xmm7[2,0]
	vfmadd231ps	-48(%rsi,%rcx,4), %xmm8, %xmm1 ## xmm1 = (xmm8 * mem) + xmm1
	vfmadd231ps	-32(%rsi,%rcx,4), %xmm6, %xmm0 ## xmm0 = (xmm6 * mem) + xmm0
	vfmadd231ps	-16(%rsi,%rcx,4), %xmm5, %xmm2 ## xmm2 = (xmm5 * mem) + xmm2
	vfmadd231ps	(%rsi,%rcx,4), %xmm4, %xmm3 ## xmm3 = (xmm4 * mem) + xmm3
	addq	$16, %rcx
	addq	$512, %rax                      ## imm = 0x200
	cmpq	%rcx, %rdx
	jne	LBB62_4
## %bb.5:                               ## %middle.block
	vaddps	%xmm1, %xmm0, %xmm0
	vaddps	%xmm0, %xmm2, %xmm0
	vaddps	%xmm0, %xmm3, %xmm0
	vpermilpd	$1, %xmm0, %xmm1        ## xmm1 = xmm0[1,0]
	vaddps	%xmm1, %xmm0, %xmm0
	vmovshdup	%xmm0, %xmm1            ## xmm1 = xmm0[1,1,3,3]
	vaddss	%xmm1, %xmm0, %xmm1
LBB62_6:                                ## %"for filter1_im_0_d_def__.s2.r1158$x.us.preheader"
	movq	%rdx, %rax
	shlq	$5, %rax
	addq	%r11, %rax
	leaq	(%r8,%r9,4), %rsi
	.p2align	4, 0x90
LBB62_7:                                ## %"for filter1_im_0_d_def__.s2.r1158$x.us"
                                        ## =>This Inner Loop Header: Depth=1
	vmovss	(%rsi,%rdx,4), %xmm0            ## xmm0 = mem[0],zero,zero,zero
	vfmadd231ss	(%rax), %xmm0, %xmm1    ## xmm1 = (xmm0 * mem) + xmm1
	incq	%rdx
	addq	$32, %rax
	cmpq	%rdx, %rdi
	jne	LBB62_7
## %bb.8:                               ## %"end for filter1_im_0_d_def__.s2.r1158$x.loopexit.us"
	vmovss	%xmm1, (%r14,%r10,4)
	movq	%r10, %r15
	orq	$1, %r15
	vmovss	(%r14,%r15,4), %xmm1            ## xmm1 = mem[0],zero,zero,zero
	cmpl	$16, %edi
	ja	LBB62_14
## %bb.9:
	xorl	%eax, %eax
	jmp	LBB62_10
LBB62_14:                               ## %vector.ph38
	movl	%edi, %eax
	andl	$15, %eax
	testq	%rax, %rax
	movl	$16, %ecx
	cmovneq	%rax, %rcx
	movq	%rdi, %rax
	subq	%rcx, %rax
	vxorps	%xmm0, %xmm0, %xmm0
	vblendps	$1, %xmm1, %xmm0, %xmm1         ## xmm1 = xmm1[0],xmm0[1,2,3]
	leaq	388(%r11), %rbx
	leaq	(%r8,%r9,4), %rcx
	addq	$48, %rcx
	xorl	%edx, %edx
	vxorps	%xmm2, %xmm2, %xmm2
	vxorps	%xmm3, %xmm3, %xmm3
	.p2align	4, 0x90
LBB62_15:                               ## %vector.body36
                                        ## =>This Inner Loop Header: Depth=1
	vmovups	-288(%rbx), %xmm4
	vunpcklpd	-320(%rbx), %xmm4, %xmm4 ## xmm4 = xmm4[0],mem[0]
	vmovups	-384(%rbx), %xmm5
	vunpcklps	-352(%rbx), %xmm5, %xmm5 ## xmm5 = xmm5[0],mem[0],xmm5[1],mem[1]
	vmovups	-160(%rbx), %xmm6
	vunpcklpd	-192(%rbx), %xmm6, %xmm6 ## xmm6 = xmm6[0],mem[0]
	vmovups	-256(%rbx), %xmm7
	vunpcklps	-224(%rbx), %xmm7, %xmm7 ## xmm7 = xmm7[0],mem[0],xmm7[1],mem[1]
	vshufps	$36, %xmm4, %xmm5, %xmm8        ## xmm8 = xmm5[0,1],xmm4[2,0]
	vmovups	-32(%rbx), %xmm5
	vunpcklpd	-64(%rbx), %xmm5, %xmm5 ## xmm5 = xmm5[0],mem[0]
	vshufps	$36, %xmm6, %xmm7, %xmm6        ## xmm6 = xmm7[0,1],xmm6[2,0]
	vmovups	-128(%rbx), %xmm7
	vunpcklps	-96(%rbx), %xmm7, %xmm7 ## xmm7 = xmm7[0],mem[0],xmm7[1],mem[1]
	vshufps	$36, %xmm5, %xmm7, %xmm5        ## xmm5 = xmm7[0,1],xmm5[2,0]
	vmovups	96(%rbx), %xmm7
	vunpcklpd	64(%rbx), %xmm7, %xmm7  ## xmm7 = xmm7[0],mem[0]
	vmovups	(%rbx), %xmm4
	vunpcklps	32(%rbx), %xmm4, %xmm4  ## xmm4 = xmm4[0],mem[0],xmm4[1],mem[1]
	vshufps	$36, %xmm7, %xmm4, %xmm4        ## xmm4 = xmm4[0,1],xmm7[2,0]
	vfmadd231ps	-48(%rcx,%rdx,4), %xmm8, %xmm1 ## xmm1 = (xmm8 * mem) + xmm1
	vfmadd231ps	-32(%rcx,%rdx,4), %xmm6, %xmm0 ## xmm0 = (xmm6 * mem) + xmm0
	vfmadd231ps	-16(%rcx,%rdx,4), %xmm5, %xmm2 ## xmm2 = (xmm5 * mem) + xmm2
	vfmadd231ps	(%rcx,%rdx,4), %xmm4, %xmm3 ## xmm3 = (xmm4 * mem) + xmm3
	addq	$16, %rdx
	addq	$512, %rbx                      ## imm = 0x200
	cmpq	%rdx, %rax
	jne	LBB62_15
## %bb.16:                              ## %middle.block34
	vaddps	%xmm1, %xmm0, %xmm0
	vaddps	%xmm0, %xmm2, %xmm0
	vaddps	%xmm0, %xmm3, %xmm0
	vpermilpd	$1, %xmm0, %xmm1        ## xmm1 = xmm0[1,0]
	vaddps	%xmm1, %xmm0, %xmm0
	vmovshdup	%xmm0, %xmm1            ## xmm1 = xmm0[1,1,3,3]
	vaddss	%xmm1, %xmm0, %xmm1
LBB62_10:                               ## %"for filter1_im_0_d_def__.s2.r1158$x.us.1.preheader"
	movq	%rax, %rcx
	shlq	$5, %rcx
	addq	%r11, %rcx
	addq	$4, %rcx
	.p2align	4, 0x90
LBB62_11:                               ## %"for filter1_im_0_d_def__.s2.r1158$x.us.1"
                                        ## =>This Inner Loop Header: Depth=1
	vmovss	(%rsi,%rax,4), %xmm0            ## xmm0 = mem[0],zero,zero,zero
	vfmadd231ss	(%rcx), %xmm0, %xmm1    ## xmm1 = (xmm0 * mem) + xmm1
	incq	%rax
	addq	$32, %rcx
	cmpq	%rax, %rdi
	jne	LBB62_11
## %bb.12:                              ## %"end for filter1_im_0_d_def__.s2.r1158$x.loopexit.us.1"
	vmovss	%xmm1, (%r14,%r15,4)
	movq	%r10, %r15
	orq	$2, %r15
	vmovss	(%r14,%r15,4), %xmm1            ## xmm1 = mem[0],zero,zero,zero
	cmpl	$17, %edi
	jae	LBB62_17
## %bb.13:
	xorl	%eax, %eax
	jmp	LBB62_20
LBB62_17:                               ## %vector.ph69
	movl	%edi, %eax
	andl	$15, %eax
	testq	%rax, %rax
	movl	$16, %ecx
	cmovneq	%rax, %rcx
	movq	%rdi, %rax
	subq	%rcx, %rax
	vxorps	%xmm0, %xmm0, %xmm0
	vblendps	$1, %xmm1, %xmm0, %xmm1         ## xmm1 = xmm1[0],xmm0[1,2,3]
	leaq	392(%r11), %rbx
	leaq	(%r8,%r9,4), %rcx
	addq	$48, %rcx
	xorl	%edx, %edx
	vxorps	%xmm2, %xmm2, %xmm2
	vxorps	%xmm3, %xmm3, %xmm3
	.p2align	4, 0x90
LBB62_18:                               ## %vector.body67
                                        ## =>This Inner Loop Header: Depth=1
	vmovups	-288(%rbx), %xmm4
	vunpcklpd	-320(%rbx), %xmm4, %xmm4 ## xmm4 = xmm4[0],mem[0]
	vmovups	-384(%rbx), %xmm5
	vunpcklps	-352(%rbx), %xmm5, %xmm5 ## xmm5 = xmm5[0],mem[0],xmm5[1],mem[1]
	vmovups	-160(%rbx), %xmm6
	vunpcklpd	-192(%rbx), %xmm6, %xmm6 ## xmm6 = xmm6[0],mem[0]
	vmovups	-256(%rbx), %xmm7
	vunpcklps	-224(%rbx), %xmm7, %xmm7 ## xmm7 = xmm7[0],mem[0],xmm7[1],mem[1]
	vshufps	$36, %xmm4, %xmm5, %xmm8        ## xmm8 = xmm5[0,1],xmm4[2,0]
	vmovups	-32(%rbx), %xmm5
	vunpcklpd	-64(%rbx), %xmm5, %xmm5 ## xmm5 = xmm5[0],mem[0]
	vshufps	$36, %xmm6, %xmm7, %xmm6        ## xmm6 = xmm7[0,1],xmm6[2,0]
	vmovups	-128(%rbx), %xmm7
	vunpcklps	-96(%rbx), %xmm7, %xmm7 ## xmm7 = xmm7[0],mem[0],xmm7[1],mem[1]
	vshufps	$36, %xmm5, %xmm7, %xmm5        ## xmm5 = xmm7[0,1],xmm5[2,0]
	vmovups	96(%rbx), %xmm7
	vunpcklpd	64(%rbx), %xmm7, %xmm7  ## xmm7 = xmm7[0],mem[0]
	vmovups	(%rbx), %xmm4
	vunpcklps	32(%rbx), %xmm4, %xmm4  ## xmm4 = xmm4[0],mem[0],xmm4[1],mem[1]
	vshufps	$36, %xmm7, %xmm4, %xmm4        ## xmm4 = xmm4[0,1],xmm7[2,0]
	vfmadd231ps	-48(%rcx,%rdx,4), %xmm8, %xmm1 ## xmm1 = (xmm8 * mem) + xmm1
	vfmadd231ps	-32(%rcx,%rdx,4), %xmm6, %xmm0 ## xmm0 = (xmm6 * mem) + xmm0
	vfmadd231ps	-16(%rcx,%rdx,4), %xmm5, %xmm2 ## xmm2 = (xmm5 * mem) + xmm2
	vfmadd231ps	(%rcx,%rdx,4), %xmm4, %xmm3 ## xmm3 = (xmm4 * mem) + xmm3
	addq	$16, %rdx
	addq	$512, %rbx                      ## imm = 0x200
	cmpq	%rdx, %rax
	jne	LBB62_18
## %bb.19:                              ## %middle.block65
	vaddps	%xmm1, %xmm0, %xmm0
	vaddps	%xmm0, %xmm2, %xmm0
	vaddps	%xmm0, %xmm3, %xmm0
	vpermilpd	$1, %xmm0, %xmm1        ## xmm1 = xmm0[1,0]
	vaddps	%xmm1, %xmm0, %xmm0
	vmovshdup	%xmm0, %xmm1            ## xmm1 = xmm0[1,1,3,3]
	vaddss	%xmm1, %xmm0, %xmm1
LBB62_20:                               ## %"for filter1_im_0_d_def__.s2.r1158$x.us.2.preheader"
	movq	%rax, %rcx
	shlq	$5, %rcx
	addq	%r11, %rcx
	addq	$8, %rcx
	.p2align	4, 0x90
LBB62_21:                               ## %"for filter1_im_0_d_def__.s2.r1158$x.us.2"
                                        ## =>This Inner Loop Header: Depth=1
	vmovss	(%rsi,%rax,4), %xmm0            ## xmm0 = mem[0],zero,zero,zero
	vfmadd231ss	(%rcx), %xmm0, %xmm1    ## xmm1 = (xmm0 * mem) + xmm1
	incq	%rax
	addq	$32, %rcx
	cmpq	%rax, %rdi
	jne	LBB62_21
## %bb.22:                              ## %"end for filter1_im_0_d_def__.s2.r1158$x.loopexit.us.2"
	vmovss	%xmm1, (%r14,%r15,4)
	movq	%r10, %r15
	orq	$3, %r15
	vmovss	(%r14,%r15,4), %xmm1            ## xmm1 = mem[0],zero,zero,zero
	cmpl	$17, %edi
	jae	LBB62_24
## %bb.23:
	xorl	%eax, %eax
	jmp	LBB62_27
LBB62_24:                               ## %vector.ph100
	movl	%edi, %eax
	andl	$15, %eax
	testq	%rax, %rax
	movl	$16, %ecx
	cmovneq	%rax, %rcx
	movq	%rdi, %rax
	subq	%rcx, %rax
	vxorps	%xmm0, %xmm0, %xmm0
	vblendps	$1, %xmm1, %xmm0, %xmm1         ## xmm1 = xmm1[0],xmm0[1,2,3]
	leaq	396(%r11), %rbx
	leaq	(%r8,%r9,4), %rcx
	addq	$48, %rcx
	xorl	%edx, %edx
	vxorps	%xmm2, %xmm2, %xmm2
	vxorps	%xmm3, %xmm3, %xmm3
	.p2align	4, 0x90
LBB62_25:                               ## %vector.body98
                                        ## =>This Inner Loop Header: Depth=1
	vmovups	-288(%rbx), %xmm4
	vunpcklpd	-320(%rbx), %xmm4, %xmm4 ## xmm4 = xmm4[0],mem[0]
	vmovups	-384(%rbx), %xmm5
	vunpcklps	-352(%rbx), %xmm5, %xmm5 ## xmm5 = xmm5[0],mem[0],xmm5[1],mem[1]
	vmovups	-160(%rbx), %xmm6
	vunpcklpd	-192(%rbx), %xmm6, %xmm6 ## xmm6 = xmm6[0],mem[0]
	vmovups	-256(%rbx), %xmm7
	vunpcklps	-224(%rbx), %xmm7, %xmm7 ## xmm7 = xmm7[0],mem[0],xmm7[1],mem[1]
	vshufps	$36, %xmm4, %xmm5, %xmm8        ## xmm8 = xmm5[0,1],xmm4[2,0]
	vmovups	-32(%rbx), %xmm5
	vunpcklpd	-64(%rbx), %xmm5, %xmm5 ## xmm5 = xmm5[0],mem[0]
	vshufps	$36, %xmm6, %xmm7, %xmm6        ## xmm6 = xmm7[0,1],xmm6[2,0]
	vmovups	-128(%rbx), %xmm7
	vunpcklps	-96(%rbx), %xmm7, %xmm7 ## xmm7 = xmm7[0],mem[0],xmm7[1],mem[1]
	vshufps	$36, %xmm5, %xmm7, %xmm5        ## xmm5 = xmm7[0,1],xmm5[2,0]
	vmovups	96(%rbx), %xmm7
	vunpcklpd	64(%rbx), %xmm7, %xmm7  ## xmm7 = xmm7[0],mem[0]
	vmovups	(%rbx), %xmm4
	vunpcklps	32(%rbx), %xmm4, %xmm4  ## xmm4 = xmm4[0],mem[0],xmm4[1],mem[1]
	vshufps	$36, %xmm7, %xmm4, %xmm4        ## xmm4 = xmm4[0,1],xmm7[2,0]
	vfmadd231ps	-48(%rcx,%rdx,4), %xmm8, %xmm1 ## xmm1 = (xmm8 * mem) + xmm1
	vfmadd231ps	-32(%rcx,%rdx,4), %xmm6, %xmm0 ## xmm0 = (xmm6 * mem) + xmm0
	vfmadd231ps	-16(%rcx,%rdx,4), %xmm5, %xmm2 ## xmm2 = (xmm5 * mem) + xmm2
	vfmadd231ps	(%rcx,%rdx,4), %xmm4, %xmm3 ## xmm3 = (xmm4 * mem) + xmm3
	addq	$16, %rdx
	addq	$512, %rbx                      ## imm = 0x200
	cmpq	%rdx, %rax
	jne	LBB62_25
## %bb.26:                              ## %middle.block96
	vaddps	%xmm1, %xmm0, %xmm0
	vaddps	%xmm0, %xmm2, %xmm0
	vaddps	%xmm0, %xmm3, %xmm0
	vpermilpd	$1, %xmm0, %xmm1        ## xmm1 = xmm0[1,0]
	vaddps	%xmm1, %xmm0, %xmm0
	vmovshdup	%xmm0, %xmm1            ## xmm1 = xmm0[1,1,3,3]
	vaddss	%xmm1, %xmm0, %xmm1
LBB62_27:                               ## %"for filter1_im_0_d_def__.s2.r1158$x.us.3.preheader"
	movq	%rax, %rcx
	shlq	$5, %rcx
	addq	%r11, %rcx
	addq	$12, %rcx
	.p2align	4, 0x90
LBB62_28:                               ## %"for filter1_im_0_d_def__.s2.r1158$x.us.3"
                                        ## =>This Inner Loop Header: Depth=1
	vmovss	(%rsi,%rax,4), %xmm0            ## xmm0 = mem[0],zero,zero,zero
	vfmadd231ss	(%rcx), %xmm0, %xmm1    ## xmm1 = (xmm0 * mem) + xmm1
	incq	%rax
	addq	$32, %rcx
	cmpq	%rax, %rdi
	jne	LBB62_28
## %bb.29:                              ## %"end for filter1_im_0_d_def__.s2.r1158$x.loopexit.us.3"
	vmovss	%xmm1, (%r14,%r15,4)
	movq	%r10, %r15
	orq	$4, %r15
	vmovss	(%r14,%r15,4), %xmm1            ## xmm1 = mem[0],zero,zero,zero
	cmpl	$17, %edi
	jae	LBB62_31
## %bb.30:
	xorl	%eax, %eax
	jmp	LBB62_34
LBB62_31:                               ## %vector.ph131
	movl	%edi, %eax
	andl	$15, %eax
	testq	%rax, %rax
	movl	$16, %ecx
	cmovneq	%rax, %rcx
	movq	%rdi, %rax
	subq	%rcx, %rax
	vxorps	%xmm0, %xmm0, %xmm0
	vblendps	$1, %xmm1, %xmm0, %xmm1         ## xmm1 = xmm1[0],xmm0[1,2,3]
	leaq	400(%r11), %rbx
	leaq	(%r8,%r9,4), %rcx
	addq	$48, %rcx
	xorl	%edx, %edx
	vxorps	%xmm2, %xmm2, %xmm2
	vxorps	%xmm3, %xmm3, %xmm3
	.p2align	4, 0x90
LBB62_32:                               ## %vector.body129
                                        ## =>This Inner Loop Header: Depth=1
	vmovups	-288(%rbx), %xmm4
	vunpcklpd	-320(%rbx), %xmm4, %xmm4 ## xmm4 = xmm4[0],mem[0]
	vmovups	-384(%rbx), %xmm5
	vunpcklps	-352(%rbx), %xmm5, %xmm5 ## xmm5 = xmm5[0],mem[0],xmm5[1],mem[1]
	vmovups	-160(%rbx), %xmm6
	vunpcklpd	-192(%rbx), %xmm6, %xmm6 ## xmm6 = xmm6[0],mem[0]
	vmovups	-256(%rbx), %xmm7
	vunpcklps	-224(%rbx), %xmm7, %xmm7 ## xmm7 = xmm7[0],mem[0],xmm7[1],mem[1]
	vshufps	$36, %xmm4, %xmm5, %xmm8        ## xmm8 = xmm5[0,1],xmm4[2,0]
	vmovups	-32(%rbx), %xmm5
	vunpcklpd	-64(%rbx), %xmm5, %xmm5 ## xmm5 = xmm5[0],mem[0]
	vshufps	$36, %xmm6, %xmm7, %xmm6        ## xmm6 = xmm7[0,1],xmm6[2,0]
	vmovups	-128(%rbx), %xmm7
	vunpcklps	-96(%rbx), %xmm7, %xmm7 ## xmm7 = xmm7[0],mem[0],xmm7[1],mem[1]
	vshufps	$36, %xmm5, %xmm7, %xmm5        ## xmm5 = xmm7[0,1],xmm5[2,0]
	vmovups	96(%rbx), %xmm7
	vunpcklpd	64(%rbx), %xmm7, %xmm7  ## xmm7 = xmm7[0],mem[0]
	vmovups	(%rbx), %xmm4
	vunpcklps	32(%rbx), %xmm4, %xmm4  ## xmm4 = xmm4[0],mem[0],xmm4[1],mem[1]
	vshufps	$36, %xmm7, %xmm4, %xmm4        ## xmm4 = xmm4[0,1],xmm7[2,0]
	vfmadd231ps	-48(%rcx,%rdx,4), %xmm8, %xmm1 ## xmm1 = (xmm8 * mem) + xmm1
	vfmadd231ps	-32(%rcx,%rdx,4), %xmm6, %xmm0 ## xmm0 = (xmm6 * mem) + xmm0
	vfmadd231ps	-16(%rcx,%rdx,4), %xmm5, %xmm2 ## xmm2 = (xmm5 * mem) + xmm2
	vfmadd231ps	(%rcx,%rdx,4), %xmm4, %xmm3 ## xmm3 = (xmm4 * mem) + xmm3
	addq	$16, %rdx
	addq	$512, %rbx                      ## imm = 0x200
	cmpq	%rdx, %rax
	jne	LBB62_32
## %bb.33:                              ## %middle.block127
	vaddps	%xmm1, %xmm0, %xmm0
	vaddps	%xmm0, %xmm2, %xmm0
	vaddps	%xmm0, %xmm3, %xmm0
	vpermilpd	$1, %xmm0, %xmm1        ## xmm1 = xmm0[1,0]
	vaddps	%xmm1, %xmm0, %xmm0
	vmovshdup	%xmm0, %xmm1            ## xmm1 = xmm0[1,1,3,3]
	vaddss	%xmm1, %xmm0, %xmm1
LBB62_34:                               ## %"for filter1_im_0_d_def__.s2.r1158$x.us.4.preheader"
	movq	%rax, %rcx
	shlq	$5, %rcx
	addq	%r11, %rcx
	addq	$16, %rcx
	.p2align	4, 0x90
LBB62_35:                               ## %"for filter1_im_0_d_def__.s2.r1158$x.us.4"
                                        ## =>This Inner Loop Header: Depth=1
	vmovss	(%rsi,%rax,4), %xmm0            ## xmm0 = mem[0],zero,zero,zero
	vfmadd231ss	(%rcx), %xmm0, %xmm1    ## xmm1 = (xmm0 * mem) + xmm1
	incq	%rax
	addq	$32, %rcx
	cmpq	%rax, %rdi
	jne	LBB62_35
## %bb.36:                              ## %"end for filter1_im_0_d_def__.s2.r1158$x.loopexit.us.4"
	vmovss	%xmm1, (%r14,%r15,4)
	movq	%r10, %r15
	orq	$5, %r15
	vmovss	(%r14,%r15,4), %xmm1            ## xmm1 = mem[0],zero,zero,zero
	cmpl	$17, %edi
	jae	LBB62_38
## %bb.37:
	xorl	%eax, %eax
	jmp	LBB62_41
LBB62_38:                               ## %vector.ph162
	movl	%edi, %eax
	andl	$15, %eax
	testq	%rax, %rax
	movl	$16, %ecx
	cmovneq	%rax, %rcx
	movq	%rdi, %rax
	subq	%rcx, %rax
	vxorps	%xmm0, %xmm0, %xmm0
	vblendps	$1, %xmm1, %xmm0, %xmm1         ## xmm1 = xmm1[0],xmm0[1,2,3]
	leaq	404(%r11), %rbx
	leaq	(%r8,%r9,4), %rcx
	addq	$48, %rcx
	xorl	%edx, %edx
	vxorps	%xmm2, %xmm2, %xmm2
	vxorps	%xmm3, %xmm3, %xmm3
	.p2align	4, 0x90
LBB62_39:                               ## %vector.body160
                                        ## =>This Inner Loop Header: Depth=1
	vmovups	-288(%rbx), %xmm4
	vunpcklpd	-320(%rbx), %xmm4, %xmm4 ## xmm4 = xmm4[0],mem[0]
	vmovups	-384(%rbx), %xmm5
	vunpcklps	-352(%rbx), %xmm5, %xmm5 ## xmm5 = xmm5[0],mem[0],xmm5[1],mem[1]
	vmovups	-160(%rbx), %xmm6
	vunpcklpd	-192(%rbx), %xmm6, %xmm6 ## xmm6 = xmm6[0],mem[0]
	vmovups	-256(%rbx), %xmm7
	vunpcklps	-224(%rbx), %xmm7, %xmm7 ## xmm7 = xmm7[0],mem[0],xmm7[1],mem[1]
	vshufps	$36, %xmm4, %xmm5, %xmm8        ## xmm8 = xmm5[0,1],xmm4[2,0]
	vmovups	-32(%rbx), %xmm5
	vunpcklpd	-64(%rbx), %xmm5, %xmm5 ## xmm5 = xmm5[0],mem[0]
	vshufps	$36, %xmm6, %xmm7, %xmm6        ## xmm6 = xmm7[0,1],xmm6[2,0]
	vmovups	-128(%rbx), %xmm7
	vunpcklps	-96(%rbx), %xmm7, %xmm7 ## xmm7 = xmm7[0],mem[0],xmm7[1],mem[1]
	vshufps	$36, %xmm5, %xmm7, %xmm5        ## xmm5 = xmm7[0,1],xmm5[2,0]
	vmovups	96(%rbx), %xmm7
	vunpcklpd	64(%rbx), %xmm7, %xmm7  ## xmm7 = xmm7[0],mem[0]
	vmovups	(%rbx), %xmm4
	vunpcklps	32(%rbx), %xmm4, %xmm4  ## xmm4 = xmm4[0],mem[0],xmm4[1],mem[1]
	vshufps	$36, %xmm7, %xmm4, %xmm4        ## xmm4 = xmm4[0,1],xmm7[2,0]
	vfmadd231ps	-48(%rcx,%rdx,4), %xmm8, %xmm1 ## xmm1 = (xmm8 * mem) + xmm1
	vfmadd231ps	-32(%rcx,%rdx,4), %xmm6, %xmm0 ## xmm0 = (xmm6 * mem) + xmm0
	vfmadd231ps	-16(%rcx,%rdx,4), %xmm5, %xmm2 ## xmm2 = (xmm5 * mem) + xmm2
	vfmadd231ps	(%rcx,%rdx,4), %xmm4, %xmm3 ## xmm3 = (xmm4 * mem) + xmm3
	addq	$16, %rdx
	addq	$512, %rbx                      ## imm = 0x200
	cmpq	%rdx, %rax
	jne	LBB62_39
## %bb.40:                              ## %middle.block158
	vaddps	%xmm1, %xmm0, %xmm0
	vaddps	%xmm0, %xmm2, %xmm0
	vaddps	%xmm0, %xmm3, %xmm0
	vpermilpd	$1, %xmm0, %xmm1        ## xmm1 = xmm0[1,0]
	vaddps	%xmm1, %xmm0, %xmm0
	vmovshdup	%xmm0, %xmm1            ## xmm1 = xmm0[1,1,3,3]
	vaddss	%xmm1, %xmm0, %xmm1
LBB62_41:                               ## %"for filter1_im_0_d_def__.s2.r1158$x.us.5.preheader"
	movq	%rax, %rcx
	shlq	$5, %rcx
	addq	%r11, %rcx
	addq	$20, %rcx
	.p2align	4, 0x90
LBB62_42:                               ## %"for filter1_im_0_d_def__.s2.r1158$x.us.5"
                                        ## =>This Inner Loop Header: Depth=1
	vmovss	(%rsi,%rax,4), %xmm0            ## xmm0 = mem[0],zero,zero,zero
	vfmadd231ss	(%rcx), %xmm0, %xmm1    ## xmm1 = (xmm0 * mem) + xmm1
	incq	%rax
	addq	$32, %rcx
	cmpq	%rax, %rdi
	jne	LBB62_42
## %bb.43:                              ## %"end for filter1_im_0_d_def__.s2.r1158$x.loopexit.us.5"
	vmovss	%xmm1, (%r14,%r15,4)
	movq	%r10, %r15
	orq	$6, %r15
	vmovss	(%r14,%r15,4), %xmm1            ## xmm1 = mem[0],zero,zero,zero
	cmpl	$17, %edi
	jae	LBB62_45
## %bb.44:
	xorl	%eax, %eax
	jmp	LBB62_48
LBB62_45:                               ## %vector.ph193
	movl	%edi, %eax
	andl	$15, %eax
	testq	%rax, %rax
	movl	$16, %ecx
	cmovneq	%rax, %rcx
	movq	%rdi, %rax
	subq	%rcx, %rax
	vxorps	%xmm0, %xmm0, %xmm0
	vblendps	$1, %xmm1, %xmm0, %xmm1         ## xmm1 = xmm1[0],xmm0[1,2,3]
	leaq	408(%r11), %rbx
	leaq	(%r8,%r9,4), %rcx
	addq	$48, %rcx
	xorl	%edx, %edx
	vxorps	%xmm2, %xmm2, %xmm2
	vxorps	%xmm3, %xmm3, %xmm3
	.p2align	4, 0x90
LBB62_46:                               ## %vector.body191
                                        ## =>This Inner Loop Header: Depth=1
	vmovups	-288(%rbx), %xmm4
	vunpcklpd	-320(%rbx), %xmm4, %xmm4 ## xmm4 = xmm4[0],mem[0]
	vmovups	-384(%rbx), %xmm5
	vunpcklps	-352(%rbx), %xmm5, %xmm5 ## xmm5 = xmm5[0],mem[0],xmm5[1],mem[1]
	vmovups	-160(%rbx), %xmm6
	vunpcklpd	-192(%rbx), %xmm6, %xmm6 ## xmm6 = xmm6[0],mem[0]
	vmovups	-256(%rbx), %xmm7
	vunpcklps	-224(%rbx), %xmm7, %xmm7 ## xmm7 = xmm7[0],mem[0],xmm7[1],mem[1]
	vshufps	$36, %xmm4, %xmm5, %xmm8        ## xmm8 = xmm5[0,1],xmm4[2,0]
	vmovups	-32(%rbx), %xmm5
	vunpcklpd	-64(%rbx), %xmm5, %xmm5 ## xmm5 = xmm5[0],mem[0]
	vshufps	$36, %xmm6, %xmm7, %xmm6        ## xmm6 = xmm7[0,1],xmm6[2,0]
	vmovups	-128(%rbx), %xmm7
	vunpcklps	-96(%rbx), %xmm7, %xmm7 ## xmm7 = xmm7[0],mem[0],xmm7[1],mem[1]
	vshufps	$36, %xmm5, %xmm7, %xmm5        ## xmm5 = xmm7[0,1],xmm5[2,0]
	vmovups	96(%rbx), %xmm7
	vunpcklpd	64(%rbx), %xmm7, %xmm7  ## xmm7 = xmm7[0],mem[0]
	vmovups	(%rbx), %xmm4
	vunpcklps	32(%rbx), %xmm4, %xmm4  ## xmm4 = xmm4[0],mem[0],xmm4[1],mem[1]
	vshufps	$36, %xmm7, %xmm4, %xmm4        ## xmm4 = xmm4[0,1],xmm7[2,0]
	vfmadd231ps	-48(%rcx,%rdx,4), %xmm8, %xmm1 ## xmm1 = (xmm8 * mem) + xmm1
	vfmadd231ps	-32(%rcx,%rdx,4), %xmm6, %xmm0 ## xmm0 = (xmm6 * mem) + xmm0
	vfmadd231ps	-16(%rcx,%rdx,4), %xmm5, %xmm2 ## xmm2 = (xmm5 * mem) + xmm2
	vfmadd231ps	(%rcx,%rdx,4), %xmm4, %xmm3 ## xmm3 = (xmm4 * mem) + xmm3
	addq	$16, %rdx
	addq	$512, %rbx                      ## imm = 0x200
	cmpq	%rdx, %rax
	jne	LBB62_46
## %bb.47:                              ## %middle.block189
	vaddps	%xmm1, %xmm0, %xmm0
	vaddps	%xmm0, %xmm2, %xmm0
	vaddps	%xmm0, %xmm3, %xmm0
	vpermilpd	$1, %xmm0, %xmm1        ## xmm1 = xmm0[1,0]
	vaddps	%xmm1, %xmm0, %xmm0
	vmovshdup	%xmm0, %xmm1            ## xmm1 = xmm0[1,1,3,3]
	vaddss	%xmm1, %xmm0, %xmm1
LBB62_48:                               ## %"for filter1_im_0_d_def__.s2.r1158$x.us.6.preheader"
	movq	%rax, %rcx
	shlq	$5, %rcx
	addq	%r11, %rcx
	addq	$24, %rcx
	.p2align	4, 0x90
LBB62_49:                               ## %"for filter1_im_0_d_def__.s2.r1158$x.us.6"
                                        ## =>This Inner Loop Header: Depth=1
	vmovss	(%rsi,%rax,4), %xmm0            ## xmm0 = mem[0],zero,zero,zero
	vfmadd231ss	(%rcx), %xmm0, %xmm1    ## xmm1 = (xmm0 * mem) + xmm1
	incq	%rax
	addq	$32, %rcx
	cmpq	%rax, %rdi
	jne	LBB62_49
## %bb.50:                              ## %"end for filter1_im_0_d_def__.s2.r1158$x.loopexit.us.6"
	vmovss	%xmm1, (%r14,%r15,4)
	orq	$7, %r10
	vmovss	(%r14,%r10,4), %xmm1            ## xmm1 = mem[0],zero,zero,zero
	cmpl	$17, %edi
	jae	LBB62_52
## %bb.51:
	xorl	%eax, %eax
	jmp	LBB62_55
LBB62_52:                               ## %vector.ph224
	movl	%edi, %eax
	andl	$15, %eax
	testq	%rax, %rax
	movl	$16, %ecx
	cmovneq	%rax, %rcx
	movq	%rdi, %rax
	subq	%rcx, %rax
	vxorps	%xmm0, %xmm0, %xmm0
	vblendps	$1, %xmm1, %xmm0, %xmm1         ## xmm1 = xmm1[0],xmm0[1,2,3]
	leaq	412(%r11), %rdx
	leaq	(%r8,%r9,4), %rcx
	addq	$48, %rcx
	xorl	%ebx, %ebx
	vxorps	%xmm2, %xmm2, %xmm2
	vxorps	%xmm3, %xmm3, %xmm3
	.p2align	4, 0x90
LBB62_53:                               ## %vector.body222
                                        ## =>This Inner Loop Header: Depth=1
	vmovups	-288(%rdx), %xmm4
	vunpcklpd	-320(%rdx), %xmm4, %xmm4 ## xmm4 = xmm4[0],mem[0]
	vmovups	-384(%rdx), %xmm5
	vunpcklps	-352(%rdx), %xmm5, %xmm5 ## xmm5 = xmm5[0],mem[0],xmm5[1],mem[1]
	vmovups	-160(%rdx), %xmm6
	vunpcklpd	-192(%rdx), %xmm6, %xmm6 ## xmm6 = xmm6[0],mem[0]
	vmovups	-256(%rdx), %xmm7
	vunpcklps	-224(%rdx), %xmm7, %xmm7 ## xmm7 = xmm7[0],mem[0],xmm7[1],mem[1]
	vshufps	$36, %xmm4, %xmm5, %xmm8        ## xmm8 = xmm5[0,1],xmm4[2,0]
	vmovups	-32(%rdx), %xmm5
	vunpcklpd	-64(%rdx), %xmm5, %xmm5 ## xmm5 = xmm5[0],mem[0]
	vshufps	$36, %xmm6, %xmm7, %xmm6        ## xmm6 = xmm7[0,1],xmm6[2,0]
	vmovups	-128(%rdx), %xmm7
	vunpcklps	-96(%rdx), %xmm7, %xmm7 ## xmm7 = xmm7[0],mem[0],xmm7[1],mem[1]
	vshufps	$36, %xmm5, %xmm7, %xmm5        ## xmm5 = xmm7[0,1],xmm5[2,0]
	vmovups	96(%rdx), %xmm7
	vunpcklpd	64(%rdx), %xmm7, %xmm7  ## xmm7 = xmm7[0],mem[0]
	vmovups	(%rdx), %xmm4
	vunpcklps	32(%rdx), %xmm4, %xmm4  ## xmm4 = xmm4[0],mem[0],xmm4[1],mem[1]
	vshufps	$36, %xmm7, %xmm4, %xmm4        ## xmm4 = xmm4[0,1],xmm7[2,0]
	vfmadd231ps	-48(%rcx,%rbx,4), %xmm8, %xmm1 ## xmm1 = (xmm8 * mem) + xmm1
	vfmadd231ps	-32(%rcx,%rbx,4), %xmm6, %xmm0 ## xmm0 = (xmm6 * mem) + xmm0
	vfmadd231ps	-16(%rcx,%rbx,4), %xmm5, %xmm2 ## xmm2 = (xmm5 * mem) + xmm2
	vfmadd231ps	(%rcx,%rbx,4), %xmm4, %xmm3 ## xmm3 = (xmm4 * mem) + xmm3
	addq	$16, %rbx
	addq	$512, %rdx                      ## imm = 0x200
	cmpq	%rbx, %rax
	jne	LBB62_53
## %bb.54:                              ## %middle.block220
	vaddps	%xmm1, %xmm0, %xmm0
	vaddps	%xmm0, %xmm2, %xmm0
	vaddps	%xmm0, %xmm3, %xmm0
	vpermilpd	$1, %xmm0, %xmm1        ## xmm1 = xmm0[1,0]
	vaddps	%xmm1, %xmm0, %xmm0
	vmovshdup	%xmm0, %xmm1            ## xmm1 = xmm0[1,1,3,3]
	vaddss	%xmm1, %xmm0, %xmm1
LBB62_55:                               ## %"for filter1_im_0_d_def__.s2.r1158$x.us.7.preheader"
	movq	%rax, %rcx
	shlq	$5, %rcx
	addq	%r11, %rcx
	addq	$28, %rcx
	.p2align	4, 0x90
LBB62_56:                               ## %"for filter1_im_0_d_def__.s2.r1158$x.us.7"
                                        ## =>This Inner Loop Header: Depth=1
	vmovss	(%rsi,%rax,4), %xmm0            ## xmm0 = mem[0],zero,zero,zero
	vfmadd231ss	(%rcx), %xmm0, %xmm1    ## xmm1 = (xmm0 * mem) + xmm1
	incq	%rax
	addq	$32, %rcx
	cmpq	%rax, %rdi
	jne	LBB62_56
## %bb.57:                              ## %"end for filter1_im_0_d_def__.s2.r1158$x.loopexit.us.7"
	vmovss	%xmm1, (%r14,%r10,4)
LBB62_58:                               ## %destructor_block
	xorl	%eax, %eax
	popq	%rbx
	popq	%r14
	popq	%r15
	retq
                                        ## -- End function
	.p2align	4, 0x90                         ## -- Begin function train_cost_model.par_for.updated_filter1.s1.v245.v245.v245
_train_cost_model.par_for.updated_filter1.s1.v245.v245.v245: ## @train_cost_model.par_for.updated_filter1.s1.v245.v245.v245
## %bb.0:                               ## %entry
	pushq	%r15
	pushq	%r14
	pushq	%rbx
                                        ## kill: def $esi killed $esi def $rsi
	movslq	(%rdx), %r8
	movslq	4(%rdx), %r9
	movq	8(%rdx), %rax
	movl	%esi, %ecx
	andl	$-2, %ecx
	leal	(%rsi,%rsi), %r10d
	andl	$2, %r10d
	movslq	%ecx, %rdi
	movq	%rdi, %r15
	imulq	%r9, %r15
	addq	%r8, %r15
	leaq	(%r15,%r10,8), %r14
	leaq	1(%r10), %r11
	movq	%r10, %rcx
	shlq	$8, %rcx
	leaq	(%rcx,%rdi), %rbx
	vmovss	(%rax,%rbx,4), %xmm0            ## xmm0 = mem[0],zero,zero,zero
	vmovss	512(%rax,%rbx,4), %xmm1         ## xmm1 = mem[0],zero,zero,zero
	vinsertps	$16, 128(%rax,%rbx,4), %xmm0, %xmm0 ## xmm0 = xmm0[0],mem[0],xmm0[2,3]
	vinsertps	$32, 256(%rax,%rbx,4), %xmm0, %xmm0 ## xmm0 = xmm0[0,1],mem[0],xmm0[3]
	vinsertps	$48, 384(%rax,%rbx,4), %xmm0, %xmm0 ## xmm0 = xmm0[0,1,2],mem[0]
	vinsertps	$16, 640(%rax,%rbx,4), %xmm1, %xmm1 ## xmm1 = xmm1[0],mem[0],xmm1[2,3]
	vinsertps	$32, 768(%rax,%rbx,4), %xmm1, %xmm1 ## xmm1 = xmm1[0,1],mem[0],xmm1[3]
	vinsertps	$48, 896(%rax,%rbx,4), %xmm1, %xmm1 ## xmm1 = xmm1[0,1,2],mem[0]
	leaq	8(%r15,%r10,8), %rbx
	movq	24(%rdx), %rdx
	vmovups	%xmm1, 16(%rdx,%r14,4)
	vmovups	%xmm0, (%rdx,%r14,4)
	shlq	$8, %r11
	addq	%r11, %rdi
	vmovss	(%rax,%rdi,4), %xmm0            ## xmm0 = mem[0],zero,zero,zero
	vmovss	512(%rax,%rdi,4), %xmm1         ## xmm1 = mem[0],zero,zero,zero
	vinsertps	$16, 128(%rax,%rdi,4), %xmm0, %xmm0 ## xmm0 = xmm0[0],mem[0],xmm0[2,3]
	vinsertps	$32, 256(%rax,%rdi,4), %xmm0, %xmm0 ## xmm0 = xmm0[0,1],mem[0],xmm0[3]
	vinsertps	$48, 384(%rax,%rdi,4), %xmm0, %xmm0 ## xmm0 = xmm0[0,1,2],mem[0]
	vinsertps	$16, 640(%rax,%rdi,4), %xmm1, %xmm1 ## xmm1 = xmm1[0],mem[0],xmm1[2,3]
	vinsertps	$32, 768(%rax,%rdi,4), %xmm1, %xmm1 ## xmm1 = xmm1[0,1],mem[0],xmm1[3]
	vinsertps	$48, 896(%rax,%rdi,4), %xmm1, %xmm1 ## xmm1 = xmm1[0,1,2],mem[0]
	vmovups	%xmm1, 16(%rdx,%rbx,4)
	orl	$1, %esi
	movslq	%esi, %rsi
	imulq	%rsi, %r9
	addq	%r8, %r9
	leaq	(%r9,%r10,8), %rdi
	vmovups	%xmm0, (%rdx,%rbx,4)
	addq	%rsi, %rcx
	vmovss	(%rax,%rcx,4), %xmm0            ## xmm0 = mem[0],zero,zero,zero
	vmovss	512(%rax,%rcx,4), %xmm1         ## xmm1 = mem[0],zero,zero,zero
	vinsertps	$16, 128(%rax,%rcx,4), %xmm0, %xmm0 ## xmm0 = xmm0[0],mem[0],xmm0[2,3]
	vinsertps	$32, 256(%rax,%rcx,4), %xmm0, %xmm0 ## xmm0 = xmm0[0,1],mem[0],xmm0[3]
	vinsertps	$48, 384(%rax,%rcx,4), %xmm0, %xmm0 ## xmm0 = xmm0[0,1,2],mem[0]
	vinsertps	$16, 640(%rax,%rcx,4), %xmm1, %xmm1 ## xmm1 = xmm1[0],mem[0],xmm1[2,3]
	vinsertps	$32, 768(%rax,%rcx,4), %xmm1, %xmm1 ## xmm1 = xmm1[0,1],mem[0],xmm1[3]
	vinsertps	$48, 896(%rax,%rcx,4), %xmm1, %xmm1 ## xmm1 = xmm1[0,1,2],mem[0]
	vmovups	%xmm1, 16(%rdx,%rdi,4)
	leaq	(%r9,%r10,8), %rcx
	addq	$8, %rcx
	vmovups	%xmm0, (%rdx,%rdi,4)
	addq	%rsi, %r11
	vmovss	(%rax,%r11,4), %xmm0            ## xmm0 = mem[0],zero,zero,zero
	vmovss	512(%rax,%r11,4), %xmm1         ## xmm1 = mem[0],zero,zero,zero
	vinsertps	$16, 128(%rax,%r11,4), %xmm0, %xmm0 ## xmm0 = xmm0[0],mem[0],xmm0[2,3]
	vinsertps	$32, 256(%rax,%r11,4), %xmm0, %xmm0 ## xmm0 = xmm0[0,1],mem[0],xmm0[3]
	vinsertps	$48, 384(%rax,%r11,4), %xmm0, %xmm0 ## xmm0 = xmm0[0,1,2],mem[0]
	vinsertps	$16, 640(%rax,%r11,4), %xmm1, %xmm1 ## xmm1 = xmm1[0],mem[0],xmm1[2,3]
	vinsertps	$32, 768(%rax,%r11,4), %xmm1, %xmm1 ## xmm1 = xmm1[0,1],mem[0],xmm1[3]
	vinsertps	$48, 896(%rax,%r11,4), %xmm1, %xmm1 ## xmm1 = xmm1[0,1,2],mem[0]
	vmovups	%xmm1, 16(%rdx,%rcx,4)
	vmovups	%xmm0, (%rdx,%rcx,4)
	xorl	%eax, %eax
	popq	%rbx
	popq	%r14
	popq	%r15
	retq
                                        ## -- End function
	.section	__TEXT,__literal4,4byte_literals
	.p2align	2                               ## -- Begin function train_cost_model.par_for.updated_filter1.s2.v245.v245.v245
LCPI64_0:
	.long	0x3f666666                      ## float 0.899999976
LCPI64_1:
	.long	0x3dcccccd                      ## float 0.100000001
	.section	__TEXT,__text,regular,pure_instructions
	.p2align	4, 0x90
_train_cost_model.par_for.updated_filter1.s2.v245.v245.v245: ## @train_cost_model.par_for.updated_filter1.s2.v245.v245.v245
## %bb.0:                               ## %entry
                                        ## kill: def $esi killed $esi def $rsi
	movl	(%rdx), %r8d
	movslq	4(%rdx), %rax
	movq	8(%rdx), %rcx
	movl	%esi, %edx
	andl	$-2, %edx
	leal	(%rsi,%rsi), %edi
	andl	$2, %edi
	leaq	(%rax,%rax,2), %r9
	imull	%r8d, %edx
	movslq	%edx, %r10
	leaq	(%r10,%rdi,8), %rdx
	vbroadcastss	LCPI64_0(%rip), %ymm0   ## ymm0 = [8.99999976E-1,8.99999976E-1,8.99999976E-1,8.99999976E-1,8.99999976E-1,8.99999976E-1,8.99999976E-1,8.99999976E-1]
	leaq	(%r9,%rdx), %r11
	addq	%rax, %rdx
	vbroadcastss	LCPI64_1(%rip), %ymm1   ## ymm1 = [1.00000001E-1,1.00000001E-1,1.00000001E-1,1.00000001E-1,1.00000001E-1,1.00000001E-1,1.00000001E-1,1.00000001E-1]
	vmulps	(%rcx,%r11,4), %ymm1, %ymm2
	vfmadd231ps	(%rcx,%rdx,4), %ymm0, %ymm2 ## ymm2 = (ymm0 * mem) + ymm2
	vmovups	%ymm2, (%rcx,%rdx,4)
	leaq	(%r10,%rdi,8), %rdx
	addq	$8, %rdx
	leaq	(%r9,%rdx), %r10
	addq	%rax, %rdx
	vmulps	(%rcx,%r10,4), %ymm1, %ymm2
	vfmadd231ps	(%rcx,%rdx,4), %ymm0, %ymm2 ## ymm2 = (ymm0 * mem) + ymm2
	vmovups	%ymm2, (%rcx,%rdx,4)
	orl	$1, %esi
	imull	%r8d, %esi
	movslq	%esi, %rdx
	leaq	(%rdx,%rdi,8), %rsi
	leaq	(%r9,%rsi), %r8
	addq	%rax, %rsi
	vmulps	(%rcx,%r8,4), %ymm1, %ymm2
	vfmadd231ps	(%rcx,%rsi,4), %ymm0, %ymm2 ## ymm2 = (ymm0 * mem) + ymm2
	vmovups	%ymm2, (%rcx,%rsi,4)
	leaq	(%rdx,%rdi,8), %rdx
	addq	$8, %rdx
	addq	%rdx, %rax
	addq	%rdx, %r9
	vmulps	(%rcx,%r9,4), %ymm1, %ymm1
	vfmadd231ps	(%rcx,%rax,4), %ymm0, %ymm1 ## ymm1 = (ymm0 * mem) + ymm1
	vmovups	%ymm1, (%rcx,%rax,4)
	xorl	%eax, %eax
	vzeroupper
	retq
                                        ## -- End function
	.section	__TEXT,__literal4,4byte_literals
	.p2align	2                               ## -- Begin function train_cost_model.par_for.updated_filter1.s3.v245.v245.v245
LCPI65_0:
	.long	0x3f7fbe77                      ## float 0.999000012
LCPI65_1:
	.long	0x3a83126f                      ## float 0.00100000005
	.section	__TEXT,__text,regular,pure_instructions
	.p2align	4, 0x90
_train_cost_model.par_for.updated_filter1.s3.v245.v245.v245: ## @train_cost_model.par_for.updated_filter1.s3.v245.v245.v245
## %bb.0:                               ## %entry
	pushq	%rbx
                                        ## kill: def $esi killed $esi def $rsi
	movl	(%rdx), %r8d
	movl	4(%rdx), %r10d
	movl	8(%rdx), %r9d
	movq	16(%rdx), %rax
	movl	%esi, %edx
	andl	$-2, %edx
	leal	(%rsi,%rsi), %ecx
	andl	$2, %ecx
	imull	%r9d, %edx
	leal	(%rdx,%r10), %edi
	addl	%r8d, %edx
	movslq	%edi, %r11
	movslq	%edx, %rdx
	leaq	(%rdx,%rcx,8), %rdi
	leaq	(%r11,%rcx,8), %rbx
	vmovups	(%rax,%rbx,4), %ymm1
	vbroadcastss	LCPI65_0(%rip), %ymm0   ## ymm0 = [9.99000012E-1,9.99000012E-1,9.99000012E-1,9.99000012E-1,9.99000012E-1,9.99000012E-1,9.99000012E-1,9.99000012E-1]
	vmulps	%ymm1, %ymm1, %ymm2
	vbroadcastss	LCPI65_1(%rip), %ymm1   ## ymm1 = [1.00000005E-3,1.00000005E-3,1.00000005E-3,1.00000005E-3,1.00000005E-3,1.00000005E-3,1.00000005E-3,1.00000005E-3]
	vmulps	%ymm1, %ymm2, %ymm2
	vfmadd231ps	(%rax,%rdi,4), %ymm0, %ymm2 ## ymm2 = (ymm0 * mem) + ymm2
	vmovups	%ymm2, (%rax,%rdi,4)
	leaq	8(%rdx,%rcx,8), %rdx
	leaq	8(%r11,%rcx,8), %rdi
	vmovups	(%rax,%rdi,4), %ymm2
	vmulps	%ymm2, %ymm2, %ymm2
	vmulps	%ymm1, %ymm2, %ymm2
	vfmadd231ps	(%rax,%rdx,4), %ymm0, %ymm2 ## ymm2 = (ymm0 * mem) + ymm2
	vmovups	%ymm2, (%rax,%rdx,4)
	orl	$1, %esi
	imull	%r9d, %esi
	addl	%esi, %r10d
	addl	%r8d, %esi
	movslq	%r10d, %rdx
	movslq	%esi, %rsi
	leaq	(%rsi,%rcx,8), %rdi
	leaq	(%rdx,%rcx,8), %rbx
	vmovups	(%rax,%rbx,4), %ymm2
	vmulps	%ymm2, %ymm2, %ymm2
	vmulps	%ymm1, %ymm2, %ymm2
	vfmadd231ps	(%rax,%rdi,4), %ymm0, %ymm2 ## ymm2 = (ymm0 * mem) + ymm2
	vmovups	%ymm2, (%rax,%rdi,4)
	leaq	8(%rsi,%rcx,8), %rsi
	leaq	8(%rdx,%rcx,8), %rcx
	vmovups	(%rax,%rcx,4), %ymm2
	vmulps	%ymm2, %ymm2, %ymm2
	vmulps	%ymm1, %ymm2, %ymm1
	vfmadd231ps	(%rax,%rsi,4), %ymm0, %ymm1 ## ymm1 = (ymm0 * mem) + ymm1
	vmovups	%ymm1, (%rax,%rsi,4)
	xorl	%eax, %eax
	popq	%rbx
	vzeroupper
	retq
                                        ## -- End function
	.section	__TEXT,__literal4,4byte_literals
	.p2align	2                               ## -- Begin function train_cost_model.par_for.updated_filter1.s4.v245.v245.v245
LCPI66_0:
	.long	0x3727c5ac                      ## float 9.99999974E-6
	.section	__TEXT,__text,regular,pure_instructions
	.p2align	4, 0x90
_train_cost_model.par_for.updated_filter1.s4.v245.v245.v245: ## @train_cost_model.par_for.updated_filter1.s4.v245.v245.v245
## %bb.0:                               ## %entry
	pushq	%r14
	pushq	%rbx
                                        ## kill: def $esi killed $esi def $rsi
	movl	(%rdx), %r9d
	vmovss	4(%rdx), %xmm0                  ## xmm0 = mem[0],zero,zero,zero
	movl	16(%rdx), %r10d
	movslq	20(%rdx), %r11
	movq	24(%rdx), %r8
	movq	40(%rdx), %rax
	movl	%esi, %edi
	andl	$-2, %edi
	leal	(%rsi,%rsi), %ecx
	andl	$2, %ecx
	vmulss	8(%rdx), %xmm0, %xmm0
	vbroadcastss	%xmm0, %ymm1
	vbroadcastss	12(%rdx), %ymm0
	movl	%edi, %edx
	imull	%r10d, %edx
	imull	%r9d, %edi
	movslq	%edi, %r14
	movslq	%edx, %rdx
	leaq	(%rdx,%rcx,8), %rbx
	leaq	(%rbx,%r11), %rdi
	vmulps	(%rax,%rdi,4), %ymm1, %ymm3
	leaq	(%rbx,%r11,2), %rdi
	vmulps	(%rax,%rdi,4), %ymm0, %ymm2
	leaq	(%r14,%rcx,8), %rdi
	vsqrtps	%ymm2, %ymm4
	vbroadcastss	LCPI66_0(%rip), %ymm2   ## ymm2 = [9.99999974E-6,9.99999974E-6,9.99999974E-6,9.99999974E-6,9.99999974E-6,9.99999974E-6,9.99999974E-6,9.99999974E-6]
	vaddps	%ymm2, %ymm4, %ymm4
	vdivps	%ymm4, %ymm3, %ymm3
	vmovups	(%r8,%rdi,4), %ymm4
	vsubps	%ymm3, %ymm4, %ymm3
	vmovups	%ymm3, (%rax,%rbx,4)
	leaq	8(%rdx,%rcx,8), %rdx
	leaq	(%rdx,%r11), %rdi
	vmulps	(%rax,%rdi,4), %ymm1, %ymm3
	leaq	(%rdx,%r11,2), %rdi
	vmulps	(%rax,%rdi,4), %ymm0, %ymm4
	leaq	8(%r14,%rcx,8), %rdi
	vsqrtps	%ymm4, %ymm4
	vaddps	%ymm2, %ymm4, %ymm4
	vdivps	%ymm4, %ymm3, %ymm3
	vmovups	(%r8,%rdi,4), %ymm4
	vsubps	%ymm3, %ymm4, %ymm3
	vmovups	%ymm3, (%rax,%rdx,4)
	orl	$1, %esi
	imull	%esi, %r10d
	imull	%r9d, %esi
	movslq	%r10d, %rdx
	leaq	(%rdx,%rcx,8), %rdi
	leaq	(%rdi,%r11), %rbx
	vmulps	(%rax,%rbx,4), %ymm1, %ymm3
	movslq	%esi, %rsi
	leaq	(%rdi,%r11,2), %rbx
	vmulps	(%rax,%rbx,4), %ymm0, %ymm4
	leaq	(%rsi,%rcx,8), %rbx
	vsqrtps	%ymm4, %ymm4
	vaddps	%ymm2, %ymm4, %ymm4
	vdivps	%ymm4, %ymm3, %ymm3
	vmovups	(%r8,%rbx,4), %ymm4
	vsubps	%ymm3, %ymm4, %ymm3
	vmovups	%ymm3, (%rax,%rdi,4)
	leaq	8(%rdx,%rcx,8), %rdx
	leaq	(%rdx,%r11), %rdi
	vmulps	(%rax,%rdi,4), %ymm1, %ymm1
	leaq	8(%rsi,%rcx,8), %rcx
	leaq	(%rdx,%r11,2), %rsi
	vmulps	(%rax,%rsi,4), %ymm0, %ymm0
	vmovups	(%r8,%rcx,4), %ymm3
	vsqrtps	%ymm0, %ymm0
	vaddps	%ymm2, %ymm0, %ymm0
	vdivps	%ymm0, %ymm1, %ymm0
	vsubps	%ymm0, %ymm3, %ymm0
	vmovups	%ymm0, (%rax,%rdx,4)
	xorl	%eax, %eax
	popq	%rbx
	popq	%r14
	vzeroupper
	retq
                                        ## -- End function
	.p2align	4, 0x90                         ## -- Begin function train_cost_model.par_for.updated_bias1.s1.v248.v248
_train_cost_model.par_for.updated_bias1.s1.v248.v248: ## @train_cost_model.par_for.updated_bias1.s1.v248.v248
## %bb.0:                               ## %entry
	pushq	%r14
	pushq	%rbx
	movl	(%rdx), %edi
	movslq	4(%rdx), %r8
	movq	24(%rdx), %r9
	testl	%edi, %edi
	jle	LBB67_1
## %bb.2:                               ## %"for bias1_im_0_d_def__.s1.r1178$x.preheader"
	movq	8(%rdx), %rax
	movl	%esi, %ecx
	imull	%edi, %ecx
	shll	$3, %ecx
	movslq	%ecx, %rcx
	leaq	(%rax,%rcx,4), %r10
	leaq	(,%rdi,4), %r11
	vxorps	%xmm0, %xmm0, %xmm0
	xorl	%ecx, %ecx
	.p2align	4, 0x90
LBB67_3:                                ## %"for bias1_im_0_d_def__.s1.r1178$x"
                                        ## =>This Inner Loop Header: Depth=1
	leaq	(%r10,%rcx,4), %r14
	leaq	(%r14,%r11), %rax
	addq	%r11, %rax
	leaq	(%rax,%r11), %rbx
	addq	%r11, %rbx
	leaq	(%rbx,%r11), %rdx
	vmovss	(%rax,%rdi,8), %xmm1            ## xmm1 = mem[0],zero,zero,zero
	vinsertps	$16, (%rbx,%rdi,4), %xmm1, %xmm1 ## xmm1 = xmm1[0],mem[0],xmm1[2,3]
	vinsertps	$32, (%rdx,%rdi,4), %xmm1, %xmm1 ## xmm1 = xmm1[0,1],mem[0],xmm1[3]
	addq	%r11, %rdx
	vinsertps	$48, (%rdx,%rdi,4), %xmm1, %xmm1 ## xmm1 = xmm1[0,1,2],mem[0]
	vmovss	(%r10,%rcx,4), %xmm2            ## xmm2 = mem[0],zero,zero,zero
	vinsertps	$16, (%r14,%rdi,4), %xmm2, %xmm2 ## xmm2 = xmm2[0],mem[0],xmm2[2,3]
	vinsertps	$32, (%r14,%rdi,8), %xmm2, %xmm2 ## xmm2 = xmm2[0,1],mem[0],xmm2[3]
	vinsertps	$48, (%rax,%rdi,4), %xmm2, %xmm2 ## xmm2 = xmm2[0,1,2],mem[0]
	vinsertf128	$1, %xmm1, %ymm2, %ymm1
	vaddps	%ymm1, %ymm0, %ymm0
	incq	%rcx
	cmpq	%rcx, %rdi
	jne	LBB67_3
LBB67_4:                                ## %"consume bias1_im_0_d_def__"
	movslq	%esi, %rax
	leaq	(%r8,%rax,8), %rax
	vmovups	%ymm0, (%r9,%rax,4)
	xorl	%eax, %eax
	popq	%rbx
	popq	%r14
	vzeroupper
	retq
LBB67_1:
	vxorps	%xmm0, %xmm0, %xmm0
	jmp	LBB67_4
                                        ## -- End function
	.section	__TEXT,__literal4,4byte_literals
	.p2align	2                               ## -- Begin function train_cost_model.par_for.updated_bias1.s2.v248.v248
LCPI68_0:
	.long	0x3f666666                      ## float 0.899999976
LCPI68_1:
	.long	0x3dcccccd                      ## float 0.100000001
	.section	__TEXT,__text,regular,pure_instructions
	.p2align	4, 0x90
_train_cost_model.par_for.updated_bias1.s2.v248.v248: ## @train_cost_model.par_for.updated_bias1.s2.v248.v248
## %bb.0:                               ## %entry
	movslq	(%rdx), %rax
	movq	8(%rdx), %rcx
	movslq	%esi, %rdx
	leaq	(%rax,%rdx,8), %rsi
	vbroadcastss	LCPI68_0(%rip), %ymm0   ## ymm0 = [8.99999976E-1,8.99999976E-1,8.99999976E-1,8.99999976E-1,8.99999976E-1,8.99999976E-1,8.99999976E-1,8.99999976E-1]
	leaq	(%rax,%rax,2), %rax
	leaq	(%rax,%rdx,8), %rax
	vbroadcastss	LCPI68_1(%rip), %ymm1   ## ymm1 = [1.00000001E-1,1.00000001E-1,1.00000001E-1,1.00000001E-1,1.00000001E-1,1.00000001E-1,1.00000001E-1,1.00000001E-1]
	vmulps	(%rcx,%rax,4), %ymm1, %ymm1
	vfmadd231ps	(%rcx,%rsi,4), %ymm0, %ymm1 ## ymm1 = (ymm0 * mem) + ymm1
	vmovups	%ymm1, (%rcx,%rsi,4)
	xorl	%eax, %eax
	vzeroupper
	retq
                                        ## -- End function
	.section	__TEXT,__literal4,4byte_literals
	.p2align	2                               ## -- Begin function train_cost_model.par_for.updated_bias1.s3.v248.v248
LCPI69_0:
	.long	0x3f7fbe77                      ## float 0.999000012
LCPI69_1:
	.long	0x3a83126f                      ## float 0.00100000005
	.section	__TEXT,__text,regular,pure_instructions
	.p2align	4, 0x90
_train_cost_model.par_for.updated_bias1.s3.v248.v248: ## @train_cost_model.par_for.updated_bias1.s3.v248.v248
## %bb.0:                               ## %entry
	movslq	(%rdx), %rax
	movq	8(%rdx), %rcx
	leaq	(%rax,%rax,2), %rdx
	movslq	%esi, %rsi
	leaq	(%rdx,%rsi,8), %rdx
	vmovups	(%rcx,%rdx,4), %ymm0
	leaq	(%rax,%rsi,4), %rax
	vbroadcastss	LCPI69_0(%rip), %ymm1   ## ymm1 = [9.99000012E-1,9.99000012E-1,9.99000012E-1,9.99000012E-1,9.99000012E-1,9.99000012E-1,9.99000012E-1,9.99000012E-1]
	vmulps	%ymm0, %ymm0, %ymm0
	vbroadcastss	LCPI69_1(%rip), %ymm2   ## ymm2 = [1.00000005E-3,1.00000005E-3,1.00000005E-3,1.00000005E-3,1.00000005E-3,1.00000005E-3,1.00000005E-3,1.00000005E-3]
	vmulps	%ymm2, %ymm0, %ymm0
	vfmadd231ps	(%rcx,%rax,8), %ymm1, %ymm0 ## ymm0 = (ymm1 * mem) + ymm0
	vmovups	%ymm0, (%rcx,%rax,8)
	xorl	%eax, %eax
	vzeroupper
	retq
                                        ## -- End function
	.section	__TEXT,__literal4,4byte_literals
	.p2align	2                               ## -- Begin function train_cost_model.par_for.updated_bias1.s4.v248.v248
LCPI70_0:
	.long	0xb727c5ac                      ## float -9.99999974E-6
	.section	__TEXT,__text,regular,pure_instructions
	.p2align	4, 0x90
_train_cost_model.par_for.updated_bias1.s4.v248.v248: ## @train_cost_model.par_for.updated_bias1.s4.v248.v248
## %bb.0:                               ## %entry
	vmovss	(%rdx), %xmm0                   ## xmm0 = mem[0],zero,zero,zero
	movslq	12(%rdx), %rax
	movq	32(%rdx), %rcx
	movslq	%esi, %rsi
	vmulss	4(%rdx), %xmm0, %xmm0
	leaq	(%rax,%rsi,8), %rdi
	vbroadcastss	%xmm0, %ymm0
	vmulps	(%rcx,%rdi,4), %ymm0, %ymm0
	leaq	(%rax,%rsi,4), %rax
	vbroadcastss	8(%rdx), %ymm1
	vmulps	(%rcx,%rax,8), %ymm1, %ymm1
	vsqrtps	%ymm1, %ymm1
	vbroadcastss	LCPI70_0(%rip), %ymm2   ## ymm2 = [-9.99999974E-6,-9.99999974E-6,-9.99999974E-6,-9.99999974E-6,-9.99999974E-6,-9.99999974E-6,-9.99999974E-6,-9.99999974E-6]
	vsubps	%ymm1, %ymm2, %ymm1
	vdivps	%ymm1, %ymm0, %ymm0
	movq	16(%rdx), %rax
	shlq	$5, %rsi
	vaddps	(%rax,%rsi), %ymm0, %ymm0
	vmovups	%ymm0, (%rcx,%rsi)
	xorl	%eax, %eax
	vzeroupper
	retq
                                        ## -- End function
	.p2align	4, 0x90                         ## -- Begin function train_cost_model.par_for.prediction_output.s0.n.n
_train_cost_model.par_for.prediction_output.s0.n.n: ## @train_cost_model.par_for.prediction_output.s0.n.n
## %bb.0:                               ## %entry
	pushq	%r14
	pushq	%rbx
	movslq	8(%rdx), %rax
	movq	16(%rdx), %r11
	movq	32(%rdx), %r10
	cmpl	%esi, 4(%rdx)
	jle	LBB71_2
## %bb.1:                               ## %then_bb
	movslq	%esi, %rcx
	leaq	(%rax,%rcx,8), %rax
	vmovups	(%r11,%rax,4), %ymm0
	shlq	$5, %rcx
	vmovups	%ymm0, (%r10,%rcx)
	jmp	LBB71_15
LBB71_2:                                ## %next_bb
	movl	(%rdx), %ecx
	shll	$3, %esi
	subl	%esi, %ecx
	jle	LBB71_15
## %bb.3:                               ## %"for prediction_output.s0.n.ni.preheader"
	cmpl	$8, %ecx
	movl	$8, %edx
	cmovll	%ecx, %edx
	addl	%esi, %eax
	movslq	%eax, %r8
	movslq	%esi, %r9
	cmpl	$32, %edx
	jae	LBB71_5
## %bb.4:
	xorl	%esi, %esi
	jmp	LBB71_13
LBB71_5:                                ## %vector.ph
	movl	%edx, %esi
	andl	$-32, %esi
	leaq	-32(%rsi), %rax
	movq	%rax, %rdi
	shrq	$5, %rdi
	incq	%rdi
	movl	%edi, %r14d
	andl	$3, %r14d
	cmpq	$96, %rax
	jae	LBB71_7
## %bb.6:
	xorl	%ecx, %ecx
	jmp	LBB71_9
LBB71_7:                                ## %vector.ph.new
	leaq	(%r10,%r9,4), %rbx
	addq	$480, %rbx                      ## imm = 0x1E0
	leaq	(%r11,%r8,4), %rax
	addq	$480, %rax                      ## imm = 0x1E0
	andq	$-4, %rdi
	negq	%rdi
	xorl	%ecx, %ecx
	.p2align	4, 0x90
LBB71_8:                                ## %vector.body
                                        ## =>This Inner Loop Header: Depth=1
	vmovups	-480(%rax,%rcx,4), %ymm0
	vmovups	-448(%rax,%rcx,4), %ymm1
	vmovups	-416(%rax,%rcx,4), %ymm2
	vmovups	-384(%rax,%rcx,4), %ymm3
	vmovups	%ymm0, -480(%rbx,%rcx,4)
	vmovups	%ymm1, -448(%rbx,%rcx,4)
	vmovups	%ymm2, -416(%rbx,%rcx,4)
	vmovups	%ymm3, -384(%rbx,%rcx,4)
	vmovups	-352(%rax,%rcx,4), %ymm0
	vmovups	-320(%rax,%rcx,4), %ymm1
	vmovups	-288(%rax,%rcx,4), %ymm2
	vmovups	-256(%rax,%rcx,4), %ymm3
	vmovups	%ymm0, -352(%rbx,%rcx,4)
	vmovups	%ymm1, -320(%rbx,%rcx,4)
	vmovups	%ymm2, -288(%rbx,%rcx,4)
	vmovups	%ymm3, -256(%rbx,%rcx,4)
	vmovups	-224(%rax,%rcx,4), %ymm0
	vmovups	-192(%rax,%rcx,4), %ymm1
	vmovups	-160(%rax,%rcx,4), %ymm2
	vmovups	-128(%rax,%rcx,4), %ymm3
	vmovups	%ymm0, -224(%rbx,%rcx,4)
	vmovups	%ymm1, -192(%rbx,%rcx,4)
	vmovups	%ymm2, -160(%rbx,%rcx,4)
	vmovups	%ymm3, -128(%rbx,%rcx,4)
	vmovups	-96(%rax,%rcx,4), %ymm0
	vmovups	-64(%rax,%rcx,4), %ymm1
	vmovups	-32(%rax,%rcx,4), %ymm2
	vmovups	(%rax,%rcx,4), %ymm3
	vmovups	%ymm0, -96(%rbx,%rcx,4)
	vmovups	%ymm1, -64(%rbx,%rcx,4)
	vmovups	%ymm2, -32(%rbx,%rcx,4)
	vmovups	%ymm3, (%rbx,%rcx,4)
	subq	$-128, %rcx
	addq	$4, %rdi
	jne	LBB71_8
LBB71_9:                                ## %middle.block.unr-lcssa
	testq	%r14, %r14
	je	LBB71_12
## %bb.10:                              ## %vector.body.epil.preheader
	leaq	(%rcx,%r9), %rax
	leaq	(%r10,%rax,4), %rax
	addq	$96, %rax
	addq	%r8, %rcx
	leaq	(%r11,%rcx,4), %rcx
	addq	$96, %rcx
	shlq	$7, %r14
	xorl	%edi, %edi
	.p2align	4, 0x90
LBB71_11:                               ## %vector.body.epil
                                        ## =>This Inner Loop Header: Depth=1
	vmovups	-96(%rcx,%rdi), %ymm0
	vmovups	-64(%rcx,%rdi), %ymm1
	vmovups	-32(%rcx,%rdi), %ymm2
	vmovups	(%rcx,%rdi), %ymm3
	vmovups	%ymm0, -96(%rax,%rdi)
	vmovups	%ymm1, -64(%rax,%rdi)
	vmovups	%ymm2, -32(%rax,%rdi)
	vmovups	%ymm3, (%rax,%rdi)
	subq	$-128, %rdi
	cmpq	%rdi, %r14
	jne	LBB71_11
LBB71_12:                               ## %middle.block
	cmpq	%rdx, %rsi
	je	LBB71_15
LBB71_13:                               ## %"for prediction_output.s0.n.ni.preheader5"
	leaq	(%r10,%r9,4), %rax
	leaq	(%r11,%r8,4), %rcx
	.p2align	4, 0x90
LBB71_14:                               ## %"for prediction_output.s0.n.ni"
                                        ## =>This Inner Loop Header: Depth=1
	vmovss	(%rcx,%rsi,4), %xmm0            ## xmm0 = mem[0],zero,zero,zero
	vmovss	%xmm0, (%rax,%rsi,4)
	incq	%rsi
	cmpq	%rsi, %rdx
	jne	LBB71_14
LBB71_15:                               ## %destructor_block
	xorl	%eax, %eax
	popq	%rbx
	popq	%r14
	vzeroupper
	retq
                                        ## -- End function
	.p2align	4, 0x90                         ## -- Begin function train_cost_model.par_for.sum.s0.n.n
_train_cost_model.par_for.sum.s0.n.n:   ## @train_cost_model.par_for.sum.s0.n.n
## %bb.0:                               ## %entry
	pushq	%rax
	movq	8(%rdx), %rax
	cmpl	%esi, 4(%rdx)
	jle	LBB72_2
## %bb.1:                               ## %then_bb
	movslq	%esi, %rcx
	shlq	$5, %rcx
	vxorps	%xmm0, %xmm0, %xmm0
	vmovaps	%ymm0, (%rax,%rcx)
LBB72_4:                                ## %destructor_block
	xorl	%eax, %eax
	popq	%rcx
	vzeroupper
	retq
LBB72_2:                                ## %next_bb
	movl	(%rdx), %ecx
	shll	$3, %esi
	subl	%esi, %ecx
	jle	LBB72_4
## %bb.3:                               ## %"for sum.s0.n.ni.preheader"
	cmpl	$8, %ecx
	movl	$8, %edx
	cmovll	%ecx, %edx
	movslq	%esi, %rcx
	leaq	(%rax,%rcx,4), %rdi
	decl	%edx
	leaq	4(,%rdx,4), %rdx
	xorl	%esi, %esi
	callq	_memset
	xorl	%eax, %eax
	popq	%rcx
	retq
                                        ## -- End function
	.p2align	4, 0x90                         ## -- Begin function train_cost_model.par_for.sum.s1.n.n
_train_cost_model.par_for.sum.s1.n.n:   ## @train_cost_model.par_for.sum.s1.n.n
## %bb.0:                               ## %entry
	pushq	%rbp
	pushq	%r15
	pushq	%r14
	pushq	%r13
	pushq	%r12
	pushq	%rbx
	subq	$136, %rsp
                                        ## kill: def $esi killed $esi def $rsi
	movslq	4(%rdx), %r9
	movl	8(%rdx), %r13d
	movl	16(%rdx), %eax
	movq	24(%rdx), %r10
	movq	40(%rdx), %r11
	cmpl	%esi, 12(%rdx)
	jle	LBB73_5
## %bb.1:                               ## %then_bb
	testl	%r13d, %r13d
	jle	LBB73_22
## %bb.2:                               ## %"for sum.s1.r92$y.preheader"
	shll	$3, %esi
	movl	%r9d, %ecx
	shll	$5, %ecx
	movl	%ecx, -56(%rsp)                 ## 4-byte Spill
	movslq	%esi, %rcx
	subl	%eax, %esi
	movq	%rcx, -48(%rsp)                 ## 8-byte Spill
	vmovaps	(%r11,%rcx,4), %ymm0
	vmovups	%ymm0, 64(%rsp)                 ## 32-byte Spill
	leaq	(%r9,%r9), %rcx
	leaq	(%r9,%r9,2), %rdx
	leaq	(,%r9,4), %r12
	leaq	(%r9,%r9,4), %rbp
	leaq	(%rcx,%rcx,2), %rax
	movq	%rax, -88(%rsp)                 ## 8-byte Spill
	leaq	(,%r9,8), %r8
	movq	%r8, %rax
	subq	%r9, %rax
	movq	%rax, -104(%rsp)                ## 8-byte Spill
	leaq	(%r9,%r9,8), %r15
	movq	%r9, %rdi
	shlq	$4, %rdi
	movq	%rdi, %rax
	subq	%r9, %rax
	subq	%r9, %rax
	movq	%rax, -120(%rsp)                ## 8-byte Spill
	leaq	(,%rdx,8), %rax
	subq	%r9, %rax
	movq	%rax, -128(%rsp)                ## 8-byte Spill
	movq	%r9, %r14
	shlq	$5, %r14
	subq	%r9, %r14
	movq	%r14, %rax
	subq	%r9, %rax
	vxorps	%xmm1, %xmm1, %xmm1
	leaq	(%rcx,%rcx,4), %rbx
	movq	%rbx, 56(%rsp)                  ## 8-byte Spill
	leaq	(%r9,%rbp,2), %rbx
	movq	%rbx, 48(%rsp)                  ## 8-byte Spill
	leaq	(%r12,%r12,2), %rbx
	movq	%rbx, 40(%rsp)                  ## 8-byte Spill
	movq	%rdx, -72(%rsp)                 ## 8-byte Spill
	leaq	(%r9,%rdx,4), %rdx
	movq	%rdx, 32(%rsp)                  ## 8-byte Spill
	leaq	(%rbp,%rbp,2), %rdx
	movq	%rdx, 24(%rsp)                  ## 8-byte Spill
	movq	%rdi, -112(%rsp)                ## 8-byte Spill
	leaq	(%r9,%rdi), %rdx
	movq	%rdx, 16(%rsp)                  ## 8-byte Spill
	movq	%rcx, -64(%rsp)                 ## 8-byte Spill
	leaq	(%rcx,%rcx,8), %rcx
	movq	%rcx, 8(%rsp)                   ## 8-byte Spill
	leaq	(%r9,%r15,2), %rcx
	movq	%rcx, (%rsp)                    ## 8-byte Spill
	movq	%rsi, %rdi
	movq	%r9, %rcx
	movq	%r12, -80(%rsp)                 ## 8-byte Spill
	leaq	(%r12,%r12,4), %rdx
	movq	%rdx, -8(%rsp)                  ## 8-byte Spill
	leaq	(%r9,%rbp,4), %r12
	leaq	(%r9,%r12), %rdx
	movq	%rdx, -16(%rsp)                 ## 8-byte Spill
	movq	%r8, -96(%rsp)                  ## 8-byte Spill
	leaq	(%r8,%r8,2), %rdx
	movq	%rdx, -24(%rsp)                 ## 8-byte Spill
	movq	%rbp, %r8
	leaq	(%rbp,%rbp,4), %rbp
	leaq	(%rbp,%r9), %rdx
	movq	%rdx, -32(%rsp)                 ## 8-byte Spill
	movq	%r15, %rbx
	leaq	(%r15,%r15,2), %rdx
	addq	%rdx, %r9
	leaq	(%rcx,%r9), %r15
	.p2align	4, 0x90
LBB73_3:                                ## %"for sum.s1.r92$y"
                                        ## =>This Inner Loop Header: Depth=1
	movslq	%edi, %rdi
	vmovups	(%r10,%rdi,4), %ymm2
	vminps	%ymm1, %ymm2, %ymm2
	leaq	(%rcx,%rdi), %rsi
	vmovups	(%r10,%rsi,4), %ymm3
	vminps	%ymm1, %ymm3, %ymm3
	vaddps	%ymm3, %ymm2, %ymm2
	movq	-64(%rsp), %rsi                 ## 8-byte Reload
	addq	%rdi, %rsi
	vmovups	(%r10,%rsi,4), %ymm3
	vminps	%ymm1, %ymm3, %ymm3
	movq	-72(%rsp), %rsi                 ## 8-byte Reload
	leaq	(%rsi,%rdi), %rsi
	vmovups	(%r10,%rsi,4), %ymm4
	vminps	%ymm1, %ymm4, %ymm4
	vaddps	%ymm4, %ymm3, %ymm3
	vaddps	%ymm3, %ymm2, %ymm2
	movq	-80(%rsp), %rsi                 ## 8-byte Reload
	leaq	(%rsi,%rdi), %rsi
	vmovups	(%r10,%rsi,4), %ymm3
	vminps	%ymm1, %ymm3, %ymm3
	leaq	(%r8,%rdi), %rsi
	vmovups	(%r10,%rsi,4), %ymm4
	vminps	%ymm1, %ymm4, %ymm4
	vaddps	%ymm4, %ymm3, %ymm3
	movq	-88(%rsp), %rsi                 ## 8-byte Reload
	leaq	(%rsi,%rdi), %rsi
	vmovups	(%r10,%rsi,4), %ymm4
	vminps	%ymm1, %ymm4, %ymm4
	vaddps	%ymm4, %ymm3, %ymm3
	vaddps	%ymm3, %ymm2, %ymm2
	movq	-104(%rsp), %rsi                ## 8-byte Reload
	leaq	(%rsi,%rdi), %rsi
	vmovups	(%r10,%rsi,4), %ymm3
	vminps	%ymm1, %ymm3, %ymm3
	movq	-96(%rsp), %rsi                 ## 8-byte Reload
	leaq	(%rsi,%rdi), %rsi
	vmovups	(%r10,%rsi,4), %ymm4
	vminps	%ymm1, %ymm4, %ymm4
	vaddps	%ymm4, %ymm3, %ymm3
	leaq	(%rbx,%rdi), %rsi
	vmovups	(%r10,%rsi,4), %ymm4
	vminps	%ymm1, %ymm4, %ymm4
	vaddps	%ymm4, %ymm3, %ymm3
	movq	56(%rsp), %rsi                  ## 8-byte Reload
	leaq	(%rsi,%rdi), %rsi
	vmovups	(%r10,%rsi,4), %ymm4
	vminps	%ymm1, %ymm4, %ymm4
	vaddps	%ymm4, %ymm3, %ymm3
	vaddps	%ymm3, %ymm2, %ymm2
	movq	48(%rsp), %rsi                  ## 8-byte Reload
	leaq	(%rsi,%rdi), %rsi
	vmovups	(%r10,%rsi,4), %ymm3
	vminps	%ymm1, %ymm3, %ymm3
	movq	40(%rsp), %rsi                  ## 8-byte Reload
	leaq	(%rsi,%rdi), %rsi
	vmovups	(%r10,%rsi,4), %ymm4
	vminps	%ymm1, %ymm4, %ymm4
	vaddps	%ymm4, %ymm3, %ymm3
	movq	32(%rsp), %rsi                  ## 8-byte Reload
	leaq	(%rsi,%rdi), %rsi
	vmovups	(%r10,%rsi,4), %ymm4
	vminps	%ymm1, %ymm4, %ymm4
	vaddps	%ymm4, %ymm3, %ymm3
	movq	-120(%rsp), %rsi                ## 8-byte Reload
	leaq	(%rsi,%rdi), %rsi
	vmovups	(%r10,%rsi,4), %ymm4
	vminps	%ymm1, %ymm4, %ymm4
	vaddps	%ymm4, %ymm3, %ymm3
	movq	24(%rsp), %rsi                  ## 8-byte Reload
	leaq	(%rsi,%rdi), %rsi
	vmovups	(%r10,%rsi,4), %ymm4
	vminps	%ymm1, %ymm4, %ymm4
	vaddps	%ymm4, %ymm3, %ymm3
	vaddps	%ymm3, %ymm2, %ymm0
	vmovups	%ymm0, 96(%rsp)                 ## 32-byte Spill
	movq	-112(%rsp), %rsi                ## 8-byte Reload
	leaq	(%rsi,%rdi), %rsi
	vmovups	(%r10,%rsi,4), %ymm3
	vminps	%ymm1, %ymm3, %ymm3
	movq	16(%rsp), %rsi                  ## 8-byte Reload
	leaq	(%rsi,%rdi), %rsi
	vmovups	(%r10,%rsi,4), %ymm4
	vminps	%ymm1, %ymm4, %ymm4
	vaddps	%ymm4, %ymm3, %ymm3
	movq	8(%rsp), %rsi                   ## 8-byte Reload
	leaq	(%rsi,%rdi), %rsi
	vmovups	(%r10,%rsi,4), %ymm4
	movq	(%rsp), %rsi                    ## 8-byte Reload
	leaq	(%rsi,%rdi), %rsi
	vmovups	(%r10,%rsi,4), %ymm5
	movq	-8(%rsp), %rsi                  ## 8-byte Reload
	leaq	(%rsi,%rdi), %rsi
	vmovups	(%r10,%rsi,4), %ymm6
	leaq	(%r12,%rdi), %rsi
	vmovups	(%r10,%rsi,4), %ymm7
	movq	-16(%rsp), %rsi                 ## 8-byte Reload
	leaq	(%rsi,%rdi), %rsi
	vmovups	(%r10,%rsi,4), %ymm8
	movq	-128(%rsp), %rsi                ## 8-byte Reload
	leaq	(%rsi,%rdi), %rsi
	vmovups	(%r10,%rsi,4), %ymm9
	movq	-24(%rsp), %rsi                 ## 8-byte Reload
	leaq	(%rsi,%rdi), %rsi
	vmovups	(%r10,%rsi,4), %ymm10
	leaq	(%rbp,%rdi), %rsi
	vmovups	(%r10,%rsi,4), %ymm11
	movq	-32(%rsp), %rsi                 ## 8-byte Reload
	leaq	(%rsi,%rdi), %rsi
	vmovups	(%r10,%rsi,4), %ymm12
	leaq	(%rdx,%rdi), %rsi
	vmovups	(%r10,%rsi,4), %ymm13
	leaq	(%r9,%rdi), %rsi
	vmovups	(%r10,%rsi,4), %ymm14
	leaq	(%r15,%rdi), %rsi
	vmovups	(%r10,%rsi,4), %ymm15
	leaq	(%rax,%rdi), %rsi
	vmovups	(%r10,%rsi,4), %ymm2
	leaq	(%r14,%rdi), %rsi
	vmovups	(%r10,%rsi,4), %ymm0
	vminps	%ymm1, %ymm4, %ymm4
	vaddps	%ymm4, %ymm3, %ymm3
	vminps	%ymm1, %ymm5, %ymm4
	vaddps	%ymm4, %ymm3, %ymm3
	vminps	%ymm1, %ymm6, %ymm4
	vaddps	%ymm4, %ymm3, %ymm3
	vminps	%ymm1, %ymm7, %ymm4
	vaddps	%ymm4, %ymm3, %ymm3
	vaddps	96(%rsp), %ymm3, %ymm3          ## 32-byte Folded Reload
	vminps	%ymm1, %ymm8, %ymm4
	vminps	%ymm1, %ymm9, %ymm5
	vaddps	%ymm5, %ymm4, %ymm4
	vminps	%ymm1, %ymm10, %ymm5
	vaddps	%ymm5, %ymm4, %ymm4
	vminps	%ymm1, %ymm11, %ymm5
	vaddps	%ymm5, %ymm4, %ymm4
	vminps	%ymm1, %ymm12, %ymm5
	vaddps	%ymm5, %ymm4, %ymm4
	vminps	%ymm1, %ymm13, %ymm5
	vaddps	%ymm5, %ymm4, %ymm4
	vminps	%ymm1, %ymm14, %ymm5
	vaddps	%ymm5, %ymm4, %ymm4
	vaddps	%ymm4, %ymm3, %ymm3
	vminps	%ymm1, %ymm15, %ymm4
	vminps	%ymm1, %ymm2, %ymm2
	vaddps	%ymm2, %ymm4, %ymm2
	vminps	%ymm1, %ymm0, %ymm0
	vaddps	%ymm0, %ymm2, %ymm0
	vaddps	%ymm0, %ymm3, %ymm0
	vmovups	64(%rsp), %ymm2                 ## 32-byte Reload
	vsubps	%ymm0, %ymm2, %ymm2
	vmovups	%ymm2, 64(%rsp)                 ## 32-byte Spill
	addl	-56(%rsp), %edi                 ## 4-byte Folded Reload
	decq	%r13
	jne	LBB73_3
## %bb.4:                               ## %destructor_block.loopexit
	movq	-48(%rsp), %rax                 ## 8-byte Reload
	vmovups	64(%rsp), %ymm0                 ## 32-byte Reload
	vmovaps	%ymm0, (%r11,%rax,4)
LBB73_22:                               ## %destructor_block
	xorl	%eax, %eax
	addq	$136, %rsp
	popq	%rbx
	popq	%r12
	popq	%r13
	popq	%r14
	popq	%r15
	popq	%rbp
	vzeroupper
	retq
LBB73_5:                                ## %next_bb
	testl	%r13d, %r13d
	jle	LBB73_22
## %bb.6:                               ## %next_bb
	movl	(%rdx), %edx
	shll	$3, %esi
	subl	%esi, %edx
	testl	%edx, %edx
	jle	LBB73_22
## %bb.7:                               ## %"for sum.s1.r92$y1.us.preheader"
	movslq	%esi, %rcx
	subl	%eax, %esi
	movq	%rsi, 64(%rsp)                  ## 8-byte Spill
	cmpl	$8, %edx
	movl	$8, %ebx
	cmovll	%edx, %ebx
	movl	%ebx, %eax
	andl	$-32, %eax
	movq	%rax, -64(%rsp)                 ## 8-byte Spill
	addq	$-32, %rax
	movq	%rax, -72(%rsp)                 ## 8-byte Spill
	movq	%rax, %rdx
	shrq	$5, %rdx
	incq	%rdx
	leaq	224(%r10), %rax
	movq	%rax, -88(%rsp)                 ## 8-byte Spill
	movl	%r9d, %eax
	shll	$5, %eax
	movl	%eax, -128(%rsp)                ## 4-byte Spill
	leaq	(%r11,%rcx,4), %rbp
	addq	$224, %rbp
	movq	%rdx, %rax
	movq	%rdx, -80(%rsp)                 ## 8-byte Spill
	andq	$-2, %rdx
	negq	%rdx
	movq	%rdx, -96(%rsp)                 ## 8-byte Spill
	xorl	%eax, %eax
	vxorps	%xmm0, %xmm0, %xmm0
	vxorps	%xmm1, %xmm1, %xmm1
	movl	%esi, %r14d
	movq	%r9, 96(%rsp)                   ## 8-byte Spill
	movq	%rcx, -56(%rsp)                 ## 8-byte Spill
	movq	%r13, -40(%rsp)                 ## 8-byte Spill
	jmp	LBB73_8
	.p2align	4, 0x90
LBB73_21:                               ## %"end for sum.s1.r92$x5.split.us.us"
                                        ##   in Loop: Header=BB73_8 Depth=1
	movq	-112(%rsp), %rax                ## 8-byte Reload
	incq	%rax
	movl	-120(%rsp), %r14d               ## 4-byte Reload
	addl	-128(%rsp), %r14d               ## 4-byte Folded Reload
	movq	-40(%rsp), %r13                 ## 8-byte Reload
	cmpq	%r13, %rax
	je	LBB73_22
LBB73_8:                                ## %"for sum.s1.r92$y1.us"
                                        ## =>This Loop Header: Depth=1
                                        ##     Child Loop BB73_9 Depth 2
                                        ##       Child Loop BB73_14 Depth 3
                                        ##       Child Loop BB73_19 Depth 3
	movq	%rax, -112(%rsp)                ## 8-byte Spill
	shlq	$5, %rax
	movq	%rax, -104(%rsp)                ## 8-byte Spill
	movl	%r14d, -120(%rsp)               ## 4-byte Spill
	xorl	%r8d, %r8d
	jmp	LBB73_9
	.p2align	4, 0x90
LBB73_20:                               ## %"end for sum.s1.n.ni.loopexit.us.us"
                                        ##   in Loop: Header=BB73_9 Depth=2
	incq	%r8
	addl	%r9d, %r14d
	cmpq	$32, %r8
	je	LBB73_21
LBB73_9:                                ## %"for sum.s1.r92$x4.us.us"
                                        ##   Parent Loop BB73_8 Depth=1
                                        ## =>  This Loop Header: Depth=2
                                        ##       Child Loop BB73_14 Depth 3
                                        ##       Child Loop BB73_19 Depth 3
	movslq	%r14d, %r15
	cmpl	$32, %ebx
	jae	LBB73_11
## %bb.10:                              ##   in Loop: Header=BB73_9 Depth=2
	xorl	%edi, %edi
	jmp	LBB73_18
	.p2align	4, 0x90
LBB73_11:                               ## %vector.ph
                                        ##   in Loop: Header=BB73_9 Depth=2
	cmpq	$0, -72(%rsp)                   ## 8-byte Folded Reload
	je	LBB73_12
## %bb.13:                              ## %vector.body.preheader
                                        ##   in Loop: Header=BB73_9 Depth=2
	movq	-88(%rsp), %rax                 ## 8-byte Reload
	leaq	(%rax,%r15,4), %r13
	movq	-96(%rsp), %r9                  ## 8-byte Reload
	xorl	%r12d, %r12d
	.p2align	4, 0x90
LBB73_14:                               ## %vector.body
                                        ##   Parent Loop BB73_8 Depth=1
                                        ##     Parent Loop BB73_9 Depth=2
                                        ## =>    This Inner Loop Header: Depth=3
	vmovups	-224(%rbp,%r12,4), %ymm2
	vmovups	-192(%rbp,%r12,4), %ymm3
	vmovups	-160(%rbp,%r12,4), %ymm4
	vmovups	-128(%rbp,%r12,4), %ymm5
	vmovups	-224(%r13,%r12,4), %ymm6
	vmovups	-192(%r13,%r12,4), %ymm7
	vmovups	-160(%r13,%r12,4), %ymm8
	vmovups	-128(%r13,%r12,4), %ymm9
	vminps	%ymm1, %ymm6, %ymm6
	vsubps	%ymm6, %ymm2, %ymm2
	vminps	%ymm1, %ymm7, %ymm6
	vsubps	%ymm6, %ymm3, %ymm3
	vminps	%ymm1, %ymm8, %ymm6
	vsubps	%ymm6, %ymm4, %ymm4
	vminps	%ymm1, %ymm9, %ymm6
	vsubps	%ymm6, %ymm5, %ymm5
	vmovups	%ymm2, -224(%rbp,%r12,4)
	vmovups	%ymm3, -192(%rbp,%r12,4)
	vmovups	%ymm4, -160(%rbp,%r12,4)
	vmovups	%ymm5, -128(%rbp,%r12,4)
	vmovups	-96(%rbp,%r12,4), %ymm2
	vmovups	-64(%rbp,%r12,4), %ymm3
	vmovups	-32(%rbp,%r12,4), %ymm4
	vmovups	(%rbp,%r12,4), %ymm5
	vmovups	-96(%r13,%r12,4), %ymm6
	vmovups	-64(%r13,%r12,4), %ymm7
	vmovups	-32(%r13,%r12,4), %ymm8
	vmovups	(%r13,%r12,4), %ymm9
	vminps	%ymm1, %ymm6, %ymm6
	vsubps	%ymm6, %ymm2, %ymm2
	vminps	%ymm1, %ymm7, %ymm6
	vsubps	%ymm6, %ymm3, %ymm3
	vminps	%ymm1, %ymm8, %ymm6
	vsubps	%ymm6, %ymm4, %ymm4
	vminps	%ymm1, %ymm9, %ymm6
	vsubps	%ymm6, %ymm5, %ymm5
	vmovups	%ymm2, -96(%rbp,%r12,4)
	vmovups	%ymm3, -64(%rbp,%r12,4)
	vmovups	%ymm4, -32(%rbp,%r12,4)
	vmovups	%ymm5, (%rbp,%r12,4)
	addq	$64, %r12
	addq	$2, %r9
	jne	LBB73_14
	jmp	LBB73_15
LBB73_12:                               ##   in Loop: Header=BB73_9 Depth=2
	xorl	%r12d, %r12d
LBB73_15:                               ## %middle.block.unr-lcssa
                                        ##   in Loop: Header=BB73_9 Depth=2
	testb	$1, -80(%rsp)                   ## 1-byte Folded Reload
	movq	64(%rsp), %rdx                  ## 8-byte Reload
	movq	96(%rsp), %r9                   ## 8-byte Reload
	movq	-56(%rsp), %rcx                 ## 8-byte Reload
	je	LBB73_17
## %bb.16:                              ## %vector.body.epil
                                        ##   in Loop: Header=BB73_9 Depth=2
	movq	-104(%rsp), %rax                ## 8-byte Reload
	addl	%r8d, %eax
	imull	%r9d, %eax
	addl	%edx, %eax
	cltq
	leaq	(%r12,%rcx), %rdx
	vmovups	(%r11,%rdx,4), %ymm2
	vmovups	32(%r11,%rdx,4), %ymm3
	vmovups	64(%r11,%rdx,4), %ymm4
	vmovups	96(%r11,%rdx,4), %ymm5
	addq	%r12, %rax
	vmovups	(%r10,%rax,4), %ymm6
	vmovups	32(%r10,%rax,4), %ymm7
	vmovups	64(%r10,%rax,4), %ymm8
	vmovups	96(%r10,%rax,4), %ymm9
	vminps	%ymm1, %ymm6, %ymm6
	vsubps	%ymm6, %ymm2, %ymm2
	vminps	%ymm1, %ymm7, %ymm6
	vsubps	%ymm6, %ymm3, %ymm3
	vminps	%ymm1, %ymm8, %ymm6
	vsubps	%ymm6, %ymm4, %ymm4
	vminps	%ymm1, %ymm9, %ymm6
	vsubps	%ymm6, %ymm5, %ymm5
	vmovups	%ymm2, (%r11,%rdx,4)
	vmovups	%ymm3, 32(%r11,%rdx,4)
	vmovups	%ymm4, 64(%r11,%rdx,4)
	vmovups	%ymm5, 96(%r11,%rdx,4)
LBB73_17:                               ## %middle.block
                                        ##   in Loop: Header=BB73_9 Depth=2
	movq	-64(%rsp), %rax                 ## 8-byte Reload
	movq	%rax, %rdi
	cmpq	%rbx, %rax
	je	LBB73_20
LBB73_18:                               ## %"for sum.s1.n.ni.us.us.preheader"
                                        ##   in Loop: Header=BB73_9 Depth=2
	movq	%rbx, %rax
	subq	%rdi, %rax
	addq	%rdi, %r15
	leaq	(%r10,%r15,4), %rdx
	addq	%rcx, %rdi
	leaq	(%r11,%rdi,4), %rdi
	xorl	%esi, %esi
	.p2align	4, 0x90
LBB73_19:                               ## %"for sum.s1.n.ni.us.us"
                                        ##   Parent Loop BB73_8 Depth=1
                                        ##     Parent Loop BB73_9 Depth=2
                                        ## =>    This Inner Loop Header: Depth=3
	vmovss	(%rdi,%rsi,4), %xmm2            ## xmm2 = mem[0],zero,zero,zero
	vmovss	(%rdx,%rsi,4), %xmm3            ## xmm3 = mem[0],zero,zero,zero
	vminss	%xmm0, %xmm3, %xmm3
	vsubss	%xmm3, %xmm2, %xmm2
	vmovss	%xmm2, (%rdi,%rsi,4)
	incq	%rsi
	cmpq	%rsi, %rax
	jne	LBB73_19
	jmp	LBB73_20
                                        ## -- End function
	.globl	_train_cost_model_argv          ## -- Begin function train_cost_model_argv
	.p2align	4, 0x90
_train_cost_model_argv:                 ## @train_cost_model_argv
	.cfi_startproc
## %bb.0:                               ## %entry
	pushq	%rbx
	.cfi_def_cfa_offset 16
	subq	$128, %rsp
	.cfi_def_cfa_offset 144
	.cfi_offset %rbx, -16
	movq	(%rdi), %rax
	movq	8(%rdi), %rcx
	movl	(%rax), %r10d
	movl	(%rcx), %esi
	movq	16(%rdi), %rcx
	movl	(%rcx), %edx
	movq	24(%rdi), %rcx
	movq	32(%rdi), %r8
	movq	40(%rdi), %r9
	vmovups	48(%rdi), %ymm1
	movq	80(%rdi), %r11
	movq	88(%rdi), %rax
	vmovss	(%rax), %xmm0                   ## xmm0 = mem[0],zero,zero,zero
	movq	96(%rdi), %rax
	movl	(%rax), %eax
	movq	104(%rdi), %rbx
	movl	(%rbx), %ebx
	vmovups	112(%rdi), %ymm2
	vmovups	144(%rdi), %ymm3
	movq	176(%rdi), %rdi
	vmovups	%ymm3, 88(%rsp)
	movq	%rdi, 120(%rsp)
	vmovups	%ymm2, 56(%rsp)
	movl	%ebx, 48(%rsp)
	movl	%eax, 40(%rsp)
	movq	%r11, 32(%rsp)
	vmovups	%ymm1, (%rsp)
	movl	%r10d, %edi
	vzeroupper
	callq	_train_cost_model
	addq	$128, %rsp
	popq	%rbx
	retq
	.cfi_endproc
                                        ## -- End function
	.globl	_train_cost_model_metadata      ## -- Begin function train_cost_model_metadata
	.p2align	4, 0x90
_train_cost_model_metadata:             ## @train_cost_model_metadata
## %bb.0:                               ## %entry
	leaq	l_train_cost_model_metadata_storage(%rip), %rax
	retq
                                        ## -- End function
	.section	__TEXT,__const
	.p2align	5                               ## @str
l_str:
	.asciz	"updated_head2_filter"

	.p2align	5                               ## @str.3
l_str.3:
	.asciz	"updated_head2_bias"

	.p2align	5                               ## @str.4
l_str.4:
	.asciz	"updated_head1_filter"

	.p2align	5                               ## @str.5
l_str.5:
	.asciz	"updated_head1_bias"

	.p2align	5                               ## @str.6
l_str.6:
	.asciz	"updated_filter1"

	.p2align	5                               ## @str.7
l_str.7:
	.asciz	"updated_bias1"

	.p2align	5                               ## @str.8
l_str.8:
	.asciz	"true_runtime"

	.p2align	5                               ## @str.9
l_str.9:
	.asciz	"schedule_features"

	.p2align	5                               ## @str.10
l_str.10:
	.asciz	"prediction_output"

	.p2align	5                               ## @str.11
l_str.11:
	.asciz	"pipeline_features"

	.p2align	5                               ## @str.12
l_str.12:
	.asciz	"loss_output"

	.p2align	5                               ## @str.13
l_str.13:
	.asciz	"head2_filter"

	.p2align	5                               ## @str.14
l_str.14:
	.asciz	"head2_bias"

	.p2align	5                               ## @str.15
l_str.15:
	.asciz	"head1_filter"

	.p2align	5                               ## @str.16
l_str.16:
	.asciz	"head1_bias"

	.p2align	5                               ## @str.17
l_str.17:
	.asciz	"filter1"

	.p2align	5                               ## @str.18
l_str.18:
	.asciz	"bias1"

	.p2align	5                               ## @str.19
l_str.19:
	.asciz	"Input buffer filter1"

	.p2align	5                               ## @str.20
l_str.20:
	.asciz	"Input buffer bias1"

	.p2align	5                               ## @str.21
l_str.21:
	.asciz	"Input buffer head1_bias"

	.p2align	5                               ## @str.22
l_str.22:
	.asciz	"Input buffer head1_filter"

	.p2align	5                               ## @str.23
l_str.23:
	.asciz	"Input buffer head2_bias"

	.p2align	5                               ## @str.24
l_str.24:
	.asciz	"Input buffer head2_filter"

	.p2align	5                               ## @str.25
l_str.25:
	.asciz	"Output buffer loss_output"

	.p2align	5                               ## @str.26
l_str.26:
	.asciz	"Input buffer pipeline_features"

	.p2align	5                               ## @str.27
l_str.27:
	.asciz	"Output buffer prediction_output"

	.p2align	5                               ## @str.28
l_str.28:
	.asciz	"Input buffer schedule_features"

	.p2align	5                               ## @str.29
l_str.29:
	.asciz	"Input buffer true_runtime"

	.p2align	5                               ## @str.30
l_str.30:
	.asciz	"Output buffer updated_bias1"

	.p2align	5                               ## @str.31
l_str.31:
	.asciz	"Output buffer updated_filter1"

	.p2align	5                               ## @str.32
l_str.32:
	.asciz	"Output buffer updated_head1_bias"

	.p2align	5                               ## @str.33
l_str.33:
	.asciz	"Output buffer updated_head1_filter"

	.p2align	5                               ## @str.34
l_str.34:
	.asciz	"Output buffer updated_head2_bias"

	.p2align	5                               ## @str.35
l_str.35:
	.asciz	"Output buffer updated_head2_filter"

	.p2align	5                               ## @str.36
l_str.36:
	.asciz	"bias1.stride.0"

	.p2align	5                               ## @str.37
l_str.37:
	.asciz	"1"

	.p2align	5                               ## @str.38
l_str.38:
	.asciz	"bias1.min.0"

	.p2align	5                               ## @str.39
l_str.39:
	.asciz	"0"

	.p2align	5                               ## @str.40
l_str.40:
	.asciz	"bias1.extent.0"

	.p2align	5                               ## @str.41
l_str.41:
	.asciz	"32"

	.p2align	5                               ## @str.42
l_str.42:
	.asciz	"filter1.stride.0"

	.p2align	5                               ## @str.43
l_str.43:
	.asciz	"filter1.min.0"

	.p2align	5                               ## @str.44
l_str.44:
	.asciz	"filter1.extent.0"

	.p2align	5                               ## @str.45
l_str.45:
	.asciz	"filter1.min.1"

	.p2align	5                               ## @str.46
l_str.46:
	.asciz	"filter1.extent.1"

	.p2align	5                               ## @str.47
l_str.47:
	.asciz	"head1_bias.stride.0"

	.p2align	5                               ## @str.48
l_str.48:
	.asciz	"head1_bias.min.0"

	.p2align	5                               ## @str.49
l_str.49:
	.asciz	"head1_bias.extent.0"

	.p2align	5                               ## @str.50
l_str.50:
	.asciz	"8"

	.p2align	5                               ## @str.51
l_str.51:
	.asciz	"head1_filter.stride.0"

	.p2align	5                               ## @str.52
l_str.52:
	.asciz	"head1_filter.min.0"

	.p2align	5                               ## @str.53
l_str.53:
	.asciz	"head1_filter.extent.0"

	.p2align	5                               ## @str.54
l_str.54:
	.asciz	"head1_filter.min.1"

	.p2align	5                               ## @str.55
l_str.55:
	.asciz	"head1_filter.extent.1"

	.p2align	5                               ## @str.56
l_str.56:
	.asciz	"40"

	.p2align	5                               ## @str.57
l_str.57:
	.asciz	"head1_filter.min.2"

	.p2align	5                               ## @str.58
l_str.58:
	.asciz	"head1_filter.extent.2"

	.p2align	5                               ## @str.59
l_str.59:
	.asciz	"7"

	.p2align	5                               ## @str.60
l_str.60:
	.asciz	"head2_bias.stride.0"

	.p2align	5                               ## @str.61
l_str.61:
	.asciz	"head2_bias.min.0"

	.p2align	5                               ## @str.62
l_str.62:
	.asciz	"head2_bias.extent.0"

	.p2align	5                               ## @str.63
l_str.63:
	.asciz	"24"

	.p2align	5                               ## @str.64
l_str.64:
	.asciz	"head2_filter.stride.0"

	.p2align	5                               ## @str.65
l_str.65:
	.asciz	"head2_filter.min.0"

	.p2align	5                               ## @str.66
l_str.66:
	.asciz	"head2_filter.extent.0"

	.p2align	5                               ## @str.67
l_str.67:
	.asciz	"head2_filter.min.1"

	.p2align	5                               ## @str.68
l_str.68:
	.asciz	"head2_filter.extent.1"

	.p2align	5                               ## @str.69
l_str.69:
	.asciz	"39"

	.p2align	5                               ## @str.70
l_str.70:
	.asciz	"pipeline_features.stride.0"

	.p2align	5                               ## @str.71
l_str.71:
	.asciz	"prediction_output.stride.0"

	.p2align	5                               ## @str.72
l_str.72:
	.asciz	"schedule_features.stride.0"

	.p2align	5                               ## @str.73
l_str.73:
	.asciz	"true_runtime.stride.0"

	.p2align	5                               ## @str.74
l_str.74:
	.asciz	"updated_bias1.stride.0"

	.p2align	5                               ## @str.75
l_str.75:
	.asciz	"updated_bias1.min.0"

	.p2align	5                               ## @str.76
l_str.76:
	.asciz	"updated_bias1.extent.0"

	.p2align	5                               ## @str.77
l_str.77:
	.asciz	"updated_bias1.min.1"

	.p2align	5                               ## @str.78
l_str.78:
	.asciz	"updated_bias1.extent.1"

	.p2align	5                               ## @str.79
l_str.79:
	.asciz	"4"

	.p2align	5                               ## @str.80
l_str.80:
	.asciz	"updated_filter1.stride.0"

	.p2align	5                               ## @str.81
l_str.81:
	.asciz	"updated_filter1.min.0"

	.p2align	5                               ## @str.82
l_str.82:
	.asciz	"updated_filter1.extent.0"

	.p2align	5                               ## @str.83
l_str.83:
	.asciz	"updated_filter1.min.1"

	.p2align	5                               ## @str.84
l_str.84:
	.asciz	"updated_filter1.extent.1"

	.p2align	5                               ## @str.85
l_str.85:
	.asciz	"updated_filter1.min.2"

	.p2align	5                               ## @str.86
l_str.86:
	.asciz	"updated_filter1.extent.2"

	.p2align	5                               ## @str.87
l_str.87:
	.asciz	"updated_head1_bias.stride.0"

	.p2align	5                               ## @str.88
l_str.88:
	.asciz	"updated_head1_bias.min.0"

	.p2align	5                               ## @str.89
l_str.89:
	.asciz	"updated_head1_bias.extent.0"

	.p2align	5                               ## @str.90
l_str.90:
	.asciz	"updated_head1_bias.min.1"

	.p2align	5                               ## @str.91
l_str.91:
	.asciz	"updated_head1_bias.extent.1"

	.p2align	5                               ## @str.92
l_str.92:
	.asciz	"updated_head1_filter.stride.0"

	.p2align	5                               ## @str.93
l_str.93:
	.asciz	"updated_head1_filter.min.0"

	.p2align	5                               ## @str.94
l_str.94:
	.asciz	"updated_head1_filter.extent.0"

	.p2align	5                               ## @str.95
l_str.95:
	.asciz	"updated_head1_filter.min.1"

	.p2align	5                               ## @str.96
l_str.96:
	.asciz	"updated_head1_filter.extent.1"

	.p2align	5                               ## @str.97
l_str.97:
	.asciz	"updated_head1_filter.min.2"

	.p2align	5                               ## @str.98
l_str.98:
	.asciz	"updated_head1_filter.extent.2"

	.p2align	5                               ## @str.99
l_str.99:
	.asciz	"updated_head1_filter.min.3"

	.p2align	5                               ## @str.100
l_str.100:
	.asciz	"updated_head1_filter.extent.3"

	.p2align	5                               ## @str.101
l_str.101:
	.asciz	"updated_head2_bias.stride.0"

	.p2align	5                               ## @str.102
l_str.102:
	.asciz	"updated_head2_bias.min.0"

	.p2align	5                               ## @str.103
l_str.103:
	.asciz	"updated_head2_bias.extent.0"

	.p2align	5                               ## @str.104
l_str.104:
	.asciz	"updated_head2_bias.min.1"

	.p2align	5                               ## @str.105
l_str.105:
	.asciz	"updated_head2_bias.extent.1"

	.p2align	5                               ## @str.106
l_str.106:
	.asciz	"updated_head2_filter.stride.0"

	.p2align	5                               ## @str.107
l_str.107:
	.asciz	"updated_head2_filter.min.0"

	.p2align	5                               ## @str.108
l_str.108:
	.asciz	"updated_head2_filter.extent.0"

	.p2align	5                               ## @str.109
l_str.109:
	.asciz	"updated_head2_filter.min.1"

	.p2align	5                               ## @str.110
l_str.110:
	.asciz	"updated_head2_filter.extent.1"

	.p2align	5                               ## @str.111
l_str.111:
	.asciz	"updated_head2_filter.min.2"

	.p2align	5                               ## @str.112
l_str.112:
	.asciz	"updated_head2_filter.extent.2"

	.p2align	5                               ## @str.113
l_str.113:
	.asciz	"head1_conv"

	.p2align	5                               ## @str.114
l_str.114:
	.asciz	"normalized_schedule_features"

	.p2align	5                               ## @str.115
l_str.115:
	.asciz	"head2_conv"

	.p2align	5                               ## @str.116
l_str.116:
	.asciz	"head2_relu"

	.p2align	5                               ## @str.117
l_str.117:
	.asciz	"conv1_stage2"

	.p2align	5                               ## @str.118
l_str.118:
	.asciz	"f1"

	.p2align	5                               ## @str.119
l_str.119:
	.asciz	"f0_0_d_def__"

	.p2align	5                               ## @str.120
l_str.120:
	.asciz	"f1_1_d_def__"

	.p2align	5                               ## @str.121
l_str.121:
	.asciz	"relu1_0_d_def__"

	.p2align	5                               ## @str.122
l_str.122:
	.asciz	"sum_1_d_def__"

	.p2align	5                               ## @str.123
l_str.123:
	.asciz	"conv1_stage2_1_d_def__"

	.p2align	5                               ## @str.124
l_str.124:
	.asciz	"conv1_stage1_1_d_def__"

	.p2align	5                               ## @str.126
l_str.126:
	.asciz	"head2_conv_1_d_def__"

	.p2align	5                               ## @str.127
l_str.127:
	.asciz	"sum"

	.p2align	5                               ## @str.128
l_str.128:
	.asciz	"num_stages"

	.p2align	3                               ## @0
l___unnamed_1:
	.long	1                               ## 0x1
	.long	0                               ## 0x0

	.p2align	3                               ## @1
l___unnamed_2:
	.long	13                              ## 0xd
	.long	0                               ## 0x0

	.p2align	5                               ## @str.129
l_str.129:
	.asciz	"batch_size"

	.p2align	3                               ## @2
l___unnamed_3:
	.long	1                               ## 0x1
	.long	0                               ## 0x0

	.p2align	3                               ## @3
l___unnamed_4:
	.long	80                              ## 0x50
	.long	0                               ## 0x0

	.p2align	5                               ## @str.130
l_str.130:
	.asciz	"num_cores"

	.p2align	3                               ## @4
l___unnamed_5:
	.long	1                               ## 0x1
	.long	0                               ## 0x0

	.p2align	3                               ## @5
l___unnamed_6:
	.long	32                              ## 0x20
	.long	0                               ## 0x0

	.p2align	3                               ## @6
l___unnamed_7:
	.quad	0                               ## 0x0

	.p2align	3                               ## @7
l___unnamed_8:
	.quad	40                              ## 0x28

	.p2align	3                               ## @8
l___unnamed_9:
	.quad	0                               ## 0x0

	.p2align	3                               ## @9
l___unnamed_10:
	.quad	7                               ## 0x7

	.p2align	3                               ## @10
l___unnamed_11:
	.quad	0                               ## 0x0

	.p2align	3                               ## @11
l___unnamed_12:
	.quad	13                              ## 0xd

	.section	__DATA,__const
	.p2align	4                               ## @12
l___unnamed_13:
	.quad	l___unnamed_7
	.quad	l___unnamed_8
	.quad	l___unnamed_9
	.quad	l___unnamed_10
	.quad	l___unnamed_11
	.quad	l___unnamed_12

	.section	__TEXT,__const
	.p2align	3                               ## @13
l___unnamed_14:
	.quad	0                               ## 0x0

	.p2align	3                               ## @14
l___unnamed_15:
	.quad	80                              ## 0x50

	.p2align	3                               ## @15
l___unnamed_16:
	.quad	0                               ## 0x0

	.p2align	3                               ## @16
l___unnamed_17:
	.quad	39                              ## 0x27

	.p2align	3                               ## @17
l___unnamed_18:
	.quad	0                               ## 0x0

	.p2align	3                               ## @18
l___unnamed_19:
	.quad	13                              ## 0xd

	.section	__DATA,__const
	.p2align	4                               ## @19
l___unnamed_20:
	.quad	l___unnamed_14
	.quad	l___unnamed_15
	.quad	l___unnamed_16
	.quad	l___unnamed_17
	.quad	l___unnamed_18
	.quad	l___unnamed_19

	.section	__TEXT,__const
	.p2align	3                               ## @20
l___unnamed_21:
	.quad	0                               ## 0x0

	.p2align	3                               ## @21
l___unnamed_22:
	.quad	8                               ## 0x8

	.p2align	3                               ## @22
l___unnamed_23:
	.quad	0                               ## 0x0

	.p2align	3                               ## @23
l___unnamed_24:
	.quad	40                              ## 0x28

	.p2align	3                               ## @24
l___unnamed_25:
	.quad	0                               ## 0x0

	.p2align	3                               ## @25
l___unnamed_26:
	.quad	7                               ## 0x7

	.section	__DATA,__const
	.p2align	4                               ## @26
l___unnamed_27:
	.quad	l___unnamed_21
	.quad	l___unnamed_22
	.quad	l___unnamed_23
	.quad	l___unnamed_24
	.quad	l___unnamed_25
	.quad	l___unnamed_26

	.section	__TEXT,__const
	.p2align	3                               ## @27
l___unnamed_28:
	.quad	0                               ## 0x0

	.p2align	3                               ## @28
l___unnamed_29:
	.quad	8                               ## 0x8

	.section	__DATA,__const
	.p2align	3                               ## @29
l___unnamed_30:
	.quad	l___unnamed_28
	.quad	l___unnamed_29

	.section	__TEXT,__const
	.p2align	3                               ## @30
l___unnamed_31:
	.quad	0                               ## 0x0

	.p2align	3                               ## @31
l___unnamed_32:
	.quad	24                              ## 0x18

	.p2align	3                               ## @32
l___unnamed_33:
	.quad	0                               ## 0x0

	.p2align	3                               ## @33
l___unnamed_34:
	.quad	39                              ## 0x27

	.section	__DATA,__const
	.p2align	4                               ## @34
l___unnamed_35:
	.quad	l___unnamed_31
	.quad	l___unnamed_32
	.quad	l___unnamed_33
	.quad	l___unnamed_34

	.section	__TEXT,__const
	.p2align	3                               ## @35
l___unnamed_36:
	.quad	0                               ## 0x0

	.p2align	3                               ## @36
l___unnamed_37:
	.quad	24                              ## 0x18

	.section	__DATA,__const
	.p2align	3                               ## @37
l___unnamed_38:
	.quad	l___unnamed_36
	.quad	l___unnamed_37

	.section	__TEXT,__const
	.p2align	3                               ## @38
l___unnamed_39:
	.quad	0                               ## 0x0

	.p2align	3                               ## @39
l___unnamed_40:
	.quad	32                              ## 0x20

	.p2align	3                               ## @40
l___unnamed_41:
	.quad	0                               ## 0x0

	.p2align	3                               ## @41
l___unnamed_42:
	.quad	32                              ## 0x20

	.section	__DATA,__const
	.p2align	4                               ## @42
l___unnamed_43:
	.quad	l___unnamed_39
	.quad	l___unnamed_40
	.quad	l___unnamed_41
	.quad	l___unnamed_42

	.section	__TEXT,__const
	.p2align	3                               ## @43
l___unnamed_44:
	.quad	0                               ## 0x0

	.p2align	3                               ## @44
l___unnamed_45:
	.quad	32                              ## 0x20

	.section	__DATA,__const
	.p2align	3                               ## @45
l___unnamed_46:
	.quad	l___unnamed_44
	.quad	l___unnamed_45

	.section	__TEXT,__const
	.p2align	5                               ## @str.131
l_str.131:
	.asciz	"learning_rate"

	.p2align	3                               ## @46
l___unnamed_47:
	.long	0x3f800000                      ## float 1
	.long	0x00000000                      ## float 0

	.p2align	3                               ## @47
l___unnamed_48:
	.long	0x3a83126f                      ## float 0.00100000005
	.long	0x00000000                      ## float 0

	.p2align	5                               ## @str.132
l_str.132:
	.asciz	"timestep"

	.p2align	3                               ## @48
l___unnamed_49:
	.space	8

	.p2align	3                               ## @49
l___unnamed_50:
	.long	37                              ## 0x25
	.long	0                               ## 0x0

	.p2align	5                               ## @str.133
l_str.133:
	.asciz	"reference"

	.p2align	3                               ## @50
l___unnamed_51:
	.space	8

	.p2align	3                               ## @51
l___unnamed_52:
	.space	8

	.p2align	3                               ## @52
l___unnamed_53:
	.quad	0                               ## 0x0

	.p2align	3                               ## @53
l___unnamed_54:
	.quad	80                              ## 0x50

	.section	__DATA,__const
	.p2align	3                               ## @54
l___unnamed_55:
	.quad	l___unnamed_53
	.quad	l___unnamed_54

	.section	__TEXT,__const
	.p2align	3                               ## @55
l___unnamed_56:
	.quad	0                               ## 0x0

	.p2align	3                               ## @56
l___unnamed_57:
	.quad	8                               ## 0x8

	.p2align	3                               ## @57
l___unnamed_58:
	.quad	0                               ## 0x0

	.p2align	3                               ## @58
l___unnamed_59:
	.quad	40                              ## 0x28

	.p2align	3                               ## @59
l___unnamed_60:
	.quad	0                               ## 0x0

	.p2align	3                               ## @60
l___unnamed_61:
	.quad	7                               ## 0x7

	.p2align	3                               ## @61
l___unnamed_62:
	.quad	0                               ## 0x0

	.p2align	3                               ## @62
l___unnamed_63:
	.quad	4                               ## 0x4

	.section	__DATA,__const
	.p2align	4                               ## @63
l___unnamed_64:
	.quad	l___unnamed_56
	.quad	l___unnamed_57
	.quad	l___unnamed_58
	.quad	l___unnamed_59
	.quad	l___unnamed_60
	.quad	l___unnamed_61
	.quad	l___unnamed_62
	.quad	l___unnamed_63

	.section	__TEXT,__const
	.p2align	3                               ## @64
l___unnamed_65:
	.quad	0                               ## 0x0

	.p2align	3                               ## @65
l___unnamed_66:
	.quad	8                               ## 0x8

	.p2align	3                               ## @66
l___unnamed_67:
	.quad	0                               ## 0x0

	.p2align	3                               ## @67
l___unnamed_68:
	.quad	4                               ## 0x4

	.section	__DATA,__const
	.p2align	4                               ## @68
l___unnamed_69:
	.quad	l___unnamed_65
	.quad	l___unnamed_66
	.quad	l___unnamed_67
	.quad	l___unnamed_68

	.section	__TEXT,__const
	.p2align	3                               ## @69
l___unnamed_70:
	.quad	0                               ## 0x0

	.p2align	3                               ## @70
l___unnamed_71:
	.quad	24                              ## 0x18

	.p2align	3                               ## @71
l___unnamed_72:
	.quad	0                               ## 0x0

	.p2align	3                               ## @72
l___unnamed_73:
	.quad	39                              ## 0x27

	.p2align	3                               ## @73
l___unnamed_74:
	.quad	0                               ## 0x0

	.p2align	3                               ## @74
l___unnamed_75:
	.quad	4                               ## 0x4

	.section	__DATA,__const
	.p2align	4                               ## @75
l___unnamed_76:
	.quad	l___unnamed_70
	.quad	l___unnamed_71
	.quad	l___unnamed_72
	.quad	l___unnamed_73
	.quad	l___unnamed_74
	.quad	l___unnamed_75

	.section	__TEXT,__const
	.p2align	3                               ## @76
l___unnamed_77:
	.quad	0                               ## 0x0

	.p2align	3                               ## @77
l___unnamed_78:
	.quad	24                              ## 0x18

	.p2align	3                               ## @78
l___unnamed_79:
	.quad	0                               ## 0x0

	.p2align	3                               ## @79
l___unnamed_80:
	.quad	4                               ## 0x4

	.section	__DATA,__const
	.p2align	4                               ## @80
l___unnamed_81:
	.quad	l___unnamed_77
	.quad	l___unnamed_78
	.quad	l___unnamed_79
	.quad	l___unnamed_80

	.section	__TEXT,__const
	.p2align	3                               ## @81
l___unnamed_82:
	.quad	0                               ## 0x0

	.p2align	3                               ## @82
l___unnamed_83:
	.quad	32                              ## 0x20

	.p2align	3                               ## @83
l___unnamed_84:
	.quad	0                               ## 0x0

	.p2align	3                               ## @84
l___unnamed_85:
	.quad	32                              ## 0x20

	.p2align	3                               ## @85
l___unnamed_86:
	.quad	0                               ## 0x0

	.p2align	3                               ## @86
l___unnamed_87:
	.quad	4                               ## 0x4

	.section	__DATA,__const
	.p2align	4                               ## @87
l___unnamed_88:
	.quad	l___unnamed_82
	.quad	l___unnamed_83
	.quad	l___unnamed_84
	.quad	l___unnamed_85
	.quad	l___unnamed_86
	.quad	l___unnamed_87

	.section	__TEXT,__const
	.p2align	3                               ## @88
l___unnamed_89:
	.quad	0                               ## 0x0

	.p2align	3                               ## @89
l___unnamed_90:
	.quad	32                              ## 0x20

	.p2align	3                               ## @90
l___unnamed_91:
	.quad	0                               ## 0x0

	.p2align	3                               ## @91
l___unnamed_92:
	.quad	4                               ## 0x4

	.section	__DATA,__const
	.p2align	4                               ## @92
l___unnamed_93:
	.quad	l___unnamed_89
	.quad	l___unnamed_90
	.quad	l___unnamed_91
	.quad	l___unnamed_92

	.section	__TEXT,__const
	.p2align	3                               ## @93
l___unnamed_94:
	.quad	0                               ## 0x0

	.p2align	3                               ## @94
l___unnamed_95:
	.quad	80                              ## 0x50

	.section	__DATA,__const
	.p2align	3                               ## @95
l___unnamed_96:
	.quad	l___unnamed_94
	.quad	l___unnamed_95

	.p2align	4                               ## @96
l___unnamed_97:
	.quad	l_str.128
	.long	0                               ## 0x0
	.long	0                               ## 0x0
	.byte	0                               ## 0x0
	.byte	32                              ## 0x20
	.short	1                               ## 0x1
	.space	4
	.quad	l___unnamed_1
	.quad	0
	.quad	0
	.quad	l___unnamed_2
	.quad	0
	.quad	l_str.129
	.long	0                               ## 0x0
	.long	0                               ## 0x0
	.byte	0                               ## 0x0
	.byte	32                              ## 0x20
	.short	1                               ## 0x1
	.space	4
	.quad	l___unnamed_3
	.quad	0
	.quad	0
	.quad	l___unnamed_4
	.quad	0
	.quad	l_str.130
	.long	0                               ## 0x0
	.long	0                               ## 0x0
	.byte	0                               ## 0x0
	.byte	32                              ## 0x20
	.short	1                               ## 0x1
	.space	4
	.quad	l___unnamed_5
	.quad	0
	.quad	0
	.quad	l___unnamed_6
	.quad	0
	.quad	l_str.11
	.long	1                               ## 0x1
	.long	3                               ## 0x3
	.byte	2                               ## 0x2
	.byte	32                              ## 0x20
	.short	1                               ## 0x1
	.space	4
	.quad	0
	.quad	0
	.quad	0
	.quad	0
	.quad	l___unnamed_13
	.quad	l_str.9
	.long	1                               ## 0x1
	.long	3                               ## 0x3
	.byte	2                               ## 0x2
	.byte	32                              ## 0x20
	.short	1                               ## 0x1
	.space	4
	.quad	0
	.quad	0
	.quad	0
	.quad	0
	.quad	l___unnamed_20
	.quad	l_str.15
	.long	1                               ## 0x1
	.long	3                               ## 0x3
	.byte	2                               ## 0x2
	.byte	32                              ## 0x20
	.short	1                               ## 0x1
	.space	4
	.quad	0
	.quad	0
	.quad	0
	.quad	0
	.quad	l___unnamed_27
	.quad	l_str.16
	.long	1                               ## 0x1
	.long	1                               ## 0x1
	.byte	2                               ## 0x2
	.byte	32                              ## 0x20
	.short	1                               ## 0x1
	.space	4
	.quad	0
	.quad	0
	.quad	0
	.quad	0
	.quad	l___unnamed_30
	.quad	l_str.13
	.long	1                               ## 0x1
	.long	2                               ## 0x2
	.byte	2                               ## 0x2
	.byte	32                              ## 0x20
	.short	1                               ## 0x1
	.space	4
	.quad	0
	.quad	0
	.quad	0
	.quad	0
	.quad	l___unnamed_35
	.quad	l_str.14
	.long	1                               ## 0x1
	.long	1                               ## 0x1
	.byte	2                               ## 0x2
	.byte	32                              ## 0x20
	.short	1                               ## 0x1
	.space	4
	.quad	0
	.quad	0
	.quad	0
	.quad	0
	.quad	l___unnamed_38
	.quad	l_str.17
	.long	1                               ## 0x1
	.long	2                               ## 0x2
	.byte	2                               ## 0x2
	.byte	32                              ## 0x20
	.short	1                               ## 0x1
	.space	4
	.quad	0
	.quad	0
	.quad	0
	.quad	0
	.quad	l___unnamed_43
	.quad	l_str.18
	.long	1                               ## 0x1
	.long	1                               ## 0x1
	.byte	2                               ## 0x2
	.byte	32                              ## 0x20
	.short	1                               ## 0x1
	.space	4
	.quad	0
	.quad	0
	.quad	0
	.quad	0
	.quad	l___unnamed_46
	.quad	l_str.131
	.long	0                               ## 0x0
	.long	0                               ## 0x0
	.byte	2                               ## 0x2
	.byte	32                              ## 0x20
	.short	1                               ## 0x1
	.space	4
	.quad	l___unnamed_47
	.quad	0
	.quad	0
	.quad	l___unnamed_48
	.quad	0
	.quad	l_str.132
	.long	0                               ## 0x0
	.long	0                               ## 0x0
	.byte	0                               ## 0x0
	.byte	32                              ## 0x20
	.short	1                               ## 0x1
	.space	4
	.quad	l___unnamed_49
	.quad	0
	.quad	0
	.quad	l___unnamed_50
	.quad	0
	.quad	l_str.133
	.long	0                               ## 0x0
	.long	0                               ## 0x0
	.byte	0                               ## 0x0
	.byte	32                              ## 0x20
	.short	1                               ## 0x1
	.space	4
	.quad	l___unnamed_51
	.quad	0
	.quad	0
	.quad	l___unnamed_52
	.quad	0
	.quad	l_str.8
	.long	1                               ## 0x1
	.long	1                               ## 0x1
	.byte	2                               ## 0x2
	.byte	32                              ## 0x20
	.short	1                               ## 0x1
	.space	4
	.quad	0
	.quad	0
	.quad	0
	.quad	0
	.quad	l___unnamed_55
	.quad	l_str.4
	.long	2                               ## 0x2
	.long	4                               ## 0x4
	.byte	2                               ## 0x2
	.byte	32                              ## 0x20
	.short	1                               ## 0x1
	.space	4
	.quad	0
	.quad	0
	.quad	0
	.quad	0
	.quad	l___unnamed_64
	.quad	l_str.5
	.long	2                               ## 0x2
	.long	2                               ## 0x2
	.byte	2                               ## 0x2
	.byte	32                              ## 0x20
	.short	1                               ## 0x1
	.space	4
	.quad	0
	.quad	0
	.quad	0
	.quad	0
	.quad	l___unnamed_69
	.quad	l_str
	.long	2                               ## 0x2
	.long	3                               ## 0x3
	.byte	2                               ## 0x2
	.byte	32                              ## 0x20
	.short	1                               ## 0x1
	.space	4
	.quad	0
	.quad	0
	.quad	0
	.quad	0
	.quad	l___unnamed_76
	.quad	l_str.3
	.long	2                               ## 0x2
	.long	2                               ## 0x2
	.byte	2                               ## 0x2
	.byte	32                              ## 0x20
	.short	1                               ## 0x1
	.space	4
	.quad	0
	.quad	0
	.quad	0
	.quad	0
	.quad	l___unnamed_81
	.quad	l_str.6
	.long	2                               ## 0x2
	.long	3                               ## 0x3
	.byte	2                               ## 0x2
	.byte	32                              ## 0x20
	.short	1                               ## 0x1
	.space	4
	.quad	0
	.quad	0
	.quad	0
	.quad	0
	.quad	l___unnamed_88
	.quad	l_str.7
	.long	2                               ## 0x2
	.long	2                               ## 0x2
	.byte	2                               ## 0x2
	.byte	32                              ## 0x20
	.short	1                               ## 0x1
	.space	4
	.quad	0
	.quad	0
	.quad	0
	.quad	0
	.quad	l___unnamed_93
	.quad	l_str.10
	.long	2                               ## 0x2
	.long	1                               ## 0x1
	.byte	2                               ## 0x2
	.byte	32                              ## 0x20
	.short	1                               ## 0x1
	.space	4
	.quad	0
	.quad	0
	.quad	0
	.quad	0
	.quad	l___unnamed_96
	.quad	l_str.12
	.long	2                               ## 0x2
	.long	0                               ## 0x0
	.byte	2                               ## 0x2
	.byte	32                              ## 0x20
	.short	1                               ## 0x1
	.space	4
	.quad	0
	.quad	0
	.quad	0
	.quad	0
	.quad	0

	.section	__TEXT,__const
	.p2align	5                               ## @str.134
l_str.134:
	.asciz	"x86-64-osx-avx-avx2-f16c-fma-no_runtime-sse41"

	.p2align	5                               ## @str.135
l_str.135:
	.asciz	"train_cost_model"

	.section	__DATA,__const
	.p2align	4                               ## @train_cost_model_metadata_storage
l_train_cost_model_metadata_storage:
	.long	1                               ## 0x1
	.long	23                              ## 0x17
	.quad	l___unnamed_97
	.quad	l_str.134
	.quad	l_str.135

.subsections_via_symbols
